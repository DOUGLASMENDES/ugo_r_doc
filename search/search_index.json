{
    "docs": [
        {
            "location": "/",
            "text": "Let there be light!\n\n\n\n\nA R documentation website.\n\n\n\n\nugo_r_doc\n is a corpus; a catalog of books, manuals, articles, presentations, videos, podcasts, summaries, notes, code snippets, excerpts, websites, etc.\n\n\nThe \u2018docs\u2019 is a searchable knowledge-based system.\n\n\nYou type a keyword, it leads to several sources, you identify the document, and you go retrieve the document; whether it is a digital or a material document. Fast and easy!\n\n\nThe corpus is unstructured. Knowledge is rather cumulated, layer after layer, resulting in a hotchpotch of information. Information may be repeted among many documents, with different explanations, some more comprehensive. Newer entries might also supplement or contradict older entries.",
            "title": "Home"
        },
        {
            "location": "/R_CS/",
            "text": "Foreword\n\n\nCheat sheets.\n\n\n\n\nBasics\n\u00b6\n\n\n\n\nBase R\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data\n. PDF only.\n\n\n\n\n\n\nData Import\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nControl Structures\n. PDF.\n\n\n\n\n\n\n\n\n\n\nAdvanced\n\u00b6\n\n\n\n\nAdvanced R\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio & RMarkdown\n\u00b6\n\n\n\n\nR Markdown Reference Guide\n. PDF only.\n\n\nsyntax, chunk options, pandoc options, slide formats, LaTeX options\n\n\n\n\n\n\nR Markdown\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\nSpecialization\n\u00b6\n\n\nTydyverse\n\u00b6\n\n\n\n\nTidyverse\n. PDF.\n\n\n\n\n\n\n\n\n\n\ndplyr & tidyr\n\u00b6\n\n\n\n\nGrammar of dplyr\n. PDF only (explanatory slides).\n\n\ndplyr\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndplyr & tidyr\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\nsjmisc\n\u00b6\n\n\n\n\nsjmisc\n. PDF.\n\n\n\n\n\n\n\n\n\n\ndata.table\n\u00b6\n\n\n\n\ndata.table Intro\n. PDF only (explanatory article).\n\n\ndata.table\n 1. PDF.\n\n\ndata.table\n 2. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaret\n\u00b6\n\n\n\n\ncaret\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbabilities\n\u00b6\n\n\n\n\nProbabilities\n. PDF only.\n\n\n\n\nRegular Expressions\n\u00b6\n\n\n\n\nBasic Regular Expressions\n. PDF.\n\n\n\n\n\n\n\n\n\n\nR-Quandl\n\u00b6\n\n\n\n\nQuandl\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpark\n\u00b6\n\n\n\n\nsparklyr\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\nTime series\n\u00b6\n\n\n\n\nTime Series\n. PDF only.\n\n\ninput, decomposition, tests, stochastic, graphics, miscellaneous\n\n\n\n\n\n\nxts\n. PDF only.\n\n\nTime Series in R \u2013 The Power of \nxts\n and \nzoo\n. HTML.\n\n\n\n\nSurvival, regression, data mining, machine learning\n\u00b6\n\n\n\n\nsurvminer\n. PDF only.\n\n\ncurve, ggplot2, cox model\n\n\n\n\n\n\nRegressions\n. PDF only.\n\n\nlinear model, variable selection, diagnostics, graphics, tests, variable transformation, ridge, segmented, gls, glm, nls, gnls, loess, splines, robust, structural equation, simultaneous equation, pls, principal components, quantile, linear and nonlinear mixed effects, generalized additive, survival analysis, classification & regression trees, beta\n\n\n\n\n\n\nData Mining\n. PDF only.\n\n\nassociation rules, sequential patterns, classification & prediction, regression, clustering, outliers, time series, text mining, socila networks, graph mining, spatial data, statistics, graphics, data manipulation, data access, big data, parallel computing, reports, weka, editors, guis\n\n\n\n\n\n\n\n\nBig Data Machine Learning\n. PDF only.\n\n\n\n\nlinear regression, logistic regression, regularization (ridge, lasso), neural network, support vector machine, nayesian network and na\u00efve bayes, k-nearest neighbors, decision tree, tree ensembles (bagging or random forest, boosting)\n\n\n\n\n\n\n\n\nMachine Learning Modelling in R\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning with R\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization (ggplot2, ggmap, ggvis, shiny, plotly, Leaflet)\n\u00b6\n\n\n\n\nggplot2\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggmap\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggvis\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggvis\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshiny\n. PDF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotly\n. PDF only.\n\n\n\n\n\n\n\n\nLeaflet\n. PDF.\n\n\n\n\n\n\n\n\n\n\nParameters\n\u00b6\n\n\nLines\n\n\n\n\n\n\n\n\nPoints\n\n\n\n\n\n\n\n\nGraphs\n\u00b6\n\n\n\n\nGraphs\n.\n\n\n\n\n\n\n\n\n\n\n\n\nColors\n\u00b6",
            "title": "R Cheat Sheets"
        },
        {
            "location": "/R_CS/#advanced",
            "text": "Advanced R . PDF.",
            "title": "Advanced"
        },
        {
            "location": "/R_CS/#rstudio-rmarkdown",
            "text": "R Markdown Reference Guide . PDF only.  syntax, chunk options, pandoc options, slide formats, LaTeX options    R Markdown . PDF.         RStudio . PDF.",
            "title": "RStudio &amp; RMarkdown"
        },
        {
            "location": "/R_CS/#specialization",
            "text": "",
            "title": "Specialization"
        },
        {
            "location": "/R_CS/#tydyverse",
            "text": "Tidyverse . PDF.",
            "title": "Tydyverse"
        },
        {
            "location": "/R_CS/#dplyr-tidyr",
            "text": "Grammar of dplyr . PDF only (explanatory slides).  dplyr . PDF.        dplyr & tidyr . PDF.",
            "title": "dplyr &amp; tidyr"
        },
        {
            "location": "/R_CS/#sjmisc",
            "text": "sjmisc . PDF.",
            "title": "sjmisc"
        },
        {
            "location": "/R_CS/#datatable",
            "text": "data.table Intro . PDF only (explanatory article).  data.table  1. PDF.  data.table  2. PDF.",
            "title": "data.table"
        },
        {
            "location": "/R_CS/#caret",
            "text": "caret . PDF.",
            "title": "Caret"
        },
        {
            "location": "/R_CS/#probabilities",
            "text": "Probabilities . PDF only.",
            "title": "Probabilities"
        },
        {
            "location": "/R_CS/#regular-expressions",
            "text": "Basic Regular Expressions . PDF.",
            "title": "Regular Expressions"
        },
        {
            "location": "/R_CS/#r-quandl",
            "text": "Quandl .",
            "title": "R-Quandl"
        },
        {
            "location": "/R_CS/#spark",
            "text": "sparklyr . PDF.",
            "title": "Spark"
        },
        {
            "location": "/R_CS/#time-series",
            "text": "Time Series . PDF only.  input, decomposition, tests, stochastic, graphics, miscellaneous    xts . PDF only.  Time Series in R \u2013 The Power of  xts  and  zoo . HTML.",
            "title": "Time series"
        },
        {
            "location": "/R_CS/#survival-regression-data-mining-machine-learning",
            "text": "survminer . PDF only.  curve, ggplot2, cox model    Regressions . PDF only.  linear model, variable selection, diagnostics, graphics, tests, variable transformation, ridge, segmented, gls, glm, nls, gnls, loess, splines, robust, structural equation, simultaneous equation, pls, principal components, quantile, linear and nonlinear mixed effects, generalized additive, survival analysis, classification & regression trees, beta    Data Mining . PDF only.  association rules, sequential patterns, classification & prediction, regression, clustering, outliers, time series, text mining, socila networks, graph mining, spatial data, statistics, graphics, data manipulation, data access, big data, parallel computing, reports, weka, editors, guis     Big Data Machine Learning . PDF only.   linear regression, logistic regression, regularization (ridge, lasso), neural network, support vector machine, nayesian network and na\u00efve bayes, k-nearest neighbors, decision tree, tree ensembles (bagging or random forest, boosting)     Machine Learning Modelling in R . PDF.        Machine Learning with R . PDF.",
            "title": "Survival, regression, data mining, machine learning"
        },
        {
            "location": "/R_CS/#visualization-ggplot2-ggmap-ggvis-shiny-plotly-leaflet",
            "text": "ggplot2 . PDF.         ggmap . PDF.         ggvis . PDF.        ggvis . PDF.         shiny . PDF.         plotly . PDF only.     Leaflet . PDF.",
            "title": "Visualization (ggplot2, ggmap, ggvis, shiny, plotly, Leaflet)"
        },
        {
            "location": "/R_CS/#parameters",
            "text": "Lines     Points",
            "title": "Parameters"
        },
        {
            "location": "/R_CS/#graphs",
            "text": "Graphs .",
            "title": "Graphs"
        },
        {
            "location": "/R_CS/#colors",
            "text": "",
            "title": "Colors"
        },
        {
            "location": "/An Introduction to R/",
            "text": "Foreword\n\n\nNotes, leads, and ideas on what R can do. More at:\n\n\n\n\nR-intro\n\n\ncran.r-project.org/manuals (series of official manuals)\n\n\n\n\n\n\n1, Introduction and Preliminaries\n\u00b6\n\n\n\n\nRun R (CLI).\n\n\nQuit with \nq()\n.\n\n\n\n\n\n\nRun R in an IDE (GUI); like RStudio.\n\n\nCreate a working directory \nwork\n.\n\n\nAsk help.\n\n\nhelp(function)\n; open a web documentation in a browser or in the IDE.\n\n\n?function\n; idem.\n\n\n??function\n; idem, but showing concordances.\n\n\nhelp(\"[[\")\n; idem, searching with a string.\n\n\nhelp.start()\n; show the entire manual.\n\n\n\n\n\n\nsink()\n; divert output from the console to a connection; restore the output to the console.\n\n\nobjects()\n, \nls()\n; see the objects stored in a session.\n\n\nObjects are in a file with \n.RData\n extension.\n\n\n\n\n\n\nrm(x, y, ink, ...)\n; remove stored objects.\n\n\nAll commands entered or run are recorded in a file with \n.Rhistory\n extension.\n\n\n\n\n2, Simple Manipulations; Numbers and Vectors\n\u00b6\n\n\nCreate a vector\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nx \n<-\n \nc\n(\n1\n,\n \n2\n)\n \n# assignment (universal).\n\nx\n\n[\n1\n]\n \n1\n \n2\n\n\n\nc\n(\n2\n,\n \n1\n)\n \n->\n y \n# alternative assignment.\n\ny\n\n[\n1\n]\n \n2\n \n1\n\n\nx \n=\n \nc\n(\n1\n,\n \n2\n)\n \n# alternative assignment (some limitations).\n\nx \n<<-\n \nc\n(\n1\n,\n \n2\n)\n \n# permanent assignment.\n\n\n\n# examples\n\nab \n<-\n \n9\n\nab\n\n[\n1\n]\n \n9\n\n\n\nassign\n(\n\"ab\"\n,\n \n10\n)\n\nab \n\n[\n1\n]\n \n10\n\n\n\n\n\n\n\nCreate a sequence (vector)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\nx \n<-\n \n2\n \n*\n \n1\n:\n15\n\n\nx\n\n[\n1\n]\n  \n2\n  \n4\n  \n6\n  \n8\n \n10\n \n12\n \n14\n \n16\n \n18\n \n20\n \n22\n \n24\n \n26\n \n28\n \n30\n\n\nn \n<-\n \n10\n\nx \n<-\n \n1\n:\nn\n-1\n\n\nx\n\n[\n1\n]\n \n1\n \n2\n \n3\n \n4\n \n5\n \n6\n \n7\n \n8\n \n9\n\n\nx \n<-\n \n30\n:\n1\n\nx\n\n[\n1\n]\n \n30\n \n29\n \n28\n \n27\n \n26\n \n25\n \n24\n \n23\n \n22\n \n21\n \n20\n \n19\n \n18\n \n17\n \n16\n \n15\n \n14\n \n13\n \n12\n \n11\n \n10\n  \n9\n  \n8\n  \n7\n  \n6\n  \n5\n  \n4\n  \n3\n\n\n[\n29\n]\n  \n2\n  \n1\n\n\n\nseq\n(\nfrom \n=\n \n1\n,\n to \n=\n \n30\n)\n\n\n[\n1\n]\n  \n1\n  \n2\n  \n3\n  \n4\n  \n5\n  \n6\n  \n7\n  \n8\n  \n9\n \n10\n \n11\n \n12\n \n13\n \n14\n \n15\n \n16\n \n17\n \n18\n \n19\n \n20\n \n21\n \n22\n \n23\n \n24\n \n25\n \n26\n \n27\n \n28\n\n\n[\n29\n]\n \n29\n \n30\n\n\n\nseq\n(\n1\n,\n \n30\n)\n\n\n[\n1\n]\n  \n1\n  \n2\n  \n3\n  \n4\n  \n5\n  \n6\n  \n7\n  \n8\n  \n9\n \n10\n \n11\n \n12\n \n13\n \n14\n \n15\n \n16\n \n17\n \n18\n \n19\n \n20\n \n21\n \n22\n \n23\n \n24\n \n25\n \n26\n \n27\n \n28\n\n\n[\n29\n]\n \n29\n \n30\n\n\n\nseq\n(\n30\n,\n \n1\n)\n\n\n[\n1\n]\n \n30\n \n29\n \n28\n \n27\n \n26\n \n25\n \n24\n \n23\n \n22\n \n21\n \n20\n \n19\n \n18\n \n17\n \n16\n \n15\n \n14\n \n13\n \n12\n \n11\n \n10\n  \n9\n  \n8\n  \n7\n  \n6\n  \n5\n  \n4\n  \n3\n\n\n[\n29\n]\n  \n2\n  \n1\n\n\n\nseq\n(\n1\n,\n \n30\n,\n by \n=\n \n2\n)\n\n\n[\n1\n]\n  \n1\n  \n3\n  \n5\n  \n7\n  \n9\n \n11\n \n13\n \n15\n \n17\n \n19\n \n21\n \n23\n \n25\n \n27\n \n29\n\n\n\n\n\n\n\nRepetition\n\u00b6\n\n\n1\n2\nrep\n(\n2\n,\n times \n=\n \n5\n)\n\n\n[\n1\n]\n \n2\n \n2\n \n2\n \n2\n \n2\n\n\n\n\n\n\n\n1\n2\n3\n4\nx \n<-\n \nc\n(\n\"x\"\n,\n \n\"y\"\n)[\nrep\n(\nc\n(\n1\n,\n \n2\n,\n \n2\n,\n \n1\n),\n times \n=\n \n4\n)]\n\n\nx\n\n[\n1\n]\n \n\"x\"\n \n\"y\"\n \n\"y\"\n \n\"x\"\n \n\"x\"\n \n\"y\"\n \n\"y\"\n \n\"x\"\n \n\"x\"\n \n\"y\"\n \n\"y\"\n \n\"x\"\n \n\"x\"\n \n\"y\"\n \n\"y\"\n \n\"x\"\n\n\n\n\n\n\n\nLength (vector)\n\u00b6\n\n\n\n\nlength(vector)\n\n\n\n\nBoolean\n\u00b6\n\n\n\n\nTRUE\n or \nT\n; \nT == 1\n.\n\n\nFALSE\n or \nF\n; \nF == 0\n.\n\n\n\n\nSome operators\n\n\n\n\n<\n\n\n>=\n\n\n<\n\n\n>=\n\n\n==\n\n\n!=\n\n\n<>\n\n\n&\n\n\n&&\n\n\n|\n\n\n||\n\n\nand many more.\n\n\n\n\nMissing data and more\n\u00b6\n\n\n\n\nNA; not available.\n\n\nNaN; not a number.\n\n\nInf - Inf == NaN == 0/0; infinite number.\n\n\n\n\n1\n2\n3\n4\nx \n<-\n \nc\n(\n1\n:\n3\n,\n \nNA\n)\n\n\nx\n\n[\n1\n]\n \n1\n \n2\n \n3\n \nNA\n\n\n\n\n\n\n\n\n\nis.na(var)\n.\n\n\nvar == NA\n.\n\n\nis.na(x)\n.\n\n\n!is.na(x)\n.\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nx \n<-\n \nc\n(\n-1\n:\n3\n,\n \nNA\n)\n\ny \n<-\n x\n[\n!\nis.na\n(\nx\n)\n \n&\n x \n>\n \n0\n]\n\n\nx\n\n[\n1\n]\n  \n-1\n  \n0\n  \n1\n  \n2\n  \n3\n \nNA\n\n\ny\n\n[\n1\n]\n \n1\n \n2\n \n3\n\n\n\n\n\n\n\nExtract, subset (vector)\n\u00b6\n\n\n\n\nx[i]\n; index.\n\n\n\n\nBackslash use for some characters\n\u00b6\n\n\n\n\n\\\\\n; backslash.\n\n\n\\n\n; new line.\n\n\n\\t\n; tab.\n\n\n\\b\n; backspace.\n\n\n\n\nConcatenate, paste (vector)\n\u00b6\n\n\n1\n2\n3\n4\nlabs \n<-\n \npaste\n(\nc\n(\n\"X\"\n,\n \n\"Y\"\n),\n \n1\n:\n10\n,\n sep \n=\n \n\"\"\n)\n\n\nlabs\n\n[\n1\n]\n \n\"X1\"\n  \n\"Y2\"\n  \n\"X3\"\n  \n\"Y4\"\n  \n\"X5\"\n  \n\"Y6\"\n  \n\"X7\"\n  \n\"Y8\"\n  \n\"X9\"\n  \n\"Y10\"\n\n\n\n\n\n\n\nA note of data handling and manipulations\n\u00b6\n\n\nYou can also \nsplit()\n, \nmerge()\n, \nrbind()\n, \ncbind()\n vectors. \n\n\nIt is also possible with other objects such as factors, lists, arrays, matrices, and data frames.\n\n\nThere are built-in functions to extract, exclude, subset, replace, transform or convert (\n.as\n), concatenate, paste, group, and bind.\n\n\nslice, extract, exclude, subset, replace, convert,  concatenate, paste, group, bind\n \n\n\nExclude, remove (vector)\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nz \n<-\n \n1\n:\n20\n\n\nz\n\n[\n1\n]\n  \n1\n  \n2\n  \n3\n  \n4\n  \n5\n  \n6\n  \n7\n  \n8\n  \n9\n \n10\n \n11\n \n12\n \n13\n \n14\n \n15\n \n16\n \n17\n \n18\n \n19\n \n20\n\n\nzz \n<-\n z\n[\n-\n(\n1\n:\n5\n)]\n\n\nzz\n\n[\n1\n]\n  \n6\n  \n7\n  \n8\n  \n9\n \n10\n \n11\n \n12\n \n13\n \n14\n \n15\n \n16\n \n17\n \n18\n \n19\n \n20\n\n\n\n\n\n\n\nReplace (vector)\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nx \n<-\n \nc\n(\n1\n,\n \n1\n,\n \n1\n,\n \nNA\n)\n\n\nx\n\n[\n1\n]\n  \n1\n  \n1\n  \n1\n \nNA\n\n\nx\n[\nis.na\n(\nx\n)]\n \n<-\n \n0\n\n\nx\n\n[\n1\n]\n \n1\n \n1\n \n1\n \n0\n\n\n# replaces any missing values in x by zeros\n\n\n\n\n\n\n\nAbsolute value\n\u00b6\n\n\n\n\ny \n<-\n \nabs\n(\ny\n)\n.\n\n\n\n\nConvert (\n.as\n)\n\n\n\n\nas.vector()\n.\n\n\nas.integer()\n.\n\n\nas.numeric()\n\n\nas.factor()\n.\n\n\nas.character()\n.\n\n\nand many more.\n\n\n\n\n3, Objects, their Modes and Attributes\n\u00b6\n\n\nObject type\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nobj \n<-\n \n1\n\n\nobj1 \n<-\n \nnumeric\n(\nobj\n)\n\n\nmode\n(\nobj1\n)\n\n\n[\n1\n]\n \n\"numeric\"\n\nobj2 \n<-\n \ncharacter\n(\nobj\n)\n\n\nmode\n(\nobj2\n)\n\n\n[\n1\n]\n \n\"character\"\n\n\nx \n<-\n \n1\n\n\n\n# class() != mode(), but almost\n\n\nmode\n(\nx\n)\n\n\n[\n1\n]\n \n\"numeric\"\n\n\nx \n<-\n \nfactor\n(\nx\n)\n\nx\n\n[\n1\n]\n \n1\n\nLevels\n:\n \n1\n\n\nx \n<-\n \nnumeric\n(\nx\n)\n\nx\n\n[\n1\n]\n \n0\n\n\n\n# load key:value\n\nobj\n[\n3\n]\n \n<-\n \n17\n\n\nobj\n\n[\n1\n]\n  \n1\n \nNA\n \n17\n\n\n\n\n\n\n\nClasses\n\u00b6\n\n\nclass\n\n\n\n\n\u201cnumeric\u201d.\n\n\n\u201clogical\u201d.\n\n\n\u201ccharacter\u201d.\n\n\n\u201clist\u201d.\n\n\n\u201cmatrix\u201d.\n\n\n\u201carray\u201d.\n\n\n\u201cfactor\u201d.\n\n\n\u201cdata.frame\u201d.\n\n\n\n\n1\n2\n3\n4\n5\n6\nclass\n(\nobj\n)\n\n\n[\n1\n]\n \n\"numeric\"\n\n\n\n# print the object as ordinary\n\n\nunclass\n(\nobj\n)\n\n\n[\n1\n]\n  \n1\n \nNA\n \n17\n\n\n\n\n\n\n\n4, Ordered and Unordered Factors\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nstate \n<-\n \nc\n(\n\"tas\"\n,\n\"qld\"\n,\n\"sa\"\n,\n\"sa\"\n,\n\"sa\"\n,\n\"vic\"\n,\n\"nt\"\n,\n\"act\"\n,\n\"qld\"\n,\n\"nsw\"\n,\n\"wa\"\n,\n\"nsw\"\n,\n\"nsw\"\n,\n\"vic\"\n,\n\"vic\"\n,\n\"vic\"\n,\n\"nsw\"\n,\n\"qld\"\n,\n\"qld\"\n,\n\"vic\"\n,\n\"nt\"\n,\n\"wa\"\n,\n\"wa\"\n,\n\"qld\"\n,\n\"sa\"\n,\n \n\"tas\"\n,\n\"nsw\"\n,\n \n\"nsw\"\n,\n \n\"wa\"\n,\n\"act\"\n)\n\n\nstatef \n<-\n \nfactor\n(\nstate\n)\n\n\n\n# class != mode()\n\n\nclass\n(\nstatef\n)\n\n\n[\n1\n]\n \n\"factor\"\n\n\nmode\n(\nstatef\n)\n\n\n[\n1\n]\n \n\"numeric\"\n\n\n\nlevels\n(\nstatef\n)\n\n\n[\n1\n]\n \n\"act\"\n \n\"nsw\"\n \n\"nt\"\n  \n\"qld\"\n \n\"sa\"\n  \n\"tas\"\n \n\"vic\"\n \n\"wa\"\n \n\nincomes \n<-\n \nc\n(\n60\n,\n \n49\n,\n \n40\n,\n \n61\n,\n \n64\n,\n \n60\n,\n \n59\n,\n \n54\n,\n \n62\n,\n \n69\n,\n \n70\n,\n \n42\n,\n \n56\n,\n \n61\n,\n \n61\n,\n \n61\n,\n \n58\n,\n \n51\n,\n \n48\n,\n \n65\n,\n \n49\n,\n \n49\n,\n \n41\n,\n \n48\n,\n \n52\n,\n \n46\n,\n \n59\n,\n \n46\n,\n \n58\n,\n \n43\n)\n\n\nincmeans \n<-\n \ntapply\n(\nincomes\n,\n statef\n,\n \nmean\n)\n\n\nincmeans\n  act   nsw    nt   qld    sa   tas   vic    wa \n\n48.50\n \n55.00\n \n54.00\n \n51.60\n \n54.25\n \n53.00\n \n61.60\n \n54.50\n \n\nstderr \n<-\n \nfunction\n(\nx\n)\n \n{\n \nsqrt\n(\nvar\n(\nx\n)\n \n/\n \nlength\n(\nx\n))\n \n}\n\n\n\nstderr\n(\nincomes\n)\n\n\n[\n1\n]\n \n1.524462\n\n\n\n# alternatively\n\nincster \n<-\n \ntapply\n(\nincomes\n,\n statef\n,\n \nstderr\n)\n\n\nincster\n      act       nsw        nt       qld        sa       tas       vic        wa \n\n5.5000000\n \n3.9665266\n \n5.0000000\n \n2.6570661\n \n5.3909647\n \n7.0000000\n \n0.8717798\n \n6.2249498\n  \n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# a vector of characters\n\nstateff \n<-\n \nc\n(\n\"a\"\n,\n \n\"b\"\n,\n \n\"c\"\n)\n\n\n[\n1\n]\n \n\"a\"\n \n\"b\"\n \n\"c\"\n\n\n\nas.factor\n(\nstateff\n)\n\n\n[\n1\n]\n a b \nc\n\nLevels\n:\n a b \nc\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n# Create a factor with the wrong order of levels\n\nsizes \n<-\n \nfactor\n(\nc\n(\n\"small\"\n,\n \n\"large\"\n,\n \n\"large\"\n,\n \n\"small\"\n,\n \n\"medium\"\n))\n\n\nsizes\n\n[\n1\n]\n small  large  large  small  medium\nLevels\n:\n large medium small\n\n\n# levels can be specified explicitly\n\nsizes \n<-\n \nfactor\n(\nsizes\n,\n levels \n=\n \nc\n(\n\"small\"\n,\n \n\"medium\"\n,\n \n\"large\"\n))\n\n\nsizes\n\n[\n1\n]\n small  large  large  small  medium\nLevels\n:\n small medium large\n\n\n# do the same with an ordered factor\n\nsizes \n<-\n \nordered\n(\nc\n(\n\"small\"\n,\n \n\"large\"\n,\n \n\"large\"\n,\n \n\"small\"\n,\n \n\"medium\"\n))\n\n\nsizes \n<-\n \nordered\n(\nsizes\n,\n levels \n=\n \nc\n(\n\"small\"\n,\n \n\"medium\"\n,\n \n\"large\"\n))\n\n\nsizes\n\n[\n1\n]\n small  large  large  small  medium\nLevels\n:\n small \n<\n medium \n<\n large\n\n\n# use relevel() to make a particular level first in the list. (This will not work for ordered factors.)\n\n\n\n# Create a factor with the wrong order\n\nsizes\n\n[\n1\n]\n small  large  large  small  medium\nLevels\n:\n large medium small\n\n\n# make medium first\n\nsizes \n<-\n relevel\n(\nsizes\n,\n \n\"medium\"\n)\n\n\nsizes\n\n[\n1\n]\n small  large  large  small  medium\nLevels\n:\n medium large small\n\n\n# make small first\n\nsizes \n<-\n relevel\n(\nsizes\n,\n \n\"small\"\n)\n\n\nsizes\n\n[\n1\n]\n small  large  large  small  medium\nLevels\n:\n small medium large\n\n\n# specify the proper order when the factor is created\n\nsizes \n<-\n \nfactor\n(\nc\n(\n\"small\"\n,\n \n\"large\"\n,\n \n\"large\"\n,\n \n\"small\"\n,\n \n\"medium\"\n),\n\n                  levels \n=\n \nc\n(\n\"small\"\n,\n \n\"medium\"\n,\n \n\"large\"\n))\n\n\nsizes\n\n[\n1\n]\n small  large  large  small  medium\nLevels\n:\n small medium large\n\n\n# Create a factor with the wrong order of levels\n\nsizes \n<-\n \nfactor\n(\nc\n(\n\"small\"\n,\n \n\"large\"\n,\n \n\"large\"\n,\n \n\"small\"\n,\n \n\"medium\"\n))\n\n\nsizes\n\n[\n1\n]\n small  large  large  small  medium\nLevels\n:\n large medium small\n\n\n# reverse the order of levels in a factor\n\nsizes \n<-\n \nfactor\n(\nsizes\n,\n levels\n=\nrev\n(\nlevels\n(\nsizes\n)))\n\n\nsizes\n\n[\n1\n]\n small  large  large  small  medium\nLevels\n:\n small medium large\n\n\n\n\n\n\nConvert (\n.as\n)\n\u00b6\n\n\n\n\nas.factor()\n.\n\n\n\n\n5, Arrays and Matrices\n\u00b6\n\n\nDimension\n\u00b6\n\n\n1\n2\n3\n4\nz \n<-\n \n1\n:\n1500\n\n\ndim\n(\nz\n)\n \n<-\n \nc\n(\n3\n,\n \n5\n,\n \n100\n)\n\n\n\n# gives 100 arrays of 3 lines by 5 columns\n\n\n\n\n\n\n\nCreate a matrix, an array\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\nx \n<-\n \narray\n(\n1\n:\n20\n,\n dim\n=\nc\n(\n4\n,\n \n5\n))\n\n\nx\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n \n[,\n4\n]\n \n[,\n5\n]\n\n\n[\n1\n,]\n    \n1\n    \n5\n    \n9\n   \n13\n   \n17\n\n\n[\n2\n,]\n    \n2\n    \n6\n   \n10\n   \n14\n   \n18\n\n\n[\n3\n,]\n    \n3\n    \n7\n   \n11\n   \n15\n   \n19\n\n\n[\n4\n,]\n    \n4\n    \n8\n   \n12\n   \n16\n   \n20\n\n\nx \n<-\n \narray\n(\n1\n:\n20\n,\n dim\n=\nc\n(\n2\n,\n \n5\n,\n \n2\n))\n\n\nx\n\n,\n \n,\n \n1\n\n\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n \n[,\n4\n]\n \n[,\n5\n]\n\n\n[\n1\n,]\n    \n1\n    \n3\n    \n5\n    \n7\n    \n9\n\n\n[\n2\n,]\n    \n2\n    \n4\n    \n6\n    \n8\n   \n10\n\n\n\n,\n \n,\n \n2\n\n\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n \n[,\n4\n]\n \n[,\n5\n]\n\n\n[\n1\n,]\n   \n11\n   \n13\n   \n15\n   \n17\n   \n19\n\n\n[\n2\n,]\n   \n12\n   \n14\n   \n16\n   \n18\n   \n20\n\n\n\ni \n<-\n \narray\n(\nc\n(\n1\n:\n3\n,\n \n3\n:\n1\n),\n dim \n=\n \nc\n(\n3\n,\n \n2\n))\n\n\ni\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n1\n    \n3\n\n\n[\n2\n,]\n    \n2\n    \n2\n\n\n[\n3\n,]\n    \n3\n    \n1\n\n\nj \n<-\n \narray\n(\nc\n(\n1\n:\n8\n),\n dim \n=\n \nc\n(\n2\n,\n \n2\n,\n \n2\n))\n\n\nj\n\n,\n \n,\n \n1\n\n\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n1\n    \n3\n\n\n[\n2\n,]\n    \n2\n    \n4\n\n\n\n,\n \n,\n \n2\n\n\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n5\n    \n7\n\n\n[\n2\n,]\n    \n6\n    \n8\n\n\n\nk \n<-\n \narray\n(\n1\n:\n27\n,\n \nc\n(\n3\n,\n \n3\n,\n \n3\n))\n\n\n\n>\n k\n\n,\n \n,\n \n1\n\n\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n\n\n[\n1\n,]\n    \n1\n    \n4\n    \n7\n\n\n[\n2\n,]\n    \n2\n    \n5\n    \n8\n\n\n[\n3\n,]\n    \n3\n    \n6\n    \n9\n\n\n\n,\n \n,\n \n2\n\n\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n\n\n[\n1\n,]\n   \n10\n   \n13\n   \n16\n\n\n[\n2\n,]\n   \n11\n   \n14\n   \n17\n\n\n[\n3\n,]\n   \n12\n   \n15\n   \n18\n\n\n\n,\n \n,\n \n3\n\n\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n\n\n[\n1\n,]\n   \n19\n   \n22\n   \n25\n\n\n[\n2\n,]\n   \n20\n   \n23\n   \n26\n\n\n[\n3\n,]\n   \n21\n   \n24\n   \n27\n\n\n\na \n<-\n \nmatrix\n(\n1\n,\n \n2\n,\n \n2\n)\n\n\na\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n1\n    \n1\n\n\n[\n2\n,]\n    \n1\n    \n1\n\n\nb \n<-\n \nmatrix\n(\n2\n,\n \n2\n,\n \n2\n)\n\n\nb\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n2\n    \n2\n\n\n[\n2\n,]\n    \n2\n    \n2\n\n\n\n\n\n\n\nExtract, subset (matrix)\n\u00b6\n\n\n\n\na[2, 1]\n; rows, columns.\n\n\n\n\nExtract, subset (array)\n\n\n\n\na[2, 1, 1]\n; rows, columns, matrix.\n\n\n\n\nCross product vs multiplication\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\na \n<-\n \n1\n:\n5\n\nb \n<-\n \nseq\n(\n10\n,\n \n6\n,\n \n-1\n)\n\n\na\n\n[\n1\n]\n \n1\n \n2\n \n3\n \n4\n \n5\n\nb\n\n[\n1\n]\n \n10\n  \n9\n  \n8\n  \n7\n  \n6\n\n\na \n*\n b\n\n[\n1\n]\n \n10\n \n18\n \n24\n \n28\n \n30\n\n\n\ncrossprod\n(\na\n,\n b\n)\n\n     \n[,\n1\n]\n\n\n[\n1\n,]\n  \n110\n\n\nab \n<-\n a \n%o%\n b\nab\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n \n[,\n4\n]\n \n[,\n5\n]\n\n\n[\n1\n,]\n   \n10\n    \n9\n    \n8\n    \n7\n    \n6\n\n\n[\n2\n,]\n   \n20\n   \n18\n   \n16\n   \n14\n   \n12\n\n\n[\n3\n,]\n   \n30\n   \n27\n   \n24\n   \n21\n   \n18\n\n\n[\n4\n,]\n   \n40\n   \n36\n   \n32\n   \n28\n   \n24\n\n\n[\n5\n,]\n   \n50\n   \n45\n   \n40\n   \n35\n   \n30\n\n\n\n\n\n\n\nMatrix operation\n\u00b6\n\n\nA et B are 2x2 matrices:\n\n\n\n\nA * B\n; scalar multiplication.\n\n\nA %*% B\n; matrix multiplication\n\n\nx %*% A %*% y\n; matrix multiplication.\n\n\ncrossproduct(A, B)\n; cross multiplication.\n\n\nab \n<-\n \nouter\n(\nA\n,\nB\n,\n\"*\"\n)\n; \na * b\n.\n\n\nab \n<-\n \nouter\n(\nA\n,\nB\n,\n\"+\"\n)\n; \na + b\n.\n\n\nab \n<-\n \nouter\n(\nA\n,\nB\n,\n\"-\"\n)\n; \na - b\n.\n\n\nand many more.\n\n\n\n\nDiagonal and triangle\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\nab\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n \n[,\n4\n]\n \n[,\n5\n]\n\n\n[\n1\n,]\n   \n11\n   \n10\n    \n9\n    \n8\n    \n7\n\n\n[\n2\n,]\n   \n12\n   \n11\n   \n10\n    \n9\n    \n8\n\n\n[\n3\n,]\n   \n13\n   \n12\n   \n11\n   \n10\n    \n9\n\n\n[\n4\n,]\n   \n14\n   \n13\n   \n12\n   \n11\n   \n10\n\n\n[\n5\n,]\n   \n15\n   \n14\n   \n13\n   \n12\n   \n11\n\n\n\ndiag\n(\nab\n)\n\n\n[\n1\n]\n \n11\n \n11\n \n11\n \n11\n \n11\n\n\n\nlower.tri\n(\nab\n)\n\n      \n[,\n1\n]\n  \n[,\n2\n]\n  \n[,\n3\n]\n  \n[,\n4\n]\n  \n[,\n5\n]\n\n\n[\n1\n,]\n \nFALSE\n \nFALSE\n \nFALSE\n \nFALSE\n \nFALSE\n\n\n[\n2\n,]\n  \nTRUE\n \nFALSE\n \nFALSE\n \nFALSE\n \nFALSE\n\n\n[\n3\n,]\n  \nTRUE\n  \nTRUE\n \nFALSE\n \nFALSE\n \nFALSE\n\n\n[\n4\n,]\n  \nTRUE\n  \nTRUE\n  \nTRUE\n \nFALSE\n \nFALSE\n\n\n[\n5\n,]\n  \nTRUE\n  \nTRUE\n  \nTRUE\n  \nTRUE\n \nFALSE\n\n\n\nlower.tri\n(\nab\n)\n \n*\n ab\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n \n[,\n4\n]\n \n[,\n5\n]\n\n\n[\n1\n,]\n    \n0\n    \n0\n    \n0\n    \n0\n    \n0\n\n\n[\n2\n,]\n   \n12\n    \n0\n    \n0\n    \n0\n    \n0\n\n\n[\n3\n,]\n   \n13\n   \n12\n    \n0\n    \n0\n    \n0\n\n\n[\n4\n,]\n   \n14\n   \n13\n   \n12\n    \n0\n    \n0\n\n\n[\n5\n,]\n   \n15\n   \n14\n   \n13\n   \n12\n    \n0\n\n\n\nupper.tri\n(\nab\n)\n\n      \n[,\n1\n]\n  \n[,\n2\n]\n  \n[,\n3\n]\n  \n[,\n4\n]\n  \n[,\n5\n]\n\n\n[\n1\n,]\n \nFALSE\n  \nTRUE\n  \nTRUE\n  \nTRUE\n  \nTRUE\n\n\n[\n2\n,]\n \nFALSE\n \nFALSE\n  \nTRUE\n  \nTRUE\n  \nTRUE\n\n\n[\n3\n,]\n \nFALSE\n \nFALSE\n \nFALSE\n  \nTRUE\n  \nTRUE\n\n\n[\n4\n,]\n \nFALSE\n \nFALSE\n \nFALSE\n \nFALSE\n  \nTRUE\n\n\n[\n5\n,]\n \nFALSE\n \nFALSE\n \nFALSE\n \nFALSE\n \nFALSE\n\n\n\nupper.tri\n(\nab\n)\n \n*\n ab\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n \n[,\n4\n]\n \n[,\n5\n]\n\n\n[\n1\n,]\n    \n0\n   \n10\n    \n9\n    \n8\n    \n7\n\n\n[\n2\n,]\n    \n0\n    \n0\n   \n10\n    \n9\n    \n8\n\n\n[\n3\n,]\n    \n0\n    \n0\n    \n0\n   \n10\n    \n9\n\n\n[\n4\n,]\n    \n0\n    \n0\n    \n0\n    \n0\n   \n10\n\n\n[\n5\n,]\n    \n0\n    \n0\n    \n0\n    \n0\n    \n0\n\n\n\n\n\n\n\nSolving a matrix system (and more matrix algebra)\n\u00b6\n\n\n1\n2\n3\n4\nb \n<-\n A \n%*%\n x\n\n\n# or\n\n\nsolve\n(\nA\n,\n b\n)\n\n\n\n\n\n\n\n\n\nsolve(A)\n; inverse the matrix.\n\n\n\n\nSymmetrical matrix and eigenvalue\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nb \n<-\n \nmatrix\n(\n2\n,\n \n2\n,\n \n2\n)\n\n\nb\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n2\n    \n2\n\n\n[\n2\n,]\n    \n2\n    \n2\n\n\ne \n<-\n \neigen\n(\nb\n,\n only.values \n=\n \nTRUE\n)\n\ne\n\n$\nvalues\n\n[\n1\n]\n \n4\n \n0\n\n\n\n$\nvectors\n\nNULL\n\n\n\n\n\n\n\nSingular value decomposition (matrix)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nsvd\n(\nb\n)\n\n\n$\nd\n\n[\n1\n]\n \n4\n \n0\n\n\n\n$\nu\n           \n[,\n1\n]\n       \n[,\n2\n]\n\n\n[\n1\n,]\n \n-0.7071068\n \n-0.7071068\n\n\n[\n2\n,]\n \n-0.7071068\n  \n0.7071068\n\n\n\n$\nv\n           \n[,\n1\n]\n       \n[,\n2\n]\n\n\n[\n1\n,]\n \n-0.7071068\n \n-0.7071068\n\n\n[\n2\n,]\n \n-0.7071068\n  \n0.7071068\n\n\n\n\n\n\n\nDeterminant (matrix)\n\n\n1\n2\ndet\n(\nab\n)\n\n\n[\n1\n]\n \n0\n\n\n\n\n\n\n\nLeast square fitting (matrix)\n\u00b6\n\n\n\n\nlsfit()\n or least squares estimate of \nb\n in the model: \ny = X b + e\n.\n\n\n\n\nQR decomposition (matrix)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nqr\n(\nab\n)\n\n\n$\nqr\n\n            \n[,\n1\n]\n        \n[,\n2\n]\n          \n[,\n3\n]\n          \n[,\n4\n]\n          \n[,\n5\n]\n\n\n[\n1\n,]\n \n-29.2403830\n \n-27.0174299\n \n-2.479448e+01\n \n-2.257152e+01\n \n-2.034857e+01\n\n\n[\n2\n,]\n   \n0.4103913\n   \n0.2418254\n  \n4.836508e-01\n  \n7.254763e-01\n  \n9.673017e-01\n\n\n[\n3\n,]\n   \n0.4445906\n  \n-0.1703815\n  \n3.717512e-15\n  \n1.134003e-14\n  \n1.749210e-14\n\n\n[\n4\n,]\n   \n0.4787899\n  \n-0.5015812\n  \n3.957070e-01\n  \n1.553722e-15\n  \n1.295516e-15\n\n\n[\n5\n,]\n   \n0.5129892\n  \n-0.8327809\n  \n5.076995e-01\n  \n6.936403e-01\n \n-1.685698e-16\n\n\n\n$\nrank\n\n\n[\n1\n]\n \n2\n\n\n\n$\nqraux\n\n[\n1\n]\n \n1.376192e+00\n \n1.160818e+00\n \n1.765282e+00\n \n1.720322e+00\n \n1.685698e-16\n\n\n\n$\npivot\n\n[\n1\n]\n \n1\n \n2\n \n3\n \n4\n \n5\n\n\n\nattr\n(,\n\"class\"\n)\n\n\n[\n1\n]\n \n\"qr\"\n\n\n\n\n\n\nAlso:\n\n\n\n\nqr.coef()\n.\n\n\nqr.fitted()\n.\n\n\nqr.resid()\n.\n\n\n\n\nConvert (\n.as\n)\n\u00b6\n\n\n\n\nas.array()\n.\n\n\nas.matrix()\n.\n\n\n\n\n6, Lists and Data Frames\n\u00b6\n\n\nCreate a list\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nLst \n<-\n \nlist\n(\nname \n=\n \n\"Fred\"\n,\n wife \n=\n \n\"Mary\"\n,\n no.children \n=\n \n3\n,\n child.ages \n=\n \nc\n(\n4\n,\n7\n,\n9\n))\n\n\nLst\n\n$\nname\n\n[\n1\n]\n \n\"Fred\"\n\n\n\n$\nwife\n\n[\n1\n]\n \n\"Mary\"\n\n\n\n$\nno.children\n\n[\n1\n]\n \n3\n\n\n\n$\nchild.ages\n\n[\n1\n]\n \n4\n \n7\n \n9\n\n\n\n\n\n\n\nExtract, subset (list)\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nLst\n$\nname\n\n[\n1\n]\n \n\"Fred\"\n\n\nLst\n[[\n\"name\"\n]]\n \n==\n Lst\n[[\n1\n]]\n\n\n[\n1\n]\n \nTRUE\n\n\n\nLst\n[\n5\n]\n \n<-\n \nlist\n(\n\"\"\n)\n \n# or list()  \n\nLst\n[\n\"new\"\n]\n \n<-\n \nlist\n()\n  \n# key, value\n\n\nLst\n[\n1\n]\n\n\n$\nname\n\n[\n1\n]\n \n\"Fred\"\n\n\nLst\n$\nchild.ages\n[\n1\n]\n\n\n[\n1\n]\n \n4\n\nLst\n[[\n4\n]][\n1\n]\n\n\n[\n1\n]\n \n4\n\n\n\n\n\n\n\nConcatenate, paste\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\nx \n<-\n \nc\n(\n1\n,\n2\n)\n\ny \n<-\n \nc\n(\n3\n,\n4\n)\n\n\n\nc\n(\nx\n,\n y\n)\n\n\n[\n1\n]\n \n1\n \n2\n \n3\n \n4\n\n\n\npaste\n(\nx\n,\n y\n)\n\n\n[\n1\n]\n \n\"1 3\"\n \n\"2 4\"\n\n\n\ndata.frame\n(\nx\n,\n y\n)\n\n  x y\n\n1\n \n1\n \n3\n\n\n2\n \n2\n \n4\n\n\na \n<-\n \nlist\n(\n1\n,\n \n2\n)\n\nb \n<-\n \nlist\n(\n\"a\"\n,\n \n\"b\"\n)\n\n\n\nlist\n(\na\n,\n b\n)\n\n\n[[\n1\n]]\n\n\n[[\n1\n]][[\n1\n]]\n\n\n[\n1\n]\n \n1\n\n\n\n[[\n1\n]][[\n2\n]]\n\n\n[\n1\n]\n \n2\n\n\n\n\n[[\n2\n]]\n\n\n[[\n2\n]][[\n1\n]]\n\n\n[\n1\n]\n \n\"a\"\n\n\n\n[[\n2\n]][[\n2\n]]\n\n\n[\n1\n]\n \n\"b\"\n\n\nh \n<-\n \nmatrix\n(\n2\n,\n \n2\n,\n \n2\n)\n\ng \n<-\n \nmatrix\n(\n1\n,\n \n2\n,\n \n2\n)\n\n\n\nc\n(\nh\n,\n g\n)\n\n\n[\n1\n]\n \n2\n \n2\n \n2\n \n2\n \n1\n \n1\n \n1\n \n1\n\n\n\npaste\n(\nh\n,\n g\n)\n\n\n[\n1\n]\n \n\"2 1\"\n \n\"2 1\"\n \n\"2 1\"\n \n\"2 1\"\n\n\n\nlist\n(\nh\n,\n g\n)\n\n\n[[\n1\n]]\n\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n2\n    \n2\n\n\n[\n2\n,]\n    \n2\n    \n2\n\n\n\n[[\n2\n]]\n\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n1\n    \n1\n\n\n[\n2\n,]\n    \n1\n    \n1\n\n\n\n\n\n\n\nConvert (\nas.\n)\n\u00b6\n\n\n\n\nas.matrix()\n\n\n\n\nData frame\n\u00b6\n\n\n\n\nA data frame can hold other data frames.\n\n\nA list can hold other lists.\n\n\nA vector can hold other vectors.\n\n\n\n\n\n\nEach variable can be numeric, character, logical, factor, numeric matrix, list, data.frame.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\nstate \n<-\n \nc\n(\n\"tas\"\n,\n\"qld\"\n,\n\"sa\"\n,\n\"sa\"\n,\n\"sa\"\n,\n\"vic\"\n,\n\"nt\"\n,\n\"act\"\n,\n\"qld\"\n,\n\"nsw\"\n,\n\"wa\"\n,\n\"nsw\"\n,\n\"nsw\"\n,\n\"vic\"\n,\n\"vic\"\n,\n\"vic\"\n,\n\"nsw\"\n,\n\"qld\"\n,\n\"qld\"\n,\n\"vic\"\n,\n\"nt\"\n,\n\"wa\"\n,\n\"wa\"\n,\n\"qld\"\n,\n\"sa\"\n,\n \n\"tas\"\n,\n\"nsw\"\n,\n \n\"nsw\"\n,\n \n\"wa\"\n,\n\"act\"\n)\n\n\nstatef \n<-\n \nfactor\n(\nstate\n)\n\n\nincomes \n<-\n \nc\n(\n60\n,\n \n49\n,\n \n40\n,\n \n61\n,\n \n64\n,\n \n60\n,\n \n59\n,\n \n54\n,\n \n62\n,\n \n69\n,\n \n70\n,\n \n42\n,\n \n56\n,\n \n61\n,\n \n61\n,\n \n61\n,\n \n58\n,\n \n51\n,\n \n48\n,\n \n65\n,\n \n49\n,\n \n49\n,\n \n41\n,\n \n48\n,\n \n52\n,\n \n46\n,\n \n59\n,\n \n46\n,\n \n58\n,\n \n43\n)\n\n\nincomef \n<-\n \nfactor\n(\nincomes\n)\n\n\naccountants \n<-\n \ndata.frame\n(\nhome\n=\nstatef\n,\n loot\n=\nincomes\n,\n shot\n=\nincomef\n)\n\n\n\nhead\n(\naccountants\n,\n \n10\n)\n\n   home loot shot\n\n1\n   tas   \n60\n   \n60\n\n\n2\n   qld   \n49\n   \n49\n\n\n3\n    sa   \n40\n   \n40\n\n\n4\n    sa   \n61\n   \n61\n\n\n5\n    sa   \n64\n   \n64\n\n\n6\n   vic   \n60\n   \n60\n\n\n7\n    nt   \n59\n   \n59\n\n\n8\n   act   \n54\n   \n54\n\n\n9\n   qld   \n62\n   \n62\n\n\n10\n  nsw   \n69\n   \n69\n\n\n\n# class() != mode()\n\n\nclass\n(\naccountants\n)\n\n\n[\n1\n]\n \n\"data.frame\"\n\n\n\nmode\n(\naccountants\n)\n\n\n[\n1\n]\n \n\"list\"\n\n\n\n\n\n\n\nConcatenate, paste (data frame)\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n# accountants == acc\n\n\n\nc\n(\naccountants\n,\n acc\n)\n\n\n$\nhome\n \n[\n1\n]\n tas qld sa  sa  sa  vic nt  act qld nsw wa  nsw nsw\n\n[\n14\n]\n vic vic vic nsw qld qld vic nt  wa  wa  qld sa  tas\n\n[\n27\n]\n nsw nsw wa  act\nLevels\n:\n act nsw nt qld sa tas vic wa\n\n\n$\nloot\n \n[\n1\n]\n \n60\n \n49\n \n40\n \n61\n \n64\n \n60\n \n59\n \n54\n \n62\n \n69\n \n70\n \n42\n \n56\n \n61\n \n61\n \n61\n \n58\n \n51\n\n\n[\n19\n]\n \n48\n \n65\n \n49\n \n49\n \n41\n \n48\n \n52\n \n46\n \n59\n \n46\n \n58\n \n43\n\n\n\n$\nshot\n \n[\n1\n]\n \n60\n \n49\n \n40\n \n61\n \n64\n \n60\n \n59\n \n54\n \n62\n \n69\n \n70\n \n42\n \n56\n \n61\n \n61\n \n61\n \n58\n \n51\n\n\n[\n19\n]\n \n48\n \n65\n \n49\n \n49\n \n41\n \n48\n \n52\n \n46\n \n59\n \n46\n \n58\n \n43\n\n\n20\n Levels\n:\n \n40\n \n41\n \n42\n \n43\n \n46\n \n48\n \n49\n \n51\n \n52\n \n54\n \n56\n \n58\n \n59\n \n...\n \n70\n\n\n\n$\nhome\n \n[\n1\n]\n tas qld sa  sa  sa  vic nt  act qld nsw wa  nsw nsw\n\n[\n14\n]\n vic vic vic nsw qld qld vic nt  wa  wa  qld sa  tas\n\n[\n27\n]\n nsw nsw wa  act\nLevels\n:\n act nsw nt qld sa tas vic wa\n\n\n$\nloot\n \n[\n1\n]\n \n60\n \n49\n \n40\n \n61\n \n64\n \n60\n \n59\n \n54\n \n62\n \n69\n \n70\n \n42\n \n56\n \n61\n \n61\n \n61\n \n58\n \n51\n\n\n[\n19\n]\n \n48\n \n65\n \n49\n \n49\n \n41\n \n48\n \n52\n \n46\n \n59\n \n46\n \n58\n \n43\n\n\n\n$\nshot\n \n[\n1\n]\n \n60\n \n49\n \n40\n \n61\n \n64\n \n60\n \n59\n \n54\n \n62\n \n69\n \n70\n \n42\n \n56\n \n61\n \n61\n \n61\n \n58\n \n51\n\n\n[\n19\n]\n \n48\n \n65\n \n49\n \n49\n \n41\n \n48\n \n52\n \n46\n \n59\n \n46\n \n58\n \n43\n\n\n20\n Levels\n:\n \n40\n \n41\n \n42\n \n43\n \n46\n \n48\n \n49\n \n51\n \n52\n \n54\n \n56\n \n58\n \n59\n \n...\n \n70\n\n\n\n\npaste\n(\naccountants\n,\n acc\n)\n\n\n[\n1\n]\n \n\"c(6, 4, 5, 5, 5, 7, 3, 1, 4, 2, 8, 2, 2, 7, 7, 7, 2, 4, 4, 7, 3, 8, 8, 4, 5, 6, 2, 2, 8, 1) c(6, 4, 5, 5, 5, 7, 3, 1, 4, 2, 8, 2, 2, 7, 7, 7, 2, 4, 4, 7, 3, 8, 8, 4, 5, 6, 2, 2, 8, 1)\"\n                                                            \n\n[\n2\n]\n \n\"c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43) c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\"\n\n\n[\n3\n]\n \n\"c(14, 7, 1, 15, 17, 14, 13, 10, 16, 19, 20, 3, 11, 15, 15, 15, 12, 8, 6, 18, 7, 7, 2, 6, 9, 5, 13, 5, 12, 4) c(14, 7, 1, 15, 17, 14, 13, 10, 16, 19, 20, 3, 11, 15, 15, 15, 12, 8, 6, 18, 7, 7, 2, 6, 9, 5, 13, 5, 12, 4)\"\n                          \n\n\nlist\n(\naccountants\n,\n acc\n)\n\n\n[[\n1\n]]\n\n   home loot shot\n\n1\n   tas   \n60\n   \n60\n\n\n2\n   qld   \n49\n   \n49\n\n\n3\n    sa   \n40\n   \n40\n\n\n4\n    sa   \n61\n   \n61\n\n\n5\n    sa   \n64\n   \n64\n\n\n6\n   vic   \n60\n   \n60\n\n\n7\n    nt   \n59\n   \n59\n\n\n8\n   act   \n54\n   \n54\n\n\n9\n   qld   \n62\n   \n62\n\n\n10\n  nsw   \n69\n   \n69\n\n\n...\n\n\n\n[[\n2\n]]\n\n   home loot shot\n\n1\n   tas   \n60\n   \n60\n\n\n2\n   qld   \n49\n   \n49\n\n\n3\n    sa   \n40\n   \n40\n\n\n4\n    sa   \n61\n   \n61\n\n\n5\n    sa   \n64\n   \n64\n\n\n6\n   vic   \n60\n   \n60\n\n\n7\n    nt   \n59\n   \n59\n\n\n8\n   act   \n54\n   \n54\n\n\n9\n   qld   \n62\n   \n62\n\n\n10\n  nsw   \n69\n   \n69\n\n\n...\n\n\naccountants \n+\n acc\n   home loot shot\n\n1\n    \nNA\n  \n120\n   \nNA\n\n\n2\n    \nNA\n   \n98\n   \nNA\n\n\n3\n    \nNA\n   \n80\n   \nNA\n\n\n4\n    \nNA\n  \n122\n   \nNA\n\n\n5\n    \nNA\n  \n128\n   \nNA\n\n\n6\n    \nNA\n  \n120\n   \nNA\n\n\n7\n    \nNA\n  \n118\n   \nNA\n\n\n8\n    \nNA\n  \n108\n   \nNA\n\n\n9\n    \nNA\n  \n124\n   \nNA\n\n\n10\n   \nNA\n  \n138\n   \nNA\n\n\n...\n\n\n\n\n\n\n\nConvert (\nas.\n)\n\u00b6\n\n\n\n\nas.data.frame()\n\n\n\n\nLoad data into R\n\u00b6\n\n\n\n\nread.table()\n; produce a data frame with inputs.\n\n\n\n\nA note on environment objects\n\n\n\n\nsearch()\n; objects in .GlobalEnv; including packages.\n\n\nls()\n; list these objects.\n\n\n\n\n\n\nattach()\n; attach an object to .GlobalEnv.\n\n\ndetach()\n.\n\n\n\n\n7, Reading/Writing Data from/to Files (Input/Output)\n\u00b6\n\n\nA lot more can be found on I/O: depending on the file type, the data format, the desired R object, many commands are available. Depending on a file format, look for the dedicated package on CRAN.\n\n\nread, reading, write, writing, input, output, i/o\n\n\nExamples\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n# read from a file\n\nHousePrice \n<-\n read.table\n(\n\"house.data\"\n,\n header \n=\n \nTRUE\n,\n sep \n=\n \n\";\"\n)\n\n\n\n# read a 3-variable list\n\ninput \n<-\n \nscan\n(\n\"input.dat\"\n,\n \nlist\n(\n\"\"\n,\n \n0\n,\n \n0\n))\n  \n\n\n# read a3-variable list\n\ninput \n<-\n \nscan\n(\n\"input.dat\"\n,\n \nlist\n(\nid\n=\n\"\"\n,\n x \n=\n \n0\n,\n y \n=\n \n0\n))\n\n\n\n# make 3 separate vectors\n\nlabel \n<-\n input\n[[\n1\n]]\n\nx \n<-\n input\n[[\n2\n]]\n\ny \n<-\n input\n[[\n3\n]]\n\n\n\n# label them\n\nlabel \n<-\n inp\n$\nid\nx \n<-\n inp\n$\nx\ny \n<-\n inp\n$\ny\n\n\n# read a 5-variable list and transform it into a matrix\n\nX \n<-\n \nmatrix\n(\nscan\n(\n\"light.dat\"\n,\n \n0\n),\n ncol \n=\n \n5\n,\n byrow \n=\n \nTRUE\n)\n\n\n\n\n\n\n\nInputing from file types\u2026\n\n\n\n\n.txt.\n\n\n.csv.\n\n\n.tsv.\n\n\n.hdf5.\n\n\n.bmp.\n\n\n.jpeg.\n\n\n.png.\n\n\n.tiff.\n\n\n.zip.\n\n\n.xls, spreadsheet.\n\n\ndatabases.\n\n\nstatistical programs.\n\n\nbinary files.\n\n\nand many more (some are up and coming such as julia files).\n\n\n\n\nCleaning parameter\n\u00b6\n\n\n\n\nstrip.white = TRUE\n; remove unnecessary spaces.\n\n\n\n\nChange the data frame or matrix format\n\u00b6\n\n\n\n\nstack()\n; for example, take a 6-column, 4-variable data frame and stack the 4 variables into one long column.\n\n\nunstack()\n; vice-versa.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n# 6 columns or variables\n\nStatus Age  V1    V2    V3    V4\n P \n23646\n \n45190\n \n50333\n \n55166\n \n56271\n\nCC \n26174\n \n35535\n \n38227\n \n37911\n \n41184\n\nCC \n27723\n \n25691\n \n25712\n \n26144\n \n26398\n\nCC \n27193\n \n30949\n \n29693\n \n29754\n \n30772\n\nCC \n24370\n \n50542\n \n51966\n \n54341\n \n54273\n\nCC \n28359\n \n58591\n \n58803\n \n59435\n \n61292\n\nCC \n25136\n \n45801\n \n45389\n \n47197\n \n47126\n\n\n...\n\n\n\n# 4 columns, more rows\n\nStatus Age values ind\nX1  P \n23646\n \n45190\n V1\nX2 CC \n26174\n \n35535\n V1\nX3 CC \n27723\n \n25691\n V1\nX4 CC \n27193\n \n30949\n V1\nX5 CC \n24370\n \n50542\n V1\nX6 CC \n28359\n \n58591\n V1\nX7 CC \n25136\n \n45801\n V1\nX11 P \n23646\n \n50333\n V2\n\n...\n\n\n\n\n\n\n\n\n\nreshape()\n; stack and unstack variables, according to parameters.\n\n\n\n\n1\nreshape\n(\nzz\n,\n idvar \n=\n \n\"id\"\n,\n timevar \n=\n \n\"var\"\n,\n varying \n=\n \nlist\n(\nc\n(\n\"V1\"\n,\n \n\"V2\"\n,\n \n\"V3\"\n,\n \n\"V4\"\n)),\n direction \n=\n \n\"long\"\n)\n\n\n\n\n\n\n\n\n\nftable()\n; flatten multidimensional matrices (arrays);\n\n\nread.ftable()\n.\n\n\ndata()\n; list of datasets.\n\n\ndata(package = \"rpart\")\n; load package.\n\n\nedit(input)\n; open a mini-spreadsheet.\n\n\nxnew \n<-\n edit\n(\ninput\n)\n; open and save it as a new dataset.\n\n\nand many more.\n\n\n\n\nEncoding\n\u00b6\n\n\n\n\nutf-8 for Linux and OS X.\n\n\nUCS-2LE and UTF-16 for Windows.\n\n\n\n\n1\n2\n3\n4\n# examples\n\n\nreadLines\n(\n\"filename.txt\"\n,\n encoding \n=\n \n\"UTF-8\"\n)\n\n\nreadLines\n(\n\"filename.txt\"\n,\n encoding \n=\n \n\"UCS2LE\"\n)\n\nread.delim\n(\n\"clipboard\"\n,\n fileEncoding\n=\n\"UTF-16\"\n)\n\n\n\n\n\n\n\nOutput\n\u00b6\n\n\n\n\nwrite.table()\n; many parameters.\n\n\nwrite.matrix()\n.\n\n\nwrite.foreign()\n.\n\n\n\n\nCheck a file\n\u00b6\n\n\n\n\nreadLines(\"aab.txt\")\n.\n\n\nreadLines(\"aab.txt\", 1)\n.\n\n\nunlink(\"aab.txt\")\n; delete the file on the working directory.\n\n\n\n\nDirectory management\n\u00b6\n\n\n\n\ngetwd()\n; get the current working directory.\n\n\nsetwd()\n; set the current working directory.\n\n\nfile.show(filepath)\n.\n\n\nsystem.file()\n\n\n\n\nSpreadsheet editor and edition in R\n\u00b6\n\n\n\n\nfix(c)\n; edit object c; a vetor, list, data frame, etc. in a mini-spreadsheet.\n\n\ndata.entry(c, mode=NULL, Names=NULL), dataentry(c), de(c, Modes=list(), Names=NULL)\n; idem.\n\n\nvi(file= )\n; invoke a text editor.\n\n\nemacs(file= )\n; idem.\n\n\npico(file= )\n; idem.\n\n\nxemacs(file= )\n; idem.\n\n\nxedit(file=)\n; idem.\n\n\n\n\n8, Probability Distributions\n\u00b6\n\n\nDistribution\n\u00b6\n\n\n\n\nbeta\n.\n\n\nbinom\n.\n\n\ncauchy\n.\n\n\nchisq\n.\n\n\nexp\n.\n\n\nf\n.\n\n\ngamma\n.\n\n\ngeom\n.\n\n\nhyper\n.\n\n\nlnorm\n.\n\n\nlogis\n.\n\n\nnbinom\n.\n\n\nnorm\n.\n\n\npois\n.\n\n\nsignrank\n.\n\n\nt\n.\n\n\nunif\n.\n\n\nweibull\n.\n\n\nwilcox\n.\n\n\nand many more.\n\n\n\n\nPrefixes\n\n\n\n\nd\n; density.\n\n\np\n; CDF or cumulative density function.\n\n\nq\n; quantile.\n\n\nr\n; random deviates or simulation or number generation.\n\n\n\n\nDistribution operation\n\u00b6\n\n\nCommands, prefix + distribution, examples:\n\n\n\n\ndbeta(x= )\n.\n\n\npbinom(q= , lower.tail= , log.p= )\n.\n\n\nqcauchy(p= , lower.tail= , log.p= )\n.\n\n\nrchisq(n or r= )\n.\n\n\nptukey()\n; studentized (t) distribution\n\n\nqtukey()\n.\n\n\ndmultinom()\n.\n\n\nltinomial\n.\n\n\nrmultinom()\n.\n\n\netc.\n\n\n\n\nDescriptive statistics\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n# overview\n\n\nsummary\n(\ncars\n)\n\nstr\n(\ncars\n)\n\nboxplot\n(\ncars\n$\nspeed\n)\n\nboxplot\n(\ncars\n$\ndist\n)\n\n\n\n# Tukey five-number summaries\n\nfivenum\n(\ncars\n$\nspeed\n)\n\n\n\n# histograms and bar charts\n\nstem\n(\ncars\n$\nspeed\n)\n\nhist\n(\ncars\n$\nspeed\n)\n\nbarplot\n(\ncars\n$\nspeed\n)\n\ndotchart\n(\ncars\n$\nspeed\n)\n\n\n\n# scatter plot\n\nplot\n(\ncars\n$\nspeed\n,\n cars\n$\ndist\n)\n\nlines\n(\ncars\n$\nspeed\n,\n cars\n$\ndist\n)\n\n\n\n# add a (1-d) representation\n\nplot\n(\ncars\n$\nspeed\n,\n cars\n$\ndist\n)\n\nrug\n(\ncars\n$\nspeed\n,\n ticksize \n=\n \n0.03\n)\n\nplot\n(\ncars\n$\nspeed\n,\n cars\n$\ndist\n)\n\nrug\n(\ncars\n$\ndist\n,\n ticksize \n=\n \n0.03\n)\n\n\n\n# normality of the residuals, linearity\n\nqqnorm\n(\ncars\n$\nspeed\n)\n\nqqline\n(\ncars\n$\nspeed\n)\n\n\nqqnorm\n(\ncars\n$\ndist\n)\n\nqqline\n(\ncars\n$\ndist\n)\n\n\nqqplot\n(\ncars\n$\nspeed\n,\n cars\n$\ndist\n)\n\n\n\n# normality test\n\nshapiro.test\n(\ncars\n$\nspeed\n)\n\n\n\n# Kolmogorov-Smirnov test on a sample\n\nks.test\n(\ncars\n$\nspeed\n[\n10\n],\n \n\"pnorm\"\n,\n mean \n=\n \nmean\n(\ncars\n$\nspeed\n),\n sd \n=\n \nsqrt\n(\nvar\n(\ncars\n$\nspeed\n)))\n\nks.test\n(\ncars\n$\nspeed\n[\n40\n],\n \n\"pnorm\"\n,\n mean \n=\n \nmean\n(\ncars\n$\nspeed\n),\n sd \n=\n \nsqrt\n(\nvar\n(\ncars\n$\nspeed\n)))\n\n\n\n\n\n\n\nTest\n\u00b6\n\n\n\n\nt.test(A,B)\n; true difference of means is not 0, difference means.\n\n\nvar.test(A,B)\n; true ratio of variances is not 1, difference variances.\n\n\nt.test(A,B, var.equal=TRUE)\n; true difference of means is not 0.\n\n\nwilcox.test(A,B)\n; rank sum with continuity correction, continuous distribution.\n\n\nand many more.\n\n\n\n\nNormality\n\u00b6\n\n\n\n\nplot(ecdf(A), do.points=FALSE, verticals=TRUE, xlim=range(A, B))\n\n\nplot(ecdf(B), do.points=FALSE, verticals=TRUE, add=TRUE)\n\n\nks.test(A,B); maximal vertical distance between the two ecdf\n\n\n\n\n9, Grouping, Loops and Conditional Execution\n\u00b6\n\n\nControl flow\n\u00b6\n\n\n\n\n&\n; AND.\n\n\n&&\n; AND; evaluates left to right, examining only the first element of each vector.\n\n\n|\n; OR.\n\n\n||\n; OR; evaluates left to right, examining only the first element of each vector.\n\n\nxor()\n; elementwise exclusive OR.\n\n\nisTRUE(x)\n; is true if and only if x is a length-one logical vector whose only element is TRUE and which has no attributes.\n\n\nifelse(condition, a, b)\n; if \ncondition\n is proven true, return \na\n, or else, return \nb\n.\n\n\n\n\nLoops\n\u00b6\n\n\nlooping\n\n\n\n\nfor(var in seq) expr\n.\n\n\nwhile(condition is true) expr\n.\n\n\nrepeat expr\n.\n\n\nbreak\n; break out of a for, while or repeat loop; control is transferred to the first statement outside the inner-most loop.\n\n\nnext\n; halt the processing of the current iteration and advance the looping index.\n\n\n\n\n10, Writing you own Functions (with examples)\n\u00b6\n\n\nSimple custom function\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# define\n\nmyfunction \n<-\n \nfunction\n(\narg1\n,\n arg2\n,\n \n...\n){\n\nstatements\n\nreturn\n(\nobject\n)\n\n\n}\n\n\n\n# call\n\nmyfunction\n(\narg1\n,\n arg2\n,\n \n...\n)\n\n\n\n\n\n\n\nIt is possible to remplace function arguments with variables and objects.\n\n\nComplex custom function\n\n\nInvolve conditions and loops.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\ntwosam \n<-\n \nfunction\n(\ny1\n,\n y2\n)\n \n{\n\n    n1 \n<-\n \nlength\n(\ny1\n);\n n2 \n<-\n \nlength\n(\ny2\n)\n\n    yb1 \n<-\n \nmean\n(\ny1\n);\n\n    yb2 \n<-\n \nmean\n(\ny2\n)\n\n    s1 \n<-\n var\n(\ny1\n);\n\n    s2 \n<-\n var\n(\ny2\n)\n\n    s \n<-\n \n((\nn1 \n-\n \n1\n)\n \n*\n s1 \n+\n \n(\nn2\n-1\n)\n \n*\n s2\n)\n \n/\n \n(\nn1 \n+\n n2 \n-\n \n2\n)\n\n    tst \n<-\n \n(\nyb1 \n-\n yb2\n)\n \n/\n \nsqrt\n(\ns \n*\n \n(\n1\n \n/\n n1 \n+\n \n1\n \n/\n n2\n))\n\n    tst\n\n}\n\n\n\n\n\n\n\nDefine and call on the same line\n\n\n\n\ntstat \n<-\n twosam\n(\ndata\n$\nmale\n,\n data\n$\nfemale\n);\n tstat\n\n\n\n\nSeveral commands and functions can be called on the same line with:\n\n\n\n\ncommand1; command2; function1; function2\n \n\n\n\n\nTwo ways of defining and calling a function\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nmulty \n<-\n \nfunction\n(\nx\n,\n y\n)\n \n{\n\n    x \n*\n y\n    \n}\n\n\n\n# with a binary operator\n\n\n\"%!%\"\n \n<-\n \nfunction\n(\nx\n,\n y\n)\n \n{\n\n    x \n*\n y\n    \n}\n\n\nx \n<-\n \n2\n\ny \n<-\n \n2\n\n\nmulty\n(\nx\n,\n y\n)\n\n\n[\n1\n]\n \n4\n\n\nx \n%!%\n y\n\n[\n1\n]\n \n4\n\n\n\n\n\n\n\nMatrix multiplication operator, \n%*%\n, and the outer product matrix operator, \n%o\n%\n, are other examples of binary operators.\n\n\nFunction in a function\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n# case 1\n\narea \n<-\n \nfunction\n(\nf\n,\n a\n,\n b\n,\n eps \n=\n \n1.0e-06\n,\n lim \n=\n \n10\n)\n \n{\n\n    fun1 \n<-\n \nfunction\n(\nf\n,\n a\n,\n b\n,\n fa\n,\n fb\n,\n a0\n,\n eps\n,\n lim\n,\n fun\n)\n \n{\n\n        d \n<-\n \n(\na \n+\n b\n)\n/\n2\n\n        h \n<-\n \n(\nb \n-\n a\n)\n/\n4\n\n        fd \n<-\n f\n(\nd\n)\n\n        a1 \n<-\n h \n*\n \n(\nfa \n+\n fd\n)\n\n        a2 \n<-\n h \n*\n \n(\nfd \n+\n fb\n)\n\n        \nif\n(\nabs\n(\na0 \n-\n a1 \n-\n a2\n)\n \n<\n eps \n||\n lim \n==\n \n0\n)\n\n            \nreturn\n(\na1 \n+\n a2\n)\n\n        \nelse\n \n{\n\n            \nreturn\n(\nfun\n(\nf\n,\n a\n,\n d\n,\n fa\n,\n fd\n,\n a1\n,\n eps\n,\n lim \n-\n \n1\n,\n fun\n)\n \n                fun\n(\nf\n,\n d\n,\n b\n,\n fd\n,\n fb\n,\n a2\n,\n eps\n,\n lim \n-\n \n1\n,\n fun\n))\n\n        \n}\n\n    \n}\n\n    fa \n<-\n f\n(\na\n)\n\n    fb \n<-\n f\n(\nb\n)\n\n    a0 \n<-\n \n((\nfa \n+\n fb\n)\n \n*\n \n(\nb \n-\n a\n))\n/\n2\n\n    fun1\n(\nf\n,\n a\n,\n b\n,\n fa\n,\n fb\n,\n a0\n,\n eps\n,\n lim\n,\n fun1\n)\n\n\n}\n\n\n\n# case 2\n\nf \n<-\n \nfunction\n(\nx\n)\n \n{\n\n    y \n<-\n \n2\n*\nx\n    \nprint\n(\nx\n)\n\n    \nprint\n(\ny\n)\n\n    \nprint\n(\nz\n)\n\n\n}\n\n\n\n# case 3\n\ncube \n<-\n \nfunction\n(\nn\n)\n \n{\n\n    sq \n<-\n \nfunction\n()\n n\n*\nn\n    n\n*\nsq\n()\n\n\n}\n\n\n\n# case 4\n\nopen.account \n<-\n \nfunction\n(\ntotal\n)\n \n{\n\n    \nlist\n(\n\n        deposit \n=\n \nfunction\n(\namount\n)\n \n{\n\n            \nif\n(\namount \n<=\n \n0\n)\n\n                \nstop\n(\n\"Deposits must be positive!\\n\"\n)\n\n            total \n<<-\n total \n+\n amount\n            \ncat\n(\namount\n,\n \n\"deposited. Your balance is\"\n,\n total\n,\n \n\"\\n\\n\"\n)\n\n        \n},\n\n        withdraw \n=\n \nfunction\n(\namount\n)\n \n{\n\n            \nif\n(\namount \n>\n total\n)\n\n                \nstop\n(\n\"You don\u2019t have that much money!\\n\"\n)\n\n            total \n<<-\n total \n-\n amount\n            \ncat\n(\namount\n,\n \n\"withdrawn. Your balance is\"\n,\n total\n,\n \n\"\\n\\n\"\n)\n\n        \n},\n\n        balance \n=\n \nfunction\n()\n \n{\n\n            \ncat\n(\n\"Your balance is\"\n,\n total\n,\n \n\"\\n\\n\"\n)\n\n        \n}\n\n    \n)\n\n\n}\n\n\nross \n<-\n open.account\n(\n100\n)\n\nrobert \n<-\n open.account\n(\n200\n)\n\n\nross\n$\nwithdraw\n(\n30\n)\n\nross\n$\nbalance\n()\n\nrobert\n$\nbalance\n()\n\nross\n$\ndeposit\n(\n50\n)\n\nross\n$\nbalance\n()\n\nross\n$\nwithdraw\n(\n500\n)\n\n\n\n\n\n\nName\n\n\nAdd, modify, and remove (with \nnames\n(\nx\n)\n \n<-\n \nNA\n or \n0\n) names.\n\n\n\n\nnames()\n.\n\n\nrownames()\n.\n\n\ncolnames()\n.\n\n\ndimnames()\n.\n\n\n\n\nCustomizing startup\n\u00b6\n\n\nCustomize the R environment through a directory initialization file; commands that you want to execute every time R is started under your system.\n\n\nR will always source the Rprofile.site file first. \n\n\nOn Windows, the file is in C:\\Program Files\\R\\R-n.n.n\\etc. You can also place a .Rprofile file in any directory that you are going to run R from or in the user home directory. \n\n\nIndividual users control over their workspace and allows for different startup procedures in different working directories.\n\n\nIf no .Rprofile file is found in the startup directory, then R looks for a .Rprofile file in the user\u2019s home directory and uses that (if it exists) environment variable R_PROFILE_USER is set. The file it points to is used instead of the .Rprofile files. \n\n\nFunction named .First() in either of the two profile files or in the .RData image has a special status: initialize the environment\n\n\nSequence in which files are executed is:\n\n\n\n\nRprofile.site\n\n\nthe user profile\n\n\n.RData\n\n\n.First()\n\n\n.Last(), if defined, is (normally) executed at the very end of the session.\n\n\n\n\nList function and method\n\u00b6\n\n\n\n\nmethods(class = \"data.frame\")\n; list methods associated with the class.\n\n\nmethods(plot)\n; list methods specific to the object.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nmethods\n(\nclass \n=\n \n\"data.frame\"\n)\n\n \n[\n1\n]\n \n$\n             \n$<-\n           \n[\n             \n[[\n            \n[[\n<-\n         \n \n[\n6\n]\n \n[\n<-\n           aggregate     anyDuplicated as.data.frame as.list      \n\n[\n11\n]\n as.matrix     by            cbind         coerce        dim          \n\n[\n16\n]\n dimnames      \ndimnames\n<-\n    droplevels    duplicated    edit         \n\n[\n21\n]\n format        formula       head          initialize    is.na        \n\n[\n26\n]\n Math          merge         na.exclude    na.omit       Ops          \n\n[\n31\n]\n plot          print         prompt        rbind         row.names    \n\n[\n36\n]\n \nrow.names\n<-\n   rowsum        show          slotsFromS3   split        \n\n[\n41\n]\n \nsplit\n<-\n       stack         str           subset        summary      \n\n[\n46\n]\n Summary       t             tail          transform     unique       \n\n[\n51\n]\n unstack       within       \nsee \n'?methods'\n \nfor\n accessing help and \nsource\n code\n\nmethods\n(\nplot\n)\n\n \n[\n1\n]\n plot.acf\n*\n           plot.data.frame\n*\n    plot.decomposed.ts\n*\n\n \n[\n4\n]\n plot.default        plot.dendrogram\n*\n    plot.density\n*\n      \n \n[\n7\n]\n plot.ecdf           plot.factor\n*\n        plot.formula\n*\n      \n\n[\n10\n]\n plot.function       plot.hclust\n*\n        plot.histogram\n*\n    \n\n[\n13\n]\n plot.HoltWinters\n*\n   plot.isoreg\n*\n        plot.lm\n*\n           \n\n[\n16\n]\n plot.medpolish\n*\n     plot.mlm\n*\n           plot.ppr\n*\n          \n\n[\n19\n]\n plot.prcomp\n*\n        plot.princomp\n*\n      plot.profile.nls\n*\n  \n\n[\n22\n]\n plot.R6\n*\n            plot.raster\n*\n        plot.spec\n*\n         \n\n[\n25\n]\n plot.stepfun        plot.stl\n*\n           plot.table\n*\n        \n\n[\n28\n]\n plot.ts             plot.tskernel\n*\n      plot.TukeyHSD\n*\n     \nsee \n'?methods'\n \nfor\n accessing help and \nsource\n code\n\n\n\n\n\n\nDifference:\n\n\n\n\nplot()\n; a generic method.\n\n\nplot.\n ; a specific method such as \nplot.ts()\n for example.\n\n\n\n\n11, Statistical models in R\n\u00b6\n\n\nRegression\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\ny \n<-\n \n1\n:\n10\n\nx \n<-\n \n1\n:\n10\n\n\na \n<-\n lm\n(\ny \n<\nsub\n>\nx\n)\n\na\nCall\n:\n\nlm\n(\nformula \n=\n y \n<\nsub\n>\nx\n)\n\n\nCoefficients\n:\n\n\n(\nIntercept\n)\n            x  \n  \n1.123e-15\n    \n1.000e+00\n  \n\n\n\n# no intercept, through the origin\n\nb \n<-\n lm\n(\ny \n<\nsub\n>\n0\n \n+\n x\n)\n\nb\nCall\n:\n\nlm\n(\nformula \n=\n y \n<\nsub\n>\n0\n \n+\n x\n)\n\n\nCoefficients\n:\n\nx  \n\n1\n  \n\n\n\n# no intercept\n\n\nc\n \n<-\n lm\n(\ny \n<\nsub\n>\n-1\n \n+\n x\n)\n\n\nc\n\nCall\n:\n\nlm\n(\nformula \n=\n y \n<\nsub\n>\n-1\n \n+\n x\n)\n\n\nCoefficients\n:\n\nx  \n\n1\n  \n\n\nd \n<-\n lm\n(\ny \n<\nsub\n>\nx \n-\n \n1\n)\n\nd\nCall\n:\n\nlm\n(\nformula \n=\n y \n<\nsub\n>\nx \n-\n \n1\n)\n\n\nCoefficients\n:\n\nx  \n\n1\n  \n\n\n\n# log\n\ne \n<-\n lm\n(\nlog\n(\ny\n)\n \n<\nsub\n>\nx\n)\n\ne\nCall\n:\n\nlm\n(\nformula \n=\n \nlog\n(\ny\n)\n \n<\nsub\n>\nx\n)\n\n\nCoefficients\n:\n\n\n(\nIntercept\n)\n            x  \n     \n0.2432\n       \n0.2304\n  \n\n\n\n# quadratic\n\nf \n<-\n lm\n(\ny \n<\nsub\n>\n1\n \n+\n x \n+\n \nI\n(\nx\n^\n2\n))\n\nf\nCall\n:\n\nlm\n(\nformula \n=\n y \n<\nsub\n>\n1\n \n+\n x \n+\n \nI\n(\nx\n^\n2\n))\n\n\nCoefficients\n:\n\n\n(\nIntercept\n)\n            x       \nI\n(\nx\n^\n2\n)\n  \n  \n1.123e-15\n    \n1.000e+00\n    \n5.699e-18\n  \n\n\n\n# polynomial\n\ng \n<-\n lm\n(\ny \n<\nsub\n>\nX \n+\n poly\n(\nx\n,\n \n2\n))\n\n\n\n\n# weighted regression\n\nfm1 \n<-\n lm\n(\ny \n<\nsub\n>\nx\n,\n data \n=\n dummy\n,\n weight \n=\n \n1\n \n/\n w\n^\n2\n)\n\n\n\n\n# and more\n\nnew.model \n<-\n update\n(\nold.model\n,\n new.formula\n)\n\n\nfm05 \n<-\n lm\n(\ny \n<\nsub\n>\nx1 \n+\n x2 \n+\n x3 \n+\n x4 \n+\n x5\n,\n data \n=\n production\n)\n\n\nfm6 \n<-\n update\n(\nfm06\n,\n \n.\n \n<\nsub\n>\n.\n \n+\n x6\n)\n\nsmf6 \n<-\n update\n(\nfm6\n,\n \nsqrt\n(,)\n \n<\nsub\n>\n.\n)\n\n\n\n\n\n\n\nExplore the results\n\n\n\n\nsummary(regression results)\n.\n\n\nvcov()\n; variance-covariance matrix.\n\n\naov(formula, data = data.frame)\n.\n\n\nanova(fitted.model.1, fitted.model.2, ...)\n\n\nand many more.\n\n\n\n\nanova, coeficient, coef, deviance, residuals, effects, formula, model, kappa, labels, plot, predict, proj, projection\n\n\nStepwise Regression\n\n\nSelect a suitable model by adding or dropping variables and preserving hierarchies. The best model with the smallest AIC (Akaike\u2019s Information Criterion) is discovered with the search.\n\n\nGeneralized least squares\n\u00b6\n\n\ngls, binomial, logit, probit, log, cloglog, gaussian, identity, log, inverse, gamma, identity, inverse, log, inverse.gaussian, 1/mu^2, identity, inverse, log, poisson, identity, log, sqrt, quasi-likelihood, logit, probit\n\n\n1\nfitted.model \n<-\n glm\n(\nformula\n,\n famili\n=\nfamily.generator\n,\n data \n=\n \ndata.frame\n)\n\n\n\n\n\n\n\nNonlinear least squares (MLE, Mixed, Local, Robust, Additive, Tree-based)\n\u00b6\n\n\nnls\n\n\n\n\nnlm(function)\n\n\n\n\nMaximum likehood (MLE)\n\n\nml\n\n\nWhen errors are not normal.\n\n\n\n\nml(function)\n\n\n\n\nMixed model\n\n\n\n\nnlme\n package.\n\n\n\n\nLocal Approximating Regression\n\n\nNonparametric local regression function.\n\n\n\n\nlrf \n<-\n lowess\n(\nx\n,\n y\n)\n.\n\n\n\n\nRobust regression\n\n\nMASS\n package.\n\n\nAdditive model\n\n\n\n\nacepack\n package.\n\n\nmda\n package.\n\n\ngam\n package.\n\n\nmgcv\n package.\n\n\n\n\nTree-based model\n\n\ndecision, classification tree, random forest<\\sub>\n\n\n\n\nrpart\n package.\n\n\ntree\n package.\n\n\n\n\n12, Graphical Procedures\n\u00b6\n\n\nGraphic, packages\n\u00b6\n\n\n\n\nlattice\n package.\n\n\nggplot2\n package.\n\n\ngrid\n package.\n\n\nggobi\n, \nrgl\n packages; for interactive graphics, 3D, and surfaces.\n\n\nand many more.\n\n\n\n\nBasic plot\n\u00b6\n\n\n\n\nplot()\n.\n\n\nboxplot()\n..\n\n\nlines(x,y)\n; add a line to a basic plot.\n\n\npairs()\n; multivariate, pairwise scatterplot matrix. \n\n\ncoplot(a <sub>b | c)\n; scatter plot of a \nb given c, a factor vector (levels)\n\n\ncoplot(a <sub>v | c + d)\n.\n\n\npie()\n.\n\n\nhist(x)\n; where \nnclass = n\n and \nbreaks = b\n.\n\n\nbarplot()\n; can be horizontal or vertical.\n\n\ndotchart(x, ...)\n; a case of bar chart.\n\n\nand many more with lots of options.\n\n\n\n\nQ-Q plot\n\u00b6\n\n\n\n\nqqnorm(x)\n.\n\n\nqqline(x)\n.\n\n\nqqplot(x, y)\n; comparison.\n\n\n\n\nPictures\n\u00b6\n\n\n\n\nimage(x, y, z, ...)\n; grid of rectangles with colors corresponding to the values in z.\n\n\ncontour(x, y, z, ...)\n; z add contour lines (even to an existing plot).\n\n\npersp(x, y, z, ...)\n; perspective plots of a surface over the x\u2013y plane.\n\n\n\n\nGraphic arguments and parameters\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nx1 \n<-\n rnorm\n(\n1000\n,\n \n0.4\n,\n \n0.8\n)\n\nx2 \n<-\n rnorm\n(\n1000\n,\n \n0.0\n,\n \n1.0\n)\n\nx3 \n<-\n rnorm\n(\n1000\n,\n \n-1.0\n,\n \n1.0\n)\n\nhist\n(\nx1\n,\n width \n=\n \n0.33\n,\n offset \n=\n \n0.00\n,\n col \n=\n \n\"blue\"\n,\n xlim \n=\n \nc\n(\n-4\n,\n4\n),\n\n     main \n=\n \n\"Histogram of x1, x2 & x3\"\n,\n\n     xlab \n=\n \n\"x1 - blue, x2 - red, x3 - green\"\n)\n\nhist\n(\nx2\n,\n width \n=\n \n0.33\n,\n offset \n=\n \n0.33\n,\n col \n=\n \n\"red\"\n,\n add \n=\n \nTRUE\n)\n\nhist\n(\nx3\n,\n width \n=\n \n0.33\n,\n offset \n=\n \n0.66\n,\n col \n=\n \n\"green\"\n,\n add \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n\n\nadd = TRUE\n; superimpose a plot on another plot.\n\n\naxes = FALSE\n; no axes.\n\n\naxis(side,...)\n; 1 to 4, bottom, left, top, right.\n\n\nlog = \"x\", \"y\", \"xy\"\n; difference scale.\n\n\ntype = \"p\"\n (points); \n\"l\"\n (lines), \n\"b\"\n (p+l), \n\"o\"\n (l+p), \n\"h\"\n (vertical lines from points to the zero axis), \n\"s\"\n (step-function), \n\"n\"\n (not plotting).\n\n\nxlab = \"bla\"\n.\n\n\nylab = \"bla\"\n.\n\n\nmain = \"bla\"\n.\n\n\nsub = \"bla\"\n.\n\n\ntitle(main, sub)\n.\n\n\npoints(x, y)\n; add points on top of \nplot()\n.\n\n\ntext(x, y, labels,...)\n; add text to each point.\n\n\nplot(x, t, type = \"n\"); text(x, y, names)\n; replace the dot with text.\n\n\nabline(a, b)\n; add a line.\n\n\nabline(0, 1, lty = 3)\n.\n\n\nabline(coef(fm))\n.\n\n\nabline(coef(fm1), col = \"red\")\n.\n\n\nabline(h = y)\n; or \nh = value\n; add a horizontal line.\n\n\nabline(v = x)\n; or \nv = value\n; add a vertical line.\n\n\nabline(lm.obj)\n.\n\n\nlegend(x, y, legend, ...)\n; add a legend at a specified position.\n\n\nfill = v\n; add a vector of the same length as a legend.\n\n\nlty = 2\n; line style.\n\n\nlwd = 1.5\n; line width.\n\n\npch = 0\n; dot style.\n\n\npar()\n; list of permanent graphic parameters.\n\n\npar(c(\"col\", \"lty\"))\n; limit the list of parameters.\n\n\npar(col = 4, lty = 2)\n; set the parameters for all plots.\n\n\nplot(x, y, pch = \"+\")\n; will set a temporary parameter inside a plot.\n\n\npch = \"+\"\n, \npch = 4\n.\n\n\ncol = \"red\"\n; dot color.\n\n\ncol =  \"red\"\n.\n\n\ncol.axis = \"red\"\n.\n\n\ncol.lab = \"red\"\n.\n\n\ncol.main = \"red\"\n.\n\n\ncol.sub = \"red\"\n.\n\n\nfont = \"red\"\n.\n\n\nfont.axis = \"red\"\n.\n\n\nfont.lab = \"red\"\n.\n\n\nfont.main = \"red\"\n.\n\n\nfont.sub = \"red\"\n.\n\n\nadj = -0.1\n; adjust the text to the plotting position (-1\u2026-0.5\u20260\u20260.5\u20261), from left to right, 0 being the center.\n\n\ncex = 1.5\n; character 50% larger.\n\n\ncex.axis = 1.5\n.\n\n\ncex.lab = 1.5\n.\n\n\ncex.main = 1.5\n.\n\n\ncex.sub = 1.5\n.\n\n\nlab = c(5, 7, 12)\n; the first two numbers are the desired number of tick intervals on the x and y axes respectively. The third number is the desired length of axis labels.\n\n\nlas = 1\n; orientation of axis labels (text and numbers). 0 means always parallel to axis, 1 means always horizontal, and 2 means always perpendicular to the axis.\n\n\nmgp = c(3, 1, 0)\n; position of the axis components.\n\n\ntck = 0.01\n; length of tick marks.\n\n\nxaxs = \"r\"\n; axis style.\n\n\nxaxs = \"i\"\n; inside.\n\n\nmai = c(1, 0.5, 0.5, 0)\n; margins in inches around the plot.\n\n\nmar = c(4, 2, 2, 1)\n; in text lines.\n\n\nand many more.\n\n\n\n\nR allows you to create an n by m array of plots on a single page:\n\n\n\n\nmfcol = c(3.2)\n; 3 rows, 2 columns, 6 plotting areas.\n\n\nmfrow = c(3,2)\n; idem but filled by rows.\n\n\nomi = c(1, 0.5, 0.5, 0)\n; margins between plots.\n\n\noma = c(1, 0.5, 0.5, 0)\n; margins outside plots.\n\n\nmfg = c(2, 2, 3, 2)\n; position of the current figure in a multiple figure environment. The first two numbers are the row and column of the current figure; the last two are the total number of rows and columns in the multiple figure array; in other words, 2\nnd\n row and 2\nnd\n column in a 3x2 panel.\n\n\nfig = c(4, 9, 1, 4) / 10\n; position of the current figure on the page. Values are the positions of the left, right, bottom and top edges, respectively, as a percentage of the page measured from the bottom left corner.\n\n\nand many more.\n\n\n\n\nGeometric shapes\n\u00b6\n\n\n\n\npolygon(x, y, ...)\n; x, y are vectors containing the coordinates of the vertices of the polygon.\n\n\n\n\nGraphic help\n\n\n\n\nhelp(plotmath)\n.\n\n\nexample(plotmath)\n.\n\n\ndemo(plotmath)\n.\n\n\nhelp(Hershey)\n.\n\n\ndemo(Hershey)\n.\n\n\nhelp(Japanese)\n.\n\n\ndemo(Japanese)\n.\n\n\n\n\nGraphic text and mouse\n\u00b6\n\n\nLeave graphic marks and texts.\n\n\n\n\nlocator(n, type)\n; select with mouse a max of n locations to mark on the graph (left click, right click to stop).\n\n\ntext(locator(1), \"Outlier\", adj=0)\n; select with mouse a location to add a string on the graph.\n\n\nidentify(x, y)\n; add a label to a dot with mouse.\n\n\nidentify(x, y, labels)\n.\n\n\nidentify(x, y, \"yes\")\n.\n\n\n\n\nGraphic device\n\u00b6\n\n\n\n\nsplit.screen()\n; FALSE or number of regions within the current device which can, to some extent, be treated as separate graphics devices. It is useful for generating multiple plots on a single device.\n\n\nlayout()\n; divides the device up into as many rows and columns as there are in matrix mat.\n\n\n\n\nGraphic device driver\n\n\nOpen a special graphics window.\n\n\n\n\nX11()\n; under UNIX.\n\n\nwindows()\n; under Windows.\n\n\nwin.printer()\n.\n\n\nwin.metafile()\n.\n\n\n\n\n\n\nquartz()\n; under OS X.\n\n\ndev.new()\n; returns the return value of the device opened, usually invisible NULL.\n\n\ngrid()\n adds an rectangular grid to an existing plot.\n\n\npostscript()\n; starts the graphics device driver for producing PostScript graphics.\n\n\npostscript(\"file.ps\", horizontal=FALSE, height=5, pointsize=10)\n.\n\n\npostscript(\"plot1.eps\", horizontal=FALSE, onefile=FALSE, height=8, width=6, pointsize=10)\n.\n\n\n\n\n\n\npdf()\n; starts the graphics device driver for producing PDF graphics.\n\n\npng()\n; idem.\n\n\njpeg()\n; idem.\n\n\ntiff()\n; idem.\n\n\nbitmap()\n; idem.\n\n\ndev.off()\n; shuts down the specified (by default the current) device.\n\n\ndev.set()\n; dev.set makes the specified device the active device.\n\n\ndev.list()\n; returns the numbers of all open devices.\n\n\ndev.next()\n, \ndev.prev()\n; return the number and name of the next / previous device in the list of devices.\n\n\ngraphics.off(); terminate all devices.\n\n\n\n\n13, Packages\n\u00b6\n\n\n\n\ninstall.package()\n; on the computer.\n\n\nlibrary()\n; load it.\n\n\nsearch()\n; check what is loaded.\n\n\nloadedNamespaces()\n; idem.\n\n\nhelp.start()\n; start the HTML help system, package section.\n\n\n\n\nFind out about packages:\n\n\n\n\nCRAN.R-project.org\n\n\nwww.bioconductor.org\n\n\nwww.omegahat.org\n\n\n\n\n14, OS Facilities\n\u00b6\n\n\nManage files with Linux or Windows or RStudio.",
            "title": "An Introduction to R"
        },
        {
            "location": "/An Introduction to R/#2-simple-manipulations-numbers-and-vectors",
            "text": "",
            "title": "2, Simple Manipulations; Numbers and Vectors"
        },
        {
            "location": "/An Introduction to R/#create-a-vector",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 x  <-   c ( 1 ,   2 )   # assignment (universal). \nx [ 1 ]   1   2  c ( 2 ,   1 )   ->  y  # alternative assignment. \ny [ 1 ]   2   1 \n\nx  =   c ( 1 ,   2 )   # alternative assignment (some limitations). \nx  <<-   c ( 1 ,   2 )   # permanent assignment.  # examples \nab  <-   9 \nab [ 1 ]   9  assign ( \"ab\" ,   10 ) \nab  [ 1 ]   10    Create a sequence (vector)   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30 x  <-   2   *   1 : 15 \n\nx [ 1 ]    2    4    6    8   10   12   14   16   18   20   22   24   26   28   30 \n\nn  <-   10 \nx  <-   1 : n -1 \n\nx [ 1 ]   1   2   3   4   5   6   7   8   9 \n\nx  <-   30 : 1 \nx [ 1 ]   30   29   28   27   26   25   24   23   22   21   20   19   18   17   16   15   14   13   12   11   10    9    8    7    6    5    4    3  [ 29 ]    2    1  seq ( from  =   1 ,  to  =   30 )  [ 1 ]    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28  [ 29 ]   29   30  seq ( 1 ,   30 )  [ 1 ]    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28  [ 29 ]   29   30  seq ( 30 ,   1 )  [ 1 ]   30   29   28   27   26   25   24   23   22   21   20   19   18   17   16   15   14   13   12   11   10    9    8    7    6    5    4    3  [ 29 ]    2    1  seq ( 1 ,   30 ,  by  =   2 )  [ 1 ]    1    3    5    7    9   11   13   15   17   19   21   23   25   27   29",
            "title": "Create a vector"
        },
        {
            "location": "/An Introduction to R/#repetition",
            "text": "1\n2 rep ( 2 ,  times  =   5 )  [ 1 ]   2   2   2   2   2    1\n2\n3\n4 x  <-   c ( \"x\" ,   \"y\" )[ rep ( c ( 1 ,   2 ,   2 ,   1 ),  times  =   4 )] \n\nx [ 1 ]   \"x\"   \"y\"   \"y\"   \"x\"   \"x\"   \"y\"   \"y\"   \"x\"   \"x\"   \"y\"   \"y\"   \"x\"   \"x\"   \"y\"   \"y\"   \"x\"",
            "title": "Repetition"
        },
        {
            "location": "/An Introduction to R/#length-vector",
            "text": "length(vector)",
            "title": "Length (vector)"
        },
        {
            "location": "/An Introduction to R/#boolean",
            "text": "TRUE  or  T ;  T == 1 .  FALSE  or  F ;  F == 0 .   Some operators   <  >=  <  >=  ==  !=  <>  &  &&  |  ||  and many more.",
            "title": "Boolean"
        },
        {
            "location": "/An Introduction to R/#missing-data-and-more",
            "text": "NA; not available.  NaN; not a number.  Inf - Inf == NaN == 0/0; infinite number.   1\n2\n3\n4 x  <-   c ( 1 : 3 ,   NA ) \n\nx [ 1 ]   1   2   3   NA     is.na(var) .  var == NA .  is.na(x) .  !is.na(x) .   1\n2\n3\n4\n5\n6\n7\n8 x  <-   c ( -1 : 3 ,   NA ) \ny  <-  x [ ! is.na ( x )   &  x  >   0 ] \n\nx [ 1 ]    -1    0    1    2    3   NA \n\ny [ 1 ]   1   2   3",
            "title": "Missing data and more"
        },
        {
            "location": "/An Introduction to R/#extract-subset-vector",
            "text": "x[i] ; index.",
            "title": "Extract, subset (vector)"
        },
        {
            "location": "/An Introduction to R/#backslash-use-for-some-characters",
            "text": "\\\\ ; backslash.  \\n ; new line.  \\t ; tab.  \\b ; backspace.",
            "title": "Backslash use for some characters"
        },
        {
            "location": "/An Introduction to R/#concatenate-paste-vector",
            "text": "1\n2\n3\n4 labs  <-   paste ( c ( \"X\" ,   \"Y\" ),   1 : 10 ,  sep  =   \"\" ) \n\nlabs [ 1 ]   \"X1\"    \"Y2\"    \"X3\"    \"Y4\"    \"X5\"    \"Y6\"    \"X7\"    \"Y8\"    \"X9\"    \"Y10\"",
            "title": "Concatenate, paste (vector)"
        },
        {
            "location": "/An Introduction to R/#a-note-of-data-handling-and-manipulations",
            "text": "You can also  split() ,  merge() ,  rbind() ,  cbind()  vectors.   It is also possible with other objects such as factors, lists, arrays, matrices, and data frames.  There are built-in functions to extract, exclude, subset, replace, transform or convert ( .as ), concatenate, paste, group, and bind.  slice, extract, exclude, subset, replace, convert,  concatenate, paste, group, bind",
            "title": "A note of data handling and manipulations"
        },
        {
            "location": "/An Introduction to R/#exclude-remove-vector",
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 z  <-   1 : 20 \n\nz [ 1 ]    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18   19   20 \n\nzz  <-  z [ - ( 1 : 5 )] \n\nzz [ 1 ]    6    7    8    9   10   11   12   13   14   15   16   17   18   19   20",
            "title": "Exclude, remove (vector)"
        },
        {
            "location": "/An Introduction to R/#replace-vector",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 x  <-   c ( 1 ,   1 ,   1 ,   NA ) \n\nx [ 1 ]    1    1    1   NA \n\nx [ is.na ( x )]   <-   0 \n\nx [ 1 ]   1   1   1   0  # replaces any missing values in x by zeros",
            "title": "Replace (vector)"
        },
        {
            "location": "/An Introduction to R/#absolute-value",
            "text": "y  <-   abs ( y ) .   Convert ( .as )   as.vector() .  as.integer() .  as.numeric()  as.factor() .  as.character() .  and many more.",
            "title": "Absolute value"
        },
        {
            "location": "/An Introduction to R/#3-objects-their-modes-and-attributes",
            "text": "",
            "title": "3, Objects, their Modes and Attributes"
        },
        {
            "location": "/An Introduction to R/#object-type",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29 obj  <-   1 \n\nobj1  <-   numeric ( obj )  mode ( obj1 )  [ 1 ]   \"numeric\" \nobj2  <-   character ( obj )  mode ( obj2 )  [ 1 ]   \"character\" \n\nx  <-   1  # class() != mode(), but almost  mode ( x )  [ 1 ]   \"numeric\" \n\nx  <-   factor ( x ) \nx [ 1 ]   1 \nLevels :   1 \n\nx  <-   numeric ( x ) \nx [ 1 ]   0  # load key:value \nobj [ 3 ]   <-   17 \n\nobj [ 1 ]    1   NA   17",
            "title": "Object type"
        },
        {
            "location": "/An Introduction to R/#classes",
            "text": "class   \u201cnumeric\u201d.  \u201clogical\u201d.  \u201ccharacter\u201d.  \u201clist\u201d.  \u201cmatrix\u201d.  \u201carray\u201d.  \u201cfactor\u201d.  \u201cdata.frame\u201d.   1\n2\n3\n4\n5\n6 class ( obj )  [ 1 ]   \"numeric\"  # print the object as ordinary  unclass ( obj )  [ 1 ]    1   NA   17",
            "title": "Classes"
        },
        {
            "location": "/An Introduction to R/#4-ordered-and-unordered-factors",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32 state  <-   c ( \"tas\" , \"qld\" , \"sa\" , \"sa\" , \"sa\" , \"vic\" , \"nt\" , \"act\" , \"qld\" , \"nsw\" , \"wa\" , \"nsw\" , \"nsw\" , \"vic\" , \"vic\" , \"vic\" , \"nsw\" , \"qld\" , \"qld\" , \"vic\" , \"nt\" , \"wa\" , \"wa\" , \"qld\" , \"sa\" ,   \"tas\" , \"nsw\" ,   \"nsw\" ,   \"wa\" , \"act\" ) \n\nstatef  <-   factor ( state )  # class != mode()  class ( statef )  [ 1 ]   \"factor\"  mode ( statef )  [ 1 ]   \"numeric\"  levels ( statef )  [ 1 ]   \"act\"   \"nsw\"   \"nt\"    \"qld\"   \"sa\"    \"tas\"   \"vic\"   \"wa\"  \n\nincomes  <-   c ( 60 ,   49 ,   40 ,   61 ,   64 ,   60 ,   59 ,   54 ,   62 ,   69 ,   70 ,   42 ,   56 ,   61 ,   61 ,   61 ,   58 ,   51 ,   48 ,   65 ,   49 ,   49 ,   41 ,   48 ,   52 ,   46 ,   59 ,   46 ,   58 ,   43 ) \n\nincmeans  <-   tapply ( incomes ,  statef ,   mean ) \n\nincmeans\n  act   nsw    nt   qld    sa   tas   vic    wa  48.50   55.00   54.00   51.60   54.25   53.00   61.60   54.50  \n\nstderr  <-   function ( x )   {   sqrt ( var ( x )   /   length ( x ))   }  stderr ( incomes )  [ 1 ]   1.524462  # alternatively \nincster  <-   tapply ( incomes ,  statef ,   stderr ) \n\nincster\n      act       nsw        nt       qld        sa       tas       vic        wa  5.5000000   3.9665266   5.0000000   2.6570661   5.3909647   7.0000000   0.8717798   6.2249498      1\n2\n3\n4\n5\n6\n7 # a vector of characters \nstateff  <-   c ( \"a\" ,   \"b\" ,   \"c\" )  [ 1 ]   \"a\"   \"b\"   \"c\"  as.factor ( stateff )  [ 1 ]  a b  c \nLevels :  a b  c     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65 # Create a factor with the wrong order of levels \nsizes  <-   factor ( c ( \"small\" ,   \"large\" ,   \"large\" ,   \"small\" ,   \"medium\" )) \n\nsizes [ 1 ]  small  large  large  small  medium\nLevels :  large medium small # levels can be specified explicitly \nsizes  <-   factor ( sizes ,  levels  =   c ( \"small\" ,   \"medium\" ,   \"large\" )) \n\nsizes [ 1 ]  small  large  large  small  medium\nLevels :  small medium large # do the same with an ordered factor \nsizes  <-   ordered ( c ( \"small\" ,   \"large\" ,   \"large\" ,   \"small\" ,   \"medium\" )) \n\nsizes  <-   ordered ( sizes ,  levels  =   c ( \"small\" ,   \"medium\" ,   \"large\" )) \n\nsizes [ 1 ]  small  large  large  small  medium\nLevels :  small  <  medium  <  large # use relevel() to make a particular level first in the list. (This will not work for ordered factors.)  # Create a factor with the wrong order \nsizes [ 1 ]  small  large  large  small  medium\nLevels :  large medium small # make medium first \nsizes  <-  relevel ( sizes ,   \"medium\" ) \n\nsizes [ 1 ]  small  large  large  small  medium\nLevels :  medium large small # make small first \nsizes  <-  relevel ( sizes ,   \"small\" ) \n\nsizes [ 1 ]  small  large  large  small  medium\nLevels :  small medium large # specify the proper order when the factor is created \nsizes  <-   factor ( c ( \"small\" ,   \"large\" ,   \"large\" ,   \"small\" ,   \"medium\" ), \n                  levels  =   c ( \"small\" ,   \"medium\" ,   \"large\" )) \n\nsizes [ 1 ]  small  large  large  small  medium\nLevels :  small medium large # Create a factor with the wrong order of levels \nsizes  <-   factor ( c ( \"small\" ,   \"large\" ,   \"large\" ,   \"small\" ,   \"medium\" )) \n\nsizes [ 1 ]  small  large  large  small  medium\nLevels :  large medium small # reverse the order of levels in a factor \nsizes  <-   factor ( sizes ,  levels = rev ( levels ( sizes ))) \n\nsizes [ 1 ]  small  large  large  small  medium\nLevels :  small medium large",
            "title": "4, Ordered and Unordered Factors"
        },
        {
            "location": "/An Introduction to R/#convert-as",
            "text": "as.factor() .",
            "title": "Convert (.as)"
        },
        {
            "location": "/An Introduction to R/#5-arrays-and-matrices",
            "text": "",
            "title": "5, Arrays and Matrices"
        },
        {
            "location": "/An Introduction to R/#dimension",
            "text": "1\n2\n3\n4 z  <-   1 : 1500  dim ( z )   <-   c ( 3 ,   5 ,   100 )  # gives 100 arrays of 3 lines by 5 columns    Create a matrix, an array   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87 x  <-   array ( 1 : 20 ,  dim = c ( 4 ,   5 )) \n\nx\n      [, 1 ]   [, 2 ]   [, 3 ]   [, 4 ]   [, 5 ]  [ 1 ,]      1      5      9     13     17  [ 2 ,]      2      6     10     14     18  [ 3 ,]      3      7     11     15     19  [ 4 ,]      4      8     12     16     20 \n\nx  <-   array ( 1 : 20 ,  dim = c ( 2 ,   5 ,   2 )) \n\nx ,   ,   1 \n\n      [, 1 ]   [, 2 ]   [, 3 ]   [, 4 ]   [, 5 ]  [ 1 ,]      1      3      5      7      9  [ 2 ,]      2      4      6      8     10  ,   ,   2 \n\n      [, 1 ]   [, 2 ]   [, 3 ]   [, 4 ]   [, 5 ]  [ 1 ,]     11     13     15     17     19  [ 2 ,]     12     14     16     18     20 \n\n\ni  <-   array ( c ( 1 : 3 ,   3 : 1 ),  dim  =   c ( 3 ,   2 )) \n\ni\n      [, 1 ]   [, 2 ]  [ 1 ,]      1      3  [ 2 ,]      2      2  [ 3 ,]      3      1 \n\nj  <-   array ( c ( 1 : 8 ),  dim  =   c ( 2 ,   2 ,   2 )) \n\nj ,   ,   1 \n\n      [, 1 ]   [, 2 ]  [ 1 ,]      1      3  [ 2 ,]      2      4  ,   ,   2 \n\n      [, 1 ]   [, 2 ]  [ 1 ,]      5      7  [ 2 ,]      6      8 \n\n\nk  <-   array ( 1 : 27 ,   c ( 3 ,   3 ,   3 ))  >  k ,   ,   1 \n\n      [, 1 ]   [, 2 ]   [, 3 ]  [ 1 ,]      1      4      7  [ 2 ,]      2      5      8  [ 3 ,]      3      6      9  ,   ,   2 \n\n      [, 1 ]   [, 2 ]   [, 3 ]  [ 1 ,]     10     13     16  [ 2 ,]     11     14     17  [ 3 ,]     12     15     18  ,   ,   3 \n\n      [, 1 ]   [, 2 ]   [, 3 ]  [ 1 ,]     19     22     25  [ 2 ,]     20     23     26  [ 3 ,]     21     24     27 \n\n\na  <-   matrix ( 1 ,   2 ,   2 ) \n\na\n      [, 1 ]   [, 2 ]  [ 1 ,]      1      1  [ 2 ,]      1      1 \n\nb  <-   matrix ( 2 ,   2 ,   2 ) \n\nb\n      [, 1 ]   [, 2 ]  [ 1 ,]      2      2  [ 2 ,]      2      2",
            "title": "Dimension"
        },
        {
            "location": "/An Introduction to R/#extract-subset-matrix",
            "text": "a[2, 1] ; rows, columns.   Extract, subset (array)   a[2, 1, 1] ; rows, columns, matrix.",
            "title": "Extract, subset (matrix)"
        },
        {
            "location": "/An Introduction to R/#cross-product-vs-multiplication",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 a  <-   1 : 5 \nb  <-   seq ( 10 ,   6 ,   -1 ) \n\na [ 1 ]   1   2   3   4   5 \nb [ 1 ]   10    9    8    7    6 \n\na  *  b [ 1 ]   10   18   24   28   30  crossprod ( a ,  b ) \n      [, 1 ]  [ 1 ,]    110 \n\nab  <-  a  %o%  b\nab\n      [, 1 ]   [, 2 ]   [, 3 ]   [, 4 ]   [, 5 ]  [ 1 ,]     10      9      8      7      6  [ 2 ,]     20     18     16     14     12  [ 3 ,]     30     27     24     21     18  [ 4 ,]     40     36     32     28     24  [ 5 ,]     50     45     40     35     30",
            "title": "Cross product vs multiplication"
        },
        {
            "location": "/An Introduction to R/#matrix-operation",
            "text": "A et B are 2x2 matrices:   A * B ; scalar multiplication.  A %*% B ; matrix multiplication  x %*% A %*% y ; matrix multiplication.  crossproduct(A, B) ; cross multiplication.  ab  <-   outer ( A , B , \"*\" ) ;  a * b .  ab  <-   outer ( A , B , \"+\" ) ;  a + b .  ab  <-   outer ( A , B , \"-\" ) ;  a - b .  and many more.",
            "title": "Matrix operation"
        },
        {
            "location": "/An Introduction to R/#diagonal-and-triangle",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42 ab\n      [, 1 ]   [, 2 ]   [, 3 ]   [, 4 ]   [, 5 ]  [ 1 ,]     11     10      9      8      7  [ 2 ,]     12     11     10      9      8  [ 3 ,]     13     12     11     10      9  [ 4 ,]     14     13     12     11     10  [ 5 ,]     15     14     13     12     11  diag ( ab )  [ 1 ]   11   11   11   11   11  lower.tri ( ab ) \n       [, 1 ]    [, 2 ]    [, 3 ]    [, 4 ]    [, 5 ]  [ 1 ,]   FALSE   FALSE   FALSE   FALSE   FALSE  [ 2 ,]    TRUE   FALSE   FALSE   FALSE   FALSE  [ 3 ,]    TRUE    TRUE   FALSE   FALSE   FALSE  [ 4 ,]    TRUE    TRUE    TRUE   FALSE   FALSE  [ 5 ,]    TRUE    TRUE    TRUE    TRUE   FALSE  lower.tri ( ab )   *  ab\n      [, 1 ]   [, 2 ]   [, 3 ]   [, 4 ]   [, 5 ]  [ 1 ,]      0      0      0      0      0  [ 2 ,]     12      0      0      0      0  [ 3 ,]     13     12      0      0      0  [ 4 ,]     14     13     12      0      0  [ 5 ,]     15     14     13     12      0  upper.tri ( ab ) \n       [, 1 ]    [, 2 ]    [, 3 ]    [, 4 ]    [, 5 ]  [ 1 ,]   FALSE    TRUE    TRUE    TRUE    TRUE  [ 2 ,]   FALSE   FALSE    TRUE    TRUE    TRUE  [ 3 ,]   FALSE   FALSE   FALSE    TRUE    TRUE  [ 4 ,]   FALSE   FALSE   FALSE   FALSE    TRUE  [ 5 ,]   FALSE   FALSE   FALSE   FALSE   FALSE  upper.tri ( ab )   *  ab\n      [, 1 ]   [, 2 ]   [, 3 ]   [, 4 ]   [, 5 ]  [ 1 ,]      0     10      9      8      7  [ 2 ,]      0      0     10      9      8  [ 3 ,]      0      0      0     10      9  [ 4 ,]      0      0      0      0     10  [ 5 ,]      0      0      0      0      0",
            "title": "Diagonal and triangle"
        },
        {
            "location": "/An Introduction to R/#solving-a-matrix-system-and-more-matrix-algebra",
            "text": "1\n2\n3\n4 b  <-  A  %*%  x # or  solve ( A ,  b )     solve(A) ; inverse the matrix.   Symmetrical matrix and eigenvalue   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 b  <-   matrix ( 2 ,   2 ,   2 ) \n\nb\n      [, 1 ]   [, 2 ]  [ 1 ,]      2      2  [ 2 ,]      2      2 \n\ne  <-   eigen ( b ,  only.values  =   TRUE ) \ne $ values [ 1 ]   4   0  $ vectors NULL    Singular value decomposition (matrix)   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 svd ( b )  $ d [ 1 ]   4   0  $ u\n            [, 1 ]         [, 2 ]  [ 1 ,]   -0.7071068   -0.7071068  [ 2 ,]   -0.7071068    0.7071068  $ v\n            [, 1 ]         [, 2 ]  [ 1 ,]   -0.7071068   -0.7071068  [ 2 ,]   -0.7071068    0.7071068    Determinant (matrix)  1\n2 det ( ab )  [ 1 ]   0",
            "title": "Solving a matrix system (and more matrix algebra)"
        },
        {
            "location": "/An Introduction to R/#least-square-fitting-matrix",
            "text": "lsfit()  or least squares estimate of  b  in the model:  y = X b + e .   QR decomposition (matrix)   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 qr ( ab )  $ qr \n             [, 1 ]          [, 2 ]            [, 3 ]            [, 4 ]            [, 5 ]  [ 1 ,]   -29.2403830   -27.0174299   -2.479448e+01   -2.257152e+01   -2.034857e+01  [ 2 ,]     0.4103913     0.2418254    4.836508e-01    7.254763e-01    9.673017e-01  [ 3 ,]     0.4445906    -0.1703815    3.717512e-15    1.134003e-14    1.749210e-14  [ 4 ,]     0.4787899    -0.5015812    3.957070e-01    1.553722e-15    1.295516e-15  [ 5 ,]     0.5129892    -0.8327809    5.076995e-01    6.936403e-01   -1.685698e-16  $ rank  [ 1 ]   2  $ qraux [ 1 ]   1.376192e+00   1.160818e+00   1.765282e+00   1.720322e+00   1.685698e-16  $ pivot [ 1 ]   1   2   3   4   5  attr (, \"class\" )  [ 1 ]   \"qr\"    Also:   qr.coef() .  qr.fitted() .  qr.resid() .",
            "title": "Least square fitting (matrix)"
        },
        {
            "location": "/An Introduction to R/#convert-as_1",
            "text": "as.array() .  as.matrix() .",
            "title": "Convert (.as)"
        },
        {
            "location": "/An Introduction to R/#6-lists-and-data-frames",
            "text": "",
            "title": "6, Lists and Data Frames"
        },
        {
            "location": "/An Introduction to R/#create-a-list",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 Lst  <-   list ( name  =   \"Fred\" ,  wife  =   \"Mary\" ,  no.children  =   3 ,  child.ages  =   c ( 4 , 7 , 9 )) \n\nLst $ name [ 1 ]   \"Fred\"  $ wife [ 1 ]   \"Mary\"  $ no.children [ 1 ]   3  $ child.ages [ 1 ]   4   7   9",
            "title": "Create a list"
        },
        {
            "location": "/An Introduction to R/#extract-subset-list",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 Lst $ name [ 1 ]   \"Fred\" \n\nLst [[ \"name\" ]]   ==  Lst [[ 1 ]]  [ 1 ]   TRUE \n\n\nLst [ 5 ]   <-   list ( \"\" )   # or list()   \nLst [ \"new\" ]   <-   list ()    # key, value \n\nLst [ 1 ]  $ name [ 1 ]   \"Fred\" \n\nLst $ child.ages [ 1 ]  [ 1 ]   4 \nLst [[ 4 ]][ 1 ]  [ 1 ]   4",
            "title": "Extract, subset (list)"
        },
        {
            "location": "/An Introduction to R/#concatenate-paste",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52 x  <-   c ( 1 , 2 ) \ny  <-   c ( 3 , 4 )  c ( x ,  y )  [ 1 ]   1   2   3   4  paste ( x ,  y )  [ 1 ]   \"1 3\"   \"2 4\"  data.frame ( x ,  y ) \n  x y 1   1   3  2   2   4 \n\na  <-   list ( 1 ,   2 ) \nb  <-   list ( \"a\" ,   \"b\" )  list ( a ,  b )  [[ 1 ]]  [[ 1 ]][[ 1 ]]  [ 1 ]   1  [[ 1 ]][[ 2 ]]  [ 1 ]   2  [[ 2 ]]  [[ 2 ]][[ 1 ]]  [ 1 ]   \"a\"  [[ 2 ]][[ 2 ]]  [ 1 ]   \"b\" \n\nh  <-   matrix ( 2 ,   2 ,   2 ) \ng  <-   matrix ( 1 ,   2 ,   2 )  c ( h ,  g )  [ 1 ]   2   2   2   2   1   1   1   1  paste ( h ,  g )  [ 1 ]   \"2 1\"   \"2 1\"   \"2 1\"   \"2 1\"  list ( h ,  g )  [[ 1 ]] \n      [, 1 ]   [, 2 ]  [ 1 ,]      2      2  [ 2 ,]      2      2  [[ 2 ]] \n      [, 1 ]   [, 2 ]  [ 1 ,]      1      1  [ 2 ,]      1      1",
            "title": "Concatenate, paste"
        },
        {
            "location": "/An Introduction to R/#convert-as_2",
            "text": "as.matrix()",
            "title": "Convert (as.)"
        },
        {
            "location": "/An Introduction to R/#data-frame",
            "text": "A data frame can hold other data frames.  A list can hold other lists.  A vector can hold other vectors.    Each variable can be numeric, character, logical, factor, numeric matrix, list, data.frame.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29 state  <-   c ( \"tas\" , \"qld\" , \"sa\" , \"sa\" , \"sa\" , \"vic\" , \"nt\" , \"act\" , \"qld\" , \"nsw\" , \"wa\" , \"nsw\" , \"nsw\" , \"vic\" , \"vic\" , \"vic\" , \"nsw\" , \"qld\" , \"qld\" , \"vic\" , \"nt\" , \"wa\" , \"wa\" , \"qld\" , \"sa\" ,   \"tas\" , \"nsw\" ,   \"nsw\" ,   \"wa\" , \"act\" ) \n\nstatef  <-   factor ( state ) \n\nincomes  <-   c ( 60 ,   49 ,   40 ,   61 ,   64 ,   60 ,   59 ,   54 ,   62 ,   69 ,   70 ,   42 ,   56 ,   61 ,   61 ,   61 ,   58 ,   51 ,   48 ,   65 ,   49 ,   49 ,   41 ,   48 ,   52 ,   46 ,   59 ,   46 ,   58 ,   43 ) \n\nincomef  <-   factor ( incomes ) \n\naccountants  <-   data.frame ( home = statef ,  loot = incomes ,  shot = incomef )  head ( accountants ,   10 ) \n   home loot shot 1    tas    60     60  2    qld    49     49  3     sa    40     40  4     sa    61     61  5     sa    64     64  6    vic    60     60  7     nt    59     59  8    act    54     54  9    qld    62     62  10   nsw    69     69  # class() != mode()  class ( accountants )  [ 1 ]   \"data.frame\"  mode ( accountants )  [ 1 ]   \"list\"",
            "title": "Data frame"
        },
        {
            "location": "/An Introduction to R/#concatenate-paste-data-frame",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81 # accountants == acc  c ( accountants ,  acc )  $ home\n  [ 1 ]  tas qld sa  sa  sa  vic nt  act qld nsw wa  nsw nsw [ 14 ]  vic vic vic nsw qld qld vic nt  wa  wa  qld sa  tas [ 27 ]  nsw nsw wa  act\nLevels :  act nsw nt qld sa tas vic wa $ loot\n  [ 1 ]   60   49   40   61   64   60   59   54   62   69   70   42   56   61   61   61   58   51  [ 19 ]   48   65   49   49   41   48   52   46   59   46   58   43  $ shot\n  [ 1 ]   60   49   40   61   64   60   59   54   62   69   70   42   56   61   61   61   58   51  [ 19 ]   48   65   49   49   41   48   52   46   59   46   58   43  20  Levels :   40   41   42   43   46   48   49   51   52   54   56   58   59   ...   70  $ home\n  [ 1 ]  tas qld sa  sa  sa  vic nt  act qld nsw wa  nsw nsw [ 14 ]  vic vic vic nsw qld qld vic nt  wa  wa  qld sa  tas [ 27 ]  nsw nsw wa  act\nLevels :  act nsw nt qld sa tas vic wa $ loot\n  [ 1 ]   60   49   40   61   64   60   59   54   62   69   70   42   56   61   61   61   58   51  [ 19 ]   48   65   49   49   41   48   52   46   59   46   58   43  $ shot\n  [ 1 ]   60   49   40   61   64   60   59   54   62   69   70   42   56   61   61   61   58   51  [ 19 ]   48   65   49   49   41   48   52   46   59   46   58   43  20  Levels :   40   41   42   43   46   48   49   51   52   54   56   58   59   ...   70  paste ( accountants ,  acc )  [ 1 ]   \"c(6, 4, 5, 5, 5, 7, 3, 1, 4, 2, 8, 2, 2, 7, 7, 7, 2, 4, 4, 7, 3, 8, 8, 4, 5, 6, 2, 2, 8, 1) c(6, 4, 5, 5, 5, 7, 3, 1, 4, 2, 8, 2, 2, 7, 7, 7, 2, 4, 4, 7, 3, 8, 8, 4, 5, 6, 2, 2, 8, 1)\"                                                              [ 2 ]   \"c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43) c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)\"  [ 3 ]   \"c(14, 7, 1, 15, 17, 14, 13, 10, 16, 19, 20, 3, 11, 15, 15, 15, 12, 8, 6, 18, 7, 7, 2, 6, 9, 5, 13, 5, 12, 4) c(14, 7, 1, 15, 17, 14, 13, 10, 16, 19, 20, 3, 11, 15, 15, 15, 12, 8, 6, 18, 7, 7, 2, 6, 9, 5, 13, 5, 12, 4)\"                            list ( accountants ,  acc )  [[ 1 ]] \n   home loot shot 1    tas    60     60  2    qld    49     49  3     sa    40     40  4     sa    61     61  5     sa    64     64  6    vic    60     60  7     nt    59     59  8    act    54     54  9    qld    62     62  10   nsw    69     69  ...  [[ 2 ]] \n   home loot shot 1    tas    60     60  2    qld    49     49  3     sa    40     40  4     sa    61     61  5     sa    64     64  6    vic    60     60  7     nt    59     59  8    act    54     54  9    qld    62     62  10   nsw    69     69  ... \n\naccountants  +  acc\n   home loot shot 1      NA    120     NA  2      NA     98     NA  3      NA     80     NA  4      NA    122     NA  5      NA    128     NA  6      NA    120     NA  7      NA    118     NA  8      NA    108     NA  9      NA    124     NA  10     NA    138     NA  ...",
            "title": "Concatenate, paste (data frame)"
        },
        {
            "location": "/An Introduction to R/#convert-as_3",
            "text": "as.data.frame()",
            "title": "Convert (as.)"
        },
        {
            "location": "/An Introduction to R/#load-data-into-r",
            "text": "read.table() ; produce a data frame with inputs.   A note on environment objects   search() ; objects in .GlobalEnv; including packages.  ls() ; list these objects.    attach() ; attach an object to .GlobalEnv.  detach() .",
            "title": "Load data into R"
        },
        {
            "location": "/An Introduction to R/#7-readingwriting-data-fromto-files-inputoutput",
            "text": "A lot more can be found on I/O: depending on the file type, the data format, the desired R object, many commands are available. Depending on a file format, look for the dedicated package on CRAN.  read, reading, write, writing, input, output, i/o  Examples   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 # read from a file \nHousePrice  <-  read.table ( \"house.data\" ,  header  =   TRUE ,  sep  =   \";\" )  # read a 3-variable list \ninput  <-   scan ( \"input.dat\" ,   list ( \"\" ,   0 ,   0 ))    # read a3-variable list \ninput  <-   scan ( \"input.dat\" ,   list ( id = \"\" ,  x  =   0 ,  y  =   0 ))  # make 3 separate vectors \nlabel  <-  input [[ 1 ]] \nx  <-  input [[ 2 ]] \ny  <-  input [[ 3 ]]  # label them \nlabel  <-  inp $ id\nx  <-  inp $ x\ny  <-  inp $ y # read a 5-variable list and transform it into a matrix \nX  <-   matrix ( scan ( \"light.dat\" ,   0 ),  ncol  =   5 ,  byrow  =   TRUE )    Inputing from file types\u2026   .txt.  .csv.  .tsv.  .hdf5.  .bmp.  .jpeg.  .png.  .tiff.  .zip.  .xls, spreadsheet.  databases.  statistical programs.  binary files.  and many more (some are up and coming such as julia files).",
            "title": "7, Reading/Writing Data from/to Files (Input/Output)"
        },
        {
            "location": "/An Introduction to R/#cleaning-parameter",
            "text": "strip.white = TRUE ; remove unnecessary spaces.",
            "title": "Cleaning parameter"
        },
        {
            "location": "/An Introduction to R/#change-the-data-frame-or-matrix-format",
            "text": "stack() ; for example, take a 6-column, 4-variable data frame and stack the 4 variables into one long column.  unstack() ; vice-versa.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 # 6 columns or variables \nStatus Age  V1    V2    V3    V4\n P  23646   45190   50333   55166   56271 \nCC  26174   35535   38227   37911   41184 \nCC  27723   25691   25712   26144   26398 \nCC  27193   30949   29693   29754   30772 \nCC  24370   50542   51966   54341   54273 \nCC  28359   58591   58803   59435   61292 \nCC  25136   45801   45389   47197   47126  ...  # 4 columns, more rows \nStatus Age values ind\nX1  P  23646   45190  V1\nX2 CC  26174   35535  V1\nX3 CC  27723   25691  V1\nX4 CC  27193   30949  V1\nX5 CC  24370   50542  V1\nX6 CC  28359   58591  V1\nX7 CC  25136   45801  V1\nX11 P  23646   50333  V2 ...     reshape() ; stack and unstack variables, according to parameters.   1 reshape ( zz ,  idvar  =   \"id\" ,  timevar  =   \"var\" ,  varying  =   list ( c ( \"V1\" ,   \"V2\" ,   \"V3\" ,   \"V4\" )),  direction  =   \"long\" )     ftable() ; flatten multidimensional matrices (arrays);  read.ftable() .  data() ; list of datasets.  data(package = \"rpart\") ; load package.  edit(input) ; open a mini-spreadsheet.  xnew  <-  edit ( input ) ; open and save it as a new dataset.  and many more.",
            "title": "Change the data frame or matrix format"
        },
        {
            "location": "/An Introduction to R/#encoding",
            "text": "utf-8 for Linux and OS X.  UCS-2LE and UTF-16 for Windows.   1\n2\n3\n4 # examples  readLines ( \"filename.txt\" ,  encoding  =   \"UTF-8\" )  readLines ( \"filename.txt\" ,  encoding  =   \"UCS2LE\" ) \nread.delim ( \"clipboard\" ,  fileEncoding = \"UTF-16\" )",
            "title": "Encoding"
        },
        {
            "location": "/An Introduction to R/#output",
            "text": "write.table() ; many parameters.  write.matrix() .  write.foreign() .",
            "title": "Output"
        },
        {
            "location": "/An Introduction to R/#check-a-file",
            "text": "readLines(\"aab.txt\") .  readLines(\"aab.txt\", 1) .  unlink(\"aab.txt\") ; delete the file on the working directory.",
            "title": "Check a file"
        },
        {
            "location": "/An Introduction to R/#directory-management",
            "text": "getwd() ; get the current working directory.  setwd() ; set the current working directory.  file.show(filepath) .  system.file()",
            "title": "Directory management"
        },
        {
            "location": "/An Introduction to R/#spreadsheet-editor-and-edition-in-r",
            "text": "fix(c) ; edit object c; a vetor, list, data frame, etc. in a mini-spreadsheet.  data.entry(c, mode=NULL, Names=NULL), dataentry(c), de(c, Modes=list(), Names=NULL) ; idem.  vi(file= ) ; invoke a text editor.  emacs(file= ) ; idem.  pico(file= ) ; idem.  xemacs(file= ) ; idem.  xedit(file=) ; idem.",
            "title": "Spreadsheet editor and edition in R"
        },
        {
            "location": "/An Introduction to R/#8-probability-distributions",
            "text": "",
            "title": "8, Probability Distributions"
        },
        {
            "location": "/An Introduction to R/#distribution",
            "text": "beta .  binom .  cauchy .  chisq .  exp .  f .  gamma .  geom .  hyper .  lnorm .  logis .  nbinom .  norm .  pois .  signrank .  t .  unif .  weibull .  wilcox .  and many more.   Prefixes   d ; density.  p ; CDF or cumulative density function.  q ; quantile.  r ; random deviates or simulation or number generation.",
            "title": "Distribution"
        },
        {
            "location": "/An Introduction to R/#distribution-operation",
            "text": "Commands, prefix + distribution, examples:   dbeta(x= ) .  pbinom(q= , lower.tail= , log.p= ) .  qcauchy(p= , lower.tail= , log.p= ) .  rchisq(n or r= ) .  ptukey() ; studentized (t) distribution  qtukey() .  dmultinom() .  ltinomial .  rmultinom() .  etc.",
            "title": "Distribution operation"
        },
        {
            "location": "/An Introduction to R/#descriptive-statistics",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40 # overview  summary ( cars ) \nstr ( cars ) \nboxplot ( cars $ speed ) \nboxplot ( cars $ dist )  # Tukey five-number summaries \nfivenum ( cars $ speed )  # histograms and bar charts \nstem ( cars $ speed ) \nhist ( cars $ speed ) \nbarplot ( cars $ speed ) \ndotchart ( cars $ speed )  # scatter plot \nplot ( cars $ speed ,  cars $ dist ) \nlines ( cars $ speed ,  cars $ dist )  # add a (1-d) representation \nplot ( cars $ speed ,  cars $ dist ) \nrug ( cars $ speed ,  ticksize  =   0.03 ) \nplot ( cars $ speed ,  cars $ dist ) \nrug ( cars $ dist ,  ticksize  =   0.03 )  # normality of the residuals, linearity \nqqnorm ( cars $ speed ) \nqqline ( cars $ speed ) \n\nqqnorm ( cars $ dist ) \nqqline ( cars $ dist ) \n\nqqplot ( cars $ speed ,  cars $ dist )  # normality test \nshapiro.test ( cars $ speed )  # Kolmogorov-Smirnov test on a sample \nks.test ( cars $ speed [ 10 ],   \"pnorm\" ,  mean  =   mean ( cars $ speed ),  sd  =   sqrt ( var ( cars $ speed ))) \nks.test ( cars $ speed [ 40 ],   \"pnorm\" ,  mean  =   mean ( cars $ speed ),  sd  =   sqrt ( var ( cars $ speed )))",
            "title": "Descriptive statistics"
        },
        {
            "location": "/An Introduction to R/#test",
            "text": "t.test(A,B) ; true difference of means is not 0, difference means.  var.test(A,B) ; true ratio of variances is not 1, difference variances.  t.test(A,B, var.equal=TRUE) ; true difference of means is not 0.  wilcox.test(A,B) ; rank sum with continuity correction, continuous distribution.  and many more.",
            "title": "Test"
        },
        {
            "location": "/An Introduction to R/#normality",
            "text": "plot(ecdf(A), do.points=FALSE, verticals=TRUE, xlim=range(A, B))  plot(ecdf(B), do.points=FALSE, verticals=TRUE, add=TRUE)  ks.test(A,B); maximal vertical distance between the two ecdf",
            "title": "Normality"
        },
        {
            "location": "/An Introduction to R/#9-grouping-loops-and-conditional-execution",
            "text": "",
            "title": "9, Grouping, Loops and Conditional Execution"
        },
        {
            "location": "/An Introduction to R/#control-flow",
            "text": "& ; AND.  && ; AND; evaluates left to right, examining only the first element of each vector.  | ; OR.  || ; OR; evaluates left to right, examining only the first element of each vector.  xor() ; elementwise exclusive OR.  isTRUE(x) ; is true if and only if x is a length-one logical vector whose only element is TRUE and which has no attributes.  ifelse(condition, a, b) ; if  condition  is proven true, return  a , or else, return  b .",
            "title": "Control flow"
        },
        {
            "location": "/An Introduction to R/#loops",
            "text": "looping   for(var in seq) expr .  while(condition is true) expr .  repeat expr .  break ; break out of a for, while or repeat loop; control is transferred to the first statement outside the inner-most loop.  next ; halt the processing of the current iteration and advance the looping index.",
            "title": "Loops"
        },
        {
            "location": "/An Introduction to R/#10-writing-you-own-functions-with-examples",
            "text": "Simple custom function  1\n2\n3\n4\n5\n6\n7\n8 # define \nmyfunction  <-   function ( arg1 ,  arg2 ,   ... ){ \nstatements return ( object )  }  # call \nmyfunction ( arg1 ,  arg2 ,   ... )    It is possible to remplace function arguments with variables and objects.  Complex custom function  Involve conditions and loops.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 twosam  <-   function ( y1 ,  y2 )   { \n    n1  <-   length ( y1 );  n2  <-   length ( y2 ) \n    yb1  <-   mean ( y1 ); \n    yb2  <-   mean ( y2 ) \n    s1  <-  var ( y1 ); \n    s2  <-  var ( y2 ) \n    s  <-   (( n1  -   1 )   *  s1  +   ( n2 -1 )   *  s2 )   /   ( n1  +  n2  -   2 ) \n    tst  <-   ( yb1  -  yb2 )   /   sqrt ( s  *   ( 1   /  n1  +   1   /  n2 )) \n    tst }    Define and call on the same line   tstat  <-  twosam ( data $ male ,  data $ female );  tstat   Several commands and functions can be called on the same line with:   command1; command2; function1; function2     Two ways of defining and calling a function   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 multy  <-   function ( x ,  y )   { \n    x  *  y\n     }  # with a binary operator  \"%!%\"   <-   function ( x ,  y )   { \n    x  *  y\n     } \n\nx  <-   2 \ny  <-   2 \n\nmulty ( x ,  y )  [ 1 ]   4 \n\nx  %!%  y [ 1 ]   4    Matrix multiplication operator,  %*% , and the outer product matrix operator,  %o % , are other examples of binary operators.",
            "title": "10, Writing you own Functions (with examples)"
        },
        {
            "location": "/An Introduction to R/#function-in-a-function",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65 # case 1 \narea  <-   function ( f ,  a ,  b ,  eps  =   1.0e-06 ,  lim  =   10 )   { \n    fun1  <-   function ( f ,  a ,  b ,  fa ,  fb ,  a0 ,  eps ,  lim ,  fun )   { \n        d  <-   ( a  +  b ) / 2 \n        h  <-   ( b  -  a ) / 4 \n        fd  <-  f ( d ) \n        a1  <-  h  *   ( fa  +  fd ) \n        a2  <-  h  *   ( fd  +  fb ) \n         if ( abs ( a0  -  a1  -  a2 )   <  eps  ||  lim  ==   0 ) \n             return ( a1  +  a2 ) \n         else   { \n             return ( fun ( f ,  a ,  d ,  fa ,  fd ,  a1 ,  eps ,  lim  -   1 ,  fun )  \n                fun ( f ,  d ,  b ,  fd ,  fb ,  a2 ,  eps ,  lim  -   1 ,  fun )) \n         } \n     } \n    fa  <-  f ( a ) \n    fb  <-  f ( b ) \n    a0  <-   (( fa  +  fb )   *   ( b  -  a )) / 2 \n    fun1 ( f ,  a ,  b ,  fa ,  fb ,  a0 ,  eps ,  lim ,  fun1 )  }  # case 2 \nf  <-   function ( x )   { \n    y  <-   2 * x\n     print ( x ) \n     print ( y ) \n     print ( z )  }  # case 3 \ncube  <-   function ( n )   { \n    sq  <-   function ()  n * n\n    n * sq ()  }  # case 4 \nopen.account  <-   function ( total )   { \n     list ( \n        deposit  =   function ( amount )   { \n             if ( amount  <=   0 ) \n                 stop ( \"Deposits must be positive!\\n\" ) \n            total  <<-  total  +  amount\n             cat ( amount ,   \"deposited. Your balance is\" ,  total ,   \"\\n\\n\" ) \n         }, \n        withdraw  =   function ( amount )   { \n             if ( amount  >  total ) \n                 stop ( \"You don\u2019t have that much money!\\n\" ) \n            total  <<-  total  -  amount\n             cat ( amount ,   \"withdrawn. Your balance is\" ,  total ,   \"\\n\\n\" ) \n         }, \n        balance  =   function ()   { \n             cat ( \"Your balance is\" ,  total ,   \"\\n\\n\" ) \n         } \n     )  } \n\nross  <-  open.account ( 100 ) \nrobert  <-  open.account ( 200 ) \n\nross $ withdraw ( 30 ) \nross $ balance () \nrobert $ balance () \nross $ deposit ( 50 ) \nross $ balance () \nross $ withdraw ( 500 )    Name  Add, modify, and remove (with  names ( x )   <-   NA  or  0 ) names.   names() .  rownames() .  colnames() .  dimnames() .",
            "title": "Function in a function"
        },
        {
            "location": "/An Introduction to R/#customizing-startup",
            "text": "Customize the R environment through a directory initialization file; commands that you want to execute every time R is started under your system.  R will always source the Rprofile.site file first.   On Windows, the file is in C:\\Program Files\\R\\R-n.n.n\\etc. You can also place a .Rprofile file in any directory that you are going to run R from or in the user home directory.   Individual users control over their workspace and allows for different startup procedures in different working directories.  If no .Rprofile file is found in the startup directory, then R looks for a .Rprofile file in the user\u2019s home directory and uses that (if it exists) environment variable R_PROFILE_USER is set. The file it points to is used instead of the .Rprofile files.   Function named .First() in either of the two profile files or in the .RData image has a special status: initialize the environment  Sequence in which files are executed is:   Rprofile.site  the user profile  .RData  .First()  .Last(), if defined, is (normally) executed at the very end of the session.",
            "title": "Customizing startup"
        },
        {
            "location": "/An Introduction to R/#list-function-and-method",
            "text": "methods(class = \"data.frame\") ; list methods associated with the class.  methods(plot) ; list methods specific to the object.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 methods ( class  =   \"data.frame\" ) \n  [ 1 ]   $               $<-             [               [[              [[ <-          \n  [ 6 ]   [ <-            aggregate     anyDuplicated as.data.frame as.list       [ 11 ]  as.matrix     by            cbind         coerce        dim           [ 16 ]  dimnames       dimnames <-     droplevels    duplicated    edit          [ 21 ]  format        formula       head          initialize    is.na         [ 26 ]  Math          merge         na.exclude    na.omit       Ops           [ 31 ]  plot          print         prompt        rbind         row.names     [ 36 ]   row.names <-    rowsum        show          slotsFromS3   split         [ 41 ]   split <-        stack         str           subset        summary       [ 46 ]  Summary       t             tail          transform     unique        [ 51 ]  unstack       within       \nsee  '?methods'   for  accessing help and  source  code\n\nmethods ( plot ) \n  [ 1 ]  plot.acf *            plot.data.frame *     plot.decomposed.ts * \n  [ 4 ]  plot.default        plot.dendrogram *     plot.density *       \n  [ 7 ]  plot.ecdf           plot.factor *         plot.formula *        [ 10 ]  plot.function       plot.hclust *         plot.histogram *      [ 13 ]  plot.HoltWinters *    plot.isoreg *         plot.lm *             [ 16 ]  plot.medpolish *      plot.mlm *            plot.ppr *            [ 19 ]  plot.prcomp *         plot.princomp *       plot.profile.nls *    [ 22 ]  plot.R6 *             plot.raster *         plot.spec *           [ 25 ]  plot.stepfun        plot.stl *            plot.table *          [ 28 ]  plot.ts             plot.tskernel *       plot.TukeyHSD *      \nsee  '?methods'   for  accessing help and  source  code   Difference:   plot() ; a generic method.  plot.  ; a specific method such as  plot.ts()  for example.",
            "title": "List function and method"
        },
        {
            "location": "/An Introduction to R/#11-statistical-models-in-r",
            "text": "",
            "title": "11, Statistical models in R"
        },
        {
            "location": "/An Introduction to R/#regression",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82 y  <-   1 : 10 \nx  <-   1 : 10 \n\na  <-  lm ( y  < sub > x ) \na\nCall : \nlm ( formula  =  y  < sub > x ) \n\nCoefficients :  ( Intercept )             x  \n   1.123e-15      1.000e+00    # no intercept, through the origin \nb  <-  lm ( y  < sub > 0   +  x ) \nb\nCall : \nlm ( formula  =  y  < sub > 0   +  x ) \n\nCoefficients : \nx   1    # no intercept  c   <-  lm ( y  < sub > -1   +  x )  c \nCall : \nlm ( formula  =  y  < sub > -1   +  x ) \n\nCoefficients : \nx   1   \n\n\nd  <-  lm ( y  < sub > x  -   1 ) \nd\nCall : \nlm ( formula  =  y  < sub > x  -   1 ) \n\nCoefficients : \nx   1    # log \ne  <-  lm ( log ( y )   < sub > x ) \ne\nCall : \nlm ( formula  =   log ( y )   < sub > x ) \n\nCoefficients :  ( Intercept )             x  \n      0.2432         0.2304    # quadratic \nf  <-  lm ( y  < sub > 1   +  x  +   I ( x ^ 2 )) \nf\nCall : \nlm ( formula  =  y  < sub > 1   +  x  +   I ( x ^ 2 )) \n\nCoefficients :  ( Intercept )             x        I ( x ^ 2 )   \n   1.123e-15      1.000e+00      5.699e-18    # polynomial \ng  <-  lm ( y  < sub > X  +  poly ( x ,   2 ))  # weighted regression \nfm1  <-  lm ( y  < sub > x ,  data  =  dummy ,  weight  =   1   /  w ^ 2 )  # and more \nnew.model  <-  update ( old.model ,  new.formula ) \n\nfm05  <-  lm ( y  < sub > x1  +  x2  +  x3  +  x4  +  x5 ,  data  =  production ) \n\nfm6  <-  update ( fm06 ,   .   < sub > .   +  x6 ) \nsmf6  <-  update ( fm6 ,   sqrt (,)   < sub > . )    Explore the results   summary(regression results) .  vcov() ; variance-covariance matrix.  aov(formula, data = data.frame) .  anova(fitted.model.1, fitted.model.2, ...)  and many more.   anova, coeficient, coef, deviance, residuals, effects, formula, model, kappa, labels, plot, predict, proj, projection  Stepwise Regression  Select a suitable model by adding or dropping variables and preserving hierarchies. The best model with the smallest AIC (Akaike\u2019s Information Criterion) is discovered with the search.",
            "title": "Regression"
        },
        {
            "location": "/An Introduction to R/#generalized-least-squares",
            "text": "gls, binomial, logit, probit, log, cloglog, gaussian, identity, log, inverse, gamma, identity, inverse, log, inverse.gaussian, 1/mu^2, identity, inverse, log, poisson, identity, log, sqrt, quasi-likelihood, logit, probit  1 fitted.model  <-  glm ( formula ,  famili = family.generator ,  data  =   data.frame )",
            "title": "Generalized least squares"
        },
        {
            "location": "/An Introduction to R/#nonlinear-least-squares-mle-mixed-local-robust-additive-tree-based",
            "text": "nls   nlm(function)   Maximum likehood (MLE)  ml  When errors are not normal.   ml(function)   Mixed model   nlme  package.   Local Approximating Regression  Nonparametric local regression function.   lrf  <-  lowess ( x ,  y ) .   Robust regression  MASS  package.  Additive model   acepack  package.  mda  package.  gam  package.  mgcv  package.   Tree-based model  decision, classification tree, random forest<\\sub>   rpart  package.  tree  package.",
            "title": "Nonlinear least squares (MLE, Mixed, Local, Robust, Additive, Tree-based)"
        },
        {
            "location": "/An Introduction to R/#12-graphical-procedures",
            "text": "",
            "title": "12, Graphical Procedures"
        },
        {
            "location": "/An Introduction to R/#graphic-packages",
            "text": "lattice  package.  ggplot2  package.  grid  package.  ggobi ,  rgl  packages; for interactive graphics, 3D, and surfaces.  and many more.",
            "title": "Graphic, packages"
        },
        {
            "location": "/An Introduction to R/#basic-plot",
            "text": "plot() .  boxplot() ..  lines(x,y) ; add a line to a basic plot.  pairs() ; multivariate, pairwise scatterplot matrix.   coplot(a <sub>b | c) ; scatter plot of a  b given c, a factor vector (levels)  coplot(a <sub>v | c + d) .  pie() .  hist(x) ; where  nclass = n  and  breaks = b .  barplot() ; can be horizontal or vertical.  dotchart(x, ...) ; a case of bar chart.  and many more with lots of options.",
            "title": "Basic plot"
        },
        {
            "location": "/An Introduction to R/#q-q-plot",
            "text": "qqnorm(x) .  qqline(x) .  qqplot(x, y) ; comparison.",
            "title": "Q-Q plot"
        },
        {
            "location": "/An Introduction to R/#pictures",
            "text": "image(x, y, z, ...) ; grid of rectangles with colors corresponding to the values in z.  contour(x, y, z, ...) ; z add contour lines (even to an existing plot).  persp(x, y, z, ...) ; perspective plots of a surface over the x\u2013y plane.",
            "title": "Pictures"
        },
        {
            "location": "/An Introduction to R/#graphic-arguments-and-parameters",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 x1  <-  rnorm ( 1000 ,   0.4 ,   0.8 ) \nx2  <-  rnorm ( 1000 ,   0.0 ,   1.0 ) \nx3  <-  rnorm ( 1000 ,   -1.0 ,   1.0 ) \nhist ( x1 ,  width  =   0.33 ,  offset  =   0.00 ,  col  =   \"blue\" ,  xlim  =   c ( -4 , 4 ), \n     main  =   \"Histogram of x1, x2 & x3\" , \n     xlab  =   \"x1 - blue, x2 - red, x3 - green\" ) \nhist ( x2 ,  width  =   0.33 ,  offset  =   0.33 ,  col  =   \"red\" ,  add  =   TRUE ) \nhist ( x3 ,  width  =   0.33 ,  offset  =   0.66 ,  col  =   \"green\" ,  add  =   TRUE )     add = TRUE ; superimpose a plot on another plot.  axes = FALSE ; no axes.  axis(side,...) ; 1 to 4, bottom, left, top, right.  log = \"x\", \"y\", \"xy\" ; difference scale.  type = \"p\"  (points);  \"l\"  (lines),  \"b\"  (p+l),  \"o\"  (l+p),  \"h\"  (vertical lines from points to the zero axis),  \"s\"  (step-function),  \"n\"  (not plotting).  xlab = \"bla\" .  ylab = \"bla\" .  main = \"bla\" .  sub = \"bla\" .  title(main, sub) .  points(x, y) ; add points on top of  plot() .  text(x, y, labels,...) ; add text to each point.  plot(x, t, type = \"n\"); text(x, y, names) ; replace the dot with text.  abline(a, b) ; add a line.  abline(0, 1, lty = 3) .  abline(coef(fm)) .  abline(coef(fm1), col = \"red\") .  abline(h = y) ; or  h = value ; add a horizontal line.  abline(v = x) ; or  v = value ; add a vertical line.  abline(lm.obj) .  legend(x, y, legend, ...) ; add a legend at a specified position.  fill = v ; add a vector of the same length as a legend.  lty = 2 ; line style.  lwd = 1.5 ; line width.  pch = 0 ; dot style.  par() ; list of permanent graphic parameters.  par(c(\"col\", \"lty\")) ; limit the list of parameters.  par(col = 4, lty = 2) ; set the parameters for all plots.  plot(x, y, pch = \"+\") ; will set a temporary parameter inside a plot.  pch = \"+\" ,  pch = 4 .  col = \"red\" ; dot color.  col =  \"red\" .  col.axis = \"red\" .  col.lab = \"red\" .  col.main = \"red\" .  col.sub = \"red\" .  font = \"red\" .  font.axis = \"red\" .  font.lab = \"red\" .  font.main = \"red\" .  font.sub = \"red\" .  adj = -0.1 ; adjust the text to the plotting position (-1\u2026-0.5\u20260\u20260.5\u20261), from left to right, 0 being the center.  cex = 1.5 ; character 50% larger.  cex.axis = 1.5 .  cex.lab = 1.5 .  cex.main = 1.5 .  cex.sub = 1.5 .  lab = c(5, 7, 12) ; the first two numbers are the desired number of tick intervals on the x and y axes respectively. The third number is the desired length of axis labels.  las = 1 ; orientation of axis labels (text and numbers). 0 means always parallel to axis, 1 means always horizontal, and 2 means always perpendicular to the axis.  mgp = c(3, 1, 0) ; position of the axis components.  tck = 0.01 ; length of tick marks.  xaxs = \"r\" ; axis style.  xaxs = \"i\" ; inside.  mai = c(1, 0.5, 0.5, 0) ; margins in inches around the plot.  mar = c(4, 2, 2, 1) ; in text lines.  and many more.   R allows you to create an n by m array of plots on a single page:   mfcol = c(3.2) ; 3 rows, 2 columns, 6 plotting areas.  mfrow = c(3,2) ; idem but filled by rows.  omi = c(1, 0.5, 0.5, 0) ; margins between plots.  oma = c(1, 0.5, 0.5, 0) ; margins outside plots.  mfg = c(2, 2, 3, 2) ; position of the current figure in a multiple figure environment. The first two numbers are the row and column of the current figure; the last two are the total number of rows and columns in the multiple figure array; in other words, 2 nd  row and 2 nd  column in a 3x2 panel.  fig = c(4, 9, 1, 4) / 10 ; position of the current figure on the page. Values are the positions of the left, right, bottom and top edges, respectively, as a percentage of the page measured from the bottom left corner.  and many more.",
            "title": "Graphic arguments and parameters"
        },
        {
            "location": "/An Introduction to R/#geometric-shapes",
            "text": "polygon(x, y, ...) ; x, y are vectors containing the coordinates of the vertices of the polygon.   Graphic help   help(plotmath) .  example(plotmath) .  demo(plotmath) .  help(Hershey) .  demo(Hershey) .  help(Japanese) .  demo(Japanese) .",
            "title": "Geometric shapes"
        },
        {
            "location": "/An Introduction to R/#graphic-text-and-mouse",
            "text": "Leave graphic marks and texts.   locator(n, type) ; select with mouse a max of n locations to mark on the graph (left click, right click to stop).  text(locator(1), \"Outlier\", adj=0) ; select with mouse a location to add a string on the graph.  identify(x, y) ; add a label to a dot with mouse.  identify(x, y, labels) .  identify(x, y, \"yes\") .",
            "title": "Graphic text and mouse"
        },
        {
            "location": "/An Introduction to R/#graphic-device",
            "text": "split.screen() ; FALSE or number of regions within the current device which can, to some extent, be treated as separate graphics devices. It is useful for generating multiple plots on a single device.  layout() ; divides the device up into as many rows and columns as there are in matrix mat.   Graphic device driver  Open a special graphics window.   X11() ; under UNIX.  windows() ; under Windows.  win.printer() .  win.metafile() .    quartz() ; under OS X.  dev.new() ; returns the return value of the device opened, usually invisible NULL.  grid()  adds an rectangular grid to an existing plot.  postscript() ; starts the graphics device driver for producing PostScript graphics.  postscript(\"file.ps\", horizontal=FALSE, height=5, pointsize=10) .  postscript(\"plot1.eps\", horizontal=FALSE, onefile=FALSE, height=8, width=6, pointsize=10) .    pdf() ; starts the graphics device driver for producing PDF graphics.  png() ; idem.  jpeg() ; idem.  tiff() ; idem.  bitmap() ; idem.  dev.off() ; shuts down the specified (by default the current) device.  dev.set() ; dev.set makes the specified device the active device.  dev.list() ; returns the numbers of all open devices.  dev.next() ,  dev.prev() ; return the number and name of the next / previous device in the list of devices.  graphics.off(); terminate all devices.",
            "title": "Graphic device"
        },
        {
            "location": "/An Introduction to R/#13-packages",
            "text": "install.package() ; on the computer.  library() ; load it.  search() ; check what is loaded.  loadedNamespaces() ; idem.  help.start() ; start the HTML help system, package section.   Find out about packages:   CRAN.R-project.org  www.bioconductor.org  www.omegahat.org",
            "title": "13, Packages"
        },
        {
            "location": "/An Introduction to R/#14-os-facilities",
            "text": "Manage files with Linux or Windows or RStudio.",
            "title": "14, OS Facilities"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/",
            "text": "Foreword\n\n\nNotes, leads, and ideas on what R can do. \nREF: reference(s) to the book.\n From Springer, 2014.\n\n\n\n\nA, Presentation\n\u00b6\n\n\nGUI\n\u00b6\n\n\n\n\n\n\nRcommander; \nRcmdr\n package.\n\n\n\n\n\n\nThe R Commander: A Basic-Statistics GUI for R\n\n\n\n\n\n\nREF: p.3-5\n\n\nB, Datasets\n\u00b6\n\n\n\n\nDatasets.\n\n\n\n\nPART 1, BASICS\n\u00b6\n\n\nChapter 1, Basic Concept, Organizing Data\n\u00b6\n\n\nEditors\n\u00b6\n\n\n\n\nRStudio.\n\n\nTinn-R.\n\n\nJGR.\n\n\nEmacs/ESS.\n\n\n\n\nData entry\n\u00b6\n\n\n\n\nx \n<-\n \n2\n.\n\n\n2 -> 2\n.\n\n\n\n\nExponent\n\u00b6\n\n\n\n\nexp(1)\n.\n\n\n\n\nLogarithm\n\u00b6\n\n\n\n\nlog(3)\n.\n\n\nlog(x = 3)\n.\n\n\nlog(x = 3, base(exp(1))\n.\n\n\nlog(3, exp(1))\n.\n\n\n\n\nFactorial\n\u00b6\n\n\n\n\nfactorial(2)\n.\n\n\n\n\nFind out about an object, a variable (\nis.\n)\n\u00b6\n\n\n\n\nis.character()\n.\n\n\nis.vector()\n.\n\n\nis.character()\n.\n\n\nis.character()\n\n\nand many more.\n\n\n\n\nConvert (\nas.\n)\n\u00b6\n\n\n\n\nas.character()\n.\n\n\nas.raw()\n.\n\n\nas.date()\n.\n\n\nand many more.\n\n\n\n\nArray and matrix\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nmatrix\n(\n1\n:\n12\n,\n nrow \n=\n \n4\n,\n ncol \n=\n \n3\n,\n byrow \n=\n \nFALSE\n)\n\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n\n\n[\n1\n,]\n    \n1\n    \n5\n    \n9\n\n\n[\n2\n,]\n    \n2\n    \n6\n   \n10\n\n\n[\n3\n,]\n    \n3\n    \n7\n   \n11\n\n\n[\n4\n,]\n    \n4\n    \n8\n   \n12\n\n\n\nmatrix\n(\n1\n:\n12\n,\n nrow \n=\n \n4\n,\n ncol \n=\n \n3\n,\n byrow \n=\n \nTRUE\n)\n\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n\n\n[\n1\n,]\n    \n1\n    \n2\n    \n3\n\n\n[\n2\n,]\n    \n4\n    \n5\n    \n6\n\n\n[\n3\n,]\n    \n7\n    \n8\n    \n9\n\n\n[\n4\n,]\n   \n10\n   \n11\n   \n12\n\n\n\narray\n(\n1\n:\n12\n,\n dim \n=\n \nc\n(\n2\n,\n \n2\n,\n \n3\n))\n\n\n,\n \n,\n \n1\n\n\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n1\n    \n3\n\n\n[\n2\n,]\n    \n2\n    \n4\n\n\n\n,\n \n,\n \n2\n\n\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n5\n    \n7\n\n\n[\n2\n,]\n    \n6\n    \n8\n\n\n\n,\n \n,\n \n3\n\n\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n9\n   \n11\n\n\n[\n2\n,]\n   \n10\n   \n12\n\n\n\n\n\n\n\nVector\n\u00b6\n\n\n\n\nvec \n<-\n \nc\n(\n1.1\n,\n \n2.2\n,\n \n3.5\n)\n.\n\n\nvec \n<-\n \n1\n:\n3\n.\n\n\n\n\nSequence\n\u00b6\n\n\n\n\nseq(1:3)\n.\n\n\n1\n:\n3\n.\n\n\n\n\nList\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nc\n(\n1\n:\n3\n)\n \n# vector\n\n\n[\n1\n]\n \n1\n \n2\n \n3\n\n\n\n# vs\n\n\n\nlist\n(\n1\n:\n3\n)\n \n# list\n\n\n[[\n1\n]]\n\n\n[\n1\n]\n \n1\n \n2\n \n3\n\n\n\n\n\n\n\nData Frame\n\u00b6\n\n\ntable, tabular\n\n\n\n\ndata.frame(name = c(), name = c(), name = c(), etc)\n; each column is a vector with a name.\n\n\n\n\nTime series\n\u00b6\n\n\n1\n2\n3\n4\n5\n>\n ts\n(\n1\n:\n10\n,\n frequency\n=\n4\n,\n start\n=\nc\n(\n1959\n,\n2\n))\n\n     Qtr1 Qtr2 Qtr3 Qtr4\n\n1959\n         \n1\n    \n2\n    \n3\n\n\n1960\n    \n4\n    \n5\n    \n6\n    \n7\n\n\n1961\n    \n8\n    \n9\n   \n10\n     \n\n\n\n\n\n\nClass and mode\n\u00b6\n\n\ntype, data, variable, object\n\n\n\n\nmode()\n.\n\n\nclass\n()\n; mode != class.\n\n\ntypeof()\n; type of storage.\n\n\n\n\nChapter 2, Import-Export and Producing Data\n\u00b6\n\n\nimport, export, i/o\n\n\nInput data from files\n\u00b6\n\n\n1\nread.table\n(\nfile \n=\n path\n/\nfile.txt\n,\n header \n=\n \nTRUE\n,\n sep\n=\n \n\"\\t\"\n,\n dec\n=\n\".\"\n,\n row.names \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\nattach(data)\n; dataset is attached to .GlobalEnv.\n\n\nsearch()\n; search objects in .GlobalEnv, including attached dataset,\n\n\ndetach(data)\n.\n\n\n\n\nMore about .GlobalEnv in Chapter 7, Session Management.\n\n\nRead .csv and .tsv\n\n\n\n\nread.csv()\n.\n\n\nread.csv2()\n.\n\n\nread.delim()\n.\n\n\nread.delim2()\n.\n\n\nand many more.\n\n\n\n\nRead text files\n\n\n\n\nread.ftable(\"file.txt\", row.var.names = c(...), col.vars = list())\n.\n\n\nscan(\"file.txt\", skip = 9, nlines = 1, what = \"\", dec = \"\")\n.\n\n\nand many more.\n\n\n\n\nRead software files\n\n\n\n\n.sav\n; SPSS.\n\n\nread.spss\n.\n\n\n\n\n\n\n.mtp\n; Minitab.\n\n\nread.mtp\n.\n\n\n\n\n\n\n.xpt\n; SAS en data.frame.\n\n\nread.xport\n.\n\n\n\n\n\n\n.mat\n; Matlab.\n\n\nreadMat()\n.\n\n\n\n\n\n\nand many other formats and commands.\n\n\n\n\nOuput data and export\n\u00b6\n\n\n\n\nlookup.xport()\n; for SAS.\n\n\nwrite.table(data, file=\"file.txt\", sep=\"\\t\")\n.\n\n\nxlsReadWrite()\n.\n\n\nand many more.\n\n\n\n\nMeasure computation time\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# start the timer\n\ntmps \n<-\n \nSys.time\n()\n\n\n\n# run\n\ndbsnp \n<-\n read.table\n(\n\"file\"\n)\n\n\n\n# stop the timer\n\n\nSys.time\n()\n \n-\n tmps\n\n\n\n\n\n\nProduce by repetition\n\u00b6\n\n\nrepeat\n\n\n1\n2\nrep\n(\n1\n:\n4\n,\n reach \n=\n \n2\n,\n len \n=\n \n10\n)\n\n\n[\n1\n]\n \n1\n \n2\n \n3\n \n4\n \n1\n \n2\n \n3\n \n4\n \n1\n \n2\n\n\n\n\n\n\n\nProduce random numbers\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n# generate random numbers between 0 and 1\n\nrunif\n(\n5\n)\n\n\n[\n1\n]\n \n0.2424283\n \n0.6140730\n \n0.4824881\n \n0.7263319\n \n0.1381030\n\n\nrunif\n(\n5\n,\n min \n=\n \n2\n,\n max \n=\n \n7\n)\n\n\n[\n1\n]\n \n4.588744\n \n5.522278\n \n4.307162\n \n6.248397\n \n3.854982\n\n\n\n\n\n\n\nMore on random number in Chapter 10, Random Variables, Laws, and Simulation.\n\n\nProduce random number following a distribution\n\n\n1\n2\n3\n# generate random numbers following the normal distribution\n\nrnorm\n(\n5\n)\n\n\n[\n1\n]\n  \n0.8752170\n  \n1.3869022\n \n-0.4419174\n \n-0.6129075\n \n-1.6987139\n\n\n\n\n\n\n\nGenerate numbers with other distributions.\n\n\nProduce random number by sampling a population\n\n\n1\n2\n3\n4\n5\nurne \n<-\n \n0\n:\n9\n\n\n\n# 20 draws from 'urne'\n\n\nsample\n(\nurne\n,\n \n20\n,\n replace \n=\n \nTRUE\n)\n\n\n[\n1\n]\n \n5\n \n4\n \n2\n \n2\n \n9\n \n7\n \n4\n \n6\n \n2\n \n2\n \n7\n \n8\n \n3\n \n3\n \n9\n \n6\n \n6\n \n1\n \n1\n \n0\n\n\n\n\n\n\n\nProduce data by manual input (vector-like)\n\u00b6\n\n\n\n\nFirst,  \nz \n<-\n \nscan\n()\n.\n\n\nSecond, input data in the prompt.\n\n\n\n\nProduce data with a mini-spreadsheet (tabular-like)\n\u00b6\n\n\n\n\nx \n<-\n \nas.data.frame\n(\nde\n(\n\"\"\n))\n; open a spreadsheet.\n\n\ndata.entry(\"\")\n; alternatively.\n\n\nInput data; column are variables like in a data frame.\n\n\n\n\n\n\nfix(x)\n; invokes edit on x, then assigns the new (edited) version of x to the user\u2019s workspace.\n\n\n\n\nList of objects\n\u00b6\n\n\n\n\nls()\n; list of objects.\n\n\nrm(list = ls())\n; remove all the objects.\n\n\n\n\nRead from and write to a database\n\u00b6\n\n\n\n\nRODBC\n package.\n\n\nodbcConnect()\n.\n\n\nsqlQuery()\n.\n\n\nodbcClose()\n.\n\n\n\n\nREF: p.82-84\n\n\nFile management\n\u00b6\n\n\n\n\nfile.choose()\n; open a window.\n\n\n\n\nRead from the clipboard\n\u00b6\n\n\n\n\nFirst, copy from a spreadsheet or a table.\n\n\nSecond, \nread.clipboard()\n.\n\n\n\n\nChapter 3, Data Manipulation\n\u00b6\n\n\nArithmetics\n\u00b6\n\n\n1\n2\n3\n4\n5\nx \n<-\n \nc\n(\n1\n,\n2\n,\n3\n)\n\ny \n<-\n \nc\n(\n4\n,\n5\n,\n6\n)\n\n\nx \n+\n y\n\n[\n1\n]\n \n5\n \n7\n \n9\n\n\n\n\n\n\n\nBuilt-in functions\n\u00b6\n\n\n\n\nlength(vec)\n; length.\n\n\nsort(vec, decreasing = TRUE)\n; sort.\n\n\nrev(vec)\n; inverse sorting.\n\n\norder(vec)\n; sort a vector according to the names or a list of strings.\n\n\nnames\n(\nvec\n)\n \n<-\n \n1\n:\n9\n; attribute names.\n\n\nrank(vec)\n; rank the elements.\n\n\nunique(vec)\n; remove doubles.\n\n\nduplicated(vec)\n; create a TRUE/FALSE vector indicating doubles.\n\n\nx %% y\n;  modulus (x mod y).\n\n\nx %/% y\n; integer division.\n\n\n\n\nDimension functions\n\u00b6\n\n\nnumber, row, column, dimension\n\n\n\n\ndim(df)\n.\n\n\nnrow(df)\n.\n\n\nncol(df)\n.\n\n\n\n\nName functions\n\u00b6\n\n\n\n\ndimnames(df)\n.\n\n\nnames(df)\n, \ncolnames(df)\n.\n\n\nrownames(df)\n.\n\n\n\n\nMerge functions\n\u00b6\n\n\ncombine\n\n\n\n\ncbind()\n.\n\n\nrbind()\n.\n\n\n\n\nREF: p.98\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\ny \n<-\n \narray\n(\n1\n:\n12\n,\n dim \n=\n \nc\n(\n4\n,\n \n3\n))\n\n\ny\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n\n\n[\n1\n,]\n    \n1\n    \n5\n    \n9\n\n\n[\n2\n,]\n    \n2\n    \n6\n   \n10\n\n\n[\n3\n,]\n    \n3\n    \n7\n   \n11\n\n\n[\n4\n,]\n    \n4\n    \n8\n   \n12\n\n\ny \n<-\n \ncbind\n(\ny\n,\n \nc\n(\n100\n,\n \n101\n,\n \n102\n,\n \n103\n))\n\n\ny\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n \n[,\n4\n]\n\n\n[\n1\n,]\n    \n1\n    \n5\n    \n9\n  \n100\n\n\n[\n2\n,]\n    \n2\n    \n6\n   \n10\n  \n101\n\n\n[\n3\n,]\n    \n3\n    \n7\n   \n11\n  \n102\n\n\n[\n4\n,]\n    \n4\n    \n8\n   \n12\n  \n103\n\n\n\nmerge\n(\nx\n,\n y\n)\n\n  V1 V2 V3  V4\n\n1\n  \n1\n  \n5\n  \n9\n \n100\n\n\n2\n  \n2\n  \n6\n \n10\n \n101\n\n\n3\n  \n3\n  \n7\n \n11\n \n102\n\n\n4\n  \n4\n  \n8\n \n12\n \n103\n\n\n\n# idem with rows\n\n\n\n\n\n\n\nREF: p.96-98\n\n\n\n\ngtools\n package.\n\n\nsmartbind(x,y)\n; for two data frames, similar to merge.\n\n\n\n\napply\n functions and family\n\u00b6\n\n\nexcel, wrangle\n\n\nAmong the most useful function for \u2018wrangling\u2019 data. Excel-like power. When and how to use them.\n\n\n\n\napply()\n.\n\n\nlapply()\n.\n\n\nsapply()\n.\n\n\nmapply()\n.\n\n\nby()\n.\n\n\nwith()\n.\n\n\nreplicate()\n.\n\n\ntransform()\n.\n\n\nrowSums(df)\n.\n\n\ncolSums(df)\n.\n\n\nrowMeans(df)\n.\n\n\ncolMeans(df)\n.\n\n\nsweep()\n.\n\n\nstack()\n.\n\n\nunstack()\n.\n\n\naggregate()\n\n\n\n\nREF: p.99\n\n\nSweep functions\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nu\n  V1 V2 V3  V4\n\n1\n  \n1\n  \n5\n  \n9\n \n100\n\n\n2\n  \n2\n  \n6\n \n10\n \n101\n\n\n3\n  \n3\n  \n7\n \n11\n \n102\n\n\n4\n  \n4\n  \n8\n \n12\n \n103\n\n\n\n# removes pattern '3, 5, 3, 5, etc.'\n\n\nsweep\n(\nu\n,\n MARGIN \n=\n \n1\n,\n STATS \n=\n \nc\n(\n3\n,\n \n5\n),\n FUN \n=\n \n\"-\"\n)\n\n  V1 V2 V3 V4\n\n1\n \n-2\n  \n2\n  \n6\n \n97\n\n\n2\n \n-3\n  \n1\n  \n5\n \n96\n\n\n3\n  \n0\n  \n4\n  \n8\n \n99\n\n\n4\n \n-1\n  \n3\n  \n7\n \n98\n\n\n\n# divide by a vector\n\n\nsweep\n(\nu\n,\n MARGIN \n=\n \n2\n,\n STATS \n=\n \nc\n(\n2\n,\n \n2\n,\n \n3\n,\n \n3\n),\n FUN \n=\n \n\"/\"\n)\n\n   V1  V2       V3       V4\n\n1\n \n0.5\n \n2.5\n \n3.000000\n \n33.33333\n\n\n2\n \n1.0\n \n3.0\n \n3.333333\n \n33.66667\n\n\n3\n \n1.5\n \n3.5\n \n3.666667\n \n34.00000\n\n\n4\n \n2.0\n \n4.0\n \n4.000000\n \n34.33333\n\n\n\n\n\n\n\nREF: p.100-101\n\n\nStack functions\n\u00b6\n\n\nstack, unstack\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\nu\n  V1 V2 V3  V4\n\n1\n  \n1\n  \n5\n  \n9\n \n100\n\n\n2\n  \n2\n  \n6\n \n10\n \n101\n\n\n3\n  \n3\n  \n7\n \n11\n \n102\n\n\n4\n  \n4\n  \n8\n \n12\n \n103\n\n\nv\n<-\n stack\n(\nu\n)\n\nv\n   values ind\n\n1\n       \n1\n  V1\n\n2\n       \n2\n  V1\n\n3\n       \n3\n  V1\n\n4\n       \n4\n  V1\n\n5\n       \n5\n  V2\n\n6\n       \n6\n  V2\n\n7\n       \n7\n  V2\n\n8\n       \n8\n  V2\n\n9\n       \n9\n  V3\n\n10\n     \n10\n  V3\n\n11\n     \n11\n  V3\n\n12\n     \n12\n  V3\n\n13\n    \n100\n  V4\n\n14\n    \n101\n  V4\n\n15\n    \n102\n  V4\n\n16\n    \n103\n  V4\n\nw \n<-\n unstack\n(\nv\n)\n\nw\n  V1 V2 V3  V4\n\n1\n  \n1\n  \n5\n  \n9\n \n100\n\n\n2\n  \n2\n  \n6\n \n10\n \n101\n\n\n3\n  \n3\n  \n7\n \n11\n \n102\n\n\n4\n  \n4\n  \n8\n \n12\n \n103\n\n\n\n\n\n\n\nAggregation functions\n\u00b6\n\n\naggregate\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nw\n  V1 V2 V3  V4\n\n1\n  \n1\n  \n5\n  \n9\n \n100\n\n\n2\n  \n2\n  \n6\n \n10\n \n101\n\n\n3\n  \n3\n  \n7\n \n11\n \n102\n\n\n4\n  \n4\n  \n8\n \n12\n \n103\n\n\nfac \n<-\n \nc\n(\n\"a\"\n,\n \n\"b\"\n,\n \n\"b\"\n,\n \n\"a\"\n)\n\nx \n<-\n \ncbind\n(\nw\n,\n fac\n)\n\nx\n  V1 V2 V3  V4 fac\n\n1\n  \n1\n  \n5\n  \n9\n \n100\n   a\n\n2\n  \n2\n  \n6\n \n10\n \n101\n   b\n\n3\n  \n3\n  \n7\n \n11\n \n102\n   b\n\n4\n  \n4\n  \n8\n \n12\n \n103\n   a\n\naggregate\n(\nw\n,\n by \n=\n \nlist\n(\nx\n$\nfac\n),\n \nsum\n)\n\n  Group.1 V1 V2 V3  V4\n\n1\n       a  \n5\n \n13\n \n21\n \n203\n\n\n2\n       b  \n5\n \n13\n \n21\n \n203\n\n\n\n\n\n\n\nREF: p.101\n\n\nBoolean & logical functions\n\u00b6\n\n\n\n\nlogical(2)\n; generate two FALSE in a vector; change the length.\n\n\n!logical(2)\n; generate two TRUE.\n\n\nas.logical(vec)\n.\n\n\nis.logical(vec)\n.\n\n\nisTRUE()\n.\n\n\n&\n; AND.\n\n\n&&\n; sequential AND.\n\n\n|\n; OR.\n\n\n||\n; sequential OR.\n\n\nPrefer \n&&\n over \n&\n, and \n||\n over \n|\n. Assessments go from left to right, and keep on going as long as the conditions are TRUE.\n\n\nxor()\n; exclusive OR.\n\n\nif\n, \nelse\n.\n\n\nany()\n; if one or another is TRUE.\n\n\nall()\n; if all are TRUE.\n\n\nidentical()\n; if all are identical.\n\n\nall.equal()\n.\n\n\n==\n or \nall.equal\n (and \n!=\n) can yield a FALSE because decimal are different on large numbers.\n\n\nall.equal(x, y, tolerance = 10^-6)\n fixes the problem.\n\n\n\n\n\n\nifelse(cond, a, b)\n; if \ncond\n is TRUE, \na\n, else, \nb\n.\n\n\nx\n \n<-\n \nc\n(\n3\n:-\n2\n);\n \nsqrt\n(\nifelse\n(\nx\n \n>=\n \n0\n,\n \nx\n,\n \nNA\n)\n.\n\n\n\n\n\n\n\n\nREF: p.126-127\n\n\nVenn functions\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nA \n<-\n \n1\n:\n3\n\nB \n<-\n \n3\n:\n6\n\n\n\nis.element\n(\n1\n,\n A\n)\n\n\n[\n1\n]\n \nTRUE\n\n\nis.element\n(\n4\n,\n A\n)\n\n\n[\n1\n]\n \nFALSE\n\n\nis.element\n(\n4\n,\n B\n)\n\n\n[\n1\n]\n \nTRUE\n\n\n\nall\n(\nA \n%in%\n B\n)\n\n\n[\n1\n]\n \nFALSE\n\n\nall\n(\nB \n%in%\n A\n)\n\n\n[\n1\n]\n \nFALSE\n\n\n\nintersect\n(\nA\n,\n B\n)\n\n\n[\n1\n]\n \n3\n\n\nunion\n(\nA\n,\n B\n)\n\n\n[\n1\n]\n \n1\n \n2\n \n3\n \n4\n \n5\n \n6\n\n\n\nsetdiff\n(\nA\n,\n B\n)\n\n\n[\n1\n]\n \n1\n \n2\n\n\nsetdiff\n(\nB\n,\n A\n)\n\n\n[\n1\n]\n \n4\n \n5\n \n6\n\n\nintersect\n(\nA\n,\n B\n)\n\n\n[\n1\n]\n \n3\n\n\n\n\n\n\n\nVector functions\n\u00b6\n\n\n\n\nvec[2]\n; extract.\n\n\nvec[2:5]\n; extract.\n\n\nvec[c(T, F, T)]\n; extraction with filter.\n\n\nvec[vec > 4]\n; conditional extraction.\n\n\nvec[vec == 3]\n.\n\n\nvec[which.max(z)]\n; extract the maximum value.\n\n\nvec[which.min(z)]\n.\n\n\nvec > 4\n; yield a vector of TRUE or FALSE.\n\n\nvec[-2]\n; exclude.\n\n\nvec[-c(1,5)]\n; exclude.\n\n\n\n\nSearch functions\n\u00b6\n\n\n\n\nmasque \n<-\n \nc\n(\nTRUE\n,\n \nFALSE\n)\n.\n\n\nwhich(masque)\n; return the TRUE indices.\n\n\nwhich.min(x)\n; return the index with minimum value.\n\n\nwhich.max(x)\n.\n\n\n\n\nReplace functions\n\u00b6\n\n\n\n\nz\n[\nc\n(\n1\n,\n \n5\n)]\n \n<-\n \n1\n; replace value 1 and 5 by 1.\n\n\nz\n[\nwhich.max\n(\nz\n)]\n \n<-\n \n0\n; replace the maximum value.\n\n\nz\n[\nz \n==\n \n0\n]\n \n<-\n \n8\n; replace zeros and FALSE.\n\n\n\n\nExtend a vector\n\u00b6\n\n\n\n\nvecA\n.\n\n\nvecB \n<-\n \nc\n(\nvecA\n,\n \n4\n,\n \n5\n)\n.\n\n\nvecC \n<-\n \nc\n(\nvecA\n[\n1\n:\n4\n],\n \n8\n,\n \n5\n,\n vecA\n[\n5\n:\n9\n])\n.\n\n\n\n\nMatrix and array\n\u00b6\n\n\n\n\nmat[r, c]\n; extract.\n\n\nmat[1, 2]\n.\n\n\nmat[,2]\n; all rows, column 2 only.\n\n\nmat[1, ]\n; all columns, row 1 only.\n\n\nmat[c(1, 3), c(4:5)]\n.\n\n\nmat[, 1, drop = FALSE]\n; avoid making a (horizontal) row with a (vertical) column.\n\n\nmat[ind]\n; matrix index.\n\n\narray[r, c, m]\n; extract.\n\n\n\n\nREF: p.110-113\n\n\nLists\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\nchar \n<-\n \nc\n(\n\"a\"\n,\n \n\"b\"\n,\n \n\"c\"\n)\n\nnumb \n<-\n \nc\n(\n1\n,\n \n2\n,\n \n3\n)\n\ngreek \n<-\n \nc\n(\n\"alpha\"\n,\n \n\"beta\"\n,\n \n\"gamma\"\n)\n\n\nx \n<-\n \nlist\n(\nchar\n,\n numb\n,\n greek\n)\n\nx\n\n[[\n1\n]]\n\n\n[\n1\n]\n \n\"a\"\n \n\"b\"\n \n\"c\"\n\n\n\n[[\n2\n]]\n\n\n[\n1\n]\n \n1\n \n2\n \n3\n\n\n\n[[\n3\n]]\n\n\n[\n1\n]\n \n\"alpha\"\n \n\"beta\"\n  \n\"gamma\"\n\n\n\nnames\n(\nx\n)\n \n<-\n \nc\n(\n\"char\"\n,\n \n\"numb\"\n,\n \n\"greek\"\n)\n\nx\n\n$\nchar\n\n[\n1\n]\n \n\"a\"\n \n\"b\"\n \n\"c\"\n\n\n\n$\nnumb\n\n[\n1\n]\n \n1\n \n2\n \n3\n\n\n\n$\ngreek\n\n[\n1\n]\n \n\"alpha\"\n \n\"beta\"\n  \n\"gamma\"\n\n\nx\n[\n2\n]\n\n\n$\nnumb\n\n[\n1\n]\n \n1\n \n2\n \n3\n\n\nx\n[[\n2\n]][\n2\n]\n\n\n[\n1\n]\n \n2\n\n\nx\n$\nnumb\n[\n2\n]\n\n\n[\n1\n]\n \n2\n\n\n\n\n\n\n\nREF: p.113-115\n\n\nString\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n\"bla bla bla\"\n\n\n[\n1\n]\n \n\"bla bla bla\"\n\n\n\nnoquote\n(\n\"bla bla bla\"\n)\n\n\n[\n1\n]\n bla bla bla\n\n\nsQuote\n(\n\"bla bla bla\"\n)\n\n\n[\n1\n]\n \n\"\u2018bla bla bla\u2019\"\n\n\n\ndQuote\n(\n\"bla bla bla\"\n)\n\n\n[\n1\n]\n \n\"\u201cbla bla bla\u201d\"\n\n\n\n\n\n\n\nText\n\u00b6\n\n\nwrangle, text, string, character, natural language processing, nlp\n\n\n\n\n\n\nformat()\n; and arguments:\n\n\n\n\ndigits\n.\n\n\ntrim\n.\n\n\ndigit\n.\n\n\nnsmall\n.\n\n\njustify\n.\n\n\nwidth\n.\n\n\nna.encode\n.\n\n\ndecimal.mark\n.\n\n\ndrop0trailing\n.\n\n\nand many more.\n\n\n\n\n\n\n\n\ncat(\"current working dir: \", wd)\n; print the objects, concatenate the representations.\n\n\n\n\nprintf(\"hello %d\\n\", 56)\n; mix text and data; pythonic print.\n\n\nprint(paste0(\"current working dir: \", wd))\n.\n\n\nnchar()\n; number of characters.\n\n\nx[nchar(x) > 2]\n.\n\n\nx[x %n% c(letters, LETTERS)]\n; retrieve letters, patterns or strings in a text object; alike Venn.\n\n\npaste(ch1, ch2, sep = \"-\")\n; concatenate.\n\n\npaste0(ch1, ch2)\n; concatenate.\n\n\nsubstring(\"abcdef\", first = 1:3, last = 2:4)\n; create subsets \nab\n, \nbc\n, \ncd\n.\n\n\nstrsplit(c(\"\",\"\"), split=\" \")\n; break down a string.\n\n\ngrep(\"i\", c())\n; extract an object index.\n\n\ngsub(\"i\", \"L\", c())\n; substitute.\n\n\nsub()\n; substitute the first occurrence.\n\n\ntolower()\n.\n\n\ntoupper()\n.\n\n\n\n\nDate and time\n\u00b6\n\n\nconvert, extract\n\n\n\n\nSys.time()\n.\n\n\ndate()\n.\n\n\nSys.setlocale()\n.\n\n\nas.numeric()\n.\n\n\nstrptime()\n; extract time.\n\n\nas.POSIXlt()\n.\n\n\n\n\nREF: p.120-123\n\n\nCustom functions (two examples)\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\na \n<-\n \n2\n\nb \n<-\n \n-3\n\n\n\n# quadratic\n\nf \n<-\n \nfunction\n \n(\nx\n)\n \n{\n x\n**\n2\n \n-\n \n2\n*\nx \n-\n \n2\n \n}\n\n\nf\n(\na\n)\n\n\n[\n1\n]\n \n-2\n\n\nf\n(\nb\n)\n\n\n[\n1\n]\n \n13\n\n\n\n# test\n\ng \n<-\n \nfunction\n \n(\nx\n,\n y\n)\n \n{\n\n  \nif\n \n(\nx \n<=\n y\n)\n \n{\n\n    z \n<-\n y \n-\n x\n    \nprint\n(\n\"x smaller\"\n)\n\n  \n}\n \nelse\n \n{\n\n    z \n<-\n x \n-\n y\n    \nprint\n(\n\"x larger\"\n)\n\n  \n}\n\n\n}\n \n\ng\n(\na\n,\n b\n)\n\n\n[\n1\n]\n \n\"x larger\"\n\n\ng\n(\na\n,\n \nabs\n(\nb\n))\n\n\n[\n1\n]\n \n\"x smaller\"\n\n\n\n\n\n\n\nMore about custom functions in \nChapter 6, Initiation to R Programming\n.\n\n\nLoops structure\n\u00b6\n\n\n\n\nfor\n.\n\n\nwhile\n.\n\n\nrepeat\n.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# while\n\n\nwhile\n(\nx \n+\n y \n<\n \n7\n)\n \n{\n x \n<-\n x \n+\n y \n}\n\n\n\n# for\n\n\nfor\n \n(\ni \nin\n \n1\n:\n4\n)\n \n{\n\n    \nif\n \n(\ni \n==\n \n3\n)\n \nbreak\n\n    \nfor\n \n(\nj \nin\n \n6\n:\n8\n)\n \n{\n\n        \nif\n \n(\nj \n==\n \n7\n)\n \nnext\n\n        j \n<-\n i \n+\n j\n    \n}\n\n\n}\n\n\n\n# repeat\n\ni \n<-\n \n0\n\n\nrepeat\n \n{\n\n    i \n<-\n i \n+\n \n1\n\n    \nif\n \n(\ni \n==\n \n4\n)\n \nbreak\n\n\n}\n\n\n\n\n\n\n\nLoop example\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\nIMC \n<-\n \nfunction\n \n(\npoids\n,\n taille\n)\n \n{\n\n  imc \n<-\n poids \n/\n taille\n^\n2\n\n  \nnames\n(\nimc\n)\n \n<-\n \n\"IMC\"\n\n  \nreturn\n(\nimc\n)\n\n\n}\n\n\nIMC\n(\n100\n,\n \n1.90\n)\n\n     IMC \n\n27.70083\n \n\np \n<-\n \nc\n(\n100\n,\n \n101\n,\n \n95\n,\n \n97\n)\n\nt \n<-\n \nc\n(\n1.90\n,\n \n1.75\n,\n \n1.68\n,\n \n1.92\n)\n\n\nIMC\n(\np\n,\n \nt\n)\n\n     IMC     \n<\nNA\n>\n     \n<\nNA\n>\n     \n<\nNA\n>\n \n\n27.70083\n \n32.97959\n \n33.65930\n \n26.31293\n \n\n\nfor\n \n(\ni \nin\n \n1\n:\n4\n)\n \n{\n\n  \nprint\n(\nIMC\n(\ndata\n[\ni\n,\n1\n],\n data\n[\ni\n,\n2\n]))\n\n\n}\n\n     IMC \n\n27.70083\n \n     IMC \n\n32.97959\n \n    IMC \n\n33.6593\n \n     IMC \n\n26.31293\n \n\n\n\n\n\n\nLoops increase computation time\n\n\n1\n2\n3\n4\n5\n6\n7\nsystem.time\n(\nfor\n \n(\ni \nin\n \n1\n:\n1000000\n)\n \nsqrt\n(\ni\n))\n\nutilisateur     syst\u00e8me      \u00e9coul\u00e9 \n       \n0.19\n        \n0.01\n        \n0.22\n \n\n\nsystem.time\n(\nsqrt\n(\n1\n:\n1000000\n))\n\nutilisateur     syst\u00e8me      \u00e9coul\u00e9 \n       \n0.07\n        \n0.00\n        \n0.07\n \n\n\n\n\n\n\nREF: p.133-136\n\n\nBinary and decimal\n\u00b6\n\n\nconvert\n\n\n\n\nbin2dec()\n.\n\n\ndec2bin()\n.\n\n\n\n\nREF: p.136-144, 147-152\n\n\nChapter 4, R Documentation\n\u00b6\n\n\nHelp\n\u00b6\n\n\n\n\n?functionname\n.\n\n\nhelp(functionname)\n.\n\n\nhelp.start()\n; user manual in the browser.\n\n\napropos('mean')\n; object with the name \u2018mean\u2019.\n\n\n\n\nLibrary\n\n\n\n\ninstall.packages('package')\n; should be done as an administrator or superuser directly in R, not an editor. Once the package is installed, it can be loaded on the PC by any user in R or an editor (such as RStudio).\n\n\nlibrary(package = 'package')\n; load a package.\n\n\nlibrary(help = package)\n; help on a specific package.\n\n\nlibrary(help = base)\n; for example.\n\n\nlibrary(help = utils)\n; for example.\n\n\nlibrary(help = datasets)\n; for example.\n\n\nlibrary(help = graphics)\n; for example.\n\n\nlibrary(help = grDevices)\n; for example.\n\n\n\n\n\n\nlibrary(lib.loc = .Library)\n; find which package (system packages, but not user packages).\n\n\nfind('function')\n; find which package (system).\n\n\ndata()\n; datasets available.\n\n\ndemo()\n; available demos by package.\n\n\ndemo(graphics)\n; give examples.\n\n\nexample(mean)\n; give examples.\n\n\n\n\nResources\n\u00b6\n\n\n\n\nnmz\n; search the content of the R functions, package vignette, and task views.\n\n\nR-Project Mailing Lists\n; mailing lists.\n\n\nRSeek\n; search engine.\n\n\nStackOverflow\n; forum.\n\n\nAbcd\u2019R\n; scripts.\n\n\nCIRAD\n; forum.\n\n\nDeveloppez.net\n; forum.\n\n\nETHZ\n; mailing list.\n\n\nETHZ Manual\n.\n\n\nhttp://a\n; manuals and resources.\n\nstat.ethz.ch/mailman/listinfo/r-annonce\n\n\nETHZ List Info\n.\n\n\nR Programming Wikibooks\n; wiki.\n\n\nCRAN-R\n.\n\n\nCRAN-R Doc\n; PDF and documentation.\n\n\nCRAN-R Views\n; projects.\n\n\nR Journal\n; publication.\n\n\nJournal of Statistical Software\n; publication.\n\n\n\n\nChapter 5, Techniques for Plotting Graphics\n\u00b6\n\n\nGraphic windows\n\u00b6\n\n\nOf course, these commands are automated in an editor (RStudio).\n\n\n\n\ndev.new()\n; graphic window on Windows.\n\n\nwindows()\n; graphic window on Windows.\n\n\nwin.graph()\n; graphic window on Windows.\n\n\nX11()\n; graphic window on Linux.\n\n\nAdd parameters in the command:\n\n\nwidth = , height =\n.\n\n\npointsize =\n.\n\n\nxpinch = , ypinch =\n; pixels per inch.\n\n\nxpos = , ypos =\n; position of the upper left corner, in pixel.\n\n\n\n\n\n\ndev.set(num)\n; activate window number \u2018num\u2019; there can be several graphic windows.\n\n\ndev.off(num)\n; close a window.\n\n\ngraphics.off()\n; close all windows.\n\n\ndev.list()\n; return window numbers.\n\n\ndev.cur()\n; return the current number, active window.\n\n\ndev.print(png, file = , width = , height = )\n; print.\n\n\nsavePlot(filename = , type = \"png\")\n; save.\n\n\npng(file=\" \", width = , height = )\n; save directly in .png.\n\n\njpeg()\n.\n\n\nbitmap()\n.\n\n\npostscript()\n.\n\n\npdf()\n.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n# examples\n\n\ndev.new\n(\nwidth \n=\n \n200\n,\n height \n=\n \n200\n,\n xpos \n=\n \n100\n,\n ypos \n=\n \n100\n)\n\n\nNULL\n\n\ndev.list\n()\n\nRStudioGD       png   windows \n        \n2\n         \n3\n         \n4\n \n\ndev.cur\n()\n\nwindows \n      \n4\n\n\nhist\n(\nrunif\n(\n100\n))\n \n# create a graphic\n\n\n\n# in the current working directory\n\ndev.print\n(\npng\n,\n file \n=\n \n\"mygraph.png\"\n,\n width \n=\n \n480\n,\n height \n=\n \n480\n)\n\nsavePlot\n(\nfilename \n=\n \n\"mygraph.png\"\n,\n type \n=\n \n\"png\"\n)\n\npdf\n(\nfile \n=\n \n\"mygraph.pdf\"\n)\n\n\ndev.off\n()\n\n\n\n\n\n\n\nMultiple windows\n\u00b6\n\n\n\n\npar(mfrow = c(3, 2))\n; create 6 graphic boxes, 3 rows X 2 columns.\n\n\n\n\nREF: p.166-167\n\n\n1\n2\n3\n4\n5\n6\n7\n8\ndev.new\n()\n\nmat \n<-\n \nmatrix\n(\nc\n(\n2\n,\n \n3\n,\n \n0\n,\n \n1\n,\n \n0\n,\n \n0\n,\n \n0\n,\n \n4\n,\n \n0\n,\n \n0\n,\n \n0\n,\n \n5\n),\n \n4\n,\n \n3\n,\n byrow \n=\n \nTRUE\n)\n\nlayout\n(\nmat\n)\n\n\n\ndev.new\n()\n\nlayout\n(\nmat\n,\n widths \n=\n \nc\n(\n1\n,\n \n5\n,\n \n14\n),\n heights \n=\n \nc\n(\n1\n,\n2\n,\n4\n,\n1\n))\n\nlayout.show\n(\n5\n)\n\n\n\n\n\n\n\nREF: p.168\n\n\nDraw on a graphic\n\u00b6\n\n\n\n\nsegments(x0 = 0, y0 = 0, x1 = 1, y1 = 1)\n; draw lines on a plot.\n\n\nlines(x = c(1,0), y = c(0,1))\n.\n\n\nabline(h = 0, v = 0)\n; add a line to a plot.\n\n\nabline(a = 1, b = 1)\n.\n\n\n\n\nRandom numbers\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nx \n<-\n runif\n(\n12\n)\n\nx\n\n[\n1\n]\n \n0.03344539\n \n0.22711659\n \n0.66696650\n \n0.02840671\n \n0.61067995\n \n0.92957527\n\n\n[\n7\n]\n \n0.26962190\n \n0.60013387\n \n0.04831111\n \n0.12603905\n \n0.41913598\n \n0.13142315\n\n\n\n# ranking\n\ni \n<-\n \norder\n(\nx\n)\n\ni\n\n[\n1\n]\n  \n4\n  \n1\n  \n9\n \n10\n \n12\n  \n2\n  \n7\n \n11\n  \n8\n  \n5\n  \n3\n  \n6\n\n\n\n# reorder\n\nx \n<-\n x\n[\ni\n]\n\nx\n\n[\n1\n]\n \n0.02840671\n \n0.03344539\n \n0.04831111\n \n0.12603905\n \n0.13142315\n \n0.22711659\n\n\n[\n7\n]\n \n0.26962190\n \n0.41913598\n \n0.60013387\n \n0.61067995\n \n0.66696650\n \n0.92957527\n\n\n\n\n\n\n\nMore on random number in Chapter 10, Random Variables, Laws, and Simulation.\n\n\nExamples\n\n\n\n\nexample(polygon)\n; see examples.\n\n\ncurve(x**3 - 3*x, from = -2, to = 2)\n; trace a curve according to a function.\n\n\nhist(rnorm(10000), prob = TRUE, breaks = 100)\n.\n\n\nplot(runif(7), type = \"h\", axes = F); box(lty = \"1373\")\n\n\nplot(1:10, runif(10), type = \"l\", col = \"orangered\")\n\n\n\n\nHelp\n\n\n\n\ncolors()[grep(\"orange\", colors())]\n; list of tones.\n\n\n\n\nColors\n\u00b6\n\n\n\n\nrgb(red = 26, green = 204, blue = 76, maxColorValue = 255)\n; return the code.\n\n\nrgb(red = 0.1, green = 0.8, blue = 0.3)\n.\n\n\ncol2rgb(\"#1AVV4C\")\n; inversely.\n\n\nR generates 256\u00b3 colors, 15M.\n\n\nrainbow(#)\n; show 1, 8, 17 or more colors; R can generate 256\u00b3 colors (over 15M),\n\n\npie(rep(1,200), labels = \"\", col = rainbow(200), border = NA)\n; show the colors.\n\n\nplot(1:40, col = rainbow(n), pch = 19, cex = 2),     grid(col = \"grey50\")\n; show.\n\n\nRColorBrewer\n package.\n\n\n\n\n1\n2\nlibrary\n(\n'RColorBrewer'\n)\n\ndisplay.brewer.all\n()\n\n\n\n\n\n\n\nImages\n\u00b6\n\n\n\n\ncaTools\n package to manage images.\n\n\nread.gif()\n.\n\n\nimage()\n.\n\n\n\n\nWrite and add marks on a graphic\n\u00b6\n\n\n\n\ntext(coord x, coord y, 'text')\n; write.\n\n\ndemo(plotmath)\n; see examples.\n\n\nmtext('bas', side = 1)\n; write \u2018bas\u2019 on the graphic box; 4 sides of the box (bottom, left, top, right).\n\n\nlocator()\n; point with the mouse on a graphic to record coordinates; \nesc\n to show the coordinates.\n\nplot(1,1); locate area to be annotated with \ntext()\n.\n\n\ntext(locator(1), label=\"ici\")\n; add a label, one or several occurrences, by pointing the location with the mouse.\n\n\nidentify(occurrence, label)\n; add one or more labels by pointing the location with the mouse. \n\n\n\n\nGraphic parameters and graphic windows\n\u00b6\n\n\nmargin, border, box, square, space, height, width, color, text, title, label, mark, font, police, character, language, size, length, size, tick, coordinate, x, y, log, scale, axis, axes, format, alignment, point\n\n\nAdvanced graphic package\n\u00b6\n\n\n\n\nrgl\n.\n\n\nlattice\n.\n\n\nggplot2\n.\n\n\n\n\nREF: p.190-201\n\n\nChapter 6, Initiation to R Programming\n\u00b6\n\n\n1\n2\n3\nfunction\n \n(\n<\nparameters\n>\n)\n \n{\n\n    \n<\nbody\n>\n\n\n}\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nlance \n<-\n \nfunction\n \n(\nnom\n)\n \n{\n\n  \ncat\n(\n\"Bonjour\"\n,\n nom\n,\n \n\"!\"\n)\n\n\n}\n\n\nlance\n(\n'allo'\n)\n\nBonjour Alex \n!\n\n\nbonjour \n<-\n \nfunction\n \n(\nnom\n=\n\"Pierre\"\n,\n langue\n=\n\"fr\"\n)\n \n{\n\n  \ncat\n(\nswitch\n(\nlangue\n,\n fr\n=\n\"Bonjour\"\n,\n esp\n=\n\"Hola\"\n,\n ang\n=\n\"Hi\"\n),\n nom\n,\n \n\"!\"\n)\n\n\n}\n\n\nbonjour\n()\n\nBonjour Pierre \n!\n\nbonjour\n(\nnom\n=\n\"Ben\"\n)\n \n# replace the default value.\n\nBonjour Ben \n!\n\nbonjour\n(\nlangue\n=\n\"ang\"\n)\n\nHi Pierre \n!\n\nbonjour\n(\nlang \n=\n \n\"ang\"\n)\n \n# partial call\n\nHi Pierre \n!\n\nbonjour\n(\nl\n=\n\"ang\"\n)\n\nHi Pierre \n!\n\nbonjour\n(\nl\n=\n\"a\"\n)\n\n Pierre \n!\n\n\n\n\n\n\n\nREF: p.218-219\n\n\nUnion\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n\"%union%\"\n \n<-\n \nfunction\n \n(\nA\n,\nB\n)\n \n{\n \nunion\n(\nA\n,\nB\n)\n \n}\n\nA \n<-\n \nc\n(\n4\n,\n5\n,\n2\n,\n7\n)\n\nB \n<-\n \nc\n(\n2\n,\n1\n,\n7\n,\n3\n)\n\n\nA \n%union%\n B\n\n[\n1\n]\n \n4\n \n5\n \n2\n \n7\n \n1\n \n3\n\n\n\n\n\n\n\nClass\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nobj \n<-\n \n1\n:\n10\n\n\n\nclass\n(\nobj\n)\n\n\n[\n1\n]\n \n\"integer\"\n\n\n\nclass\n(\nobj\n)\n \n<-\n \n\"TheClass\"\n\n\n\nclass\n(\nobj\n)\n\n\n[\n1\n]\n \n\"TheClass\"\n\n\n\ninherits\n(\nobj\n,\n \n\"TheClass\"\n)\n\n\n[\n1\n]\n \nTRUE\n\n\n\n\n\n\n\nMethods\n\u00b6\n\n\n1\n2\n3\n4\nx \n<-\n \n1\n:\n10\n\n\n\nprint.default\n(\nx\n)\n\n\n[\n1\n]\n \n1\n \n2\n \n3\n \n4\n \n5\n \n6\n \n7\n \n8\n \n9\n \n10\n\n\n\n\n\n\n\nREF: p.227-231\n\n\nCombine and permute\n\u00b6\n\n\n\n\ncombinat\n package.\n\n\ncombn(5,3)\n; combine 3 numbers from 1:5.\n\n\nchoose(200,3)\n; choose 3 numbers from 1:200.\n\n\npermn(n,m)\n.\n\n\n\n\nVery time consuming!\n\n\nMore power, speed\n\u00b6\n\n\n\n\nThe core of R in programmed in C. Converting a R function into C is easy. C is faster.\n\n\nCall it through API C.\n\n\nWith R graphic interface and C computation speed, it is the best of both world.\n\n\nEasier to set up on Linux or OS X (the OS has default compilers. Use the \nRcpp\n package. \n\n\nOn Windows, there is a need for \nRtools\n. \n\n\nR can be compiled (byte compiler) with the \nRevoScaleR\n package (parallel computing).\n\n\nbigmemory\n, \nff\n, packages; split a matrix into sub-matrices, perform computation and combine the results (parallel computing). \n\n\nUse a multi-core architecture with the \nparallel\n package.\n\n\nHighPerformanceComputing list of packages\n.\n\n\n\n\nTry parallel computing with a Monte Carlo with the \nparallel\n package\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nmyfunc \n<-\n \nfunction\n(\nM\n=\n1000\n)\n \n{\n\n    decision \n<-\n \n0\n\n    \nfor\n \n(\ni \nin\n \n1\n:\nM\n)\n \n{\n\n      x \n<-\n rnorm\n(\n100\n)\n\n      \nif\n \n(\nshapiro.test\n(\nx\n)\n$\np \n<\n \n0.05\n)\n decision \n<-\n decision \n+1\n\n    \n}\n\n\nreturn\n(\ndecision\n)\n\n\n}\n\n\n\nsystem.time\n({\n\n    M \n<-\n \n60000\n\n    decision \n<-\n myfunc\n(\nM\n)\n\n    \nprint\n(\ndecision\n/\nM\n)\n\n\n})\n\n\n\n\n\n\n\n\n\nFor a parallel execution.\n\n\nStart menu.\n\n\nType devmgmt.msc\n\n\nUnder Processors in Linux, type \ntop\n in the terminal.\n\n\nThen, type \n1\n in R.\n\n\nEnter \ndetectCores()\n from the \nparallel\n package.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nrequire\n(\n\"parallel\"\n)\n\n\nsystem.time\n({\n\n    nbcores \n<-\n \n6\n \n# less than detectCores() - 1\n\n    M \n<-\n \n60000\n\n    cl \n<-\n makeCluster\n(\nnbcores\n,\n type \n=\n \n\"PSOCK\"\n)\n\n    out \n<-\n clusterCall\n(\ncl\n,\n myfunc\n,\n \nround\n(\nM\n/\nnbcores\n))\n\n    stopCluster\n(\ncl\n)\n\n    decision \n<-\n \n0\n\n    \nfor\n \n(\nclus \nin\n \n1\n:\nnbcores\n)\n \n{\n\n        decision \n<-\n decision \n+\n out\n[[\nclus\n]]\n\n    \n}\n\n    \nprint\n(\ndecision\n/\n(\nround\n(\nM\n/\nnbcores\n)\n*\nnbcores\n))\n\n\n})\n\n\n\n\n\n\n\n\n\nThe process number (PID) of each computation node (core) in the cluster.\n\n\n\n\n1\n2\n3\n4\nrequire\n(\n\"parallel\"\n)\n\n\nSys.getpid\n()\n\ncl \n<-\n makeCluster\n(\n4\n,\ntype\n=\n\"PSOCK\"\n)\n\nout \n<-\n clusterCall\n(\ncl\n,\n \nSys.getpid\n))\n\n\n\n\n\n\n\nInvolve the graphical card for more power\n\u00b6\n\n\n\n\nRun computations with the graphical card, the GPU.\n\n\ngputools\n package.\n\n\n\n\n\n\n\n\nREF: p.303-308\n\n\nChapter 7, Session Management\n\u00b6\n\n\nwork, session, save, object, instruction, graphic, create, package\n\n\nEnvironment\n\u00b6\n\n\n\n\nglobalenv()\n; .GlobalEnv.\n\n\nnew.env()\n; new environment with its own functions, variable, etc.\n\n\nls()\n; list of objects in the environment.\n\n\nobjects()\n; idem.\n\n\nrm()\n; remove one or more objects.\n\n\nrm(list = ls())\n; remove all.\n\n\n.RData; workspace file extension.\n\n\nfile.RData; R file.\n\n\nsave.image(\"file.RData\")\n; save a R workspace to the current working directory. Several workspace can have their own objects.\n\n\nload(\"file.RData\")\n; load a R workspace from the current working directory.\n\n\nload(file.choose())\n; open the current working directory.\n\n\ngetwd()\n; get the current working directory.\n\n\nsetwd()\n; set the current working directory.\n\n\n.Rhistory; history file extension.\n\n\nhistory()\n; consult the R log.\n\n\nsavehistory()\n; save the log to a file.\n\n\nloadhistory()\n; load the log from a file.\n\n\nsearch()\n; list of attached packages.\n\n\nsearchpaths()\n;  list of paths.\n\n\nlibrary()\n; list of packages in memory.\n\n\nrequire(\"packages\")\n, \nlibrary(\"packages\")\n; load a package.\n\n\nattach(data)\n; attach a dataset to .GlobalEnv..\n\n\ndetach(data)\n.\n\n\nls(pos = 1)\n, \nls()\n; object list in database 1.\n\n\nls(pos = 2)\n; object list in database 2.\n\n\nls(pos = match(\"package:datasets\", search()))\n; list datasets.\n\n\nls(data)\n; list object related to the dataset.\n\n\nfix(data)\n; open a spreadsheet with the data.\n\n\nsink(file = \"sortie.txt\")\n; save a file on the current working directory.\n\n\nsink()\n; stop the recording.\n\n\n\n\nFile manipulation\n\u00b6\n\n\n\n\nLow-level interface to the computer\u2019s file system.\n\n\nCreate a file.\n\n\nExecute the command.\n\n\nCheck out the result in the text files themselves.\n\n\nfile.create(\"sorty.txt\", showWarnings = TRUE)\n.\n\n\nfile.exists(\"sorty.txt\")\n.\n\n\nfile.remove(\"sorty.txt\")\n.\n\n\nfile.rename(\"sorty.txt\", \"sorti.txt\")\n.\n\n\nfile.append(\"sorty.txt\", \"sorti2.txt\")\n.\n\n\nfile.copy(\"sorty.txt\", \"sorti2.txt\")\n.\n\n\nfile.symlink(\"sorty.txt\", \"sorti2.txt\")\n.\n\n\nfile.path(\"sorty.txt\")\n.\n\n\nfile.show()\n.\n\n\nlist.files()\n.\n\n\nunlink()\n.\n\n\nbasename()\n.\n\n\npath.expand()\n.\n\n\nlist.files()\n.\n\n\nfile.exists()\n.\n\n\nmemory.size()\n.\n\n\nmemory.limit()\n.\n\n\n\n\nREF: p.320-330\n\n\nMemory management\n\u00b6\n\n\n\n\nThe KSysGuard software analyzes memory usage in real time. It is a task manager and performance monitor for UNIX-like OS.\n\n\n\n\nCreate a package\n\u00b6\n\n\n\n\nHow to: build a list of R instruction, load a dataset, create function, and output the results.\n\n\nRun a series of scripts in batch, run a script at a distance.\n\n\nSet the PATH to an executable Rgui.exe.\n\n\nOpen a R script without opening R.\n\n\nCreate a runthis script, apply \nchmod\n to use and execute it. The bash script launch R commands (it must begin with #!/in/bash).\n\n\nand much more.\n\n\n\n\nREF: p.332-338\n\n\nPART 2, MATHEMATICS AND BASIC STATISTICS\n\u00b6\n\n\nChapter 8, Basic Mathematics, matrix algebra, integration, optimization\n\u00b6\n\n\nMath functions\n\u00b6\n\n\nREF: p.342\n\n\nMatrix calculation\n\u00b6\n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\nA \n<-\n \nmatrix\n(\nc\n(\n2\n,\n3\n,\n5\n,\n4\n),\n nrow \n=\n \n2\n,\n ncol \n=\n \n2\n)\n\nB \n<-\n \nmatrix\n(\nc\n(\n1\n,\n2\n,\n2\n,\n7\n),\n nrow \n=\n \n2\n,\n ncol \n=\n \n2\n)\n\n\nA\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n2\n    \n5\n\n\n[\n2\n,]\n    \n3\n    \n4\n\n\nB\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n1\n    \n2\n\n\n[\n2\n,]\n    \n2\n    \n7\n\n\nA \n+\n B\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n3\n    \n7\n\n\n[\n2\n,]\n    \n5\n   \n11\n\n\nA \n-\n B\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n1\n    \n3\n\n\n[\n2\n,]\n    \n1\n   \n-3\n\n\n\n# scalar multiplication\n\nA \n*\n B\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n2\n   \n10\n\n\n[\n2\n,]\n    \n6\n   \n28\n\n\n\n# matrix multiplication\n\nA \n%*%\n B\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n   \n12\n   \n39\n\n\n[\n2\n,]\n   \n11\n   \n34\n\n\n\n# scalar multiplication\n\na \n<-\n \n10\n\na \n*\n A\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n   \n20\n   \n50\n\n\n[\n2\n,]\n   \n30\n   \n40\n\n\n\n# transpose\n\n\nt\n(\nA\n)\n\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n2\n    \n3\n\n\n[\n2\n,]\n    \n5\n    \n4\n\n\n\n# inverse\n\n\nsolve\n(\nB\n)\n\n           \n[,\n1\n]\n       \n[,\n2\n]\n\n\n[\n1\n,]\n  \n2.3333333\n \n-0.6666667\n\n\n[\n2\n,]\n \n-0.6666667\n  \n0.3333333\n\n\n\nsolve\n(\nA\n)\n \n%*%\n B\n           \n[,\n1\n]\n      \n[,\n2\n]\n\n\n[\n1\n,]\n  \n0.8571429\n  \n3.857143\n\n\n[\n2\n,]\n \n-0.1428571\n \n-1.142857\n\n\n\nt\n(\nA\n)\n \n*\n B\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n2\n    \n6\n\n\n[\n2\n,]\n   \n10\n   \n28\n\n\nx \n<-\n \nseq\n(\n1\n,\n4\n)\n\ny \n<-\n \nseq\n(\n4\n,\n7\n)\n\n\nx\n\n[\n1\n]\n \n1\n \n2\n \n3\n \n4\n\ny\n\n[\n1\n]\n \n4\n \n5\n \n6\n \n7\n\n\n\nouter\n(\nx\n,\n y\n,\n FUN \n=\n \n\"*\"\n)\n\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n \n[,\n4\n]\n\n\n[\n1\n,]\n    \n4\n    \n5\n    \n6\n    \n7\n\n\n[\n2\n,]\n    \n8\n   \n10\n   \n12\n   \n14\n\n\n[\n3\n,]\n   \n12\n   \n15\n   \n18\n   \n21\n\n\n[\n4\n,]\n   \n16\n   \n20\n   \n24\n   \n28\n\n\n\n# Kronecker\n\n\nkronecker\n(\nA\n,\n B\n)\n\n     \n[,\n1\n]\n \n[,\n2\n]\n \n[,\n3\n]\n \n[,\n4\n]\n\n\n[\n1\n,]\n    \n2\n    \n4\n    \n5\n   \n10\n\n\n[\n2\n,]\n    \n4\n   \n14\n   \n10\n   \n35\n\n\n[\n3\n,]\n    \n3\n    \n6\n    \n4\n    \n8\n\n\n[\n4\n,]\n    \n6\n   \n21\n    \n8\n   \n28\n\n\n\n# triangle\n\n\nlower.tri\n(\nA\n)\n\n      \n[,\n1\n]\n  \n[,\n2\n]\n\n\n[\n1\n,]\n \nFALSE\n \nFALSE\n\n\n[\n2\n,]\n  \nTRUE\n \nFALSE\n\n\n\nlower.tri\n(\nA\n,\n diag \n=\n \nTRUE\n)\n\n     \n[,\n1\n]\n  \n[,\n2\n]\n\n\n[\n1\n,]\n \nTRUE\n \nFALSE\n\n\n[\n2\n,]\n \nTRUE\n  \nTRUE\n\n\n\nlower.tri\n(\nA\n)\n \n*\n A\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n    \n0\n    \n0\n\n\n[\n2\n,]\n    \n3\n    \n0\n\n\n\n# diagonal\n\n\ndiag\n(\nA\n)\n\n\n[\n1\n]\n \n2\n \n4\n\n\n\nsum\n(\ndiag\n(\nA\n))\n\n\n[\n1\n]\n \n6\n\n\n\n# kappa\n\n\nkappa\n(\nA\n,\n exact \n=\n \nTRUE\n)\n\n\n[\n1\n]\n \n7.582401\n\n\n\n# matrix reduction\n\n\nscale\n(\nA\n,\n scale \n=\n \nFALSE\n)\n\n     \n[,\n1\n]\n \n[,\n2\n]\n\n\n[\n1\n,]\n \n-0.5\n  \n0.5\n\n\n[\n2\n,]\n  \n0.5\n \n-0.5\n\n\nattr\n(,\n\"scaled:center\"\n)\n\n\n[\n1\n]\n \n2.5\n \n4.5\n\n\n\nscale\n(\nA\n,\n center \n=\n \nFALSE\n,\n scale \n=\n \napply\n(\nA\n,\n \n2\n,\n sd\n))\n\n         \n[,\n1\n]\n     \n[,\n2\n]\n\n\n[\n1\n,]\n \n2.828427\n \n7.071068\n\n\n[\n2\n,]\n \n4.242641\n \n5.656854\n\n\nattr\n(,\n\"scaled:scale\"\n)\n\n\n[\n1\n]\n \n0.7071068\n \n0.7071068\n\n\n\n# eigenvalue\n\n\neigen\n(\nA\n)\n\n\n$\nvalues\n\n[\n1\n]\n  \n7\n \n-1\n\n\n\n$\nvectors\n           \n[,\n1\n]\n       \n[,\n2\n]\n\n\n[\n1\n,]\n \n-0.7071068\n \n-0.8574929\n\n\n[\n2\n,]\n \n-0.7071068\n  \n0.5144958\n\n\n\n# singular value vector\n\n\nsvd\n(\nA\n)\n\n\n$\nd\n\n[\n1\n]\n \n7.285383\n \n0.960828\n\n\n\n$\nu\n           \n[,\n1\n]\n       \n[,\n2\n]\n\n\n[\n1\n,]\n \n-0.7337222\n \n-0.6794496\n\n\n[\n2\n,]\n \n-0.6794496\n  \n0.7337222\n\n\n\n$\nv\n           \n[,\n1\n]\n       \n[,\n2\n]\n\n\n[\n1\n,]\n \n-0.4812092\n  \n0.8766058\n\n\n[\n2\n,]\n \n-0.8766058\n \n-0.4812092\n\n\n\n# Cholesky\n\n\nchol2inv\n(\nA\n)\n \n          \n[,\n1\n]\n     \n[,\n2\n]\n\n\n[\n1\n,]\n  \n0.640625\n \n-0.15625\n\n\n[\n2\n,]\n \n-0.156250\n  \n0.06250\n\n\n\n# QR\n\n\nqr\n(\nA\n)\n\n\n$\nqr\n\n           \n[,\n1\n]\n      \n[,\n2\n]\n\n\n[\n1\n,]\n \n-3.6055513\n \n-6.101702\n\n\n[\n2\n,]\n  \n0.8320503\n \n-1.941451\n\n\n\n$\nrank\n\n\n[\n1\n]\n \n2\n\n\n\n$\nqraux\n\n[\n1\n]\n \n1.554700\n \n1.941451\n\n\n\n$\npivot\n\n[\n1\n]\n \n1\n \n2\n\n\n\nattr\n(,\n\"class\"\n)\n\n\n[\n1\n]\n \n\"qr\"\n\n\n\n\n\n\n\nIntegral calculus\n\u00b6\n\n\nintegration\n\n\n1\n2\n3\n4\nmyf \n<-\n \nfunction\n(\nx\n)\n \n{\n \nexp\n(\n-\nx\n^\n2\n \n/\n \n2\n)\n \n/\n \nsqrt\n(\n2\n  \n*\npi\n)\n \n}\n\n\nintegrate\n(\nmyf\n,\n lower \n=\n \n-\nInf\n,\n upper \n=\n \nInf\n)\n$\nvalue\n\n[\n1\n]\n \n1\n\n\n\n\n\n\n\nDifferential calculus\n\n\nderivative\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nD\n(\nexpression\n(\nsin\n(\ncos\n(\nx \n+\n y\n^\n2\n))),\n \n\"x\"\n)\n\n\n-\n(\ncos\n(\ncos\n(\nx \n+\n y\n^\n2\n))\n \n*\n \nsin\n(\nx \n+\n y\n^\n2\n))\n\n\nf \n<-\n deriv\n(\n<\nsub\n>\nx\n^\n2\n,\n \n\"x\"\n,\n \nTRUE\n)\n\nf\n(\n3\n)\n\n\n[\n1\n]\n \n9\n\n\nattr\n(,\n\"gradient\"\n)\n\n     x\n\n[\n1\n,]\n \n6\n\n\n\n\n\n\n\n\n\nnumDeriv\n package.\n\n\ngrad()\n; first-degree derivative.\n\n\nhessian()\n; second-degree derivative.\n\n\nand many more.\n\n\n\n\n\n\n\n\nOptimisation\n\u00b6\n\n\nlinear, programming, constraints, min-max\n\n\n1\n2\n3\n4\n5\n# compute a 1-variable min, max (y <sub>x)\n\n\noptimize\n(\nfunction\n(\nx\n)\n \n{\n \ncos\n(\nx\n^\n2\n)\n \n},\n lower \n=\n \n0\n,\n upper \n=\n \n2\n,\n maximum \n=\n \nFALSE\n)\n\n\n\nsource\n(\n'</sub>/.active-rstudio-document'\n,\n echo \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n\n\nnlm()\n; compute a 2-variable min, max (z \nx + y).\n\n\nnlminb()\n; add contraints on x and y, add parameters \nupper\n and \nlower\n.\n\n\noptim()\n.\n\n\nconstrOptim()\n.\n\n\nand many more.\n\n\n\n\nUnit root\n\n\n1\n2\n3\nunitroot\n(\nf\n=\nfunction\n(\nx\n)\n \n{\n \ncos\n(\nx\n^\n2\n)\n \n},\n lower \n=\n \n0\n,\nupper \n=\n \n2\n,\ntol \n=\n \n0.00001\n)\n$\nroot\n\n\npolyroot\n(\nx\n(\n3\n,\n \n-8\n,\n \n1\n))\n \n# for p(x) = 3 - 8x + x\u00b2\n\n\n\n\n\n\n\n\n\ncummax()\n.\n\n\ncummin()\n.\n\n\ncumprod()\n.\n\n\ncumsum()\n.\n\n\nand many more.\n\n\n\n\nChapter 9, Descriptive Statistics\n\u00b6\n\n\nFactor, levels, labels\n\u00b6\n\n\n\n\nfactor(c())\n\n\nas.factor()\n.\n\n\nis.factor()\n.\n\n\nlevels\n(\nvar\n)\n \n<-\n \nc\n()\n.\n\n\nlabels\n(\nvar\n)\n \n<-\n \nc\n()\n.\n\n\nlevels(var)\n; output the levels.\n\n\nlabels(var)\n; output the labels.\n\n\nnlevels(var)\n; output the number of levels.\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nmydata \n<-\n \nfactor\n(\nmydata\n,\n\n    levels \n=\n \nc\n(\n1\n,\n2\n,\n3\n),\n\n    labels \n=\n \nc\n(\n\"red\"\n,\n \n\"blue\"\n,\n \n\"green\"\n)\n\n    \n)\n \n\nmydata \n<-\n \nordered\n(\nmydata\n,\n\n    levels \n=\n \nc\n(\n1\n,\n3\n,\n \n5\n),\n\n    labels \n=\n \nc\n(\n\"Low\"\n,\n \n\"Medium\"\n,\n \n\"High\"\n)\n\n    \n)\n \n\n\n\n\n\n\nNames\n\u00b6\n\n\n\n\nnames\n(\nvar\n)\n \n<-\n \nc\n()\n; add names to a vector, data frame, list.\n\n\ncolnames\n(\nvar\n)\n \n<-\n \nc\n()\n; idem.\n\n\nrownames()\n; left-most column.\n\n\ndimnames()\n; add names to an array.\n\n\n\n\nOrder\n\u00b6\n\n\n\n\nsort(vec, decreasing = TRUE)\n.\n\n\nrev(vec)\n; inverse sorting.\n\n\norder(vec)\n; sort a vector with names or a list of strings.\n\n\nordered(vec)\n.\n\n\nas.ordered()\n.\n\n\nis.ordered()\n.\n\n\nand many more.\n\n\n\n\nConsult keywords \u2018arithmetics\u2019 and \u2018random numbers\u2019, where ordering data is commonly used.\n\n\nConvert (\nas.\n)\n\u00b6\n\n\n\n\nis.integer()\n.\n\n\nas.integer()\n.\n\n\nas.double()\n.\n\n\nis.double()\n.\n\n\nis.numeric()\n.\n\n\nas.numeric()\n.\n\n\nis.character()\n.\n\n\nas.character()\n.\n\n\nand many more.\n\n\n\n\nTable, proportion table\n\u00b6\n\n\ntabular, comparison, 2-dimensional, 2, two, dimensions\n\n\n\n\ntable(var1, var2)\n; 2-dimensional view; cross table.\n\n\nas.table(var)\n; convert.\n\n\ncut()\n; divide the range into intervals.\n\n\ntable(cut(x, res$breaks, include.lowest = TRUE))\n.\n\n\n\n\n\n\naddmargins(x, FUN = sum, quiet = TRUE)\n; add a column or row with sums, means, etc.\n\n\nread.ftable()\n; frequencies.\n\n\ntablefreq \n<-\n mytable \n/\n \nsum\n(\nmytable\n)\n.\n\n\nmargin.table(tablefreq, 1)\n; margin, right and bottom.\n\n\ntablefreq[ ,ncol()]\n; extract the total.\n\n\ntablefreq[nrow(), ]\n; extract the total.\n\n\n\n\n\n\nprop.table(mytable, 1)\n; percentage view; 1, row sum = 100%.\n\n\nprop.table(mytable, 2)\n; percentage view; 2, row sum = 100%.\n\n\nwhich.max(table)\n; find the max, min, mean, etc.\n\n\n\n\nDescriptive statistics\n\u00b6\n\n\n\n\nmean(x)\n.\n\n\nmedian(x)\n.\n\n\nquantile(x, probs = c(0.1, 0.9))\n.\n\n\nprobs = 1:10 / 10\n.\n\n\n\n\n\n\nmax(x)\n.\n\n\nmin(x)\n.\n\n\ndiff(range(x))\n.\n\n\nIQR(x)\n.\n\n\nvar.pop(x)\n, \nvar(x)\n.\n\n\nsd.pop(x)\n, \nsd(x)\n.\n\n\nco.var(x)\n.\n\n\nmad(x)\n; absolute deviation from the median.\n\n\nmean(abs(x - mean(x)))\n.\n\n\n\n\n\n\nskew(x)\n.\n\n\nkurt(x)\n.\n\n\nchisq.test()\n.\n\n\nround()\n.\n\n\nsum()\n.\n\n\nnrow()\n.\n\n\nncol()\n.\n\n\ncor(var1, var2)\n.\n\n\nmethod = \"kendall\", \"spearman\"\n\n\n\n\n\n\nrank()\n.\n\n\nrgrs\n package .\n\n\ncramer.v()\n.\n\n\n\n\n\n\n\n\nREF: p.378-379\n\n\nGraphic descriptive statistics\n\u00b6\n\n\n\n\nplot()\n.\n\n\ndotchart(table(), col = c(\"\", \"\", \"\" ,\"\") ,pch = , main = , lcolor = )\n.\n\n\nbarplot()\n.\n\n\nbarplot(var, col = , pareto = TRUE)\n.\n\n\nbarplot(sort(table(var)), TRUE))\n.\n\n\nbarplot(xlim = , width = , space = , names.arg = , legend = , density = , ylab = , lwd = )\n.\n\n\npie()\n.\n\n\npoints(barplot(), cumsum(var), type=\"l\")\n.\n\n\nboxplot()\n.\n\n\nstem()\n.\n\n\naplpack\n package.\n\n\nstem.left()\n\n\n\n\n\n\nhist()\n.\n\n\nsegments()\n.\n\n\n\n\nREF: p.392-394, 397-398, 400-410\n\n\nChapter 10, Random Variables, Laws, and Simulation\n\u00b6\n\n\n\n\nx %% m\n; modulo or modulus.\n\n\n\n\nRandomness\n\u00b6\n\n\n\n\nrunif(1)\n; generate a pseudo-random number between 0 and 1.\n\n\nset.seed()\n; \u2018shuffle the dice!\u2019.\n\n\nx \n<-\n \nfunction\n()\n \n{\n runif\n(\n1\n)\n \n}\n; generate random numbers, following the uniform distribution.\n\n\nrnorm(1)\n; generate a random numbers, following the normal distribution.\n\n\ngenerate random numbers with a randomness function and \ncurve()\n.\n\n\n\n\n1\n2\n3\n4\n5\n6\nx \n<-\n \nfunction\n()\n \n{\n rnorm\n(\n1\n,\n \n7\n,\n \n1\n)\n \n}\n\n\n# avg = 7, sd = 1, 1 obs\n\n\ncurve\n(\nrnorm\n(\nx\n,\n \n7\n,\n \n1\n),\n xlim \n=\n \nc\n(\n-1\n,\n10\n))\n\n\nplot\n(\ndensity\n(\nrnorm\n(\n1000\n,\n \n7\n,\n \n1\n)),\nxlim \n=\n \nc\n(\n-1\n,\n \n10\n),\n main\n=\n\"Density Curve\"\n)\n\n\n\n\n\n\n\nREF: p.422-423\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nmean\n(\nrunif\n(\n1\n))\n\n\n[\n1\n]\n \n0.6586903\n\n\n\nmean\n(\nrunif\n(\n10\n))\n\n\n[\n1\n]\n \n0.5196868\n\n\n\nmean\n(\nrunif\n(\n100\n))\n\n\n[\n1\n]\n \n0.5345603\n\n\n\nmean\n(\nrunif\n(\n1000\n))\n\n\n[\n1\n]\n \n0.5042301\n\n\n\nmean\n(\nrunif\n(\n10000\n))\n\n\n[\n1\n]\n \n0.5021896\n\n\n\n# getting closer to 0.5 as the sample increases\n\n\n\n\n\n\n\n\n\nConvergenceConcepts\n package to view the law of great numbers.\n\n\n\n\nDice\n\n\nREF: p.430-431\n\n\nBootstrap\n\n\nREF: p.436\n\n\nLaws\n\n\nREF: p.437-447\n\n\nChapter 11, Confidence Intervals and Hypothesis Testing\n\u00b6\n\n\nConfidence intervals\n\u00b6\n\n\n\n\nUse t-values with at least 30 observation in the sample.\n\n\nWith smaller samples (or larger too), use bootstraps to simulate populations from the \nboot\n package (\nboot()\n and \nboot.ci()\n).\n\n\nFor proportion with large samples, use the \nepitools\n package and \nbinom.approx()\n. With smaller samples, go with \nbinom.test()\n.\n\n\nVariance confidence intervals; test for normality with \nsigma2.test()\n.\n\n\nFor non-parametric sample, again, simulate populations with \nboot()\n and \nboot.ci()\n.\n\n\nFor median, use \nqbinom()\n.\n\n\nFor correlation, use \ncor.test()\n.\n\n\n\n\nREF: p.450-456\n\n\nTest\n\u00b6\n\n\n\n\nIn a test, \n\\alpha\n\\alpha\n is the signification threshold, \nH_0\nH_0\n is the tested hypothesis.\n\n\nAverage tests;  compare the theoretical average to a reference value with the \nt.test()\n.\n\n\nCompare two theoretical averages with a \nt.test()\n.\n\n\nCompare pair samples with \nt.test(paired=TRUE)\n.\n\n\nTest variance(s) with ANOVA.\n\n\nCompare a theoretical variance with a reference value with \nsigma2.test()\n.\n\n\nCompare two theoretical variances with \nvar.test()\n.\n\n\nCompare a theoretical proportion with a reference value with \nprop.test()\n.\n\n\nCompare two theoretical proportions with \nprop.test(\n).\n\n\nTest the theoretical correlation coefficient vs a reference value with \ncor.test()\n and \ncor-.test()\n.\n\n\nTest two theoretical correlation coefficients vs a reference value with \ncor.test.2.sample()\n.\n\n\nTest of independence or chi\u00b2 with \nchisq.test()\n.\n\n\nYates chi\u00b2, adjustment chi\u00b2 with \nchisq.test()\n.\n\n\nFisher test with \nfisher.test()\n.\n\n\nAdequacy test or Shapiro-Walk test with \nshapiro.test()\n.\n\n\nPositional test or sign test or, median sign test with \nprop.test()\n and \nbinom.test()\n.\n\n\nMedian sign test for two independent samples with \nchisq.test()\n and \nfisher.test()\n.\n\n\nSign test for two matching samples with \nprop.test()\n and \nbinom.test()\n.\n\n\nWilcoxon rank test or Mann-Whitney test for two independent samples with \nwilcox.test()\n.\n\n\nWilcoxon test for two matching samples with \nwilcox.test()\n.\n\n\n\n\nREF: p.459-488\n\n\nChapter 12, Simple and Multiple Linear Regression\n\u00b6\n\n\nRegression\n\u00b6\n\n\n\n\nlm(y <sub>x\n.\n\n\nlm(y <sub>0 + x)\n; no intercept.\n\n\nmodel \n<-\n lm\n(\ny \n<\nsub\n>\nx\n)\n; run the regression.\n\n\nsummary(model)\n; extract the results.\n\n\nplot(y <sub>x)\n; plot the results.\n\n\nabline(model)\n; add a line on the observations.\n\n\nconfint(model)\n; confidence intervals; 95% or 2.5% on both sides.\n\n\ncoefficients(model)\n; extract one or several coefficient.\n\n\nmodel$coefficients\n.\n\n\nmodel$call\n.\n\n\nmodel$residuals\n.\n\n\nanova(model)\n.\n\n\npredict(model, data.frame(LWT = prediction), interval = \"prediction\")\n\n\nand many more.\n\n\n\n\nprediction, result, extraction, residual\n\n\nREF: p.498-499\n\n\nNormality\n\u00b6\n\n\nhistogram, test, residual, quantile-quantile, quantile, qq\n \n\n\n1\n2\npar\n(\nmfrow\n=\nc\n(\n1\n,\n2\n))\n\nhist\n(\nresiduals\n(\nmodel\n),\n main \n=\n \n\"Histogram\"\n)\n\n\n\n\n\n\n\n\n\nqqnorm(resid(model), datax = TRUE)\n; quantile-quantile.\n\n\nqqplot()\n.\n\n\nqqline()\n.\n\n\nplot(model, 1:6, col.smooth = \"red\")\n; 6 graphics.\n\n\njarque.bera.test(residuals(model))\n; from the \ntseries\n package.\n\n\ndwtest()\n; Durbin-Watson test from the \nlmtest\n package.\n\n\n\n\nREF: p.502-503\n\n\nCorrelation\n\u00b6\n\n\ntest, explanatory variable interaction, colinearity,  best subset\n\n\n\n\npairs(newdata, lower.panel = panel.smooth, upper.panel = add.cor)\n.\n\n\n\n\nREF: p.506\n\n\nModel improvement\n\u00b6\n\n\n\n\nVariables selection.\n\n\nBest subset, leaps and bounds.\n\n\nForward selection.\n\n\nBackward selection.\n\n\nStepwise selection.\n\n\nResidual analysis.\n\n\nand many more.\n\n\n\n\nREF: p.511-535\n\n\nPolynomial regression\n\u00b6\n\n\nREF: p.535-540\n\n\nChapter 13, Elementary Variance Analysis\n\u00b6\n\n\nanova, repeated measure, between, within, inspection, hypothesis, comparison, factor, table, parameter, repeated\n\n\nREF: p.542-571\n\n\nAPPENDIX\n\u00b6\n\n\nAppendix, Installing the R Software and Packages\n\u00b6\n\n\ninstallation, package\n\n\ninstall.packages('package')\n.\n\n\n\n\nBe sure to launch RStudio as an administrator or a superuser to install a package in R (not in RStudio); the package is accessible to all users. Then load the package in R or RStudio.\n\n\nThen attach the package to the work session with \nlibrary('package')\n or \nrequire('packages')\n.\n\n\n\n\nAnswers to the Exercises\n\u00b6\n\n\nREF:  p.625-674",
            "title": "Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#gui",
            "text": "Rcommander;  Rcmdr  package.    The R Commander: A Basic-Statistics GUI for R    REF: p.3-5",
            "title": "GUI"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#b-datasets",
            "text": "Datasets.",
            "title": "B, Datasets"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#part-1-basics",
            "text": "",
            "title": "PART 1, BASICS"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-1-basic-concept-organizing-data",
            "text": "",
            "title": "Chapter 1, Basic Concept, Organizing Data"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#editors",
            "text": "RStudio.  Tinn-R.  JGR.  Emacs/ESS.",
            "title": "Editors"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#data-entry",
            "text": "x  <-   2 .  2 -> 2 .",
            "title": "Data entry"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#exponent",
            "text": "exp(1) .",
            "title": "Exponent"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#logarithm",
            "text": "log(3) .  log(x = 3) .  log(x = 3, base(exp(1)) .  log(3, exp(1)) .",
            "title": "Logarithm"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#factorial",
            "text": "factorial(2) .",
            "title": "Factorial"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#find-out-about-an-object-a-variable-is",
            "text": "is.character() .  is.vector() .  is.character() .  is.character()  and many more.",
            "title": "Find out about an object, a variable (is.)"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#convert-as",
            "text": "as.character() .  as.raw() .  as.date() .  and many more.",
            "title": "Convert (as.)"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#array-and-matrix",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32 matrix ( 1 : 12 ,  nrow  =   4 ,  ncol  =   3 ,  byrow  =   FALSE ) \n      [, 1 ]   [, 2 ]   [, 3 ]  [ 1 ,]      1      5      9  [ 2 ,]      2      6     10  [ 3 ,]      3      7     11  [ 4 ,]      4      8     12  matrix ( 1 : 12 ,  nrow  =   4 ,  ncol  =   3 ,  byrow  =   TRUE ) \n      [, 1 ]   [, 2 ]   [, 3 ]  [ 1 ,]      1      2      3  [ 2 ,]      4      5      6  [ 3 ,]      7      8      9  [ 4 ,]     10     11     12  array ( 1 : 12 ,  dim  =   c ( 2 ,   2 ,   3 ))  ,   ,   1 \n\n      [, 1 ]   [, 2 ]  [ 1 ,]      1      3  [ 2 ,]      2      4  ,   ,   2 \n\n      [, 1 ]   [, 2 ]  [ 1 ,]      5      7  [ 2 ,]      6      8  ,   ,   3 \n\n      [, 1 ]   [, 2 ]  [ 1 ,]      9     11  [ 2 ,]     10     12",
            "title": "Array and matrix"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#vector",
            "text": "vec  <-   c ( 1.1 ,   2.2 ,   3.5 ) .  vec  <-   1 : 3 .",
            "title": "Vector"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#sequence",
            "text": "seq(1:3) .  1 : 3 .",
            "title": "Sequence"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#list",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 c ( 1 : 3 )   # vector  [ 1 ]   1   2   3  # vs  list ( 1 : 3 )   # list  [[ 1 ]]  [ 1 ]   1   2   3",
            "title": "List"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#data-frame",
            "text": "table, tabular   data.frame(name = c(), name = c(), name = c(), etc) ; each column is a vector with a name.",
            "title": "Data Frame"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#time-series",
            "text": "1\n2\n3\n4\n5 >  ts ( 1 : 10 ,  frequency = 4 ,  start = c ( 1959 , 2 )) \n     Qtr1 Qtr2 Qtr3 Qtr4 1959           1      2      3  1960      4      5      6      7  1961      8      9     10",
            "title": "Time series"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#class-and-mode",
            "text": "type, data, variable, object   mode() .  class () ; mode != class.  typeof() ; type of storage.",
            "title": "Class and mode"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-2-import-export-and-producing-data",
            "text": "import, export, i/o",
            "title": "Chapter 2, Import-Export and Producing Data"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#input-data-from-files",
            "text": "1 read.table ( file  =  path / file.txt ,  header  =   TRUE ,  sep =   \"\\t\" ,  dec = \".\" ,  row.names  =   1 )     attach(data) ; dataset is attached to .GlobalEnv.  search() ; search objects in .GlobalEnv, including attached dataset,  detach(data) .   More about .GlobalEnv in Chapter 7, Session Management.  Read .csv and .tsv   read.csv() .  read.csv2() .  read.delim() .  read.delim2() .  and many more.   Read text files   read.ftable(\"file.txt\", row.var.names = c(...), col.vars = list()) .  scan(\"file.txt\", skip = 9, nlines = 1, what = \"\", dec = \"\") .  and many more.   Read software files   .sav ; SPSS.  read.spss .    .mtp ; Minitab.  read.mtp .    .xpt ; SAS en data.frame.  read.xport .    .mat ; Matlab.  readMat() .    and many other formats and commands.",
            "title": "Input data from files"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#ouput-data-and-export",
            "text": "lookup.xport() ; for SAS.  write.table(data, file=\"file.txt\", sep=\"\\t\") .  xlsReadWrite() .  and many more.",
            "title": "Ouput data and export"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#measure-computation-time",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 # start the timer \ntmps  <-   Sys.time ()  # run \ndbsnp  <-  read.table ( \"file\" )  # stop the timer  Sys.time ()   -  tmps",
            "title": "Measure computation time"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#produce-by-repetition",
            "text": "repeat  1\n2 rep ( 1 : 4 ,  reach  =   2 ,  len  =   10 )  [ 1 ]   1   2   3   4   1   2   3   4   1   2",
            "title": "Produce by repetition"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#produce-random-numbers",
            "text": "1\n2\n3\n4\n5\n6 # generate random numbers between 0 and 1 \nrunif ( 5 )  [ 1 ]   0.2424283   0.6140730   0.4824881   0.7263319   0.1381030 \n\nrunif ( 5 ,  min  =   2 ,  max  =   7 )  [ 1 ]   4.588744   5.522278   4.307162   6.248397   3.854982    More on random number in Chapter 10, Random Variables, Laws, and Simulation.  Produce random number following a distribution  1\n2\n3 # generate random numbers following the normal distribution \nrnorm ( 5 )  [ 1 ]    0.8752170    1.3869022   -0.4419174   -0.6129075   -1.6987139    Generate numbers with other distributions.  Produce random number by sampling a population  1\n2\n3\n4\n5 urne  <-   0 : 9  # 20 draws from 'urne'  sample ( urne ,   20 ,  replace  =   TRUE )  [ 1 ]   5   4   2   2   9   7   4   6   2   2   7   8   3   3   9   6   6   1   1   0",
            "title": "Produce random numbers"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#produce-data-by-manual-input-vector-like",
            "text": "First,   z  <-   scan () .  Second, input data in the prompt.",
            "title": "Produce data by manual input (vector-like)"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#produce-data-with-a-mini-spreadsheet-tabular-like",
            "text": "x  <-   as.data.frame ( de ( \"\" )) ; open a spreadsheet.  data.entry(\"\") ; alternatively.  Input data; column are variables like in a data frame.    fix(x) ; invokes edit on x, then assigns the new (edited) version of x to the user\u2019s workspace.",
            "title": "Produce data with a mini-spreadsheet (tabular-like)"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#list-of-objects",
            "text": "ls() ; list of objects.  rm(list = ls()) ; remove all the objects.",
            "title": "List of objects"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#read-from-and-write-to-a-database",
            "text": "RODBC  package.  odbcConnect() .  sqlQuery() .  odbcClose() .   REF: p.82-84",
            "title": "Read from and write to a database"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#file-management",
            "text": "file.choose() ; open a window.",
            "title": "File management"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#read-from-the-clipboard",
            "text": "First, copy from a spreadsheet or a table.  Second,  read.clipboard() .",
            "title": "Read from the clipboard"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-3-data-manipulation",
            "text": "",
            "title": "Chapter 3, Data Manipulation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#arithmetics",
            "text": "1\n2\n3\n4\n5 x  <-   c ( 1 , 2 , 3 ) \ny  <-   c ( 4 , 5 , 6 ) \n\nx  +  y [ 1 ]   5   7   9",
            "title": "Arithmetics"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#built-in-functions",
            "text": "length(vec) ; length.  sort(vec, decreasing = TRUE) ; sort.  rev(vec) ; inverse sorting.  order(vec) ; sort a vector according to the names or a list of strings.  names ( vec )   <-   1 : 9 ; attribute names.  rank(vec) ; rank the elements.  unique(vec) ; remove doubles.  duplicated(vec) ; create a TRUE/FALSE vector indicating doubles.  x %% y ;  modulus (x mod y).  x %/% y ; integer division.",
            "title": "Built-in functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#dimension-functions",
            "text": "number, row, column, dimension   dim(df) .  nrow(df) .  ncol(df) .",
            "title": "Dimension functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#name-functions",
            "text": "dimnames(df) .  names(df) ,  colnames(df) .  rownames(df) .",
            "title": "Name functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#merge-functions",
            "text": "combine   cbind() .  rbind() .   REF: p.98   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 y  <-   array ( 1 : 12 ,  dim  =   c ( 4 ,   3 )) \n\ny\n      [, 1 ]   [, 2 ]   [, 3 ]  [ 1 ,]      1      5      9  [ 2 ,]      2      6     10  [ 3 ,]      3      7     11  [ 4 ,]      4      8     12 \n\ny  <-   cbind ( y ,   c ( 100 ,   101 ,   102 ,   103 )) \n\ny\n      [, 1 ]   [, 2 ]   [, 3 ]   [, 4 ]  [ 1 ,]      1      5      9    100  [ 2 ,]      2      6     10    101  [ 3 ,]      3      7     11    102  [ 4 ,]      4      8     12    103  merge ( x ,  y ) \n  V1 V2 V3  V4 1    1    5    9   100  2    2    6   10   101  3    3    7   11   102  4    4    8   12   103  # idem with rows    REF: p.96-98   gtools  package.  smartbind(x,y) ; for two data frames, similar to merge.",
            "title": "Merge functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#apply-functions-and-family",
            "text": "excel, wrangle  Among the most useful function for \u2018wrangling\u2019 data. Excel-like power. When and how to use them.   apply() .  lapply() .  sapply() .  mapply() .  by() .  with() .  replicate() .  transform() .  rowSums(df) .  colSums(df) .  rowMeans(df) .  colMeans(df) .  sweep() .  stack() .  unstack() .  aggregate()   REF: p.99",
            "title": "apply functions and family"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#sweep-functions",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 u\n  V1 V2 V3  V4 1    1    5    9   100  2    2    6   10   101  3    3    7   11   102  4    4    8   12   103  # removes pattern '3, 5, 3, 5, etc.'  sweep ( u ,  MARGIN  =   1 ,  STATS  =   c ( 3 ,   5 ),  FUN  =   \"-\" ) \n  V1 V2 V3 V4 1   -2    2    6   97  2   -3    1    5   96  3    0    4    8   99  4   -1    3    7   98  # divide by a vector  sweep ( u ,  MARGIN  =   2 ,  STATS  =   c ( 2 ,   2 ,   3 ,   3 ),  FUN  =   \"/\" ) \n   V1  V2       V3       V4 1   0.5   2.5   3.000000   33.33333  2   1.0   3.0   3.333333   33.66667  3   1.5   3.5   3.666667   34.00000  4   2.0   4.0   4.000000   34.33333    REF: p.100-101",
            "title": "Sweep functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#stack-functions",
            "text": "stack, unstack   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 u\n  V1 V2 V3  V4 1    1    5    9   100  2    2    6   10   101  3    3    7   11   102  4    4    8   12   103 \n\nv <-  stack ( u ) \nv\n   values ind 1         1   V1 2         2   V1 3         3   V1 4         4   V1 5         5   V2 6         6   V2 7         7   V2 8         8   V2 9         9   V3 10       10   V3 11       11   V3 12       12   V3 13      100   V4 14      101   V4 15      102   V4 16      103   V4\n\nw  <-  unstack ( v ) \nw\n  V1 V2 V3  V4 1    1    5    9   100  2    2    6   10   101  3    3    7   11   102  4    4    8   12   103",
            "title": "Stack functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#aggregation-functions",
            "text": "aggregate   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 w\n  V1 V2 V3  V4 1    1    5    9   100  2    2    6   10   101  3    3    7   11   102  4    4    8   12   103 \n\nfac  <-   c ( \"a\" ,   \"b\" ,   \"b\" ,   \"a\" ) \nx  <-   cbind ( w ,  fac ) \nx\n  V1 V2 V3  V4 fac 1    1    5    9   100    a 2    2    6   10   101    b 3    3    7   11   102    b 4    4    8   12   103    a\n\naggregate ( w ,  by  =   list ( x $ fac ),   sum ) \n  Group.1 V1 V2 V3  V4 1        a   5   13   21   203  2        b   5   13   21   203    REF: p.101",
            "title": "Aggregation functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#boolean-logical-functions",
            "text": "logical(2) ; generate two FALSE in a vector; change the length.  !logical(2) ; generate two TRUE.  as.logical(vec) .  is.logical(vec) .  isTRUE() .  & ; AND.  && ; sequential AND.  | ; OR.  || ; sequential OR.  Prefer  &&  over  & , and  ||  over  | . Assessments go from left to right, and keep on going as long as the conditions are TRUE.  xor() ; exclusive OR.  if ,  else .  any() ; if one or another is TRUE.  all() ; if all are TRUE.  identical() ; if all are identical.  all.equal() .  ==  or  all.equal  (and  != ) can yield a FALSE because decimal are different on large numbers.  all.equal(x, y, tolerance = 10^-6)  fixes the problem.    ifelse(cond, a, b) ; if  cond  is TRUE,  a , else,  b .  x   <-   c ( 3 :- 2 );   sqrt ( ifelse ( x   >=   0 ,   x ,   NA ) .     REF: p.126-127",
            "title": "Boolean &amp; logical functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#venn-functions",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 A  <-   1 : 3 \nB  <-   3 : 6  is.element ( 1 ,  A )  [ 1 ]   TRUE  is.element ( 4 ,  A )  [ 1 ]   FALSE  is.element ( 4 ,  B )  [ 1 ]   TRUE  all ( A  %in%  B )  [ 1 ]   FALSE  all ( B  %in%  A )  [ 1 ]   FALSE  intersect ( A ,  B )  [ 1 ]   3  union ( A ,  B )  [ 1 ]   1   2   3   4   5   6  setdiff ( A ,  B )  [ 1 ]   1   2  setdiff ( B ,  A )  [ 1 ]   4   5   6  intersect ( A ,  B )  [ 1 ]   3",
            "title": "Venn functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#vector-functions",
            "text": "vec[2] ; extract.  vec[2:5] ; extract.  vec[c(T, F, T)] ; extraction with filter.  vec[vec > 4] ; conditional extraction.  vec[vec == 3] .  vec[which.max(z)] ; extract the maximum value.  vec[which.min(z)] .  vec > 4 ; yield a vector of TRUE or FALSE.  vec[-2] ; exclude.  vec[-c(1,5)] ; exclude.",
            "title": "Vector functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#search-functions",
            "text": "masque  <-   c ( TRUE ,   FALSE ) .  which(masque) ; return the TRUE indices.  which.min(x) ; return the index with minimum value.  which.max(x) .",
            "title": "Search functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#replace-functions",
            "text": "z [ c ( 1 ,   5 )]   <-   1 ; replace value 1 and 5 by 1.  z [ which.max ( z )]   <-   0 ; replace the maximum value.  z [ z  ==   0 ]   <-   8 ; replace zeros and FALSE.",
            "title": "Replace functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#extend-a-vector",
            "text": "vecA .  vecB  <-   c ( vecA ,   4 ,   5 ) .  vecC  <-   c ( vecA [ 1 : 4 ],   8 ,   5 ,  vecA [ 5 : 9 ]) .",
            "title": "Extend a vector"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#matrix-and-array",
            "text": "mat[r, c] ; extract.  mat[1, 2] .  mat[,2] ; all rows, column 2 only.  mat[1, ] ; all columns, row 1 only.  mat[c(1, 3), c(4:5)] .  mat[, 1, drop = FALSE] ; avoid making a (horizontal) row with a (vertical) column.  mat[ind] ; matrix index.  array[r, c, m] ; extract.   REF: p.110-113",
            "title": "Matrix and array"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#lists",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 char  <-   c ( \"a\" ,   \"b\" ,   \"c\" ) \nnumb  <-   c ( 1 ,   2 ,   3 ) \ngreek  <-   c ( \"alpha\" ,   \"beta\" ,   \"gamma\" ) \n\nx  <-   list ( char ,  numb ,  greek ) \nx [[ 1 ]]  [ 1 ]   \"a\"   \"b\"   \"c\"  [[ 2 ]]  [ 1 ]   1   2   3  [[ 3 ]]  [ 1 ]   \"alpha\"   \"beta\"    \"gamma\"  names ( x )   <-   c ( \"char\" ,   \"numb\" ,   \"greek\" ) \nx $ char [ 1 ]   \"a\"   \"b\"   \"c\"  $ numb [ 1 ]   1   2   3  $ greek [ 1 ]   \"alpha\"   \"beta\"    \"gamma\" \n\nx [ 2 ]  $ numb [ 1 ]   1   2   3 \n\nx [[ 2 ]][ 2 ]  [ 1 ]   2 \n\nx $ numb [ 2 ]  [ 1 ]   2    REF: p.113-115",
            "title": "Lists"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#string",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 \"bla bla bla\"  [ 1 ]   \"bla bla bla\"  noquote ( \"bla bla bla\" )  [ 1 ]  bla bla bla sQuote ( \"bla bla bla\" )  [ 1 ]   \"\u2018bla bla bla\u2019\"  dQuote ( \"bla bla bla\" )  [ 1 ]   \"\u201cbla bla bla\u201d\"",
            "title": "String"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#text",
            "text": "wrangle, text, string, character, natural language processing, nlp    format() ; and arguments:   digits .  trim .  digit .  nsmall .  justify .  width .  na.encode .  decimal.mark .  drop0trailing .  and many more.     cat(\"current working dir: \", wd) ; print the objects, concatenate the representations.   printf(\"hello %d\\n\", 56) ; mix text and data; pythonic print.  print(paste0(\"current working dir: \", wd)) .  nchar() ; number of characters.  x[nchar(x) > 2] .  x[x %n% c(letters, LETTERS)] ; retrieve letters, patterns or strings in a text object; alike Venn.  paste(ch1, ch2, sep = \"-\") ; concatenate.  paste0(ch1, ch2) ; concatenate.  substring(\"abcdef\", first = 1:3, last = 2:4) ; create subsets  ab ,  bc ,  cd .  strsplit(c(\"\",\"\"), split=\" \") ; break down a string.  grep(\"i\", c()) ; extract an object index.  gsub(\"i\", \"L\", c()) ; substitute.  sub() ; substitute the first occurrence.  tolower() .  toupper() .",
            "title": "Text"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#date-and-time",
            "text": "convert, extract   Sys.time() .  date() .  Sys.setlocale() .  as.numeric() .  strptime() ; extract time.  as.POSIXlt() .   REF: p.120-123",
            "title": "Date and time"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#custom-functions-two-examples",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 a  <-   2 \nb  <-   -3  # quadratic \nf  <-   function   ( x )   {  x ** 2   -   2 * x  -   2   } \n\nf ( a )  [ 1 ]   -2 \n\nf ( b )  [ 1 ]   13  # test \ng  <-   function   ( x ,  y )   { \n   if   ( x  <=  y )   { \n    z  <-  y  -  x\n     print ( \"x smaller\" ) \n   }   else   { \n    z  <-  x  -  y\n     print ( \"x larger\" ) \n   }  }  \n\ng ( a ,  b )  [ 1 ]   \"x larger\" \n\ng ( a ,   abs ( b ))  [ 1 ]   \"x smaller\"    More about custom functions in  Chapter 6, Initiation to R Programming .",
            "title": "Custom functions (two examples)"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#loops-structure",
            "text": "for .  while .  repeat .    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # while  while ( x  +  y  <   7 )   {  x  <-  x  +  y  }  # for  for   ( i  in   1 : 4 )   { \n     if   ( i  ==   3 )   break \n     for   ( j  in   6 : 8 )   { \n         if   ( j  ==   7 )   next \n        j  <-  i  +  j\n     }  }  # repeat \ni  <-   0  repeat   { \n    i  <-  i  +   1 \n     if   ( i  ==   4 )   break  }    Loop example   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 IMC  <-   function   ( poids ,  taille )   { \n  imc  <-  poids  /  taille ^ 2 \n   names ( imc )   <-   \"IMC\" \n   return ( imc )  } \n\nIMC ( 100 ,   1.90 ) \n     IMC  27.70083  \n\np  <-   c ( 100 ,   101 ,   95 ,   97 ) \nt  <-   c ( 1.90 ,   1.75 ,   1.68 ,   1.92 ) \n\nIMC ( p ,   t ) \n     IMC      < NA >       < NA >       < NA >   27.70083   32.97959   33.65930   26.31293   for   ( i  in   1 : 4 )   { \n   print ( IMC ( data [ i , 1 ],  data [ i , 2 ]))  } \n     IMC  27.70083  \n     IMC  32.97959  \n    IMC  33.6593  \n     IMC  26.31293     Loops increase computation time  1\n2\n3\n4\n5\n6\n7 system.time ( for   ( i  in   1 : 1000000 )   sqrt ( i )) \nutilisateur     syst\u00e8me      \u00e9coul\u00e9 \n        0.19          0.01          0.22   system.time ( sqrt ( 1 : 1000000 )) \nutilisateur     syst\u00e8me      \u00e9coul\u00e9 \n        0.07          0.00          0.07     REF: p.133-136",
            "title": "Loops structure"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#binary-and-decimal",
            "text": "convert   bin2dec() .  dec2bin() .   REF: p.136-144, 147-152",
            "title": "Binary and decimal"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-4-r-documentation",
            "text": "",
            "title": "Chapter 4, R Documentation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#help",
            "text": "?functionname .  help(functionname) .  help.start() ; user manual in the browser.  apropos('mean') ; object with the name \u2018mean\u2019.   Library   install.packages('package') ; should be done as an administrator or superuser directly in R, not an editor. Once the package is installed, it can be loaded on the PC by any user in R or an editor (such as RStudio).  library(package = 'package') ; load a package.  library(help = package) ; help on a specific package.  library(help = base) ; for example.  library(help = utils) ; for example.  library(help = datasets) ; for example.  library(help = graphics) ; for example.  library(help = grDevices) ; for example.    library(lib.loc = .Library) ; find which package (system packages, but not user packages).  find('function') ; find which package (system).  data() ; datasets available.  demo() ; available demos by package.  demo(graphics) ; give examples.  example(mean) ; give examples.",
            "title": "Help"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#resources",
            "text": "nmz ; search the content of the R functions, package vignette, and task views.  R-Project Mailing Lists ; mailing lists.  RSeek ; search engine.  StackOverflow ; forum.  Abcd\u2019R ; scripts.  CIRAD ; forum.  Developpez.net ; forum.  ETHZ ; mailing list.  ETHZ Manual .  http://a ; manuals and resources. \nstat.ethz.ch/mailman/listinfo/r-annonce  ETHZ List Info .  R Programming Wikibooks ; wiki.  CRAN-R .  CRAN-R Doc ; PDF and documentation.  CRAN-R Views ; projects.  R Journal ; publication.  Journal of Statistical Software ; publication.",
            "title": "Resources"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-5-techniques-for-plotting-graphics",
            "text": "",
            "title": "Chapter 5, Techniques for Plotting Graphics"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#graphic-windows",
            "text": "Of course, these commands are automated in an editor (RStudio).   dev.new() ; graphic window on Windows.  windows() ; graphic window on Windows.  win.graph() ; graphic window on Windows.  X11() ; graphic window on Linux.  Add parameters in the command:  width = , height = .  pointsize = .  xpinch = , ypinch = ; pixels per inch.  xpos = , ypos = ; position of the upper left corner, in pixel.    dev.set(num) ; activate window number \u2018num\u2019; there can be several graphic windows.  dev.off(num) ; close a window.  graphics.off() ; close all windows.  dev.list() ; return window numbers.  dev.cur() ; return the current number, active window.  dev.print(png, file = , width = , height = ) ; print.  savePlot(filename = , type = \"png\") ; save.  png(file=\" \", width = , height = ) ; save directly in .png.  jpeg() .  bitmap() .  postscript() .  pdf() .    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 # examples \n\ndev.new ( width  =   200 ,  height  =   200 ,  xpos  =   100 ,  ypos  =   100 )  NULL \n\ndev.list () \nRStudioGD       png   windows \n         2           3           4  \n\ndev.cur () \nwindows \n       4 \n\nhist ( runif ( 100 ))   # create a graphic  # in the current working directory \ndev.print ( png ,  file  =   \"mygraph.png\" ,  width  =   480 ,  height  =   480 ) \nsavePlot ( filename  =   \"mygraph.png\" ,  type  =   \"png\" ) \npdf ( file  =   \"mygraph.pdf\" ) \n\ndev.off ()",
            "title": "Graphic windows"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#multiple-windows",
            "text": "par(mfrow = c(3, 2)) ; create 6 graphic boxes, 3 rows X 2 columns.   REF: p.166-167  1\n2\n3\n4\n5\n6\n7\n8 dev.new () \nmat  <-   matrix ( c ( 2 ,   3 ,   0 ,   1 ,   0 ,   0 ,   0 ,   4 ,   0 ,   0 ,   0 ,   5 ),   4 ,   3 ,  byrow  =   TRUE ) \nlayout ( mat ) \n\n\ndev.new () \nlayout ( mat ,  widths  =   c ( 1 ,   5 ,   14 ),  heights  =   c ( 1 , 2 , 4 , 1 )) \nlayout.show ( 5 )    REF: p.168",
            "title": "Multiple windows"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#draw-on-a-graphic",
            "text": "segments(x0 = 0, y0 = 0, x1 = 1, y1 = 1) ; draw lines on a plot.  lines(x = c(1,0), y = c(0,1)) .  abline(h = 0, v = 0) ; add a line to a plot.  abline(a = 1, b = 1) .",
            "title": "Draw on a graphic"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#random-numbers",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 x  <-  runif ( 12 ) \nx [ 1 ]   0.03344539   0.22711659   0.66696650   0.02840671   0.61067995   0.92957527  [ 7 ]   0.26962190   0.60013387   0.04831111   0.12603905   0.41913598   0.13142315  # ranking \ni  <-   order ( x ) \ni [ 1 ]    4    1    9   10   12    2    7   11    8    5    3    6  # reorder \nx  <-  x [ i ] \nx [ 1 ]   0.02840671   0.03344539   0.04831111   0.12603905   0.13142315   0.22711659  [ 7 ]   0.26962190   0.41913598   0.60013387   0.61067995   0.66696650   0.92957527    More on random number in Chapter 10, Random Variables, Laws, and Simulation.  Examples   example(polygon) ; see examples.  curve(x**3 - 3*x, from = -2, to = 2) ; trace a curve according to a function.  hist(rnorm(10000), prob = TRUE, breaks = 100) .  plot(runif(7), type = \"h\", axes = F); box(lty = \"1373\")  plot(1:10, runif(10), type = \"l\", col = \"orangered\")   Help   colors()[grep(\"orange\", colors())] ; list of tones.",
            "title": "Random numbers"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#colors",
            "text": "rgb(red = 26, green = 204, blue = 76, maxColorValue = 255) ; return the code.  rgb(red = 0.1, green = 0.8, blue = 0.3) .  col2rgb(\"#1AVV4C\") ; inversely.  R generates 256\u00b3 colors, 15M.  rainbow(#) ; show 1, 8, 17 or more colors; R can generate 256\u00b3 colors (over 15M),  pie(rep(1,200), labels = \"\", col = rainbow(200), border = NA) ; show the colors.  plot(1:40, col = rainbow(n), pch = 19, cex = 2),     grid(col = \"grey50\") ; show.  RColorBrewer  package.   1\n2 library ( 'RColorBrewer' ) \ndisplay.brewer.all ()",
            "title": "Colors"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#images",
            "text": "caTools  package to manage images.  read.gif() .  image() .",
            "title": "Images"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#write-and-add-marks-on-a-graphic",
            "text": "text(coord x, coord y, 'text') ; write.  demo(plotmath) ; see examples.  mtext('bas', side = 1) ; write \u2018bas\u2019 on the graphic box; 4 sides of the box (bottom, left, top, right).  locator() ; point with the mouse on a graphic to record coordinates;  esc  to show the coordinates. \nplot(1,1); locate area to be annotated with  text() .  text(locator(1), label=\"ici\") ; add a label, one or several occurrences, by pointing the location with the mouse.  identify(occurrence, label) ; add one or more labels by pointing the location with the mouse.",
            "title": "Write and add marks on a graphic"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#graphic-parameters-and-graphic-windows",
            "text": "margin, border, box, square, space, height, width, color, text, title, label, mark, font, police, character, language, size, length, size, tick, coordinate, x, y, log, scale, axis, axes, format, alignment, point",
            "title": "Graphic parameters and graphic windows"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#advanced-graphic-package",
            "text": "rgl .  lattice .  ggplot2 .   REF: p.190-201",
            "title": "Advanced graphic package"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-6-initiation-to-r-programming",
            "text": "1\n2\n3 function   ( < parameters > )   { \n     < body >  }     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 lance  <-   function   ( nom )   { \n   cat ( \"Bonjour\" ,  nom ,   \"!\" )  } \n\nlance ( 'allo' ) \nBonjour Alex  ! \n\nbonjour  <-   function   ( nom = \"Pierre\" ,  langue = \"fr\" )   { \n   cat ( switch ( langue ,  fr = \"Bonjour\" ,  esp = \"Hola\" ,  ang = \"Hi\" ),  nom ,   \"!\" )  } \n\nbonjour () \nBonjour Pierre  ! \nbonjour ( nom = \"Ben\" )   # replace the default value. \nBonjour Ben  ! \nbonjour ( langue = \"ang\" ) \nHi Pierre  ! \nbonjour ( lang  =   \"ang\" )   # partial call \nHi Pierre  ! \nbonjour ( l = \"ang\" ) \nHi Pierre  ! \nbonjour ( l = \"a\" ) \n Pierre  !    REF: p.218-219",
            "title": "Chapter 6, Initiation to R Programming"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#union",
            "text": "1\n2\n3\n4\n5\n6 \"%union%\"   <-   function   ( A , B )   {   union ( A , B )   } \nA  <-   c ( 4 , 5 , 2 , 7 ) \nB  <-   c ( 2 , 1 , 7 , 3 ) \n\nA  %union%  B [ 1 ]   4   5   2   7   1   3",
            "title": "Union"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#class",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 obj  <-   1 : 10  class ( obj )  [ 1 ]   \"integer\"  class ( obj )   <-   \"TheClass\"  class ( obj )  [ 1 ]   \"TheClass\"  inherits ( obj ,   \"TheClass\" )  [ 1 ]   TRUE",
            "title": "Class"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#methods",
            "text": "1\n2\n3\n4 x  <-   1 : 10  print.default ( x )  [ 1 ]   1   2   3   4   5   6   7   8   9   10    REF: p.227-231",
            "title": "Methods"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#combine-and-permute",
            "text": "combinat  package.  combn(5,3) ; combine 3 numbers from 1:5.  choose(200,3) ; choose 3 numbers from 1:200.  permn(n,m) .   Very time consuming!",
            "title": "Combine and permute"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#more-power-speed",
            "text": "The core of R in programmed in C. Converting a R function into C is easy. C is faster.  Call it through API C.  With R graphic interface and C computation speed, it is the best of both world.  Easier to set up on Linux or OS X (the OS has default compilers. Use the  Rcpp  package.   On Windows, there is a need for  Rtools .   R can be compiled (byte compiler) with the  RevoScaleR  package (parallel computing).  bigmemory ,  ff , packages; split a matrix into sub-matrices, perform computation and combine the results (parallel computing).   Use a multi-core architecture with the  parallel  package.  HighPerformanceComputing list of packages .   Try parallel computing with a Monte Carlo with the  parallel  package   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 myfunc  <-   function ( M = 1000 )   { \n    decision  <-   0 \n     for   ( i  in   1 : M )   { \n      x  <-  rnorm ( 100 ) \n       if   ( shapiro.test ( x ) $ p  <   0.05 )  decision  <-  decision  +1 \n     }  return ( decision )  }  system.time ({ \n    M  <-   60000 \n    decision  <-  myfunc ( M ) \n     print ( decision / M )  })     For a parallel execution.  Start menu.  Type devmgmt.msc  Under Processors in Linux, type  top  in the terminal.  Then, type  1  in R.  Enter  detectCores()  from the  parallel  package.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 require ( \"parallel\" )  system.time ({ \n    nbcores  <-   6   # less than detectCores() - 1 \n    M  <-   60000 \n    cl  <-  makeCluster ( nbcores ,  type  =   \"PSOCK\" ) \n    out  <-  clusterCall ( cl ,  myfunc ,   round ( M / nbcores )) \n    stopCluster ( cl ) \n    decision  <-   0 \n     for   ( clus  in   1 : nbcores )   { \n        decision  <-  decision  +  out [[ clus ]] \n     } \n     print ( decision / ( round ( M / nbcores ) * nbcores ))  })     The process number (PID) of each computation node (core) in the cluster.   1\n2\n3\n4 require ( \"parallel\" )  Sys.getpid () \ncl  <-  makeCluster ( 4 , type = \"PSOCK\" ) \nout  <-  clusterCall ( cl ,   Sys.getpid ))",
            "title": "More power, speed"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#involve-the-graphical-card-for-more-power",
            "text": "Run computations with the graphical card, the GPU.  gputools  package.     REF: p.303-308",
            "title": "Involve the graphical card for more power"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-7-session-management",
            "text": "work, session, save, object, instruction, graphic, create, package",
            "title": "Chapter 7, Session Management"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#environment",
            "text": "globalenv() ; .GlobalEnv.  new.env() ; new environment with its own functions, variable, etc.  ls() ; list of objects in the environment.  objects() ; idem.  rm() ; remove one or more objects.  rm(list = ls()) ; remove all.  .RData; workspace file extension.  file.RData; R file.  save.image(\"file.RData\") ; save a R workspace to the current working directory. Several workspace can have their own objects.  load(\"file.RData\") ; load a R workspace from the current working directory.  load(file.choose()) ; open the current working directory.  getwd() ; get the current working directory.  setwd() ; set the current working directory.  .Rhistory; history file extension.  history() ; consult the R log.  savehistory() ; save the log to a file.  loadhistory() ; load the log from a file.  search() ; list of attached packages.  searchpaths() ;  list of paths.  library() ; list of packages in memory.  require(\"packages\") ,  library(\"packages\") ; load a package.  attach(data) ; attach a dataset to .GlobalEnv..  detach(data) .  ls(pos = 1) ,  ls() ; object list in database 1.  ls(pos = 2) ; object list in database 2.  ls(pos = match(\"package:datasets\", search())) ; list datasets.  ls(data) ; list object related to the dataset.  fix(data) ; open a spreadsheet with the data.  sink(file = \"sortie.txt\") ; save a file on the current working directory.  sink() ; stop the recording.",
            "title": "Environment"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#file-manipulation",
            "text": "Low-level interface to the computer\u2019s file system.  Create a file.  Execute the command.  Check out the result in the text files themselves.  file.create(\"sorty.txt\", showWarnings = TRUE) .  file.exists(\"sorty.txt\") .  file.remove(\"sorty.txt\") .  file.rename(\"sorty.txt\", \"sorti.txt\") .  file.append(\"sorty.txt\", \"sorti2.txt\") .  file.copy(\"sorty.txt\", \"sorti2.txt\") .  file.symlink(\"sorty.txt\", \"sorti2.txt\") .  file.path(\"sorty.txt\") .  file.show() .  list.files() .  unlink() .  basename() .  path.expand() .  list.files() .  file.exists() .  memory.size() .  memory.limit() .   REF: p.320-330",
            "title": "File manipulation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#memory-management",
            "text": "The KSysGuard software analyzes memory usage in real time. It is a task manager and performance monitor for UNIX-like OS.",
            "title": "Memory management"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#create-a-package",
            "text": "How to: build a list of R instruction, load a dataset, create function, and output the results.  Run a series of scripts in batch, run a script at a distance.  Set the PATH to an executable Rgui.exe.  Open a R script without opening R.  Create a runthis script, apply  chmod  to use and execute it. The bash script launch R commands (it must begin with #!/in/bash).  and much more.   REF: p.332-338",
            "title": "Create a package"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#part-2-mathematics-and-basic-statistics",
            "text": "",
            "title": "PART 2, MATHEMATICS AND BASIC STATISTICS"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-8-basic-mathematics-matrix-algebra-integration-optimization",
            "text": "",
            "title": "Chapter 8, Basic Mathematics, matrix algebra, integration, optimization"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#math-functions",
            "text": "REF: p.342",
            "title": "Math functions"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#matrix-calculation",
            "text": "1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178 A  <-   matrix ( c ( 2 , 3 , 5 , 4 ),  nrow  =   2 ,  ncol  =   2 ) \nB  <-   matrix ( c ( 1 , 2 , 2 , 7 ),  nrow  =   2 ,  ncol  =   2 ) \n\nA\n      [, 1 ]   [, 2 ]  [ 1 ,]      2      5  [ 2 ,]      3      4 \n\nB\n      [, 1 ]   [, 2 ]  [ 1 ,]      1      2  [ 2 ,]      2      7 \n\nA  +  B\n      [, 1 ]   [, 2 ]  [ 1 ,]      3      7  [ 2 ,]      5     11 \n\nA  -  B\n      [, 1 ]   [, 2 ]  [ 1 ,]      1      3  [ 2 ,]      1     -3  # scalar multiplication \nA  *  B\n      [, 1 ]   [, 2 ]  [ 1 ,]      2     10  [ 2 ,]      6     28  # matrix multiplication \nA  %*%  B\n      [, 1 ]   [, 2 ]  [ 1 ,]     12     39  [ 2 ,]     11     34  # scalar multiplication \na  <-   10 \na  *  A\n      [, 1 ]   [, 2 ]  [ 1 ,]     20     50  [ 2 ,]     30     40  # transpose  t ( A ) \n      [, 1 ]   [, 2 ]  [ 1 ,]      2      3  [ 2 ,]      5      4  # inverse  solve ( B ) \n            [, 1 ]         [, 2 ]  [ 1 ,]    2.3333333   -0.6666667  [ 2 ,]   -0.6666667    0.3333333  solve ( A )   %*%  B\n            [, 1 ]        [, 2 ]  [ 1 ,]    0.8571429    3.857143  [ 2 ,]   -0.1428571   -1.142857  t ( A )   *  B\n      [, 1 ]   [, 2 ]  [ 1 ,]      2      6  [ 2 ,]     10     28 \n\nx  <-   seq ( 1 , 4 ) \ny  <-   seq ( 4 , 7 ) \n\nx [ 1 ]   1   2   3   4 \ny [ 1 ]   4   5   6   7  outer ( x ,  y ,  FUN  =   \"*\" ) \n      [, 1 ]   [, 2 ]   [, 3 ]   [, 4 ]  [ 1 ,]      4      5      6      7  [ 2 ,]      8     10     12     14  [ 3 ,]     12     15     18     21  [ 4 ,]     16     20     24     28  # Kronecker  kronecker ( A ,  B ) \n      [, 1 ]   [, 2 ]   [, 3 ]   [, 4 ]  [ 1 ,]      2      4      5     10  [ 2 ,]      4     14     10     35  [ 3 ,]      3      6      4      8  [ 4 ,]      6     21      8     28  # triangle  lower.tri ( A ) \n       [, 1 ]    [, 2 ]  [ 1 ,]   FALSE   FALSE  [ 2 ,]    TRUE   FALSE  lower.tri ( A ,  diag  =   TRUE ) \n      [, 1 ]    [, 2 ]  [ 1 ,]   TRUE   FALSE  [ 2 ,]   TRUE    TRUE  lower.tri ( A )   *  A\n      [, 1 ]   [, 2 ]  [ 1 ,]      0      0  [ 2 ,]      3      0  # diagonal  diag ( A )  [ 1 ]   2   4  sum ( diag ( A ))  [ 1 ]   6  # kappa  kappa ( A ,  exact  =   TRUE )  [ 1 ]   7.582401  # matrix reduction  scale ( A ,  scale  =   FALSE ) \n      [, 1 ]   [, 2 ]  [ 1 ,]   -0.5    0.5  [ 2 ,]    0.5   -0.5  attr (, \"scaled:center\" )  [ 1 ]   2.5   4.5  scale ( A ,  center  =   FALSE ,  scale  =   apply ( A ,   2 ,  sd )) \n          [, 1 ]       [, 2 ]  [ 1 ,]   2.828427   7.071068  [ 2 ,]   4.242641   5.656854  attr (, \"scaled:scale\" )  [ 1 ]   0.7071068   0.7071068  # eigenvalue  eigen ( A )  $ values [ 1 ]    7   -1  $ vectors\n            [, 1 ]         [, 2 ]  [ 1 ,]   -0.7071068   -0.8574929  [ 2 ,]   -0.7071068    0.5144958  # singular value vector  svd ( A )  $ d [ 1 ]   7.285383   0.960828  $ u\n            [, 1 ]         [, 2 ]  [ 1 ,]   -0.7337222   -0.6794496  [ 2 ,]   -0.6794496    0.7337222  $ v\n            [, 1 ]         [, 2 ]  [ 1 ,]   -0.4812092    0.8766058  [ 2 ,]   -0.8766058   -0.4812092  # Cholesky  chol2inv ( A )  \n           [, 1 ]       [, 2 ]  [ 1 ,]    0.640625   -0.15625  [ 2 ,]   -0.156250    0.06250  # QR  qr ( A )  $ qr \n            [, 1 ]        [, 2 ]  [ 1 ,]   -3.6055513   -6.101702  [ 2 ,]    0.8320503   -1.941451  $ rank  [ 1 ]   2  $ qraux [ 1 ]   1.554700   1.941451  $ pivot [ 1 ]   1   2  attr (, \"class\" )  [ 1 ]   \"qr\"",
            "title": "Matrix calculation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#integral-calculus",
            "text": "integration  1\n2\n3\n4 myf  <-   function ( x )   {   exp ( - x ^ 2   /   2 )   /   sqrt ( 2    * pi )   } \n\nintegrate ( myf ,  lower  =   - Inf ,  upper  =   Inf ) $ value [ 1 ]   1    Differential calculus  derivative  1\n2\n3\n4\n5\n6\n7\n8\n9 D ( expression ( sin ( cos ( x  +  y ^ 2 ))),   \"x\" )  - ( cos ( cos ( x  +  y ^ 2 ))   *   sin ( x  +  y ^ 2 )) \n\nf  <-  deriv ( < sub > x ^ 2 ,   \"x\" ,   TRUE ) \nf ( 3 )  [ 1 ]   9  attr (, \"gradient\" ) \n     x [ 1 ,]   6     numDeriv  package.  grad() ; first-degree derivative.  hessian() ; second-degree derivative.  and many more.",
            "title": "Integral calculus"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#optimisation",
            "text": "linear, programming, constraints, min-max  1\n2\n3\n4\n5 # compute a 1-variable min, max (y <sub>x) \n\noptimize ( function ( x )   {   cos ( x ^ 2 )   },  lower  =   0 ,  upper  =   2 ,  maximum  =   FALSE )  source ( '</sub>/.active-rstudio-document' ,  echo  =   TRUE )     nlm() ; compute a 2-variable min, max (z  x + y).  nlminb() ; add contraints on x and y, add parameters  upper  and  lower .  optim() .  constrOptim() .  and many more.   Unit root  1\n2\n3 unitroot ( f = function ( x )   {   cos ( x ^ 2 )   },  lower  =   0 , upper  =   2 , tol  =   0.00001 ) $ root polyroot ( x ( 3 ,   -8 ,   1 ))   # for p(x) = 3 - 8x + x\u00b2     cummax() .  cummin() .  cumprod() .  cumsum() .  and many more.",
            "title": "Optimisation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-9-descriptive-statistics",
            "text": "",
            "title": "Chapter 9, Descriptive Statistics"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#factor-levels-labels",
            "text": "factor(c())  as.factor() .  is.factor() .  levels ( var )   <-   c () .  labels ( var )   <-   c () .  levels(var) ; output the levels.  labels(var) ; output the labels.  nlevels(var) ; output the number of levels.   1\n2\n3\n4\n5\n6\n7\n8\n9 mydata  <-   factor ( mydata , \n    levels  =   c ( 1 , 2 , 3 ), \n    labels  =   c ( \"red\" ,   \"blue\" ,   \"green\" ) \n     )  \n\nmydata  <-   ordered ( mydata , \n    levels  =   c ( 1 , 3 ,   5 ), \n    labels  =   c ( \"Low\" ,   \"Medium\" ,   \"High\" ) \n     )",
            "title": "Factor, levels, labels"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#names",
            "text": "names ( var )   <-   c () ; add names to a vector, data frame, list.  colnames ( var )   <-   c () ; idem.  rownames() ; left-most column.  dimnames() ; add names to an array.",
            "title": "Names"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#order",
            "text": "sort(vec, decreasing = TRUE) .  rev(vec) ; inverse sorting.  order(vec) ; sort a vector with names or a list of strings.  ordered(vec) .  as.ordered() .  is.ordered() .  and many more.   Consult keywords \u2018arithmetics\u2019 and \u2018random numbers\u2019, where ordering data is commonly used.",
            "title": "Order"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#convert-as_1",
            "text": "is.integer() .  as.integer() .  as.double() .  is.double() .  is.numeric() .  as.numeric() .  is.character() .  as.character() .  and many more.",
            "title": "Convert (as.)"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#table-proportion-table",
            "text": "tabular, comparison, 2-dimensional, 2, two, dimensions   table(var1, var2) ; 2-dimensional view; cross table.  as.table(var) ; convert.  cut() ; divide the range into intervals.  table(cut(x, res$breaks, include.lowest = TRUE)) .    addmargins(x, FUN = sum, quiet = TRUE) ; add a column or row with sums, means, etc.  read.ftable() ; frequencies.  tablefreq  <-  mytable  /   sum ( mytable ) .  margin.table(tablefreq, 1) ; margin, right and bottom.  tablefreq[ ,ncol()] ; extract the total.  tablefreq[nrow(), ] ; extract the total.    prop.table(mytable, 1) ; percentage view; 1, row sum = 100%.  prop.table(mytable, 2) ; percentage view; 2, row sum = 100%.  which.max(table) ; find the max, min, mean, etc.",
            "title": "Table, proportion table"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#descriptive-statistics",
            "text": "mean(x) .  median(x) .  quantile(x, probs = c(0.1, 0.9)) .  probs = 1:10 / 10 .    max(x) .  min(x) .  diff(range(x)) .  IQR(x) .  var.pop(x) ,  var(x) .  sd.pop(x) ,  sd(x) .  co.var(x) .  mad(x) ; absolute deviation from the median.  mean(abs(x - mean(x))) .    skew(x) .  kurt(x) .  chisq.test() .  round() .  sum() .  nrow() .  ncol() .  cor(var1, var2) .  method = \"kendall\", \"spearman\"    rank() .  rgrs  package .  cramer.v() .     REF: p.378-379",
            "title": "Descriptive statistics"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#graphic-descriptive-statistics",
            "text": "plot() .  dotchart(table(), col = c(\"\", \"\", \"\" ,\"\") ,pch = , main = , lcolor = ) .  barplot() .  barplot(var, col = , pareto = TRUE) .  barplot(sort(table(var)), TRUE)) .  barplot(xlim = , width = , space = , names.arg = , legend = , density = , ylab = , lwd = ) .  pie() .  points(barplot(), cumsum(var), type=\"l\") .  boxplot() .  stem() .  aplpack  package.  stem.left()    hist() .  segments() .   REF: p.392-394, 397-398, 400-410",
            "title": "Graphic descriptive statistics"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-10-random-variables-laws-and-simulation",
            "text": "x %% m ; modulo or modulus.",
            "title": "Chapter 10, Random Variables, Laws, and Simulation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#randomness",
            "text": "runif(1) ; generate a pseudo-random number between 0 and 1.  set.seed() ; \u2018shuffle the dice!\u2019.  x  <-   function ()   {  runif ( 1 )   } ; generate random numbers, following the uniform distribution.  rnorm(1) ; generate a random numbers, following the normal distribution.  generate random numbers with a randomness function and  curve() .   1\n2\n3\n4\n5\n6 x  <-   function ()   {  rnorm ( 1 ,   7 ,   1 )   }  # avg = 7, sd = 1, 1 obs \n\ncurve ( rnorm ( x ,   7 ,   1 ),  xlim  =   c ( -1 , 10 )) \n\nplot ( density ( rnorm ( 1000 ,   7 ,   1 )), xlim  =   c ( -1 ,   10 ),  main = \"Density Curve\" )    REF: p.422-423   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 mean ( runif ( 1 ))  [ 1 ]   0.6586903  mean ( runif ( 10 ))  [ 1 ]   0.5196868  mean ( runif ( 100 ))  [ 1 ]   0.5345603  mean ( runif ( 1000 ))  [ 1 ]   0.5042301  mean ( runif ( 10000 ))  [ 1 ]   0.5021896  # getting closer to 0.5 as the sample increases     ConvergenceConcepts  package to view the law of great numbers.   Dice  REF: p.430-431  Bootstrap  REF: p.436  Laws  REF: p.437-447",
            "title": "Randomness"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-11-confidence-intervals-and-hypothesis-testing",
            "text": "",
            "title": "Chapter 11, Confidence Intervals and Hypothesis Testing"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#confidence-intervals",
            "text": "Use t-values with at least 30 observation in the sample.  With smaller samples (or larger too), use bootstraps to simulate populations from the  boot  package ( boot()  and  boot.ci() ).  For proportion with large samples, use the  epitools  package and  binom.approx() . With smaller samples, go with  binom.test() .  Variance confidence intervals; test for normality with  sigma2.test() .  For non-parametric sample, again, simulate populations with  boot()  and  boot.ci() .  For median, use  qbinom() .  For correlation, use  cor.test() .   REF: p.450-456",
            "title": "Confidence intervals"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#test",
            "text": "In a test,  \\alpha \\alpha  is the signification threshold,  H_0 H_0  is the tested hypothesis.  Average tests;  compare the theoretical average to a reference value with the  t.test() .  Compare two theoretical averages with a  t.test() .  Compare pair samples with  t.test(paired=TRUE) .  Test variance(s) with ANOVA.  Compare a theoretical variance with a reference value with  sigma2.test() .  Compare two theoretical variances with  var.test() .  Compare a theoretical proportion with a reference value with  prop.test() .  Compare two theoretical proportions with  prop.test( ).  Test the theoretical correlation coefficient vs a reference value with  cor.test()  and  cor-.test() .  Test two theoretical correlation coefficients vs a reference value with  cor.test.2.sample() .  Test of independence or chi\u00b2 with  chisq.test() .  Yates chi\u00b2, adjustment chi\u00b2 with  chisq.test() .  Fisher test with  fisher.test() .  Adequacy test or Shapiro-Walk test with  shapiro.test() .  Positional test or sign test or, median sign test with  prop.test()  and  binom.test() .  Median sign test for two independent samples with  chisq.test()  and  fisher.test() .  Sign test for two matching samples with  prop.test()  and  binom.test() .  Wilcoxon rank test or Mann-Whitney test for two independent samples with  wilcox.test() .  Wilcoxon test for two matching samples with  wilcox.test() .   REF: p.459-488",
            "title": "Test"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-12-simple-and-multiple-linear-regression",
            "text": "",
            "title": "Chapter 12, Simple and Multiple Linear Regression"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#regression",
            "text": "lm(y <sub>x .  lm(y <sub>0 + x) ; no intercept.  model  <-  lm ( y  < sub > x ) ; run the regression.  summary(model) ; extract the results.  plot(y <sub>x) ; plot the results.  abline(model) ; add a line on the observations.  confint(model) ; confidence intervals; 95% or 2.5% on both sides.  coefficients(model) ; extract one or several coefficient.  model$coefficients .  model$call .  model$residuals .  anova(model) .  predict(model, data.frame(LWT = prediction), interval = \"prediction\")  and many more.   prediction, result, extraction, residual  REF: p.498-499",
            "title": "Regression"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#normality",
            "text": "histogram, test, residual, quantile-quantile, quantile, qq    1\n2 par ( mfrow = c ( 1 , 2 )) \nhist ( residuals ( model ),  main  =   \"Histogram\" )     qqnorm(resid(model), datax = TRUE) ; quantile-quantile.  qqplot() .  qqline() .  plot(model, 1:6, col.smooth = \"red\") ; 6 graphics.  jarque.bera.test(residuals(model)) ; from the  tseries  package.  dwtest() ; Durbin-Watson test from the  lmtest  package.   REF: p.502-503",
            "title": "Normality"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#correlation",
            "text": "test, explanatory variable interaction, colinearity,  best subset   pairs(newdata, lower.panel = panel.smooth, upper.panel = add.cor) .   REF: p.506",
            "title": "Correlation"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#model-improvement",
            "text": "Variables selection.  Best subset, leaps and bounds.  Forward selection.  Backward selection.  Stepwise selection.  Residual analysis.  and many more.   REF: p.511-535",
            "title": "Model improvement"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#polynomial-regression",
            "text": "REF: p.535-540",
            "title": "Polynomial regression"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#chapter-13-elementary-variance-analysis",
            "text": "anova, repeated measure, between, within, inspection, hypothesis, comparison, factor, table, parameter, repeated  REF: p.542-571",
            "title": "Chapter 13, Elementary Variance Analysis"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#appendix",
            "text": "",
            "title": "APPENDIX"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#appendix-installing-the-r-software-and-packages",
            "text": "installation, package  install.packages('package') .   Be sure to launch RStudio as an administrator or a superuser to install a package in R (not in RStudio); the package is accessible to all users. Then load the package in R or RStudio.  Then attach the package to the work session with  library('package')  or  require('packages') .",
            "title": "Appendix, Installing the R Software and Packages"
        },
        {
            "location": "/Le logiciel R, maitriser le langage, effectuer des analyses (bio)statistiques/#answers-to-the-exercises",
            "text": "REF:  p.625-674",
            "title": "Answers to the Exercises"
        },
        {
            "location": "/Formulas_in_R/",
            "text": "Foreword\n\n\nCode snippets and excerpts from the tutorial. From DataCamp.\n\n\n\n\nOverall data structures\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n# Create variables\n\na \n<-\n \nc\n(\n1\n,\n2\n,\n3\n,\n4\n,\n5\n,\n6\n,\n7\n,\n8\n,\n9\n)\n\nb \n<-\n \nlist\n(\nx \n=\n LifeCycleSavings\n[,\n1\n],\n y \n=\n LifeCycleSavings\n[,\n2\n])\n\n\n\n# Retrieve the types of `a` and `b`\n\n\n# atomic data types\n\n\ntypeof\n(\na\n)\n\n\n\n## [1] \"double\"\n\n\n\ntypeof\n(\nb\n)\n\n\n\n## [1] \"list\"\n\n\n\n# Retrieve the classes of `a` and `b`\n\n\n# for attributes, methods attached\n\n\nclass\n(\na\n)\n\n\n\n## [1] \"numeric\"\n\n\n\nclass\n(\nb\n)\n\n\n\n## [1] \"list\"\n\n\n\n\n\n\n\nNote that if an object does not have a class attribute, it has an implicit \nclass\n, \u201cmatrix\u201d, \u201carray\u201d or the result of the \nmode()\n function.\n\n\nDates and formulas are a special class.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n# Retrieve the object type\n\n\ntypeof\n(\nquote\n(\nx \n*\n \n10\n))\n\n\n\n## [1] \"language\"\n\n\n\n# Retrieve the class\n\n\nclass\n(\nquote\n(\nx \n*\n \n10\n))\n\n\n\n## [1] \"call\"\n\n\n\n# A formula\n\n\nc\n \n<-\n y \n~\n x \n# Sepal.Width ~ Petal.Width\n\nd \n<-\n y \n~\n x \n+\n b \n# Sepal.Width ~ Petal.Width + Species\n\n\n\n# Double check the class of `c`\n\n\nclass\n(\nc\n)\n\n\n\n## [1] \"formula\"\n\n\n\n# Return the type of `d`\n\n\ntypeof\n(\nc\n)\n\n\n\n## [1] \"language\"\n\n\n\n# Retrieve the attributes of `d`\n\n\nattributes\n(\nc\n)\n\n\n\n## $class\n\n\n## [1] \"formula\"\n\n\n## \n\n\n## $.Environment\n\n\n## <environment: R_GlobalEnv>\n\n\n\n\n\n\n\nFormula structures\n\u00b6\n\n\n\n\nIndependent variables appear as \u201cpredictor (variable)\u201d, \u201ccontrolled variable\u201d, \u201cfeature (variable)\u201d, etc.\n\n\nDependent variables are \u201cresponse variable\u201d, \u201coutcome variable\u201d or \u201clabel\u201d.\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\ne \n<-\n \n~\n x \n+\n y \n+\n z\nf \n<-\n y \n~\n x \n+\n b\n\n\n# Return the length of `g`\n\n\nlength\n(\ne\n)\n\n\n\n## [1] 2\n\n\n\nlength\n(\nf\n)\n\n\n\n## [1] 3\n\n\n\n# Retrieve the elements at index 1 and 2\n\ne\n[[\n1\n]]\n\n\n\n## `~`\n\n\ne\n[[\n2\n]]\n\n\n\n## x + y + z\n\n\nf\n[[\n1\n]]\n\n\n\n## `~`\n\n\nf\n[[\n2\n]]\n\n\n\n## y\n\n\nf\n[[\n3\n]]\n\n\n\n## x + b\n\n\n\n\n\n\n\nFormulation\n\u00b6\n\n\n\n\ny ~ x + a + b\n where \u201cy is a function of x, a, and b\u201d.\n\n\nSepal.Width ~ Petal.Width | Species\n where \u201cthe sepal width is a function of petal width, conditioned on species\u201d.\n\n\n\n\n\n\n\n1\n2\n3\n\"y ~ x1 + x2\"\n\nh \n<-\n as.formula\n(\n\"y ~ x1 + x2\"\n)\n\nh \n<-\n formula\n(\n\"y ~ x1 + x2\"\n)\n\n\n\n\n\n\n\nConcatenation\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n# Create variables\n\ni \n<-\n y \n~\n x\nj \n<-\n y \n~\n x \n+\n x1\nk \n<-\n y \n~\n x \n+\n x1 \n+\n x2\n\n\n# Concatentate\n\nformulae \n<-\n \nlist\n(\nas.formula\n(\ni\n),\n as.formula\n(\nj\n),\n as.formula\n(\nk\n))\n\nformulae\n\n\n## [[1]]\n\n\n## y ~ x\n\n\n## \n\n\n## [[2]]\n\n\n## y ~ x + x1\n\n\n## \n\n\n## [[3]]\n\n\n## y ~ x + x1 + x2\n\n\n\n# Double check the class of the list elements\n\n\nclass\n(\nformulae\n[[\n1\n]])\n\n\n\n## [1] \"formula\"\n\n\n\n# Join all with `c()`\n\nl \n<-\n \nc\n(\ni\n,\n j\n,\n k\n)\n\n\n\n# Apply `as.formula` to all elements of `f`\n\n\nlapply\n(\nl\n,\n as.formula\n)\n\n\n\n## [[1]]\n\n\n## y ~ x\n\n\n## \n\n\n## [[2]]\n\n\n## y ~ x + x1\n\n\n## \n\n\n## [[3]]\n\n\n## y ~ x + x1 + x2\n\n\n\n\n\n\n\nOperators\n\u00b6\n\n\n\n\n-\n for removing terms.\n\n\n:\n for interaction for the variable\u2019 interaction, not the variable itself (regression).\n\n\n*\n for crossing: include two variables and their interaction (regression).\n\n\n%in\n%\n for nesting.\n\n\n^\n for limit crossing to the specified degree.\n\n\n-1\n for removing the intercept.\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n# Use multiple independent variables\ny ~ x1 + x2\n\n## y ~ x1 + x2\n\n# Ignore objects in an analysis\ny ~ x1 - x2\n\n## y ~ x1 - x2\n\n# Interactions\ny ~ x1 * x2\n\n## y ~ x1 * x2\n\n# or\ny ~ x1 + x2 + x1:x2\n\n## y ~ x1 + x2 + x1:x2\n\n# No intercept\ny ~ x1 * x2 -1\n\n## y ~ x1 * x2 - 1\n\ny ~ 0 + x1 * x2\n\n## y ~ 0 + x1 * x2\n\n# Nesting\ny ~ a + a:b\n\n## y ~ a + a:b\n\n# or\ny ~ a + b %in% a\n\n## y ~ a + b %in% a\n\n# Quadratic regression\ny ~ x + I(x^2)\n\n## y ~ x + I(x^2)\n\n# Polynomial regression\ny ~ x + I(x^2) + I(x^3)\n\n## y ~ x + I(x^2) + I(x^3)\n\n# Factorial ANOVA\ny ~ (a*b*c)^2\n\n## y ~ (a * b * c)^2\n\n# All independent variables\ny ~ .\n\n## y ~ .\n\n\n\n\n\n\nExamine formulas\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\nm \n<-\n formula\n(\n\"y ~ x1 + x2\"\n)\n\n\n\nattributes\n(\nm\n)\n\n\n\n## $class\n\n\n## [1] \"formula\"\n\n\n## \n\n\n## $.Environment\n\n\n## <environment: R_GlobalEnv>\n\n\n\ntypeof\n(\nm\n)\n\n\n\n## [1] \"language\"\n\n\n\nclass\n(\nm\n)\n\n\n\n## [1] \"formula\"\n\n\nterms\n(\nm\n)\n\n\n\n## y ~ x1 + x2\n\n\n## attr(,\"variables\")\n\n\n## list(y, x1, x2)\n\n\n## attr(,\"factors\")\n\n\n##    x1 x2\n\n\n## y   0  0\n\n\n## x1  1  0\n\n\n## x2  0  1\n\n\n## attr(,\"term.labels\")\n\n\n## [1] \"x1\" \"x2\"\n\n\n## attr(,\"order\")\n\n\n## [1] 1 1\n\n\n## attr(,\"intercept\")\n\n\n## [1] 1\n\n\n## attr(,\"response\")\n\n\n## [1] 1\n\n\n## attr(,\".Environment\")\n\n\n## <environment: R_GlobalEnv>\n\n\n\nprint\n(\nall.vars\n(\nm\n))\n\n\n\n## [1] \"y\"  \"x1\" \"x2\"\n\n\n\nlibrary\n(\nplyr\n)\n\nis.formula\n(\nm\n)\n\n\n\n## [1] TRUE\n\n\n\n\n\n\n\nModify formulas\n\u00b6\n\n\n1\n2\n3\nupdate(y ~ x1 + x2, ~ . + x3)\n\n## y ~ x1 + x2 + x3\n\n\n\n\n\n\nUse formulas\n\u00b6\n\n\nLinear models\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nlm.m \n<-\n lm\n(\nSepal.Width \n~\n Petal.Width \n+\n \nlog\n(\nPetal.Length\n)\n \n+\n Species\n,\n \n        data \n=\n iris\n,\n \n        subset \n=\n Sepal.Length \n>\n \n4.6\n)\n\n\n\nprint\n(\nlm.m\n)\n\n\n\n## \n\n\n## Call:\n\n\n## lm(formula = Sepal.Width ~ Petal.Width + log(Petal.Length) + \n\n\n##     Species, data = iris, subset = Sepal.Length > 4.6)\n\n\n## \n\n\n## Coefficients:\n\n\n##       (Intercept)        Petal.Width  log(Petal.Length)  \n\n\n##            3.1531             0.6620             0.4612  \n\n\n## Speciesversicolor   Speciesvirginica  \n\n\n##           -1.9265            -2.3088\n\n\n\n\n\n\n\nGet back a data.frame of the fitted object\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nstats::model.frame(formula = Sepal.Width ~ Petal.Width + log(Petal.Length) + Species, \n                   data = iris, \n                   subset = Sepal.Length > 6.9, \n                   drop.unused.levels = TRUE)\n\n##     Sepal.Width Petal.Width log(Petal.Length)    Species\n## 51          3.2         1.4          1.547563 versicolor\n## 103         3.0         2.1          1.774952  virginica\n## 106         3.0         2.1          1.887070  virginica\n## 108         2.9         1.8          1.840550  virginica\n## 110         3.6         2.5          1.808289  virginica\n## 118         3.8         2.2          1.902108  virginica\n## 119         2.6         2.3          1.931521  virginica\n## 123         2.8         2.0          1.902108  virginica\n## 126         3.2         1.8          1.791759  virginica\n## 130         3.0         1.6          1.757858  virginica\n## 131         2.8         1.9          1.808289  virginica\n## 132         3.8         2.0          1.856298  virginica\n## 136         3.0         2.3          1.808289  virginica\n\n\n\n\n\n\nMixed-effect models\n\u00b6\n\n\nThe \nnlme\n and the \nlme4\n package are dedicated to fitting linear and generalized linear mixed-effects models.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n# Load packages\nlibrary(MASS)\nlibrary(nlme)\n\n# Get some data \ndata(oats)\n\n# Adjust the data names and columns\nnames(oats) = c('block', 'variety', 'nitrogen', 'yield')\noats$mainplot = oats$variety\noats$subplot = oats$nitrogen\n\n# Fit a non-linear mixed-effects model \nnlme.m = lme(yield ~ variety*nitrogen,\n             random = ~ 1|block/mainplot,\n             data = oats)\n\n# Retrieve a summary\nsummary(nlme.m)\n\n## Linear mixed-effects model fit by REML\n##  Data: oats \n##        AIC      BIC    logLik\n##   559.0285 590.4437 -264.5143\n## \n## Random effects:\n##  Formula: ~1 | block\n##         (Intercept)\n## StdDev:    14.64496\n## \n##  Formula: ~1 | mainplot %in% block\n##         (Intercept) Residual\n## StdDev:    10.29864 13.30727\n## \n## Fixed effects: yield ~ variety * nitrogen \n##                                     Value Std.Error DF   t-value p-value\n## (Intercept)                      80.00000  9.106959 45  8.784491  0.0000\n## varietyMarvellous                 6.66667  9.715030 10  0.686222  0.5082\n## varietyVictory                   -8.50000  9.715030 10 -0.874933  0.4021\n## nitrogen0.2cwt                   18.50000  7.682956 45  2.407927  0.0202\n## nitrogen0.4cwt                   34.66667  7.682956 45  4.512152  0.0000\n## nitrogen0.6cwt                   44.83333  7.682956 45  5.835427  0.0000\n## varietyMarvellous:nitrogen0.2cwt  3.33333 10.865341 45  0.306786  0.7604\n## varietyVictory:nitrogen0.2cwt    -0.33333 10.865341 45 -0.030679  0.9757\n## varietyMarvellous:nitrogen0.4cwt -4.16667 10.865341 45 -0.383482  0.7032\n## varietyVictory:nitrogen0.4cwt     4.66667 10.865341 45  0.429500  0.6696\n## varietyMarvellous:nitrogen0.6cwt -4.66667 10.865341 45 -0.429500  0.6696\n## varietyVictory:nitrogen0.6cwt     2.16667 10.865341 45  0.199411  0.8428\n##  Correlation: \n##                                  (Intr) vrtyMr vrtyVc ntr0.2 ntr0.4 ntr0.6\n## varietyMarvellous                -0.533                                   \n## varietyVictory                   -0.533  0.500                            \n## nitrogen0.2cwt                   -0.422  0.395  0.395                     \n## nitrogen0.4cwt                   -0.422  0.395  0.395  0.500              \n## nitrogen0.6cwt                   -0.422  0.395  0.395  0.500  0.500       \n## varietyMarvellous:nitrogen0.2cwt  0.298 -0.559 -0.280 -0.707 -0.354 -0.354\n## varietyVictory:nitrogen0.2cwt     0.298 -0.280 -0.559 -0.707 -0.354 -0.354\n## varietyMarvellous:nitrogen0.4cwt  0.298 -0.559 -0.280 -0.354 -0.707 -0.354\n## varietyVictory:nitrogen0.4cwt     0.298 -0.280 -0.559 -0.354 -0.707 -0.354\n## varietyMarvellous:nitrogen0.6cwt  0.298 -0.559 -0.280 -0.354 -0.354 -0.707\n## varietyVictory:nitrogen0.6cwt     0.298 -0.280 -0.559 -0.354 -0.354 -0.707\n##                                  vM:0.2 vV:0.2 vM:0.4 vV:0.4 vM:0.6\n## varietyMarvellous                                                  \n## varietyVictory                                                     \n## nitrogen0.2cwt                                                     \n## nitrogen0.4cwt                                                     \n## nitrogen0.6cwt                                                     \n## varietyMarvellous:nitrogen0.2cwt                                   \n## varietyVictory:nitrogen0.2cwt     0.500                            \n## varietyMarvellous:nitrogen0.4cwt  0.500  0.250                     \n## varietyVictory:nitrogen0.4cwt     0.250  0.500  0.500              \n## varietyMarvellous:nitrogen0.6cwt  0.500  0.250  0.500  0.250       \n## varietyVictory:nitrogen0.6cwt     0.250  0.500  0.250  0.500  0.500\n## \n## Standardized Within-Group Residuals:\n##         Min          Q1         Med          Q3         Max \n## -1.81300913 -0.56144819  0.01758018  0.63864472  1.57034195 \n## \n## Number of Observations: 72\n## Number of Groups: \n##               block mainplot %in% block \n##                   6                  18\n\n\n\n\n\n\nNon-linear models\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Set seed\n\n\nset.seed\n(\n20160227\n)\n\n\n\n# Data\n\nx \n<-\n \nseq\n(\n0\n,\n50\n,\n1\n)\n\ny \n<-\n \n((\nrunif\n(\n1\n,\n10\n,\n20\n)\n*\nx\n)\n/\n(\nrunif\n(\n1\n,\n0\n,\n10\n)\n+\nx\n))\n+\nrnorm\n(\n51\n,\n0\n,\n1\n)\n\n\n\n# Non-linear model\n\nnls.m \n<-\n nls\n(\ny \n~\n a\n*\nx\n/\n(\nb\n+\nx\n),\n \n             start\n=\nc\n(\na\n=\n4\n,\n b\n=\n1\n))\n\n\n\n\n\n\n\nGeneralized Linear Models (GLM)\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n# Load package\n\n\nlibrary\n(\nMPDiR\n)\n\n\n\n# Get the data\n\ndata\n(\nChromatic\n)\n\n\n\n# Model\n\nglm.m \n<-\n glm\n(\nThresh \n~\n Axis\n:\n(\nI\n(\nAge\n^\n-1\n)\n \n+\n Age\n),\n\n             family \n=\n Gamma\n(\nlink \n=\n \n\"identity\"\n),\n \n             data \n=\n Chromatic\n)\n\n\n\n# Get back a summary\n\n\nsummary\n(\nglm.m\n)\n\n\n\n## \n\n\n## Call:\n\n\n## glm(formula = Thresh ~ Axis:(I(Age^-1) + Age), family = Gamma(link = \"identity\"), \n\n\n##     data = Chromatic)\n\n\n## \n\n\n## Deviance Residuals: \n\n\n##     Min       1Q   Median       3Q      Max  \n\n\n## -1.2160  -0.3728  -0.0805   0.2311   1.2932  \n\n\n## \n\n\n## Coefficients:\n\n\n##                       Estimate Std. Error t value Pr(>|t|)    \n\n\n## (Intercept)          3.282e-04  9.965e-05   3.294  0.00106 ** \n\n\n## AxisDeutan:I(Age^-1) 7.803e-03  3.686e-04  21.172  < 2e-16 ***\n\n\n## AxisProtan:I(Age^-1) 8.271e-03  3.863e-04  21.410  < 2e-16 ***\n\n\n## AxisTritan:I(Age^-1) 1.166e-02  5.284e-04  22.065  < 2e-16 ***\n\n\n## AxisDeutan:Age       1.521e-05  3.418e-06   4.450 1.06e-05 ***\n\n\n## AxisProtan:Age       1.540e-05  3.434e-06   4.484 9.10e-06 ***\n\n\n## AxisTritan:Age       4.812e-05  5.838e-06   8.241 1.48e-15 ***\n\n\n## ---\n\n\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n## \n\n\n## (Dispersion parameter for Gamma family taken to be 0.2054848)\n\n\n## \n\n\n##     Null deviance: 543.35  on 510  degrees of freedom\n\n\n## Residual deviance: 100.40  on 504  degrees of freedom\n\n\n## AIC: -4777.6\n\n\n## \n\n\n## Number of Fisher Scoring iterations: 6\n\n\n\n\n\n\n\nUse formulas in graphical functions\n\u00b6\n\n\nBasic\n\u00b6\n\n\n1\n2\n3\n4\n5\n# Get data\ndata(airquality)\n\n# Plot\nplot(Ozone ~ Wind, data = airquality, pch = as.character(Month))\n\n\n\n\n\n\n\n\nThe \nlattice\n package\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Load package\nlibrary(lattice)\n\n# Plot histogram\nhistogram(~ Ozone | factor(Month), \n          data = airquality, \n          layout = c(2, 3),\n          xlab = \"Ozone (ppb)\")\n\n\n\n\n\n\n\n\nThe \nggplot2\n package\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Load package\nlibrary(ggplot2)\n\n# Plot\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", \n              formula = y ~ splines::bs(x, 3), \n              se = FALSE)\n\n\n\n\n\n\n\n\n1\n2\n3\n4\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth(span = 0.8) +\n  facet_wrap(~drv)\n\n\n\n\n\n\n\n\nThe \nggformula\n package\n\u00b6\n\n\nggplot2\n, but provides an interface that is based on formulas like the \nlattice\n package. Check out the vignette.\n\n\n1\n2\n3\n4\n5\n# Load package\nlibrary(ggformula)\n\n# Plot\ngf_point(mpg ~ hp, data = mtcars)\n\n\n\n\n\n\n\n\nThe \ndplyr\n package\n\u00b6\n\n\nStandard and non-standard evaluation.\n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n# Load `dplyr`\nlibrary(dplyr)\n\n# NSE evaluation\nselect(iris, Sepal.Length, Petal.Length)\n\n##     Sepal.Length Petal.Length\n## 1            5.1          1.4\n## 2            4.9          1.4\n## 3            4.7          1.3\n## 4            4.6          1.5\n## 5            5.0          1.4\n## 6            5.4          1.7\n## 7            4.6          1.4\n## 8            5.0          1.5\n## 9            4.4          1.4\n## 10           4.9          1.5\n## 11           5.4          1.5\n## 12           4.8          1.6\n## 13           4.8          1.4\n## 14           4.3          1.1\n## 15           5.8          1.2\n## 16           5.7          1.5\n## 17           5.4          1.3\n## 18           5.1          1.4\n## 19           5.7          1.7\n## 20           5.1          1.5\n## 21           5.4          1.7\n## 22           5.1          1.5\n## 23           4.6          1.0\n## 24           5.1          1.7\n## 25           4.8          1.9\n## 26           5.0          1.6\n## 27           5.0          1.6\n## 28           5.2          1.5\n## 29           5.2          1.4\n## 30           4.7          1.6\n## 31           4.8          1.6\n## 32           5.4          1.5\n## 33           5.2          1.5\n## 34           5.5          1.4\n## 35           4.9          1.5\n## 36           5.0          1.2\n## 37           5.5          1.3\n## 38           4.9          1.4\n## 39           4.4          1.3\n## 40           5.1          1.5\n## 41           5.0          1.3\n## 42           4.5          1.3\n## 43           4.4          1.3\n## 44           5.0          1.6\n## 45           5.1          1.9\n## 46           4.8          1.4\n## 47           5.1          1.6\n## 48           4.6          1.4\n## 49           5.3          1.5\n## 50           5.0          1.4\n## 51           7.0          4.7\n## 52           6.4          4.5\n## 53           6.9          4.9\n## 54           5.5          4.0\n## 55           6.5          4.6\n## 56           5.7          4.5\n## 57           6.3          4.7\n## 58           4.9          3.3\n## 59           6.6          4.6\n## 60           5.2          3.9\n## 61           5.0          3.5\n## 62           5.9          4.2\n## 63           6.0          4.0\n## 64           6.1          4.7\n## 65           5.6          3.6\n## 66           6.7          4.4\n## 67           5.6          4.5\n## 68           5.8          4.1\n## 69           6.2          4.5\n## 70           5.6          3.9\n## 71           5.9          4.8\n## 72           6.1          4.0\n## 73           6.3          4.9\n## 74           6.1          4.7\n## 75           6.4          4.3\n## 76           6.6          4.4\n## 77           6.8          4.8\n## 78           6.7          5.0\n## 79           6.0          4.5\n## 80           5.7          3.5\n## 81           5.5          3.8\n## 82           5.5          3.7\n## 83           5.8          3.9\n## 84           6.0          5.1\n## 85           5.4          4.5\n## 86           6.0          4.5\n## 87           6.7          4.7\n## 88           6.3          4.4\n## 89           5.6          4.1\n## 90           5.5          4.0\n## 91           5.5          4.4\n## 92           6.1          4.6\n## 93           5.8          4.0\n## 94           5.0          3.3\n## 95           5.6          4.2\n## 96           5.7          4.2\n## 97           5.7          4.2\n## 98           6.2          4.3\n## 99           5.1          3.0\n## 100          5.7          4.1\n## 101          6.3          6.0\n## 102          5.8          5.1\n## 103          7.1          5.9\n## 104          6.3          5.6\n## 105          6.5          5.8\n## 106          7.6          6.6\n## 107          4.9          4.5\n## 108          7.3          6.3\n## 109          6.7          5.8\n## 110          7.2          6.1\n## 111          6.5          5.1\n## 112          6.4          5.3\n## 113          6.8          5.5\n## 114          5.7          5.0\n## 115          5.8          5.1\n## 116          6.4          5.3\n## 117          6.5          5.5\n## 118          7.7          6.7\n## 119          7.7          6.9\n## 120          6.0          5.0\n## 121          6.9          5.7\n## 122          5.6          4.9\n## 123          7.7          6.7\n## 124          6.3          4.9\n## 125          6.7          5.7\n## 126          7.2          6.0\n## 127          6.2          4.8\n## 128          6.1          4.9\n## 129          6.4          5.6\n## 130          7.2          5.8\n## 131          7.4          6.1\n## 132          7.9          6.4\n## 133          6.4          5.6\n## 134          6.3          5.1\n## 135          6.1          5.6\n## 136          7.7          6.1\n## 137          6.3          5.6\n## 138          6.4          5.5\n## 139          6.0          4.8\n## 140          6.9          5.4\n## 141          6.7          5.6\n## 142          6.9          5.1\n## 143          5.8          5.1\n## 144          6.8          5.9\n## 145          6.7          5.7\n## 146          6.7          5.2\n## 147          6.3          5.0\n## 148          6.5          5.2\n## 149          6.2          5.4\n## 150          5.9          5.1\n\n# standard evaluation \nselect_(iris, ~Sepal.Length)\n\n##     Sepal.Length\n## 1            5.1\n## 2            4.9\n## 3            4.7\n## 4            4.6\n## 5            5.0\n## 6            5.4\n## 7            4.6\n## 8            5.0\n## 9            4.4\n## 10           4.9\n## 11           5.4\n## 12           4.8\n## 13           4.8\n## 14           4.3\n## 15           5.8\n## 16           5.7\n## 17           5.4\n## 18           5.1\n## 19           5.7\n## 20           5.1\n## 21           5.4\n## 22           5.1\n## 23           4.6\n## 24           5.1\n## 25           4.8\n## 26           5.0\n## 27           5.0\n## 28           5.2\n## 29           5.2\n## 30           4.7\n## 31           4.8\n## 32           5.4\n## 33           5.2\n## 34           5.5\n## 35           4.9\n## 36           5.0\n## 37           5.5\n## 38           4.9\n## 39           4.4\n## 40           5.1\n## 41           5.0\n## 42           4.5\n## 43           4.4\n## 44           5.0\n## 45           5.1\n## 46           4.8\n## 47           5.1\n## 48           4.6\n## 49           5.3\n## 50           5.0\n## 51           7.0\n## 52           6.4\n## 53           6.9\n## 54           5.5\n## 55           6.5\n## 56           5.7\n## 57           6.3\n## 58           4.9\n## 59           6.6\n## 60           5.2\n## 61           5.0\n## 62           5.9\n## 63           6.0\n## 64           6.1\n## 65           5.6\n## 66           6.7\n## 67           5.6\n## 68           5.8\n## 69           6.2\n## 70           5.6\n## 71           5.9\n## 72           6.1\n## 73           6.3\n## 74           6.1\n## 75           6.4\n## 76           6.6\n## 77           6.8\n## 78           6.7\n## 79           6.0\n## 80           5.7\n## 81           5.5\n## 82           5.5\n## 83           5.8\n## 84           6.0\n## 85           5.4\n## 86           6.0\n## 87           6.7\n## 88           6.3\n## 89           5.6\n## 90           5.5\n## 91           5.5\n## 92           6.1\n## 93           5.8\n## 94           5.0\n## 95           5.6\n## 96           5.7\n## 97           5.7\n## 98           6.2\n## 99           5.1\n## 100          5.7\n## 101          6.3\n## 102          5.8\n## 103          7.1\n## 104          6.3\n## 105          6.5\n## 106          7.6\n## 107          4.9\n## 108          7.3\n## 109          6.7\n## 110          7.2\n## 111          6.5\n## 112          6.4\n## 113          6.8\n## 114          5.7\n## 115          5.8\n## 116          6.4\n## 117          6.5\n## 118          7.7\n## 119          7.7\n## 120          6.0\n## 121          6.9\n## 122          5.6\n## 123          7.7\n## 124          6.3\n## 125          6.7\n## 126          7.2\n## 127          6.2\n## 128          6.1\n## 129          6.4\n## 130          7.2\n## 131          7.4\n## 132          7.9\n## 133          6.4\n## 134          6.3\n## 135          6.1\n## 136          7.7\n## 137          6.3\n## 138          6.4\n## 139          6.0\n## 140          6.9\n## 141          6.7\n## 142          6.9\n## 143          5.8\n## 144          6.8\n## 145          6.7\n## 146          6.7\n## 147          6.3\n## 148          6.5\n## 149          6.2\n## 150          5.9\n\nselect_(iris, ~Sepal.Length, ~Petal.Length) #works\n\n##     Sepal.Length Petal.Length\n## 1            5.1          1.4\n## 2            4.9          1.4\n## 3            4.7          1.3\n## 4            4.6          1.5\n## 5            5.0          1.4\n## 6            5.4          1.7\n## 7            4.6          1.4\n## 8            5.0          1.5\n## 9            4.4          1.4\n## 10           4.9          1.5\n## 11           5.4          1.5\n## 12           4.8          1.6\n## 13           4.8          1.4\n## 14           4.3          1.1\n## 15           5.8          1.2\n## 16           5.7          1.5\n## 17           5.4          1.3\n## 18           5.1          1.4\n## 19           5.7          1.7\n## 20           5.1          1.5\n## 21           5.4          1.7\n## 22           5.1          1.5\n## 23           4.6          1.0\n## 24           5.1          1.7\n## 25           4.8          1.9\n## 26           5.0          1.6\n## 27           5.0          1.6\n## 28           5.2          1.5\n## 29           5.2          1.4\n## 30           4.7          1.6\n## 31           4.8          1.6\n## 32           5.4          1.5\n## 33           5.2          1.5\n## 34           5.5          1.4\n## 35           4.9          1.5\n## 36           5.0          1.2\n## 37           5.5          1.3\n## 38           4.9          1.4\n## 39           4.4          1.3\n## 40           5.1          1.5\n## 41           5.0          1.3\n## 42           4.5          1.3\n## 43           4.4          1.3\n## 44           5.0          1.6\n## 45           5.1          1.9\n## 46           4.8          1.4\n## 47           5.1          1.6\n## 48           4.6          1.4\n## 49           5.3          1.5\n## 50           5.0          1.4\n## 51           7.0          4.7\n## 52           6.4          4.5\n## 53           6.9          4.9\n## 54           5.5          4.0\n## 55           6.5          4.6\n## 56           5.7          4.5\n## 57           6.3          4.7\n## 58           4.9          3.3\n## 59           6.6          4.6\n## 60           5.2          3.9\n## 61           5.0          3.5\n## 62           5.9          4.2\n## 63           6.0          4.0\n## 64           6.1          4.7\n## 65           5.6          3.6\n## 66           6.7          4.4\n## 67           5.6          4.5\n## 68           5.8          4.1\n## 69           6.2          4.5\n## 70           5.6          3.9\n## 71           5.9          4.8\n## 72           6.1          4.0\n## 73           6.3          4.9\n## 74           6.1          4.7\n## 75           6.4          4.3\n## 76           6.6          4.4\n## 77           6.8          4.8\n## 78           6.7          5.0\n## 79           6.0          4.5\n## 80           5.7          3.5\n## 81           5.5          3.8\n## 82           5.5          3.7\n## 83           5.8          3.9\n## 84           6.0          5.1\n## 85           5.4          4.5\n## 86           6.0          4.5\n## 87           6.7          4.7\n## 88           6.3          4.4\n## 89           5.6          4.1\n## 90           5.5          4.0\n## 91           5.5          4.4\n## 92           6.1          4.6\n## 93           5.8          4.0\n## 94           5.0          3.3\n## 95           5.6          4.2\n## 96           5.7          4.2\n## 97           5.7          4.2\n## 98           6.2          4.3\n## 99           5.1          3.0\n## 100          5.7          4.1\n## 101          6.3          6.0\n## 102          5.8          5.1\n## 103          7.1          5.9\n## 104          6.3          5.6\n## 105          6.5          5.8\n## 106          7.6          6.6\n## 107          4.9          4.5\n## 108          7.3          6.3\n## 109          6.7          5.8\n## 110          7.2          6.1\n## 111          6.5          5.1\n## 112          6.4          5.3\n## 113          6.8          5.5\n## 114          5.7          5.0\n## 115          5.8          5.1\n## 116          6.4          5.3\n## 117          6.5          5.5\n## 118          7.7          6.7\n## 119          7.7          6.9\n## 120          6.0          5.0\n## 121          6.9          5.7\n## 122          5.6          4.9\n## 123          7.7          6.7\n## 124          6.3          4.9\n## 125          6.7          5.7\n## 126          7.2          6.0\n## 127          6.2          4.8\n## 128          6.1          4.9\n## 129          6.4          5.6\n## 130          7.2          5.8\n## 131          7.4          6.1\n## 132          7.9          6.4\n## 133          6.4          5.6\n## 134          6.3          5.1\n## 135          6.1          5.6\n## 136          7.7          6.1\n## 137          6.3          5.6\n## 138          6.4          5.5\n## 139          6.0          4.8\n## 140          6.9          5.4\n## 141          6.7          5.6\n## 142          6.9          5.1\n## 143          5.8          5.1\n## 144          6.8          5.9\n## 145          6.7          5.7\n## 146          6.7          5.2\n## 147          6.3          5.0\n## 148          6.5          5.2\n## 149          6.2          5.4\n## 150          5.9          5.1\n\nselect_(iris, quote(Sepal.Length), quote(Petal.Length)) # yes!\n\n##     Sepal.Length Petal.Length\n## 1            5.1          1.4\n## 2            4.9          1.4\n## 3            4.7          1.3\n## 4            4.6          1.5\n## 5            5.0          1.4\n## 6            5.4          1.7\n## 7            4.6          1.4\n## 8            5.0          1.5\n## 9            4.4          1.4\n## 10           4.9          1.5\n## 11           5.4          1.5\n## 12           4.8          1.6\n## 13           4.8          1.4\n## 14           4.3          1.1\n## 15           5.8          1.2\n## 16           5.7          1.5\n## 17           5.4          1.3\n## 18           5.1          1.4\n## 19           5.7          1.7\n## 20           5.1          1.5\n## 21           5.4          1.7\n## 22           5.1          1.5\n## 23           4.6          1.0\n## 24           5.1          1.7\n## 25           4.8          1.9\n## 26           5.0          1.6\n## 27           5.0          1.6\n## 28           5.2          1.5\n## 29           5.2          1.4\n## 30           4.7          1.6\n## 31           4.8          1.6\n## 32           5.4          1.5\n## 33           5.2          1.5\n## 34           5.5          1.4\n## 35           4.9          1.5\n## 36           5.0          1.2\n## 37           5.5          1.3\n## 38           4.9          1.4\n## 39           4.4          1.3\n## 40           5.1          1.5\n## 41           5.0          1.3\n## 42           4.5          1.3\n## 43           4.4          1.3\n## 44           5.0          1.6\n## 45           5.1          1.9\n## 46           4.8          1.4\n## 47           5.1          1.6\n## 48           4.6          1.4\n## 49           5.3          1.5\n## 50           5.0          1.4\n## 51           7.0          4.7\n## 52           6.4          4.5\n## 53           6.9          4.9\n## 54           5.5          4.0\n## 55           6.5          4.6\n## 56           5.7          4.5\n## 57           6.3          4.7\n## 58           4.9          3.3\n## 59           6.6          4.6\n## 60           5.2          3.9\n## 61           5.0          3.5\n## 62           5.9          4.2\n## 63           6.0          4.0\n## 64           6.1          4.7\n## 65           5.6          3.6\n## 66           6.7          4.4\n## 67           5.6          4.5\n## 68           5.8          4.1\n## 69           6.2          4.5\n## 70           5.6          3.9\n## 71           5.9          4.8\n## 72           6.1          4.0\n## 73           6.3          4.9\n## 74           6.1          4.7\n## 75           6.4          4.3\n## 76           6.6          4.4\n## 77           6.8          4.8\n## 78           6.7          5.0\n## 79           6.0          4.5\n## 80           5.7          3.5\n## 81           5.5          3.8\n## 82           5.5          3.7\n## 83           5.8          3.9\n## 84           6.0          5.1\n## 85           5.4          4.5\n## 86           6.0          4.5\n## 87           6.7          4.7\n## 88           6.3          4.4\n## 89           5.6          4.1\n## 90           5.5          4.0\n## 91           5.5          4.4\n## 92           6.1          4.6\n## 93           5.8          4.0\n## 94           5.0          3.3\n## 95           5.6          4.2\n## 96           5.7          4.2\n## 97           5.7          4.2\n## 98           6.2          4.3\n## 99           5.1          3.0\n## 100          5.7          4.1\n## 101          6.3          6.0\n## 102          5.8          5.1\n## 103          7.1          5.9\n## 104          6.3          5.6\n## 105          6.5          5.8\n## 106          7.6          6.6\n## 107          4.9          4.5\n## 108          7.3          6.3\n## 109          6.7          5.8\n## 110          7.2          6.1\n## 111          6.5          5.1\n## 112          6.4          5.3\n## 113          6.8          5.5\n## 114          5.7          5.0\n## 115          5.8          5.1\n## 116          6.4          5.3\n## 117          6.5          5.5\n## 118          7.7          6.7\n## 119          7.7          6.9\n## 120          6.0          5.0\n## 121          6.9          5.7\n## 122          5.6          4.9\n## 123          7.7          6.7\n## 124          6.3          4.9\n## 125          6.7          5.7\n## 126          7.2          6.0\n## 127          6.2          4.8\n## 128          6.1          4.9\n## 129          6.4          5.6\n## 130          7.2          5.8\n## 131          7.4          6.1\n## 132          7.9          6.4\n## 133          6.4          5.6\n## 134          6.3          5.1\n## 135          6.1          5.6\n## 136          7.7          6.1\n## 137          6.3          5.6\n## 138          6.4          5.5\n## 139          6.0          4.8\n## 140          6.9          5.4\n## 141          6.7          5.6\n## 142          6.9          5.1\n## 143          5.8          5.1\n## 144          6.8          5.9\n## 145          6.7          5.7\n## 146          6.7          5.2\n## 147          6.3          5.0\n## 148          6.5          5.2\n## 149          6.2          5.4\n## 150          5.9          5.1\n\nselect_(iris, \"Sepal.Length\", \"Petal.Length\", \"Species\")\n\n##     Sepal.Length Petal.Length    Species\n## 1            5.1          1.4     setosa\n## 2            4.9          1.4     setosa\n## 3            4.7          1.3     setosa\n## 4            4.6          1.5     setosa\n## 5            5.0          1.4     setosa\n## 6            5.4          1.7     setosa\n## 7            4.6          1.4     setosa\n## 8            5.0          1.5     setosa\n## 9            4.4          1.4     setosa\n## 10           4.9          1.5     setosa\n## 11           5.4          1.5     setosa\n## 12           4.8          1.6     setosa\n## 13           4.8          1.4     setosa\n## 14           4.3          1.1     setosa\n## 15           5.8          1.2     setosa\n## 16           5.7          1.5     setosa\n## 17           5.4          1.3     setosa\n## 18           5.1          1.4     setosa\n## 19           5.7          1.7     setosa\n## 20           5.1          1.5     setosa\n## 21           5.4          1.7     setosa\n## 22           5.1          1.5     setosa\n## 23           4.6          1.0     setosa\n## 24           5.1          1.7     setosa\n## 25           4.8          1.9     setosa\n## 26           5.0          1.6     setosa\n## 27           5.0          1.6     setosa\n## 28           5.2          1.5     setosa\n## 29           5.2          1.4     setosa\n## 30           4.7          1.6     setosa\n## 31           4.8          1.6     setosa\n## 32           5.4          1.5     setosa\n## 33           5.2          1.5     setosa\n## 34           5.5          1.4     setosa\n## 35           4.9          1.5     setosa\n## 36           5.0          1.2     setosa\n## 37           5.5          1.3     setosa\n## 38           4.9          1.4     setosa\n## 39           4.4          1.3     setosa\n## 40           5.1          1.5     setosa\n## 41           5.0          1.3     setosa\n## 42           4.5          1.3     setosa\n## 43           4.4          1.3     setosa\n## 44           5.0          1.6     setosa\n## 45           5.1          1.9     setosa\n## 46           4.8          1.4     setosa\n## 47           5.1          1.6     setosa\n## 48           4.6          1.4     setosa\n## 49           5.3          1.5     setosa\n## 50           5.0          1.4     setosa\n## 51           7.0          4.7 versicolor\n## 52           6.4          4.5 versicolor\n## 53           6.9          4.9 versicolor\n## 54           5.5          4.0 versicolor\n## 55           6.5          4.6 versicolor\n## 56           5.7          4.5 versicolor\n## 57           6.3          4.7 versicolor\n## 58           4.9          3.3 versicolor\n## 59           6.6          4.6 versicolor\n## 60           5.2          3.9 versicolor\n## 61           5.0          3.5 versicolor\n## 62           5.9          4.2 versicolor\n## 63           6.0          4.0 versicolor\n## 64           6.1          4.7 versicolor\n## 65           5.6          3.6 versicolor\n## 66           6.7          4.4 versicolor\n## 67           5.6          4.5 versicolor\n## 68           5.8          4.1 versicolor\n## 69           6.2          4.5 versicolor\n## 70           5.6          3.9 versicolor\n## 71           5.9          4.8 versicolor\n## 72           6.1          4.0 versicolor\n## 73           6.3          4.9 versicolor\n## 74           6.1          4.7 versicolor\n## 75           6.4          4.3 versicolor\n## 76           6.6          4.4 versicolor\n## 77           6.8          4.8 versicolor\n## 78           6.7          5.0 versicolor\n## 79           6.0          4.5 versicolor\n## 80           5.7          3.5 versicolor\n## 81           5.5          3.8 versicolor\n## 82           5.5          3.7 versicolor\n## 83           5.8          3.9 versicolor\n## 84           6.0          5.1 versicolor\n## 85           5.4          4.5 versicolor\n## 86           6.0          4.5 versicolor\n## 87           6.7          4.7 versicolor\n## 88           6.3          4.4 versicolor\n## 89           5.6          4.1 versicolor\n## 90           5.5          4.0 versicolor\n## 91           5.5          4.4 versicolor\n## 92           6.1          4.6 versicolor\n## 93           5.8          4.0 versicolor\n## 94           5.0          3.3 versicolor\n## 95           5.6          4.2 versicolor\n## 96           5.7          4.2 versicolor\n## 97           5.7          4.2 versicolor\n## 98           6.2          4.3 versicolor\n## 99           5.1          3.0 versicolor\n## 100          5.7          4.1 versicolor\n## 101          6.3          6.0  virginica\n## 102          5.8          5.1  virginica\n## 103          7.1          5.9  virginica\n## 104          6.3          5.6  virginica\n## 105          6.5          5.8  virginica\n## 106          7.6          6.6  virginica\n## 107          4.9          4.5  virginica\n## 108          7.3          6.3  virginica\n## 109          6.7          5.8  virginica\n## 110          7.2          6.1  virginica\n## 111          6.5          5.1  virginica\n## 112          6.4          5.3  virginica\n## 113          6.8          5.5  virginica\n## 114          5.7          5.0  virginica\n## 115          5.8          5.1  virginica\n## 116          6.4          5.3  virginica\n## 117          6.5          5.5  virginica\n## 118          7.7          6.7  virginica\n## 119          7.7          6.9  virginica\n## 120          6.0          5.0  virginica\n## 121          6.9          5.7  virginica\n## 122          5.6          4.9  virginica\n## 123          7.7          6.7  virginica\n## 124          6.3          4.9  virginica\n## 125          6.7          5.7  virginica\n## 126          7.2          6.0  virginica\n## 127          6.2          4.8  virginica\n## 128          6.1          4.9  virginica\n## 129          6.4          5.6  virginica\n## 130          7.2          5.8  virginica\n## 131          7.4          6.1  virginica\n## 132          7.9          6.4  virginica\n## 133          6.4          5.6  virginica\n## 134          6.3          5.1  virginica\n## 135          6.1          5.6  virginica\n## 136          7.7          6.1  virginica\n## 137          6.3          5.6  virginica\n## 138          6.4          5.5  virginica\n## 139          6.0          4.8  virginica\n## 140          6.9          5.4  virginica\n## 141          6.7          5.6  virginica\n## 142          6.9          5.1  virginica\n## 143          5.8          5.1  virginica\n## 144          6.8          5.9  virginica\n## 145          6.7          5.7  virginica\n## 146          6.7          5.2  virginica\n## 147          6.3          5.0  virginica\n## 148          6.5          5.2  virginica\n## 149          6.2          5.4  virginica\n## 150          5.9          5.1  virginica\n\n\n\n\n\n\nOther packages\n\u00b6\n\n\n\n\nFormula\n.\n\n\nformula.tools\n.",
            "title": "Formulas in R"
        },
        {
            "location": "/Formulas_in_R/#formula-structures",
            "text": "Independent variables appear as \u201cpredictor (variable)\u201d, \u201ccontrolled variable\u201d, \u201cfeature (variable)\u201d, etc.  Dependent variables are \u201cresponse variable\u201d, \u201coutcome variable\u201d or \u201clabel\u201d.     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32 e  <-   ~  x  +  y  +  z\nf  <-  y  ~  x  +  b # Return the length of `g`  length ( e )  ## [1] 2  length ( f )  ## [1] 3  # Retrieve the elements at index 1 and 2 \ne [[ 1 ]]  ## `~` \n\ne [[ 2 ]]  ## x + y + z \n\nf [[ 1 ]]  ## `~` \n\nf [[ 2 ]]  ## y \n\nf [[ 3 ]]  ## x + b",
            "title": "Formula structures"
        },
        {
            "location": "/Formulas_in_R/#formulation",
            "text": "y ~ x + a + b  where \u201cy is a function of x, a, and b\u201d.  Sepal.Width ~ Petal.Width | Species  where \u201cthe sepal width is a function of petal width, conditioned on species\u201d.    1\n2\n3 \"y ~ x1 + x2\" \nh  <-  as.formula ( \"y ~ x1 + x2\" ) \nh  <-  formula ( \"y ~ x1 + x2\" )",
            "title": "Formulation"
        },
        {
            "location": "/Formulas_in_R/#concatenation",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37 # Create variables \ni  <-  y  ~  x\nj  <-  y  ~  x  +  x1\nk  <-  y  ~  x  +  x1  +  x2 # Concatentate \nformulae  <-   list ( as.formula ( i ),  as.formula ( j ),  as.formula ( k )) \nformulae ## [[1]]  ## y ~ x  ##   ## [[2]]  ## y ~ x + x1  ##   ## [[3]]  ## y ~ x + x1 + x2  # Double check the class of the list elements  class ( formulae [[ 1 ]])  ## [1] \"formula\"  # Join all with `c()` \nl  <-   c ( i ,  j ,  k )  # Apply `as.formula` to all elements of `f`  lapply ( l ,  as.formula )  ## [[1]]  ## y ~ x  ##   ## [[2]]  ## y ~ x + x1  ##   ## [[3]]  ## y ~ x + x1 + x2",
            "title": "Concatenation"
        },
        {
            "location": "/Formulas_in_R/#operators",
            "text": "-  for removing terms.  :  for interaction for the variable\u2019 interaction, not the variable itself (regression).  *  for crossing: include two variables and their interaction (regression).  %in %  for nesting.  ^  for limit crossing to the specified degree.  -1  for removing the intercept.     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58 # Use multiple independent variables\ny ~ x1 + x2\n\n## y ~ x1 + x2\n\n# Ignore objects in an analysis\ny ~ x1 - x2\n\n## y ~ x1 - x2\n\n# Interactions\ny ~ x1 * x2\n\n## y ~ x1 * x2\n\n# or\ny ~ x1 + x2 + x1:x2\n\n## y ~ x1 + x2 + x1:x2\n\n# No intercept\ny ~ x1 * x2 -1\n\n## y ~ x1 * x2 - 1\n\ny ~ 0 + x1 * x2\n\n## y ~ 0 + x1 * x2\n\n# Nesting\ny ~ a + a:b\n\n## y ~ a + a:b\n\n# or\ny ~ a + b %in% a\n\n## y ~ a + b %in% a\n\n# Quadratic regression\ny ~ x + I(x^2)\n\n## y ~ x + I(x^2)\n\n# Polynomial regression\ny ~ x + I(x^2) + I(x^3)\n\n## y ~ x + I(x^2) + I(x^3)\n\n# Factorial ANOVA\ny ~ (a*b*c)^2\n\n## y ~ (a * b * c)^2\n\n# All independent variables\ny ~ .\n\n## y ~ .",
            "title": "Operators"
        },
        {
            "location": "/Formulas_in_R/#examine-formulas",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47 m  <-  formula ( \"y ~ x1 + x2\" )  attributes ( m )  ## $class  ## [1] \"formula\"  ##   ## $.Environment  ## <environment: R_GlobalEnv>  typeof ( m )  ## [1] \"language\"  class ( m )  ## [1] \"formula\" \n\nterms ( m )  ## y ~ x1 + x2  ## attr(,\"variables\")  ## list(y, x1, x2)  ## attr(,\"factors\")  ##    x1 x2  ## y   0  0  ## x1  1  0  ## x2  0  1  ## attr(,\"term.labels\")  ## [1] \"x1\" \"x2\"  ## attr(,\"order\")  ## [1] 1 1  ## attr(,\"intercept\")  ## [1] 1  ## attr(,\"response\")  ## [1] 1  ## attr(,\".Environment\")  ## <environment: R_GlobalEnv>  print ( all.vars ( m ))  ## [1] \"y\"  \"x1\" \"x2\"  library ( plyr ) \nis.formula ( m )  ## [1] TRUE",
            "title": "Examine formulas"
        },
        {
            "location": "/Formulas_in_R/#modify-formulas",
            "text": "1\n2\n3 update(y ~ x1 + x2, ~ . + x3)\n\n## y ~ x1 + x2 + x3",
            "title": "Modify formulas"
        },
        {
            "location": "/Formulas_in_R/#use-formulas",
            "text": "",
            "title": "Use formulas"
        },
        {
            "location": "/Formulas_in_R/#linear-models",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 lm.m  <-  lm ( Sepal.Width  ~  Petal.Width  +   log ( Petal.Length )   +  Species ,  \n        data  =  iris ,  \n        subset  =  Sepal.Length  >   4.6 )  print ( lm.m )  ##   ## Call:  ## lm(formula = Sepal.Width ~ Petal.Width + log(Petal.Length) +   ##     Species, data = iris, subset = Sepal.Length > 4.6)  ##   ## Coefficients:  ##       (Intercept)        Petal.Width  log(Petal.Length)    ##            3.1531             0.6620             0.4612    ## Speciesversicolor   Speciesvirginica    ##           -1.9265            -2.3088",
            "title": "Linear models"
        },
        {
            "location": "/Formulas_in_R/#get-back-a-dataframe-of-the-fitted-object",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 stats::model.frame(formula = Sepal.Width ~ Petal.Width + log(Petal.Length) + Species, \n                   data = iris, \n                   subset = Sepal.Length > 6.9, \n                   drop.unused.levels = TRUE)\n\n##     Sepal.Width Petal.Width log(Petal.Length)    Species\n## 51          3.2         1.4          1.547563 versicolor\n## 103         3.0         2.1          1.774952  virginica\n## 106         3.0         2.1          1.887070  virginica\n## 108         2.9         1.8          1.840550  virginica\n## 110         3.6         2.5          1.808289  virginica\n## 118         3.8         2.2          1.902108  virginica\n## 119         2.6         2.3          1.931521  virginica\n## 123         2.8         2.0          1.902108  virginica\n## 126         3.2         1.8          1.791759  virginica\n## 130         3.0         1.6          1.757858  virginica\n## 131         2.8         1.9          1.808289  virginica\n## 132         3.8         2.0          1.856298  virginica\n## 136         3.0         2.3          1.808289  virginica",
            "title": "Get back a data.frame of the fitted object"
        },
        {
            "location": "/Formulas_in_R/#mixed-effect-models",
            "text": "The  nlme  and the  lme4  package are dedicated to fitting linear and generalized linear mixed-effects models.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82 # Load packages\nlibrary(MASS)\nlibrary(nlme)\n\n# Get some data \ndata(oats)\n\n# Adjust the data names and columns\nnames(oats) = c('block', 'variety', 'nitrogen', 'yield')\noats$mainplot = oats$variety\noats$subplot = oats$nitrogen\n\n# Fit a non-linear mixed-effects model \nnlme.m = lme(yield ~ variety*nitrogen,\n             random = ~ 1|block/mainplot,\n             data = oats)\n\n# Retrieve a summary\nsummary(nlme.m)\n\n## Linear mixed-effects model fit by REML\n##  Data: oats \n##        AIC      BIC    logLik\n##   559.0285 590.4437 -264.5143\n## \n## Random effects:\n##  Formula: ~1 | block\n##         (Intercept)\n## StdDev:    14.64496\n## \n##  Formula: ~1 | mainplot %in% block\n##         (Intercept) Residual\n## StdDev:    10.29864 13.30727\n## \n## Fixed effects: yield ~ variety * nitrogen \n##                                     Value Std.Error DF   t-value p-value\n## (Intercept)                      80.00000  9.106959 45  8.784491  0.0000\n## varietyMarvellous                 6.66667  9.715030 10  0.686222  0.5082\n## varietyVictory                   -8.50000  9.715030 10 -0.874933  0.4021\n## nitrogen0.2cwt                   18.50000  7.682956 45  2.407927  0.0202\n## nitrogen0.4cwt                   34.66667  7.682956 45  4.512152  0.0000\n## nitrogen0.6cwt                   44.83333  7.682956 45  5.835427  0.0000\n## varietyMarvellous:nitrogen0.2cwt  3.33333 10.865341 45  0.306786  0.7604\n## varietyVictory:nitrogen0.2cwt    -0.33333 10.865341 45 -0.030679  0.9757\n## varietyMarvellous:nitrogen0.4cwt -4.16667 10.865341 45 -0.383482  0.7032\n## varietyVictory:nitrogen0.4cwt     4.66667 10.865341 45  0.429500  0.6696\n## varietyMarvellous:nitrogen0.6cwt -4.66667 10.865341 45 -0.429500  0.6696\n## varietyVictory:nitrogen0.6cwt     2.16667 10.865341 45  0.199411  0.8428\n##  Correlation: \n##                                  (Intr) vrtyMr vrtyVc ntr0.2 ntr0.4 ntr0.6\n## varietyMarvellous                -0.533                                   \n## varietyVictory                   -0.533  0.500                            \n## nitrogen0.2cwt                   -0.422  0.395  0.395                     \n## nitrogen0.4cwt                   -0.422  0.395  0.395  0.500              \n## nitrogen0.6cwt                   -0.422  0.395  0.395  0.500  0.500       \n## varietyMarvellous:nitrogen0.2cwt  0.298 -0.559 -0.280 -0.707 -0.354 -0.354\n## varietyVictory:nitrogen0.2cwt     0.298 -0.280 -0.559 -0.707 -0.354 -0.354\n## varietyMarvellous:nitrogen0.4cwt  0.298 -0.559 -0.280 -0.354 -0.707 -0.354\n## varietyVictory:nitrogen0.4cwt     0.298 -0.280 -0.559 -0.354 -0.707 -0.354\n## varietyMarvellous:nitrogen0.6cwt  0.298 -0.559 -0.280 -0.354 -0.354 -0.707\n## varietyVictory:nitrogen0.6cwt     0.298 -0.280 -0.559 -0.354 -0.354 -0.707\n##                                  vM:0.2 vV:0.2 vM:0.4 vV:0.4 vM:0.6\n## varietyMarvellous                                                  \n## varietyVictory                                                     \n## nitrogen0.2cwt                                                     \n## nitrogen0.4cwt                                                     \n## nitrogen0.6cwt                                                     \n## varietyMarvellous:nitrogen0.2cwt                                   \n## varietyVictory:nitrogen0.2cwt     0.500                            \n## varietyMarvellous:nitrogen0.4cwt  0.500  0.250                     \n## varietyVictory:nitrogen0.4cwt     0.250  0.500  0.500              \n## varietyMarvellous:nitrogen0.6cwt  0.500  0.250  0.500  0.250       \n## varietyVictory:nitrogen0.6cwt     0.250  0.500  0.250  0.500  0.500\n## \n## Standardized Within-Group Residuals:\n##         Min          Q1         Med          Q3         Max \n## -1.81300913 -0.56144819  0.01758018  0.63864472  1.57034195 \n## \n## Number of Observations: 72\n## Number of Groups: \n##               block mainplot %in% block \n##                   6                  18",
            "title": "Mixed-effect models"
        },
        {
            "location": "/Formulas_in_R/#non-linear-models",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Set seed  set.seed ( 20160227 )  # Data \nx  <-   seq ( 0 , 50 , 1 ) \ny  <-   (( runif ( 1 , 10 , 20 ) * x ) / ( runif ( 1 , 0 , 10 ) + x )) + rnorm ( 51 , 0 , 1 )  # Non-linear model \nnls.m  <-  nls ( y  ~  a * x / ( b + x ),  \n             start = c ( a = 4 ,  b = 1 ))",
            "title": "Non-linear models"
        },
        {
            "location": "/Formulas_in_R/#generalized-linear-models-glm",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42 # Load package  library ( MPDiR )  # Get the data \ndata ( Chromatic )  # Model \nglm.m  <-  glm ( Thresh  ~  Axis : ( I ( Age ^ -1 )   +  Age ), \n             family  =  Gamma ( link  =   \"identity\" ),  \n             data  =  Chromatic )  # Get back a summary  summary ( glm.m )  ##   ## Call:  ## glm(formula = Thresh ~ Axis:(I(Age^-1) + Age), family = Gamma(link = \"identity\"),   ##     data = Chromatic)  ##   ## Deviance Residuals:   ##     Min       1Q   Median       3Q      Max    ## -1.2160  -0.3728  -0.0805   0.2311   1.2932    ##   ## Coefficients:  ##                       Estimate Std. Error t value Pr(>|t|)      ## (Intercept)          3.282e-04  9.965e-05   3.294  0.00106 **   ## AxisDeutan:I(Age^-1) 7.803e-03  3.686e-04  21.172  < 2e-16 ***  ## AxisProtan:I(Age^-1) 8.271e-03  3.863e-04  21.410  < 2e-16 ***  ## AxisTritan:I(Age^-1) 1.166e-02  5.284e-04  22.065  < 2e-16 ***  ## AxisDeutan:Age       1.521e-05  3.418e-06   4.450 1.06e-05 ***  ## AxisProtan:Age       1.540e-05  3.434e-06   4.484 9.10e-06 ***  ## AxisTritan:Age       4.812e-05  5.838e-06   8.241 1.48e-15 ***  ## ---  ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  ##   ## (Dispersion parameter for Gamma family taken to be 0.2054848)  ##   ##     Null deviance: 543.35  on 510  degrees of freedom  ## Residual deviance: 100.40  on 504  degrees of freedom  ## AIC: -4777.6  ##   ## Number of Fisher Scoring iterations: 6",
            "title": "Generalized Linear Models (GLM)"
        },
        {
            "location": "/Formulas_in_R/#use-formulas-in-graphical-functions",
            "text": "",
            "title": "Use formulas in graphical functions"
        },
        {
            "location": "/Formulas_in_R/#basic",
            "text": "1\n2\n3\n4\n5 # Get data\ndata(airquality)\n\n# Plot\nplot(Ozone ~ Wind, data = airquality, pch = as.character(Month))",
            "title": "Basic"
        },
        {
            "location": "/Formulas_in_R/#the-lattice-package",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 # Load package\nlibrary(lattice)\n\n# Plot histogram\nhistogram(~ Ozone | factor(Month), \n          data = airquality, \n          layout = c(2, 3),\n          xlab = \"Ozone (ppb)\")",
            "title": "The lattice package"
        },
        {
            "location": "/Formulas_in_R/#the-ggplot2-package",
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 # Load package\nlibrary(ggplot2)\n\n# Plot\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", \n              formula = y ~ splines::bs(x, 3), \n              se = FALSE)    1\n2\n3\n4 ggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth(span = 0.8) +\n  facet_wrap(~drv)",
            "title": "The ggplot2 package"
        },
        {
            "location": "/Formulas_in_R/#the-ggformula-package",
            "text": "ggplot2 , but provides an interface that is based on formulas like the  lattice  package. Check out the vignette.  1\n2\n3\n4\n5 # Load package\nlibrary(ggformula)\n\n# Plot\ngf_point(mpg ~ hp, data = mtcars)",
            "title": "The ggformula package"
        },
        {
            "location": "/Formulas_in_R/#the-dplyr-package",
            "text": "Standard and non-standard evaluation.    1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774 # Load `dplyr`\nlibrary(dplyr)\n\n# NSE evaluation\nselect(iris, Sepal.Length, Petal.Length)\n\n##     Sepal.Length Petal.Length\n## 1            5.1          1.4\n## 2            4.9          1.4\n## 3            4.7          1.3\n## 4            4.6          1.5\n## 5            5.0          1.4\n## 6            5.4          1.7\n## 7            4.6          1.4\n## 8            5.0          1.5\n## 9            4.4          1.4\n## 10           4.9          1.5\n## 11           5.4          1.5\n## 12           4.8          1.6\n## 13           4.8          1.4\n## 14           4.3          1.1\n## 15           5.8          1.2\n## 16           5.7          1.5\n## 17           5.4          1.3\n## 18           5.1          1.4\n## 19           5.7          1.7\n## 20           5.1          1.5\n## 21           5.4          1.7\n## 22           5.1          1.5\n## 23           4.6          1.0\n## 24           5.1          1.7\n## 25           4.8          1.9\n## 26           5.0          1.6\n## 27           5.0          1.6\n## 28           5.2          1.5\n## 29           5.2          1.4\n## 30           4.7          1.6\n## 31           4.8          1.6\n## 32           5.4          1.5\n## 33           5.2          1.5\n## 34           5.5          1.4\n## 35           4.9          1.5\n## 36           5.0          1.2\n## 37           5.5          1.3\n## 38           4.9          1.4\n## 39           4.4          1.3\n## 40           5.1          1.5\n## 41           5.0          1.3\n## 42           4.5          1.3\n## 43           4.4          1.3\n## 44           5.0          1.6\n## 45           5.1          1.9\n## 46           4.8          1.4\n## 47           5.1          1.6\n## 48           4.6          1.4\n## 49           5.3          1.5\n## 50           5.0          1.4\n## 51           7.0          4.7\n## 52           6.4          4.5\n## 53           6.9          4.9\n## 54           5.5          4.0\n## 55           6.5          4.6\n## 56           5.7          4.5\n## 57           6.3          4.7\n## 58           4.9          3.3\n## 59           6.6          4.6\n## 60           5.2          3.9\n## 61           5.0          3.5\n## 62           5.9          4.2\n## 63           6.0          4.0\n## 64           6.1          4.7\n## 65           5.6          3.6\n## 66           6.7          4.4\n## 67           5.6          4.5\n## 68           5.8          4.1\n## 69           6.2          4.5\n## 70           5.6          3.9\n## 71           5.9          4.8\n## 72           6.1          4.0\n## 73           6.3          4.9\n## 74           6.1          4.7\n## 75           6.4          4.3\n## 76           6.6          4.4\n## 77           6.8          4.8\n## 78           6.7          5.0\n## 79           6.0          4.5\n## 80           5.7          3.5\n## 81           5.5          3.8\n## 82           5.5          3.7\n## 83           5.8          3.9\n## 84           6.0          5.1\n## 85           5.4          4.5\n## 86           6.0          4.5\n## 87           6.7          4.7\n## 88           6.3          4.4\n## 89           5.6          4.1\n## 90           5.5          4.0\n## 91           5.5          4.4\n## 92           6.1          4.6\n## 93           5.8          4.0\n## 94           5.0          3.3\n## 95           5.6          4.2\n## 96           5.7          4.2\n## 97           5.7          4.2\n## 98           6.2          4.3\n## 99           5.1          3.0\n## 100          5.7          4.1\n## 101          6.3          6.0\n## 102          5.8          5.1\n## 103          7.1          5.9\n## 104          6.3          5.6\n## 105          6.5          5.8\n## 106          7.6          6.6\n## 107          4.9          4.5\n## 108          7.3          6.3\n## 109          6.7          5.8\n## 110          7.2          6.1\n## 111          6.5          5.1\n## 112          6.4          5.3\n## 113          6.8          5.5\n## 114          5.7          5.0\n## 115          5.8          5.1\n## 116          6.4          5.3\n## 117          6.5          5.5\n## 118          7.7          6.7\n## 119          7.7          6.9\n## 120          6.0          5.0\n## 121          6.9          5.7\n## 122          5.6          4.9\n## 123          7.7          6.7\n## 124          6.3          4.9\n## 125          6.7          5.7\n## 126          7.2          6.0\n## 127          6.2          4.8\n## 128          6.1          4.9\n## 129          6.4          5.6\n## 130          7.2          5.8\n## 131          7.4          6.1\n## 132          7.9          6.4\n## 133          6.4          5.6\n## 134          6.3          5.1\n## 135          6.1          5.6\n## 136          7.7          6.1\n## 137          6.3          5.6\n## 138          6.4          5.5\n## 139          6.0          4.8\n## 140          6.9          5.4\n## 141          6.7          5.6\n## 142          6.9          5.1\n## 143          5.8          5.1\n## 144          6.8          5.9\n## 145          6.7          5.7\n## 146          6.7          5.2\n## 147          6.3          5.0\n## 148          6.5          5.2\n## 149          6.2          5.4\n## 150          5.9          5.1\n\n# standard evaluation \nselect_(iris, ~Sepal.Length)\n\n##     Sepal.Length\n## 1            5.1\n## 2            4.9\n## 3            4.7\n## 4            4.6\n## 5            5.0\n## 6            5.4\n## 7            4.6\n## 8            5.0\n## 9            4.4\n## 10           4.9\n## 11           5.4\n## 12           4.8\n## 13           4.8\n## 14           4.3\n## 15           5.8\n## 16           5.7\n## 17           5.4\n## 18           5.1\n## 19           5.7\n## 20           5.1\n## 21           5.4\n## 22           5.1\n## 23           4.6\n## 24           5.1\n## 25           4.8\n## 26           5.0\n## 27           5.0\n## 28           5.2\n## 29           5.2\n## 30           4.7\n## 31           4.8\n## 32           5.4\n## 33           5.2\n## 34           5.5\n## 35           4.9\n## 36           5.0\n## 37           5.5\n## 38           4.9\n## 39           4.4\n## 40           5.1\n## 41           5.0\n## 42           4.5\n## 43           4.4\n## 44           5.0\n## 45           5.1\n## 46           4.8\n## 47           5.1\n## 48           4.6\n## 49           5.3\n## 50           5.0\n## 51           7.0\n## 52           6.4\n## 53           6.9\n## 54           5.5\n## 55           6.5\n## 56           5.7\n## 57           6.3\n## 58           4.9\n## 59           6.6\n## 60           5.2\n## 61           5.0\n## 62           5.9\n## 63           6.0\n## 64           6.1\n## 65           5.6\n## 66           6.7\n## 67           5.6\n## 68           5.8\n## 69           6.2\n## 70           5.6\n## 71           5.9\n## 72           6.1\n## 73           6.3\n## 74           6.1\n## 75           6.4\n## 76           6.6\n## 77           6.8\n## 78           6.7\n## 79           6.0\n## 80           5.7\n## 81           5.5\n## 82           5.5\n## 83           5.8\n## 84           6.0\n## 85           5.4\n## 86           6.0\n## 87           6.7\n## 88           6.3\n## 89           5.6\n## 90           5.5\n## 91           5.5\n## 92           6.1\n## 93           5.8\n## 94           5.0\n## 95           5.6\n## 96           5.7\n## 97           5.7\n## 98           6.2\n## 99           5.1\n## 100          5.7\n## 101          6.3\n## 102          5.8\n## 103          7.1\n## 104          6.3\n## 105          6.5\n## 106          7.6\n## 107          4.9\n## 108          7.3\n## 109          6.7\n## 110          7.2\n## 111          6.5\n## 112          6.4\n## 113          6.8\n## 114          5.7\n## 115          5.8\n## 116          6.4\n## 117          6.5\n## 118          7.7\n## 119          7.7\n## 120          6.0\n## 121          6.9\n## 122          5.6\n## 123          7.7\n## 124          6.3\n## 125          6.7\n## 126          7.2\n## 127          6.2\n## 128          6.1\n## 129          6.4\n## 130          7.2\n## 131          7.4\n## 132          7.9\n## 133          6.4\n## 134          6.3\n## 135          6.1\n## 136          7.7\n## 137          6.3\n## 138          6.4\n## 139          6.0\n## 140          6.9\n## 141          6.7\n## 142          6.9\n## 143          5.8\n## 144          6.8\n## 145          6.7\n## 146          6.7\n## 147          6.3\n## 148          6.5\n## 149          6.2\n## 150          5.9\n\nselect_(iris, ~Sepal.Length, ~Petal.Length) #works\n\n##     Sepal.Length Petal.Length\n## 1            5.1          1.4\n## 2            4.9          1.4\n## 3            4.7          1.3\n## 4            4.6          1.5\n## 5            5.0          1.4\n## 6            5.4          1.7\n## 7            4.6          1.4\n## 8            5.0          1.5\n## 9            4.4          1.4\n## 10           4.9          1.5\n## 11           5.4          1.5\n## 12           4.8          1.6\n## 13           4.8          1.4\n## 14           4.3          1.1\n## 15           5.8          1.2\n## 16           5.7          1.5\n## 17           5.4          1.3\n## 18           5.1          1.4\n## 19           5.7          1.7\n## 20           5.1          1.5\n## 21           5.4          1.7\n## 22           5.1          1.5\n## 23           4.6          1.0\n## 24           5.1          1.7\n## 25           4.8          1.9\n## 26           5.0          1.6\n## 27           5.0          1.6\n## 28           5.2          1.5\n## 29           5.2          1.4\n## 30           4.7          1.6\n## 31           4.8          1.6\n## 32           5.4          1.5\n## 33           5.2          1.5\n## 34           5.5          1.4\n## 35           4.9          1.5\n## 36           5.0          1.2\n## 37           5.5          1.3\n## 38           4.9          1.4\n## 39           4.4          1.3\n## 40           5.1          1.5\n## 41           5.0          1.3\n## 42           4.5          1.3\n## 43           4.4          1.3\n## 44           5.0          1.6\n## 45           5.1          1.9\n## 46           4.8          1.4\n## 47           5.1          1.6\n## 48           4.6          1.4\n## 49           5.3          1.5\n## 50           5.0          1.4\n## 51           7.0          4.7\n## 52           6.4          4.5\n## 53           6.9          4.9\n## 54           5.5          4.0\n## 55           6.5          4.6\n## 56           5.7          4.5\n## 57           6.3          4.7\n## 58           4.9          3.3\n## 59           6.6          4.6\n## 60           5.2          3.9\n## 61           5.0          3.5\n## 62           5.9          4.2\n## 63           6.0          4.0\n## 64           6.1          4.7\n## 65           5.6          3.6\n## 66           6.7          4.4\n## 67           5.6          4.5\n## 68           5.8          4.1\n## 69           6.2          4.5\n## 70           5.6          3.9\n## 71           5.9          4.8\n## 72           6.1          4.0\n## 73           6.3          4.9\n## 74           6.1          4.7\n## 75           6.4          4.3\n## 76           6.6          4.4\n## 77           6.8          4.8\n## 78           6.7          5.0\n## 79           6.0          4.5\n## 80           5.7          3.5\n## 81           5.5          3.8\n## 82           5.5          3.7\n## 83           5.8          3.9\n## 84           6.0          5.1\n## 85           5.4          4.5\n## 86           6.0          4.5\n## 87           6.7          4.7\n## 88           6.3          4.4\n## 89           5.6          4.1\n## 90           5.5          4.0\n## 91           5.5          4.4\n## 92           6.1          4.6\n## 93           5.8          4.0\n## 94           5.0          3.3\n## 95           5.6          4.2\n## 96           5.7          4.2\n## 97           5.7          4.2\n## 98           6.2          4.3\n## 99           5.1          3.0\n## 100          5.7          4.1\n## 101          6.3          6.0\n## 102          5.8          5.1\n## 103          7.1          5.9\n## 104          6.3          5.6\n## 105          6.5          5.8\n## 106          7.6          6.6\n## 107          4.9          4.5\n## 108          7.3          6.3\n## 109          6.7          5.8\n## 110          7.2          6.1\n## 111          6.5          5.1\n## 112          6.4          5.3\n## 113          6.8          5.5\n## 114          5.7          5.0\n## 115          5.8          5.1\n## 116          6.4          5.3\n## 117          6.5          5.5\n## 118          7.7          6.7\n## 119          7.7          6.9\n## 120          6.0          5.0\n## 121          6.9          5.7\n## 122          5.6          4.9\n## 123          7.7          6.7\n## 124          6.3          4.9\n## 125          6.7          5.7\n## 126          7.2          6.0\n## 127          6.2          4.8\n## 128          6.1          4.9\n## 129          6.4          5.6\n## 130          7.2          5.8\n## 131          7.4          6.1\n## 132          7.9          6.4\n## 133          6.4          5.6\n## 134          6.3          5.1\n## 135          6.1          5.6\n## 136          7.7          6.1\n## 137          6.3          5.6\n## 138          6.4          5.5\n## 139          6.0          4.8\n## 140          6.9          5.4\n## 141          6.7          5.6\n## 142          6.9          5.1\n## 143          5.8          5.1\n## 144          6.8          5.9\n## 145          6.7          5.7\n## 146          6.7          5.2\n## 147          6.3          5.0\n## 148          6.5          5.2\n## 149          6.2          5.4\n## 150          5.9          5.1\n\nselect_(iris, quote(Sepal.Length), quote(Petal.Length)) # yes!\n\n##     Sepal.Length Petal.Length\n## 1            5.1          1.4\n## 2            4.9          1.4\n## 3            4.7          1.3\n## 4            4.6          1.5\n## 5            5.0          1.4\n## 6            5.4          1.7\n## 7            4.6          1.4\n## 8            5.0          1.5\n## 9            4.4          1.4\n## 10           4.9          1.5\n## 11           5.4          1.5\n## 12           4.8          1.6\n## 13           4.8          1.4\n## 14           4.3          1.1\n## 15           5.8          1.2\n## 16           5.7          1.5\n## 17           5.4          1.3\n## 18           5.1          1.4\n## 19           5.7          1.7\n## 20           5.1          1.5\n## 21           5.4          1.7\n## 22           5.1          1.5\n## 23           4.6          1.0\n## 24           5.1          1.7\n## 25           4.8          1.9\n## 26           5.0          1.6\n## 27           5.0          1.6\n## 28           5.2          1.5\n## 29           5.2          1.4\n## 30           4.7          1.6\n## 31           4.8          1.6\n## 32           5.4          1.5\n## 33           5.2          1.5\n## 34           5.5          1.4\n## 35           4.9          1.5\n## 36           5.0          1.2\n## 37           5.5          1.3\n## 38           4.9          1.4\n## 39           4.4          1.3\n## 40           5.1          1.5\n## 41           5.0          1.3\n## 42           4.5          1.3\n## 43           4.4          1.3\n## 44           5.0          1.6\n## 45           5.1          1.9\n## 46           4.8          1.4\n## 47           5.1          1.6\n## 48           4.6          1.4\n## 49           5.3          1.5\n## 50           5.0          1.4\n## 51           7.0          4.7\n## 52           6.4          4.5\n## 53           6.9          4.9\n## 54           5.5          4.0\n## 55           6.5          4.6\n## 56           5.7          4.5\n## 57           6.3          4.7\n## 58           4.9          3.3\n## 59           6.6          4.6\n## 60           5.2          3.9\n## 61           5.0          3.5\n## 62           5.9          4.2\n## 63           6.0          4.0\n## 64           6.1          4.7\n## 65           5.6          3.6\n## 66           6.7          4.4\n## 67           5.6          4.5\n## 68           5.8          4.1\n## 69           6.2          4.5\n## 70           5.6          3.9\n## 71           5.9          4.8\n## 72           6.1          4.0\n## 73           6.3          4.9\n## 74           6.1          4.7\n## 75           6.4          4.3\n## 76           6.6          4.4\n## 77           6.8          4.8\n## 78           6.7          5.0\n## 79           6.0          4.5\n## 80           5.7          3.5\n## 81           5.5          3.8\n## 82           5.5          3.7\n## 83           5.8          3.9\n## 84           6.0          5.1\n## 85           5.4          4.5\n## 86           6.0          4.5\n## 87           6.7          4.7\n## 88           6.3          4.4\n## 89           5.6          4.1\n## 90           5.5          4.0\n## 91           5.5          4.4\n## 92           6.1          4.6\n## 93           5.8          4.0\n## 94           5.0          3.3\n## 95           5.6          4.2\n## 96           5.7          4.2\n## 97           5.7          4.2\n## 98           6.2          4.3\n## 99           5.1          3.0\n## 100          5.7          4.1\n## 101          6.3          6.0\n## 102          5.8          5.1\n## 103          7.1          5.9\n## 104          6.3          5.6\n## 105          6.5          5.8\n## 106          7.6          6.6\n## 107          4.9          4.5\n## 108          7.3          6.3\n## 109          6.7          5.8\n## 110          7.2          6.1\n## 111          6.5          5.1\n## 112          6.4          5.3\n## 113          6.8          5.5\n## 114          5.7          5.0\n## 115          5.8          5.1\n## 116          6.4          5.3\n## 117          6.5          5.5\n## 118          7.7          6.7\n## 119          7.7          6.9\n## 120          6.0          5.0\n## 121          6.9          5.7\n## 122          5.6          4.9\n## 123          7.7          6.7\n## 124          6.3          4.9\n## 125          6.7          5.7\n## 126          7.2          6.0\n## 127          6.2          4.8\n## 128          6.1          4.9\n## 129          6.4          5.6\n## 130          7.2          5.8\n## 131          7.4          6.1\n## 132          7.9          6.4\n## 133          6.4          5.6\n## 134          6.3          5.1\n## 135          6.1          5.6\n## 136          7.7          6.1\n## 137          6.3          5.6\n## 138          6.4          5.5\n## 139          6.0          4.8\n## 140          6.9          5.4\n## 141          6.7          5.6\n## 142          6.9          5.1\n## 143          5.8          5.1\n## 144          6.8          5.9\n## 145          6.7          5.7\n## 146          6.7          5.2\n## 147          6.3          5.0\n## 148          6.5          5.2\n## 149          6.2          5.4\n## 150          5.9          5.1\n\nselect_(iris, \"Sepal.Length\", \"Petal.Length\", \"Species\")\n\n##     Sepal.Length Petal.Length    Species\n## 1            5.1          1.4     setosa\n## 2            4.9          1.4     setosa\n## 3            4.7          1.3     setosa\n## 4            4.6          1.5     setosa\n## 5            5.0          1.4     setosa\n## 6            5.4          1.7     setosa\n## 7            4.6          1.4     setosa\n## 8            5.0          1.5     setosa\n## 9            4.4          1.4     setosa\n## 10           4.9          1.5     setosa\n## 11           5.4          1.5     setosa\n## 12           4.8          1.6     setosa\n## 13           4.8          1.4     setosa\n## 14           4.3          1.1     setosa\n## 15           5.8          1.2     setosa\n## 16           5.7          1.5     setosa\n## 17           5.4          1.3     setosa\n## 18           5.1          1.4     setosa\n## 19           5.7          1.7     setosa\n## 20           5.1          1.5     setosa\n## 21           5.4          1.7     setosa\n## 22           5.1          1.5     setosa\n## 23           4.6          1.0     setosa\n## 24           5.1          1.7     setosa\n## 25           4.8          1.9     setosa\n## 26           5.0          1.6     setosa\n## 27           5.0          1.6     setosa\n## 28           5.2          1.5     setosa\n## 29           5.2          1.4     setosa\n## 30           4.7          1.6     setosa\n## 31           4.8          1.6     setosa\n## 32           5.4          1.5     setosa\n## 33           5.2          1.5     setosa\n## 34           5.5          1.4     setosa\n## 35           4.9          1.5     setosa\n## 36           5.0          1.2     setosa\n## 37           5.5          1.3     setosa\n## 38           4.9          1.4     setosa\n## 39           4.4          1.3     setosa\n## 40           5.1          1.5     setosa\n## 41           5.0          1.3     setosa\n## 42           4.5          1.3     setosa\n## 43           4.4          1.3     setosa\n## 44           5.0          1.6     setosa\n## 45           5.1          1.9     setosa\n## 46           4.8          1.4     setosa\n## 47           5.1          1.6     setosa\n## 48           4.6          1.4     setosa\n## 49           5.3          1.5     setosa\n## 50           5.0          1.4     setosa\n## 51           7.0          4.7 versicolor\n## 52           6.4          4.5 versicolor\n## 53           6.9          4.9 versicolor\n## 54           5.5          4.0 versicolor\n## 55           6.5          4.6 versicolor\n## 56           5.7          4.5 versicolor\n## 57           6.3          4.7 versicolor\n## 58           4.9          3.3 versicolor\n## 59           6.6          4.6 versicolor\n## 60           5.2          3.9 versicolor\n## 61           5.0          3.5 versicolor\n## 62           5.9          4.2 versicolor\n## 63           6.0          4.0 versicolor\n## 64           6.1          4.7 versicolor\n## 65           5.6          3.6 versicolor\n## 66           6.7          4.4 versicolor\n## 67           5.6          4.5 versicolor\n## 68           5.8          4.1 versicolor\n## 69           6.2          4.5 versicolor\n## 70           5.6          3.9 versicolor\n## 71           5.9          4.8 versicolor\n## 72           6.1          4.0 versicolor\n## 73           6.3          4.9 versicolor\n## 74           6.1          4.7 versicolor\n## 75           6.4          4.3 versicolor\n## 76           6.6          4.4 versicolor\n## 77           6.8          4.8 versicolor\n## 78           6.7          5.0 versicolor\n## 79           6.0          4.5 versicolor\n## 80           5.7          3.5 versicolor\n## 81           5.5          3.8 versicolor\n## 82           5.5          3.7 versicolor\n## 83           5.8          3.9 versicolor\n## 84           6.0          5.1 versicolor\n## 85           5.4          4.5 versicolor\n## 86           6.0          4.5 versicolor\n## 87           6.7          4.7 versicolor\n## 88           6.3          4.4 versicolor\n## 89           5.6          4.1 versicolor\n## 90           5.5          4.0 versicolor\n## 91           5.5          4.4 versicolor\n## 92           6.1          4.6 versicolor\n## 93           5.8          4.0 versicolor\n## 94           5.0          3.3 versicolor\n## 95           5.6          4.2 versicolor\n## 96           5.7          4.2 versicolor\n## 97           5.7          4.2 versicolor\n## 98           6.2          4.3 versicolor\n## 99           5.1          3.0 versicolor\n## 100          5.7          4.1 versicolor\n## 101          6.3          6.0  virginica\n## 102          5.8          5.1  virginica\n## 103          7.1          5.9  virginica\n## 104          6.3          5.6  virginica\n## 105          6.5          5.8  virginica\n## 106          7.6          6.6  virginica\n## 107          4.9          4.5  virginica\n## 108          7.3          6.3  virginica\n## 109          6.7          5.8  virginica\n## 110          7.2          6.1  virginica\n## 111          6.5          5.1  virginica\n## 112          6.4          5.3  virginica\n## 113          6.8          5.5  virginica\n## 114          5.7          5.0  virginica\n## 115          5.8          5.1  virginica\n## 116          6.4          5.3  virginica\n## 117          6.5          5.5  virginica\n## 118          7.7          6.7  virginica\n## 119          7.7          6.9  virginica\n## 120          6.0          5.0  virginica\n## 121          6.9          5.7  virginica\n## 122          5.6          4.9  virginica\n## 123          7.7          6.7  virginica\n## 124          6.3          4.9  virginica\n## 125          6.7          5.7  virginica\n## 126          7.2          6.0  virginica\n## 127          6.2          4.8  virginica\n## 128          6.1          4.9  virginica\n## 129          6.4          5.6  virginica\n## 130          7.2          5.8  virginica\n## 131          7.4          6.1  virginica\n## 132          7.9          6.4  virginica\n## 133          6.4          5.6  virginica\n## 134          6.3          5.1  virginica\n## 135          6.1          5.6  virginica\n## 136          7.7          6.1  virginica\n## 137          6.3          5.6  virginica\n## 138          6.4          5.5  virginica\n## 139          6.0          4.8  virginica\n## 140          6.9          5.4  virginica\n## 141          6.7          5.6  virginica\n## 142          6.9          5.1  virginica\n## 143          5.8          5.1  virginica\n## 144          6.8          5.9  virginica\n## 145          6.7          5.7  virginica\n## 146          6.7          5.2  virginica\n## 147          6.3          5.0  virginica\n## 148          6.5          5.2  virginica\n## 149          6.2          5.4  virginica\n## 150          5.9          5.1  virginica",
            "title": "The dplyr package"
        },
        {
            "location": "/Formulas_in_R/#other-packages",
            "text": "Formula .  formula.tools .",
            "title": "Other packages"
        },
        {
            "location": "/Improved_codes/",
            "text": "Foreword\n\n\nCode snippets and excerpts from the tutorial. From DataCamp.\n\n\n\n\nCreate a sequence\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Sequence a vector\n\nx \n<-\n runif\n(\n10\n)\n\n\nseq\n(\nx\n)\n\n\n\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n# Sequence an integer\n\n\nseq\n(\nnrow\n(\nmtcars\n))\n\n\n\n##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n\n\n## [24] 24 25 26 27 28 29 30 31 32\n\n\n\n\n\n\n\nCreate a vector\n\u00b6\n\n\nvector(\"type\", length)\n improves memory usage and increases speed over\n\n\nc()\n. Know upfront what type of values will go into a vector, and how\n\nlong the vector will be.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# A numeric vector with 5 elements\n\n\nvector\n(\n\"numeric\"\n,\n \n5\n)\n\n\n\n## [1] 0 0 0 0 0\n\n\n\n# A character vector with 3 elements\n\n\nvector\n(\n\"character\"\n,\n \n3\n)\n\n\n\n## [1] \"\" \"\" \"\"\n\n\nn \n<-\n \n5\n\nx \n<-\n \nvector\n(\n\"integer\"\n,\n n\n)\n\n\nfor\n \n(\ni \nin\n \nseq\n(\nn\n))\n \n{\n\n  x\n[\ni\n]\n \n<-\n i\n\n}\n\nx\n\n\n## [1] 1 2 3 4 5\n\n\n\n\n\n\n\nA speed test\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nn \n<-\n \n1e5\n\n\nx_empty \n<-\n \nc\n()\n\n\nsystem.time\n(\nfor\n(\ni \nin\n \nseq\n(\nn\n))\n x_empty \n<-\n \nc\n(\nx_empty\n,\n i\n))\n\n\n\n##    user  system elapsed \n\n\n##  15.724   0.000  15.724\n\n\nx_zeros \n<-\n \nvector\n(\n\"integer\"\n,\n n\n)\n\n\nsystem.time\n(\nfor\n(\ni \nin\n \nseq\n(\nn\n))\n x_zeros\n[\ni\n]\n \n<-\n i\n)\n\n\n\n##    user  system elapsed \n\n\n##   0.012   0.000   0.011\n\n\n\n\n\n\n\nWhich\n\u00b6\n\n\nIs it a series, a vector? What value?\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\nx \n<-\n \n3\n:\n7\n\n\n\n# Using which (not necessary)\n\nx\n[\nwhich\n(\nx \n>\n \n5\n)]\n\n\n\n## [1] 6 7\n\n\n\n# No which\n\nx\n[\nx \n>\n \n5\n]\n\n\n\n## [1] 6 7\n\n\n\n# Using which\n\n\nlength\n(\nwhich\n(\nx \n>\n \n5\n))\n\n\n\n## [1] 2\n\n\n\n# Without which\n\n\nsum\n(\nx \n>\n \n5\n)\n\n\n\n## [1] 2\n\n\ncondition \n<-\n x \n>\n \n5\n\n\ncondition\n\n\n## [1] FALSE FALSE FALSE  TRUE  TRUE\n\n\nx\n[\ncondition\n]\n\n\n\n## [1] 6 7\n\n\n\nsum\n(\ncondition\n)\n\n\n\n## [1] 2\n\n\n\nmean\n(\ncondition\n)\n\n\n\n## [1] 0.4\n\n\n\nwhich\n(\ncondition\n)\n\n\n\n## [1] 4 5\n\n\nx \n<-\n \nc\n(\n1\n,\n \n2\n,\n \n12\n)\n\n\n\n# Using `which()` and `length()` to test if any values are greater than 10\n\n\nif\n \n(\nlength\n(\nwhich\n(\nx \n>\n \n10\n))\n \n>\n \n0\n)\n\n  \nprint\n(\n\"At least one value is greater than 10\"\n)\n\n\n\n## [1] \"At least one value is greater than 10\"\n\n\n\n# Wrapping a boolean vector with `any()`\n\n\nif\n \n(\nany\n(\nx \n>\n \n10\n))\n\n  \nprint\n(\n\"At least one value is greater than 10\"\n)\n\n\n\n## [1] \"At least one value is greater than 10\"\n\n\n\n# Using `which()` and `length()` to test if all values are positive\n\n\nif\n \n(\nlength\n(\nwhich\n(\nx \n>\n \n0\n))\n \n==\n \nlength\n(\nx\n))\n\n  \nprint\n(\n\"All values are positive\"\n)\n\n\n\n## [1] \"All values are positive\"\n\n\n\n# Wrapping a boolean vector with `all()`\n\n\nif\n \n(\nall\n(\nx \n>\n \n0\n))\n\n  \nprint\n(\n\"All values are positive\"\n)\n\n\n\n## [1] \"All values are positive\"\n\n\n\n\n\n\n\nFactor\n\u00b6\n\n\nThe simple way to remove values from a factor.\n\n\nStarting with.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# A factor with four levels\n\nx \n<-\n \nfactor\n(\nc\n(\n\"a\"\n,\n \n\"b\"\n,\n \n\"c\"\n,\n \n\"d\"\n))\n\nx\n\n\n## [1] a b c d\n\n\n## Levels: a b c d\n\n\nplot\n(\nx\n)\n\n\n\n\n\n\n\n\n\nThe problem.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Drop all values for one level\n\nx \n<-\n x\n[\nx \n!=\n \n\"d\"\n]\n\n\n\n# But we still have this level!\n\nx\n\n\n## [1] a b c\n\n\n## Levels: a b c d\n\n\nplot\n(\nx\n)\n\n\n\n\n\n\n\n\n\nThe solution.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Drop all values for one level\n\nx \n<-\n \nfactor\n(\nx\n[\nx \n!=\n \n\"d\"\n])\n\nx\n\n\n## [1] a b c\n\n\n## Levels: a b c\n\n\nplot\n(\nx\n)\n\n\n\n\n\n\n\n\n\nExtract from a data.frame\n\u00b6\n\n\nSimple and fast.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# rows first, column second - not ideal\nmtcars[mtcars$cyl == 4, ]$hp\n\n##  [1]  93  62  95  66  52  65  97  66  91 113 109\n\n# column first, rows second - much better\nmtcars$hp[mtcars$cyl == 4]\n\n##  [1]  93  62  95  66  52  65  97  66  91 113 109\n\n\n\n\n\n\nA speed test\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# Simulate a data frame...\n\nn \n<-\n \n1e7\n\nd \n<-\n \ndata.frame\n(\n\n  a \n=\n \nseq\n(\nn\n),\n\n  b \n=\n runif\n(\nn\n)\n\n\n)\n\n\n\n# rows first, column second - not ideal\n\n\nsystem.time\n(\nd\n[\nd\n$\nb \n>\n \n.5\n,\n \n]\n$\na\n)\n\n\n\n##    user  system elapsed \n\n\n##   0.568   0.096   0.663\n\n\n\n# column first, rows second - much better\n\n\nsystem.time\n(\nd\n$\na\n[\nd\n$\nb \n>\n \n.5\n])\n\n\n\n##    user  system elapsed \n\n\n##   0.104   0.012   0.114",
            "title": "Improved Codes"
        },
        {
            "location": "/Improved_codes/#create-a-vector",
            "text": "vector(\"type\", length)  improves memory usage and increases speed over  c() . Know upfront what type of values will go into a vector, and how \nlong the vector will be.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # A numeric vector with 5 elements  vector ( \"numeric\" ,   5 )  ## [1] 0 0 0 0 0  # A character vector with 3 elements  vector ( \"character\" ,   3 )  ## [1] \"\" \"\" \"\" \n\nn  <-   5 \nx  <-   vector ( \"integer\" ,  n )  for   ( i  in   seq ( n ))   { \n  x [ i ]   <-  i } \nx ## [1] 1 2 3 4 5",
            "title": "Create a vector"
        },
        {
            "location": "/Improved_codes/#a-speed-test",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 n  <-   1e5 \n\nx_empty  <-   c ()  system.time ( for ( i  in   seq ( n ))  x_empty  <-   c ( x_empty ,  i ))  ##    user  system elapsed   ##  15.724   0.000  15.724 \n\nx_zeros  <-   vector ( \"integer\" ,  n )  system.time ( for ( i  in   seq ( n ))  x_zeros [ i ]   <-  i )  ##    user  system elapsed   ##   0.012   0.000   0.011",
            "title": "A speed test"
        },
        {
            "location": "/Improved_codes/#which",
            "text": "Is it a series, a vector? What value?   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69 x  <-   3 : 7  # Using which (not necessary) \nx [ which ( x  >   5 )]  ## [1] 6 7  # No which \nx [ x  >   5 ]  ## [1] 6 7  # Using which  length ( which ( x  >   5 ))  ## [1] 2  # Without which  sum ( x  >   5 )  ## [1] 2 \n\ncondition  <-  x  >   5 \n\ncondition ## [1] FALSE FALSE FALSE  TRUE  TRUE \n\nx [ condition ]  ## [1] 6 7  sum ( condition )  ## [1] 2  mean ( condition )  ## [1] 0.4  which ( condition )  ## [1] 4 5 \n\nx  <-   c ( 1 ,   2 ,   12 )  # Using `which()` and `length()` to test if any values are greater than 10  if   ( length ( which ( x  >   10 ))   >   0 ) \n   print ( \"At least one value is greater than 10\" )  ## [1] \"At least one value is greater than 10\"  # Wrapping a boolean vector with `any()`  if   ( any ( x  >   10 )) \n   print ( \"At least one value is greater than 10\" )  ## [1] \"At least one value is greater than 10\"  # Using `which()` and `length()` to test if all values are positive  if   ( length ( which ( x  >   0 ))   ==   length ( x )) \n   print ( \"All values are positive\" )  ## [1] \"All values are positive\"  # Wrapping a boolean vector with `all()`  if   ( all ( x  >   0 )) \n   print ( \"All values are positive\" )  ## [1] \"All values are positive\"",
            "title": "Which"
        },
        {
            "location": "/Improved_codes/#factor",
            "text": "The simple way to remove values from a factor.  Starting with.  1\n2\n3\n4\n5\n6\n7\n8 # A factor with four levels \nx  <-   factor ( c ( \"a\" ,   \"b\" ,   \"c\" ,   \"d\" )) \nx ## [1] a b c d  ## Levels: a b c d \n\nplot ( x )     The problem.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Drop all values for one level \nx  <-  x [ x  !=   \"d\" ]  # But we still have this level! \nx ## [1] a b c  ## Levels: a b c d \n\nplot ( x )     The solution.  1\n2\n3\n4\n5\n6\n7\n8 # Drop all values for one level \nx  <-   factor ( x [ x  !=   \"d\" ]) \nx ## [1] a b c  ## Levels: a b c \n\nplot ( x )",
            "title": "Factor"
        },
        {
            "location": "/Improved_codes/#extract-from-a-dataframe",
            "text": "Simple and fast.  1\n2\n3\n4\n5\n6\n7\n8\n9 # rows first, column second - not ideal\nmtcars[mtcars$cyl == 4, ]$hp\n\n##  [1]  93  62  95  66  52  65  97  66  91 113 109\n\n# column first, rows second - much better\nmtcars$hp[mtcars$cyl == 4]\n\n##  [1]  93  62  95  66  52  65  97  66  91 113 109",
            "title": "Extract from a data.frame"
        },
        {
            "location": "/Improved_codes/#a-speed-test_1",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # Simulate a data frame... \nn  <-   1e7 \nd  <-   data.frame ( \n  a  =   seq ( n ), \n  b  =  runif ( n )  )  # rows first, column second - not ideal  system.time ( d [ d $ b  >   .5 ,   ] $ a )  ##    user  system elapsed   ##   0.568   0.096   0.663  # column first, rows second - much better  system.time ( d $ a [ d $ b  >   .5 ])  ##    user  system elapsed   ##   0.104   0.012   0.114",
            "title": "A speed test"
        },
        {
            "location": "/Intermediate_R/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nConditionals and Control Flow\n\u00b6\n\n\nEquality (or not)\n\u00b6\n\n\n1\n2\n# Comparison of logicals\n\n\nTRUE\n \n==\n \nFALSE\n\n\n\n\n\n\n\n1\n## [1] FALSE\n\n\n\n\n\n\n1\n2\n# Comparison of numerics\n\n\n(\n-6\n \n*\n \n14\n)\n \n!=\n \n(\n17\n \n-\n \n101\n)\n\n\n\n\n\n\n\n1\n## [1] FALSE\n\n\n\n\n\n\n1\n2\n# Comparison of character strings\n\n\n'useR'\n \n==\n \n'user'\n\n\n\n\n\n\n\n1\n## [1] FALSE\n\n\n\n\n\n\n1\n2\n# Compare a logical with a numeric\n\n\nTRUE\n \n==\n \n1\n\n\n\n\n\n\n\n1\n## [1] TRUE\n\n\n\n\n\n\nGreater and less than\n\n\n1\n2\n# Comparison of numerics\n\n\n(\n-6\n*\n5\n \n+\n \n2\n)\n \n>=\n \n(\n-10\n \n+\n \n1\n)\n\n\n\n\n\n\n\n1\n## [1] FALSE\n\n\n\n\n\n\n1\n2\n# Comparison of character strings\n\n\n'raining'\n \n<=\n \n'raining dogs'\n\n\n\n\n\n\n\n1\n## [1] TRUE\n\n\n\n\n\n\n1\n2\n# Comparison of logicals\n\n\nTRUE\n \n>\n \nFALSE\n\n\n\n\n\n\n\n1\n## [1] TRUE\n\n\n\n\n\n\nCompare vectors\n\n\n1\n2\n3\n4\n5\n6\n# The linkedin and facebook vectors\n\nlinkedin \n<-\n \nc\n(\n16\n,\n \n9\n,\n \n13\n,\n \n5\n,\n \n2\n,\n \n17\n,\n \n14\n)\n\nfacebook \n<-\n \nc\n(\n17\n,\n \n7\n,\n \n5\n,\n \n16\n,\n \n8\n,\n \n13\n,\n \n14\n)\n\n\n\n# Popular days\n\nlinkedin \n>\n \n15\n\n\n\n\n\n\n\n1\n## [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n\n\n\n1\n2\n# Quiet days\n\nlinkedin \n<=\n \n5\n\n\n\n\n\n\n\n1\n## [1] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n\n\n\n\n\n\n1\n2\n# LinkedIn more popular than Facebook\n\nlinkedin \n>\n facebook\n\n\n\n\n\n\n1\n## [1] FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n\n\n\n\n\n\nCompare matrices\n\n\n1\n2\n3\n4\nviews \n<-\n \nmatrix\n(\nc\n(\nlinkedin\n,\n facebook\n),\n nrow \n=\n \n2\n,\n byrow \n=\n \nTRUE\n)\n\n\n\n# When does views equal 13?\n\nviews \n==\n \n13\n\n\n\n\n\n\n\n1\n2\n3\n##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]\n## [1,] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n## [2,] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n\n\n\n1\n2\n# When is views less than or equal to 14?\n\nviews \n<=\n \n14\n\n\n\n\n\n\n\n1\n2\n3\n##       [,1] [,2] [,3]  [,4] [,5]  [,6] [,7]\n## [1,] FALSE TRUE TRUE  TRUE TRUE FALSE TRUE\n## [2,] FALSE TRUE TRUE FALSE TRUE  TRUE TRUE\n\n\n\n\n\n\n1\n2\n# How often does facebook equal or exceed linkedin times two?\n\n\nsum\n(\nfacebook \n>=\n linkedin \n*\n \n2\n)\n\n\n\n\n\n\n\n1\n## [1] 2\n\n\n\n\n\n\n&\n and \n|\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n# The linkedin and last variable\n\nlinkedin \n<-\n \nc\n(\n16\n,\n \n9\n,\n \n13\n,\n \n5\n,\n \n2\n,\n \n17\n,\n \n14\n)\n\nlast \n<-\n \ntail\n(\nlinkedin\n,\n \n1\n)\n\n\n\n# Is last under 5 or above 10?\n\nlast \n<\n \n5\n \n|\n last \n>\n \n10\n\n\n\n\n\n\n\n1\n## [1] TRUE\n\n\n\n\n\n\n1\n2\n# Is last between 15 (exclusive) and 20 (inclusive)?\n\nlast \n>\n \n15\n \n&\n last \n<=\n \n20\n\n\n\n\n\n\n\n1\n## [1] FALSE\n\n\n\n\n\n\n1\n2\n# Is last between 0 and 5 or between 10 and 15?\n\n\n(\nlast \n>\n \n0\n \n&\n last \n<\n \n5\n)\n \n|\n \n(\nlast \n>\n \n10\n \n&\n last \n<\n \n15\n)\n\n\n\n\n\n\n\n1\n## [1] TRUE\n\n\n\n\n\n\n&\n and \n|\n (2)\n\n\n1\n2\n# linkedin exceeds 10 but facebook below 10\n\nlinkedin \n>\n \n10\n \n&\n facebook \n<\n \n10\n\n\n\n\n\n\n\n1\n## [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n\n\n\n\n\n\n1\n2\n# When were one or both visited at least 12 times?\n\nlinkedin \n>=\n \n12\n \n|\n facebook \n>=\n \n12\n\n\n\n\n\n\n\n1\n## [1]  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n\n\n\n\n\n\n1\n2\n# When is views between 11 (exclusive) and 14 (inclusive)?\n\nviews \n>\n \n11\n \n&\n views \n<=\n \n14\n\n\n\n\n\n\n\n1\n2\n3\n##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6] [,7]\n## [1,] FALSE FALSE  TRUE FALSE FALSE FALSE TRUE\n## [2,] FALSE FALSE FALSE FALSE FALSE  TRUE TRUE\n\n\n\n\n\n\nBlend it all together\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Select the second column, named day2, from li_df: second\n\nsecond \n<-\n li_df\n$\nday2\n\n\n# Build a logical vector, TRUE if value in second is extreme: extremes\n\nextremes \n<-\n \n(\nsecond \n>\n \n25\n \n|\n second \n<\n \n5\n)\n\n\n\n# Count the number of TRUEs in extremes\n\n\nsum\n(\nextremes\n)\n\n\n\n\n\n\n\n1\n## [1] 16\n\n\n\n\n\n\nThe \nif\n statement (and more)\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Variables related to your last day of recordings\n\nmedium \n<-\n \n'LinkedIn'\n\nnum_views \n<-\n \n14\n\n\n\n# Examine the if statement for medium\n\n\nif\n \n(\nmedium \n==\n \n'LinkedIn'\n)\n \n{\n\n  \nprint\n(\n'Showing LinkedIn information'\n)\n\n\n}\n\n\n\n\n\n\n\n1\n## [1] \"Showing LinkedIn information\"\n\n\n\n\n\n\n1\n2\n3\n4\n# Write the if statement for num_views\n\n\nif\n \n(\nnum_views \n>\n \n15\n)\n \n{\n\n    \nprint\n(\n'You\\'re popular!'\n)\n\n\n}\n\n\n\n\n\n\n\nAdd an \nelse\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Variables related to your last day of recordings\n\nmedium \n<-\n \n'LinkedIn'\n\nnum_views \n<-\n \n14\n\n\n\n# Control structure for medium\n\n\nif\n \n(\nmedium \n==\n \n'LinkedIn'\n)\n \n{\n\n  \nprint\n(\n'Showing LinkedIn information'\n)\n\n\n}\n \nelse\n \n{\n\n    \nprint\n(\n'Unknown medium'\n)\n\n\n}\n\n\n\n\n\n\n\n1\n## [1] \"Showing LinkedIn information\"\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# Control structure for num_views\n\n\nif\n \n(\nnum_views \n>\n \n15\n)\n \n{\n\n  \nprint\n(\n'You\\'re popular!'\n)\n\n\n}\n \nelse\n \n{\n\n    \nprint\n(\n'Try to be more visible!'\n)\n\n\n}\n\n\n\n\n\n\n\n1\n## [1] \"Try to be more visible!\"\n\n\n\n\n\n\nCustomize further: \nelse if\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# Variables related to your last day of recordings\n\nmedium \n<-\n \n'LinkedIn'\n\nnum_views \n<-\n \n14\n\n\n\n# Control structure for medium\n\n\nif\n \n(\nmedium \n==\n \n'LinkedIn'\n)\n \n{\n\n  \nprint\n(\n'Showing LinkedIn information'\n)\n\n\n}\n \nelse\n \nif\n \n(\nmedium \n==\n \n'Facebook'\n)\n \n{\n\n  \nprint\n(\n'Showing Facebook information'\n)\n\n\n}\n \nelse\n \n{\n\n  \nprint\n(\n'Unknown medium'\n)\n\n\n}\n\n\n\n\n\n\n\n1\n## [1] \"Showing LinkedIn information\"\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Control structure for num_views\n\n\nif\n \n(\nnum_views \n>\n \n15\n)\n \n{\n\n  \nprint\n(\n'You\\'re popular!'\n)\n\n\n}\n \nelse\n \nif\n \n(\nnum_views \n>\n \n10\n \n|\n num_views \n<=\n \n15\n)\n \n{\n\n  \nprint\n(\n'Your number of views is average'\n)\n\n\n}\n \nelse\n \n{\n\n  \nprint\n(\n'Try to be more visible!'\n)\n\n\n}\n\n\n\n\n\n\n\n1\n## [1] \"Your number of views is average\"\n\n\n\n\n\n\nTake control!\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# Variables related to your last day of recordings\n\nli \n<-\n \n15\n\nfb \n<-\n \n9\n\n\n\n# Code the control-flow construct\n\n\nif\n \n(\nli \n>=\n \n15\n \n&\n fb \n>=\n \n15\n)\n \n{\n\n    sms \n<-\n \n2\n*\n(\nli \n+\n fb\n)\n\n\n}\n \nelse\n \nif\n \n(\nli \n<\n \n10\n \n&\n fb \n<\n \n10\n)\n \n{\n\n    sms \n<-\n \n(\nli \n+\n fb\n)\n/\n2\n\n\n}\n \nelse\n \n{\n\n    sms \n<-\n li \n+\n fb\n\n}\n\n\n\n# Print the resulting sms to the console\n\nsms\n\n\n\n\n\n\n1\n## [1] 24\n\n\n\n\n\n\nLoops\n\u00b6\n\n\nWrite a \nwhile\n loop\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Initialize the speed variable\n\nspeed \n<-\n \n64\n\n\n\n# Code the while loop\n\n\nwhile\n \n(\nspeed \n>\n \n30\n)\n \n{\n\n  \nprint\n(\n'Slow down!'\n)\n\n  speed \n<-\n speed \n-\n \n7\n\n\n}\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n\n\n\n\n\n\n1\n2\n# Print out the speed variable\n\nspeed\n\n\n\n\n\n\n1\n## [1] 29\n\n\n\n\n\n\nThrow in more conditionals\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Initialize the speed variable\n\nspeed \n<-\n \n64\n\n\n\n# Extend/adapt the while loop\n\n\nwhile\n \n(\nspeed \n>\n \n30\n)\n \n{\n\n  \nprint\n(\npaste\n(\n'Your speed is '\n,\n speed\n))\n\n  \nif\n \n(\nspeed \n>\n \n48\n)\n \n{\n\n    \nprint\n(\n'Slow down big time!'\n)\n\n    speed \n<-\n speed \n-\n \n11\n\n  \n}\n \nelse\n \n{\n\n        \nprint\n(\n'Slow down!'\n)\n\n        speed \n<-\n speed \n-\n \n6\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [1] \"Your speed is  64\"\n## [1] \"Slow down big time!\"\n## [1] \"Your speed is  53\"\n## [1] \"Slow down big time!\"\n## [1] \"Your speed is  42\"\n## [1] \"Slow down!\"\n## [1] \"Your speed is  36\"\n## [1] \"Slow down!\"\n\n\n\n\n\n\nStop the \nwhile\n loop: \nbreak\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# Initialize the speed variable\n\nspeed \n<-\n \n88\n\n\nwhile\n \n(\nspeed \n>\n \n30\n)\n \n{\n\n  \nprint\n(\npaste\n(\n'Your speed is'\n,\nspeed\n))\n\n    \n# Break the while loop when speed exceeds 80\n\n  \nif\n \n(\nspeed \n>\n \n80\n)\n \n{\n\n    \nbreak\n\n  \n}\n \nelse\n \nif\n \n(\nspeed \n>\n \n48\n)\n \n{\n\n    \nprint\n(\n'Slow down big time!'\n)\n\n    speed \n<-\n speed \n-\n \n11\n\n  \n}\n \nelse\n \n{\n\n    \nprint\n(\n'Slow down!'\n)\n\n    speed \n<-\n speed \n-\n \n6\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\n1\n## [1] \"Your speed is 88\"\n\n\n\n\n\n\nBuild a \nwhile\n loop from scratch\n\n\nstrsplit\n; split up in a vector that contains separate letters.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Initialize i\n\ni \n<-\n \n1\n\n\n\n# Code the while loop\n\n\nwhile\n \n(\ni \n<=\n \n10\n)\n \n{\n\n  \nprint\n(\ni \n*\n \n3\n)\n\n  \nif\n \n(\n \n(\ni \n*\n \n3\n)\n \n%%\n \n8\n \n==\n \n0\n)\n \n{\n\n    \nbreak\n\n  \n}\n\n  i \n<-\n i \n+\n \n1\n\n\n}\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [1] 3\n## [1] 6\n## [1] 9\n## [1] 12\n## [1] 15\n## [1] 18\n## [1] 21\n## [1] 24\n\n\n\n\n\n\nWrite a \nfor\n loop\n\u00b6\n\n\nLoop over a vector\n\n\n1\n2\n3\n4\n5\n6\n7\n# The linkedin vector\n\nlinkedin \n<-\n \nc\n(\n16\n,\n \n9\n,\n \n13\n,\n \n5\n,\n \n2\n,\n \n17\n,\n \n14\n)\n\n\n\n# Loop version 1\n\n\nfor\n \n(\nlin \nin\n linkedin\n)\n \n{\n\n    \nprint\n(\nlin\n)\n\n\n}\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n## [1] 16\n## [1] 9\n## [1] 13\n## [1] 5\n## [1] 2\n## [1] 17\n## [1] 14\n\n\n\n\n\n\n1\n2\n3\n4\n# Loop version 2\n\n\nfor\n \n(\ni \nin\n \n1\n:\nlength\n(\nlinkedin\n))\n \n{\n\n    \nprint\n(\nlinkedin\n[\ni\n])\n\n\n}\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n## [1] 16\n## [1] 9\n## [1] 13\n## [1] 5\n## [1] 2\n## [1] 17\n## [1] 14\n\n\n\n\n\n\nLoop over a list\n\n\n[[]]\n; list of list.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# The nyc list is already specified\n\nnyc \n<-\n \nlist\n(\npop \n=\n \n8405837\n,\n \n            boroughs \n=\n \nc\n(\n'Manhattan'\n,\n \n'Bronx'\n,\n \n'Brooklyn'\n,\n \n'Queens'\n,\n \n'Staten Island'\n),\n \n            capital \n=\n \nFALSE\n)\n\n\n\n# Loop version 1\n\n\nfor\n \n(\nitem \nin\n nyc\n)\n \n{\n\n    \nprint\n(\nitem\n)\n\n\n}\n\n\n\n\n\n\n\n1\n2\n3\n4\n## [1] 8405837\n## [1] \"Manhattan\"     \"Bronx\"         \"Brooklyn\"      \"Queens\"       \n## [5] \"Staten Island\"\n## [1] FALSE\n\n\n\n\n\n\n1\n2\n3\n4\n# Loop version 2\n\n\nfor\n \n(\ni \nin\n \n1\n:\nlength\n(\nnyc\n))\n \n{\n\n    \nprint\n(\nnyc\n[[\ni\n]])\n\n\n}\n\n\n\n\n\n\n\n1\n2\n3\n4\n## [1] 8405837\n## [1] \"Manhattan\"     \"Bronx\"         \"Brooklyn\"      \"Queens\"       \n## [5] \"Staten Island\"\n## [1] FALSE\n\n\n\n\n\n\nLoop over a matrix\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# The tic-tac-toe matrix has already been defined for you\n\nttt \n<-\n \nmatrix\n(\nc\n(\n'O'\n,\n \nNA\n,\n \n'X'\n,\n \nNA\n,\n \n'O'\n,\n \nNA\n,\n \n'X'\n,\n \n'O'\n,\n \n'X'\n),\n nrow \n=\n \n3\n,\n ncol \n=\n \n3\n)\n\n\n\n# define the double for loop\n\n\nfor\n \n(\ni \nin\n \n1\n:\nnrow\n(\nttt\n))\n \n{\n\n    \nfor\n \n(\nj \nin\n \n1\n:\nncol\n(\nttt\n))\n \n{\n\n    \nprint\n(\npaste\n(\n'On row'\n,\n i\n,\n'and column'\n,\n j\n,\n'the board contains '\n,\n ttt\n[\ni\n,\nj\n]))\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## [1] \"On row 1 and column 1 the board contains  O\"\n## [1] \"On row 1 and column 2 the board contains  NA\"\n## [1] \"On row 1 and column 3 the board contains  X\"\n## [1] \"On row 2 and column 1 the board contains  NA\"\n## [1] \"On row 2 and column 2 the board contains  O\"\n## [1] \"On row 2 and column 3 the board contains  O\"\n## [1] \"On row 3 and column 1 the board contains  X\"\n## [1] \"On row 3 and column 2 the board contains  NA\"\n## [1] \"On row 3 and column 3 the board contains  X\"\n\n\n\n\n\n\nMix up loops with control flow\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# The linkedin vector\n\nlinkedin \n<-\n \nc\n(\n16\n,\n \n9\n,\n \n13\n,\n \n5\n,\n \n2\n,\n \n17\n,\n \n14\n)\n\n\n\n# Code the for loop with conditionals\n\n\nfor\n \n(\ni \nin\n \n1\n:\nlength\n(\nlinkedin\n))\n \n{\n\n    \nif\n \n(\nlinkedin\n[\ni\n]\n \n>\n \n10\n)\n \n{\n\n        \nprint\n(\n'You\\'re popular!'\n)\n\n    \n}\n \nelse\n \n{\n\n        \nprint\n(\n'Be more visible!'\n)\n\n    \n}\n\n    \nprint\n(\nlinkedin\n[\ni\n])\n\n\n}\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n## [1] \"You're popular!\"\n## [1] 16\n## [1] \"Be more visible!\"\n## [1] 9\n## [1] \"You're popular!\"\n## [1] 13\n## [1] \"Be more visible!\"\n## [1] 5\n## [1] \"Be more visible!\"\n## [1] 2\n## [1] \"You're popular!\"\n## [1] 17\n## [1] \"You're popular!\"\n## [1] 14\n\n\n\n\n\n\nNext, you break it\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n# The linkedin vector\n\nlinkedin \n<-\n \nc\n(\n16\n,\n \n9\n,\n \n13\n,\n \n5\n,\n \n2\n,\n \n17\n,\n \n14\n)\n\n\n\n# Extend the for loop\n\n\nfor\n \n(\nli \nin\n linkedin\n)\n \n{\n\n  \nif\n \n(\nli \n>\n \n10\n)\n \n{\n\n    \nprint\n(\n'You\\'re popular!'\n)\n\n  \n}\n \nelse\n \n{\n\n    \nprint\n(\n'Be more visible!'\n)\n\n  \n}\n\n    \n# Add code to conditionally break iteration\n\n  \nif\n \n(\nli \n>\n \n16\n)\n \n{\n\n    \nprint\n(\n'This is ridiculous, I\\'m outta here!'\n)\n\n    \nbreak\n\n  \n}\n\n  \n# Add code to conditionally skip iteration\n\n  \nif\n \n(\nli \n<\n \n5\n)\n \n{\n\n    \nprint\n(\n'This is too embarrassing!'\n)\n\n    \nnext\n\n    \n}\n\n  \nprint\n(\nli\n)\n\n\n}\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n## [1] \"You're popular!\"\n## [1] 16\n## [1] \"Be more visible!\"\n## [1] 9\n## [1] \"You're popular!\"\n## [1] 13\n## [1] \"Be more visible!\"\n## [1] 5\n## [1] \"Be more visible!\"\n## [1] \"This is too embarrassing!\"\n## [1] \"You're popular!\"\n## [1] \"This is ridiculous, I'm outta here!\"\n\n\n\n\n\n\nBuild a for loop from scratch\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# Pre-defined variables\n\nrquote \n<-\n \n'R\\'s internals are irrefutably intriguing'\n\n\nchars \n<-\n \nstrsplit\n(\nrquote\n,\n split \n=\n \n''\n)[[\n1\n]]\n\nrcount \n<-\n \n0\n\n\n\n# Your solution here\n\n\nfor\n \n(\ni \nin\n \n1\n:\nlength\n(\nchars\n))\n \n{\n\n    \nif\n \n(\nchars\n[\ni\n]\n \n==\n \n'u'\n)\n \n{\n\n    \nbreak\n\n    \n}\n\n    \nif\n \n(\nchars\n[\ni\n]\n \n==\n \n'r'\n \n|\n chars\n[\ni\n]\n \n==\n \n'R'\n)\n \n{\n\n        rcount \n<-\n rcount \n+\n \n1\n\n    \n}\n\n\n}\n\n\n\n# Print the resulting rcount variable to the console\n\n\nprint\n(\nrcount\n)\n\n\n\n\n\n\n\n1\n## [1] 5\n\n\n\n\n\n\nFunctions\n\u00b6\n\n\nFunction documentation\n\u00b6\n\n\n1\n2\n3\n4\n5\n# Consult the documentation on the mean() function\n\n\n?\nmean\n\n\n\n# Inspect the arguments of the mean() function\n\n\nargs\n(\nmean\n)\n\n\n\n\n\n\n\nUse a function\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# The linkedin and facebook vectors\n\nlinkedin \n<-\n \nc\n(\n16\n,\n \n9\n,\n \n13\n,\n \n5\n,\n \n2\n,\n \n17\n,\n \n14\n)\n\nfacebook \n<-\n \nc\n(\n17\n,\n \n7\n,\n \n5\n,\n \n16\n,\n \n8\n,\n \n13\n,\n \n14\n)\n\n\n\n# Calculate average number of views\n\navg_li \n<-\n \nmean\n(\nlinkedin\n)\n\navg_fb \n<-\n \nmean\n(\nfacebook\n)\n\n\n\n# Inspect avg_li and avg_fb\n\n\nprint\n(\navg_li\n)\n\n\n\n\n\n\n\n1\n## [1] 10.85714\n\n\n\n\n\n\n1\nprint\n(\navg_fb\n)\n\n\n\n\n\n\n\n1\n## [1] 11.42857\n\n\n\n\n\n\n1\navg_li\n\n\n\n\n\n\n1\n## [1] 10.85714\n\n\n\n\n\n\n1\n2\n# Calculate the mean of linkedin minus facebook\n\n\nprint\n(\nmean\n(\nlinkedin \n-\n facebook\n))\n\n\n\n\n\n\n\n1\n## [1] -0.5714286\n\n\n\n\n\n\nUse a function (2)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# The linkedin and facebook vectors\n\nlinkedin \n<-\n \nc\n(\n16\n,\n \n9\n,\n \n13\n,\n \n5\n,\n \n2\n,\n \n17\n,\n \n14\n)\n\nfacebook \n<-\n \nc\n(\n17\n,\n \n7\n,\n \n5\n,\n \n16\n,\n \n8\n,\n \n13\n,\n \n14\n)\n\n\n\n# Calculate the mean of the sum\n\navg_sum \n<-\n \nmean\n(\nlinkedin \n+\n facebook\n)\n\n\n\n# Calculate the trimmed mean of the sum\n\navg_sum_trimmed \n<-\n \nmean\n((\nlinkedin \n+\n facebook\n),\n trim \n=\n \n0.2\n)\n\n\n\n# Inspect both new variables\n\navg_sum\n\n\n\n\n\n\n1\n## [1] 22.28571\n\n\n\n\n\n\n1\navg_sum_trimmed\n\n\n\n\n\n\n1\n## [1] 22.6\n\n\n\n\n\n\nUse a function (3)\n\n\n1\n2\n3\n4\n5\n6\n# The linkedin and facebook vectors\n\nlinkedin \n<-\n \nc\n(\n16\n,\n \n9\n,\n \n13\n,\n \n5\n,\n \nNA\n,\n \n17\n,\n \n14\n)\n\nfacebook \n<-\n \nc\n(\n17\n,\n \nNA\n,\n \n5\n,\n \n16\n,\n \n8\n,\n \n13\n,\n \n14\n)\n\n\n\n# Basic average of linkedin\n\n\nprint\n(\nmean\n(\nlinkedin\n))\n\n\n\n\n\n\n\n1\n## [1] NA\n\n\n\n\n\n\n1\n2\n# Advanced average of facebook\n\n\nprint\n(\nmean\n(\nfacebook\n,\n na.rm \n=\n \nTRUE\n))\n\n\n\n\n\n\n\n1\n## [1] 12.16667\n\n\n\n\n\n\nFunctions inside functions\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n# The linkedin and facebook vectors\n\nlinkedin \n<-\n \nc\n(\n16\n,\n \n9\n,\n \n13\n,\n \n5\n,\n \nNA\n,\n \n17\n,\n \n14\n)\n\nfacebook \n<-\n \nc\n(\n17\n,\n \nNA\n,\n \n5\n,\n \n16\n,\n \n8\n,\n \n13\n,\n \n14\n)\n\n\n\n# Calculate the mean absolute deviation\n\n\nmean\n((\nabs\n(\nlinkedin \n-\n facebook\n)),\n na.rm \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n1\n## [1] 4.8\n\n\n\n\n\n\nWrite your own function\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n# Create a function pow_two()\n\npow_two \n<-\n \nfunction\n(\narg1\n)\n \n{\n\n    arg1\n^\n2\n\n\n}\n\n\n\n# Use the function \n\npow_two\n(\n12\n)\n\n\n\n\n\n\n\n1\n## [1] 144\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# Create a function sum_abs()\n\nsum_abs \n<-\n \nfunction\n(\narg2\n,\narg3\n)\n \n{\n\n    \nabs\n(\narg2\n)\n \n+\n \nabs\n(\narg3\n)\n\n\n}\n\n\n\n# Use the function\n\nsum_abs\n(\n-2\n,\n3\n)\n\n\n\n\n\n\n\n1\n## [1] 5\n\n\n\n\n\n\nWrite your own function (2)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Define the function hello()\n\nhello \n<-\n \nfunction\n()\n \n{\n\n    \nprint\n(\n'Hi there!'\n)\n\n    \nreturn\n(\nTRUE\n)\n\n\n}\n\n\n\n# Call the function hello()\n\nhello\n()\n\n\n\n\n\n\n\n1\n2\n3\n## [1] \"Hi there!\"\n\n## [1] TRUE\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Define the function my_filter()\n\nmy_filter \n<-\n \nfunction\n(\narg1\n)\n \n{\n\n    \nif\n \n(\narg1 \n>\n \n0\n)\n \n{\n\n        \nreturn\n(\narg1\n)\n\n    \n}\n \nelse\n \n{\n\n        \nreturn\n(\nNULL\n)\n\n    \n}\n\n\n}\n\n\n\n# Call the function my_filter() twice\n\nmy_filter\n(\n5\n)\n\n\n\n\n\n\n\n1\n## [1] 5\n\n\n\n\n\n\n1\nmy_filter\n(\n-5\n)\n\n\n\n\n\n\n\n1\n## NULL\n\n\n\n\n\n\nWrite your own function (3)\n\n\nVariables inside a function are not in the Global Environment.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Extend the pow_two() function\n\npow_two \n<-\n \nfunction\n(\nx\n,\n print_info \n=\n \nTRUE\n)\n \n{\n\n  y \n<-\n x \n^\n \n2\n\n  \nif\n \n(\nprint_info\n)\n \n{\n\n    \nprint\n(\npaste\n(\nx\n,\n \n'to the power two equals'\n,\n y\n))\n\n  \n}\n\n  \nreturn\n(\ny\n)\n\n\n}\n\n\n\n#pow_two(2)\n\npow_two\n(\n2\n,\n \nFALSE\n)\n\n\n\n\n\n\n\n1\n## [1] 4\n\n\n\n\n\n\nR you functional?\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n# The linkedin and facebook vectors\n\nlinkedin \n<-\n \nc\n(\n16\n,\n \n9\n,\n \n13\n,\n \n5\n,\n \nNA\n,\n \n17\n,\n \n14\n)\n\nfacebook \n<-\n \nc\n(\n17\n,\n \n7\n,\n \n5\n,\n \n16\n,\n \n8\n,\n \n13\n,\n \n14\n)\n\n\n\n# Define the interpret function\n\ninterpret \n<-\n \nfunction\n(\narg\n)\n \n{\n\n    \nif\n \n(\narg \n>\n \n15\n)\n \n{\n\n        \nprint\n(\n'You\\'re popular!'\n)\n\n        \nreturn\n(\narg\n)\n\n    \n}\n \nelse\n \n{\n\n        \nprint\n(\n'Try to be more visible!'\n)\n\n        \nreturn\n(\n0\n)\n\n    \n}\n\n\n}\n\n\ninterpret\n(\nlinkedin\n[\n1\n])\n\n\n\n\n\n\n\n1\n2\n3\n## [1] \"You're popular!\"\n\n## [1] 16\n\n\n\n\n\n\n1\ninterpret\n(\nfacebook\n[\n2\n])\n\n\n\n\n\n\n\n1\n2\n3\n## [1] \"Try to be more visible!\"\n\n## [1] 0\n\n\n\n\n\n\nR you functional? (2)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n# The linkedin and facebook vectors\n\nlinkedin \n<-\n \nc\n(\n16\n,\n \n9\n,\n \n13\n,\n \n5\n,\n \n2\n,\n \n17\n,\n \n14\n)\n\nfacebook \n<-\n \nc\n(\n17\n,\n \n7\n,\n \n5\n,\n \n16\n,\n \n8\n,\n \n13\n,\n \n14\n)\n\n\n\n# The interpret() can be used inside interpret_all()\n\ninterpret \n<-\n \nfunction\n(\nnum_views\n){\n\n  \nif\n \n(\nnum_views \n>\n \n15\n)\n \n{\n\n    \nprint\n(\n'You\\'re popular!'\n)\n\n    \nreturn\n(\nnum_views\n)\n\n  \n}\n \nelse\n \n{\n\n    \nprint\n(\n'Try to be more visible!'\n)\n\n    \nreturn\n(\n0\n)\n\n  \n}\n\n\n}\n\n\n\n# Define the interpret_all() function\n\ninterpret_all \n<-\n \nfunction\n(\ndata\n,\n logi \n=\n \nTRUE\n){\n\n  yy \n<-\n \n0\n\n  \nfor\n \n(\ni \nin\n data\n)\n \n{\n\n    yy \n<-\n yy \n+\n interpret\n(\ni\n)\n\n  \n}\n\n  \nif\n \n(\nlogi\n)\n \n{\n\n    \nreturn\n(\nyy\n)\n\n  \n}\n \nelse\n \n{\n\n    \nreturn\n(\nNULL\n)\n\n  \n}\n\n\n}\n\n\n\n# Call the interpret_all() function on both linkedin and facebook\n\ninterpret_all\n(\nlinkedin\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n\n## [1] 33\n\n\n\n\n\n\n1\ninterpret_all\n(\nfacebook\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n\n## [1] 33\n\n\n\n\n\n\nLoad an R package\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n# The mtcars vectors have already been prepared for you\n\nwt \n<-\n mtcars\n$\nwt\nhp \n<-\n mtcars\n$\nhp\n\n\n# Request the currently attached packages\n\n\nsearch\n()\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##  [1] \".GlobalEnv\"            \"package:XLConnect\"    \n##  [3] \"package:XLConnectJars\" \"package:stats\"        \n##  [5] \"package:graphics\"      \"package:grDevices\"    \n##  [7] \"package:utils\"         \"package:datasets\"     \n##  [9] \"package:methods\"       \"Autoloads\"            \n## [11] \"package:base\"\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Try the qplot() function with wt and hp\n\nplot\n(\nwt\n,\nhp\n)\n\n\n\n# Load the ggplot2 package\n\n\nlibrary\n(\n'ggplot2'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# or\n\n\nrequire\n(\n'ggplot2'\n)\n\n\n\n# Retry the qplot() function\n\nqplot\n(\nwt\n,\nhp\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# Check out the currently attached packages again\n\n\nsearch\n()\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##  [1] \".GlobalEnv\"            \"package:ggplot2\"      \n##  [3] \"package:XLConnect\"     \"package:XLConnectJars\"\n##  [5] \"package:stats\"         \"package:graphics\"     \n##  [7] \"package:grDevices\"     \"package:utils\"        \n##  [9] \"package:datasets\"      \"package:methods\"      \n## [11] \"Autoloads\"             \"package:base\"\n\n\n\n\n\n\nThe \napply\n Family\n\u00b6\n\n\nUse \nlapply\n (with a built-in R function)\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# The vector pioneers\n\npioneers \n<-\n \nc\n(\n'GAUSS:1777'\n,\n \n'BAYES:1702'\n,\n \n'PASCAL:1623'\n,\n \n'PEARSON:1857'\n)\n\n\n\n# Split names from birth year: split_math\n\nsplit_math \n<-\n \nstrsplit\n(\npioneers\n,\n \n':'\n)\n\n\n\n# Convert to lowercase strings: split_low\n\nsplit_low \n<-\n \nlapply\n(\nsplit_math\n,\ntolower\n)\n\n\n\n# Take a look at the structure of split_low\n\nstr\n(\nsplit_low\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## List of 4\n##  $ : chr [1:2] \"gauss\" \"1777\"\n##  $ : chr [1:2] \"bayes\" \"1702\"\n##  $ : chr [1:2] \"pascal\" \"1623\"\n##  $ : chr [1:2] \"pearson\" \"1857\"\n\n\n\n\n\n\nUse \nlapply\n with your own function\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Code from previous exercise\n\npioneers \n<-\n \nc\n(\n'GAUSS:1777'\n,\n \n'BAYES:1702'\n,\n \n'PASCAL:1623'\n,\n \n'PEARSON:1857'\n)\n\n\nsplit \n<-\n \nstrsplit\n(\npioneers\n,\n split \n=\n \n':'\n)\n\nsplit_low \n<-\n \nlapply\n(\nsplit\n,\n \ntolower\n)\n\n\n\n# Write function select_first()\n\nselect_first \n<-\n \nfunction\n(\nx\n)\n \n{\n\n    \nreturn\n(\nx\n[\n1\n])\n\n\n}\n\n\n\n# Apply select_first() over split_low: names\n\nnames \n<-\n \nlapply\n(\nsplit_low\n,\n select_first\n)\n\n\nprint\n(\nnames\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## [[1]]\n## [1] \"gauss\"\n## \n## [[2]]\n## [1] \"bayes\"\n## \n## [[3]]\n## [1] \"pascal\"\n## \n## [[4]]\n## [1] \"pearson\"\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Write function select_second()\n\nselect_second \n<-\n \nfunction\n(\nx\n)\n \n{\n\n    \nreturn\n(\nx\n[\n2\n])\n\n\n}\n\n\n\n# Apply select_second() over split_low: years\n\nyears \n<-\n \nlapply\n(\nsplit_low\n,\n select_second\n)\n\n\nprint\n(\nyears\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## [[1]]\n## [1] \"1777\"\n## \n## [[2]]\n## [1] \"1702\"\n## \n## [[3]]\n## [1] \"1623\"\n## \n## [[4]]\n## [1] \"1857\"\n\n\n\n\n\n\nlapply\n and anonymous functions\n\n\nAnonymous function == lambda function.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n# Definition of split_low\n\npioneers \n<-\n \nc\n(\n'GAUSS:1777'\n,\n \n'BAYES:1702'\n,\n \n'PASCAL:1623'\n,\n \n'PEARSON:1857'\n)\n\nsplit \n<-\n \nstrsplit\n(\npioneers\n,\n split \n=\n \n':'\n)\n\nsplit_low \n<-\n \nlapply\n(\nsplit\n,\n \ntolower\n)\n\n\n\n#select_first <- function(x) {\n\n\n#  x[1]\n\n\n#}\n\n\nnames \n<-\n \nlapply\n(\nsplit_low\n,\n \nfunction\n(\nx\n)\n \n{\n x\n[\n1\n]\n \n})\n\n\n\n#select_second <- function(x) {\n\n\n#  x[2]\n\n\n#}\n\n\nyears \n<-\n \nlapply\n(\nsplit_low\n,\n \nfunction\n(\nx\n)\n \n{\n x\n[\n2\n]\n \n})\n\n\n\n\n\n\n\nUse \nlapply\n with additional arguments\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# Definition of split_low\n\npioneers \n<-\n \nc\n(\n'GAUSS:1777'\n,\n \n'BAYES:1702'\n,\n \n'PASCAL:1623'\n,\n \n'PEARSON:1857'\n)\n\n\nsplit \n<-\n \nstrsplit\n(\npioneers\n,\n split \n=\n \n':'\n)\n\nsplit_low \n<-\n \nlapply\n(\nsplit\n,\n \ntolower\n)\n\n\n\n# Replace the select_*() functions by a single function: select_el\n\nselect_el \n<-\n \nfunction\n(\nx\n,\n i\n)\n \n{\n \n  x\n[\ni\n]\n \n\n}\n\n\n\n#select_second <- function(x) { \n\n\n#  x[2] \n\n\n#}\n\n\n\n# Call the select_el() function twice on split_low: names and years\n\nnames \n<-\n \nlapply\n(\nsplit_low\n,\n select_el\n,\n i\n=\n1\n)\n\nyears \n<-\n \nlapply\n(\nsplit_low\n,\n select_el\n,\n \n2\n)\n\n\n\n\n\n\n\nUse \nsapply\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\ntemp1 \n<-\n \nc\n(\n3\n,\n \n7\n,\n \n9\n,\n \n6\n,\n \n-1\n)\n\ntemp2 \n<-\n \nc\n(\n6\n,\n \n9\n,\n \n12\n,\n \n13\n,\n \n5\n)\n\ntemp3 \n<-\n \nc\n(\n4\n,\n \n8\n,\n \n3\n,\n \n-1\n,\n \n-3\n)\n\ntemp4 \n<-\n \nc\n(\n1\n,\n \n4\n,\n \n7\n,\n \n2\n,\n \n-2\n)\n\ntemp5 \n<-\n \nc\n(\n5\n,\n \n7\n,\n \n9\n,\n \n4\n,\n \n2\n)\n\ntemp6 \n<-\n \nc\n(\n-3\n,\n \n5\n,\n \n8\n,\n \n9\n,\n \n4\n)\n\ntemp7 \n<-\n \nc\n(\n3\n,\n \n6\n,\n \n9\n,\n \n4\n,\n \n1\n)\n\n\ntemp \n<-\n \nlist\n(\ntemp1\n,\n temp2\n,\n temp3\n,\n temp4\n,\n temp5\n,\n temp6\n,\n temp7\n)\n\n\n\n# Use lapply() to find each day's minimum temperature\n\n\nlapply\n(\ntemp\n,\n \nmin\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n## [[1]]\n## [1] -1\n## \n## [[2]]\n## [1] 5\n## \n## [[3]]\n## [1] -3\n## \n## [[4]]\n## [1] -2\n## \n## [[5]]\n## [1] 2\n## \n## [[6]]\n## [1] -3\n## \n## [[7]]\n## [1] 1\n\n\n\n\n\n\n1\n2\n# Use sapply() to find each day's minimum temperature\n\n\nsapply\n(\ntemp\n,\n \nmin\n)\n\n\n\n\n\n\n\n1\n## [1] -1  5 -3 -2  2 -3  1\n\n\n\n\n\n\n1\n2\n# Use lapply() to find each day's maximum temperature\n\n\nlapply\n(\ntemp\n,\n \nmax\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n## [[1]]\n## [1] 9\n## \n## [[2]]\n## [1] 13\n## \n## [[3]]\n## [1] 8\n## \n## [[4]]\n## [1] 7\n## \n## [[5]]\n## [1] 9\n## \n## [[6]]\n## [1] 9\n## \n## [[7]]\n## [1] 9\n\n\n\n\n\n\n1\n2\n# Use sapply() to find each day's maximum temperature\n\n\nsapply\n(\ntemp\n,\n \nmax\n)\n\n\n\n\n\n\n\n1\n## [1]  9 13  8  7  9  9  9\n\n\n\n\n\n\nsapply\n with your own function\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# temp is already defined in the workspace\n\n\n\n# Define a function calculates the average of the min and max of a vector: extremes_avg\n\nextremes_avg \n<-\n \nfunction\n(\nx\n)\n \n{\n\n    \nreturn\n((\nmin\n(\nx\n)\n \n+\n \nmax\n(\nx\n))\n/\n2\n)\n\n\n}\n\n\n\n# Apply extremes_avg() over temp using sapply()\n\n\nsapply\n(\ntemp\n,\n extremes_avg\n)\n\n\n\n\n\n\n\n1\n## [1] 4.0 9.0 2.5 2.5 5.5 3.0 5.0\n\n\n\n\n\n\n1\n2\n# Apply extremes_avg() over temp using lapply()\n\n\nlapply\n(\ntemp\n,\n extremes_avg\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n## [[1]]\n## [1] 4\n## \n## [[2]]\n## [1] 9\n## \n## [[3]]\n## [1] 2.5\n## \n## [[4]]\n## [1] 2.5\n## \n## [[5]]\n## [1] 5.5\n## \n## [[6]]\n## [1] 3\n## \n## [[7]]\n## [1] 5\n\n\n\n\n\n\nsapply\n with function returning vector\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# temp is already available in the workspace\n\n\n\n# Create a function that returns min and max of a vector: extremes\n\nextremes \n<-\n \nfunction\n(\nx\n)\n \n{\n\n    \nc\n(\nmin\n(\nx\n),\n \nmax\n(\nx\n))\n\n\n}\n\n\n\n# Apply extremes() over temp with sapply()\n\n\nsapply\n(\ntemp\n,\n extremes\n)\n\n\n\n\n\n\n\n1\n2\n3\n##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## [1,]   -1    5   -3   -2    2   -3    1\n## [2,]    9   13    8    7    9    9    9\n\n\n\n\n\n\n1\n2\n# Apply extremes() over temp with lapply()\n\n\nlapply\n(\ntemp\n,\n extremes\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n## [[1]]\n## [1] -1  9\n## \n## [[2]]\n## [1]  5 13\n## \n## [[3]]\n## [1] -3  8\n## \n## [[4]]\n## [1] -2  7\n## \n## [[5]]\n## [1] 2 9\n## \n## [[6]]\n## [1] -3  9\n## \n## [[7]]\n## [1] 1 9\n\n\n\n\n\n\nsapply\n can\u2019t simplify, now what?\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n# temp is already prepared for you in the workspace\n\n\n\n# Create a function that returns all values below zero: below_zero\n\nbelow_zero \n<-\n \nfunction\n(\nx\n)\n \n{\n\n    x\n[\nx\n<\n0\n]\n\n\n}\n\n\n\n#below_zero(temp) alone won't work!!!\n\n\n\n# Apply below_zero over temp using sapply(): freezing_s\n\nfreezing_s \n<-\n \nsapply\n(\ntemp\n,\n below_zero\n)\n\n\n\n# Apply below_zero over temp using lapply(): freezing_l\n\nfreezing_l \n<-\n \nlapply\n(\ntemp\n,\n below_zero\n)\n\n\n\n# Compare freezing_s to freezing_l using identical()\n\n\nidentical\n(\nfreezing_s\n,\n freezing_l\n)\n\n\n\n\n\n\n\n1\n## [1] TRUE\n\n\n\n\n\n\nsapply\n with functions that return NULL\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# temp is already available in the workspace\n\n\n\n# Write a function that 'cat()s' out the average temperatures: print_info\n\nprint_info \n<-\n \nfunction\n(\nx\n)\n \n{\n\n    \ncat\n(\n'The average temperature is'\n,\n \nmean\n(\nx\n),\n \n'\\n'\n)\n\n\n}\n\n\n\n# Apply print_info() over temp using lapply()\n\n\nlapply\n(\ntemp\n,\n print_info\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n## The average temperature is 4.8 \n## The average temperature is 9 \n## The average temperature is 2.2 \n## The average temperature is 2.4 \n## The average temperature is 5.4 \n## The average temperature is 4.6 \n## The average temperature is 4.6\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL\n\n\n\n\n\n\n1\n2\n# Apply print_info() over temp using sapply()\n\n\nsapply\n(\ntemp\n,\n print_info\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n## The average temperature is 4.8 \n## The average temperature is 9 \n## The average temperature is 2.2 \n## The average temperature is 2.4 \n## The average temperature is 5.4 \n## The average temperature is 4.6 \n## The average temperature is 4.6\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL\n\n\n\n\n\n\nUse \nvapply\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# temp is already available in the workspace\n\n\n\n# Code the basics() function\n\nbasics \n<-\n \nfunction\n(\nx\n)\n \n{\n\n    \nc\n(\nminimum \n=\n \nmin\n(\nx\n),\n average \n=\n \nmean\n(\nx\n),\n maximum \n=\n \nmax\n(\nx\n))\n\n\n}\n\n\n\n# Apply basics() over temp using vapply()\n\n\nvapply\n(\ntemp\n,\n basics\n,\n \nnumeric\n(\n3\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n##         [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## minimum -1.0    5 -3.0 -2.0  2.0 -3.0  1.0\n## average  4.8    9  2.2  2.4  5.4  4.6  4.6\n## maximum  9.0   13  8.0  7.0  9.0  9.0  9.0\n\n\n\n\n\n\nUse \nvapply\n (2)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# temp is already available in the workspace\n\n\n\n# Definition of the basics() function\n\nbasics \n<-\n \nfunction\n(\nx\n)\n \n{\n\n  \nc\n(\nmin \n=\n \nmin\n(\nx\n),\n mean \n=\n \nmean\n(\nx\n),\n median \n=\n median\n(\nx\n),\n max \n=\n \nmax\n(\nx\n))\n\n\n}\n\n\n\n# Fix the error:\n\n\nvapply\n(\ntemp\n,\n basics\n,\n \nnumeric\n(\n4\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##        [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## min    -1.0    5 -3.0 -2.0  2.0 -3.0  1.0\n## mean    4.8    9  2.2  2.4  5.4  4.6  4.6\n## median  6.0    9  3.0  2.0  5.0  5.0  4.0\n## max     9.0   13  8.0  7.0  9.0  9.0  9.0\n\n\n\n\n\n\nFrom \nsapply\n to \nvapply\n\n\n1\n2\n3\n4\n# temp is already defined in the workspace\n\n\n\n# Convert to vapply() expression\n\n\nvapply\n(\ntemp\n,\n \nmax\n,\n \nnumeric\n(\n1\n))\n\n\n\n\n\n\n\n1\n## [1]  9 13  8  7  9  9  9\n\n\n\n\n\n\n1\n2\n# Convert to vapply() expression\n\n\nvapply\n(\ntemp\n,\n \nfunction\n(\nx\n,\n y\n)\n \n{\n \nmean\n(\nx\n)\n \n>\n y \n},\n y \n=\n \n5\n,\n \nlogical\n(\n1\n))\n\n\n\n\n\n\n\n1\n## [1] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Definition of get_info (don't change)\n\nget_info \n<-\n \nfunction\n(\nx\n,\n y\n)\n \n{\n \n  \nif\n \n(\nmean\n(\nx\n)\n \n>\n y\n)\n \n{\n\n    \nreturn\n(\n'Not too cold!'\n)\n\n  \n}\n \nelse\n \n{\n\n    \nreturn\n(\n'Pretty cold!'\n)\n\n  \n}\n\n\n}\n\n\n\n# Convert to vapply() expression\n\n\nvapply\n(\ntemp\n,\n get_info\n,\n y \n=\n \n5\n,\n \ncharacter\n(\n1\n))\n\n\n\n\n\n\n\n1\n2\n## [1] \"Pretty cold!\"  \"Not too cold!\" \"Pretty cold!\"  \"Pretty cold!\" \n## [5] \"Not too cold!\" \"Pretty cold!\"  \"Pretty cold!\"\n\n\n\n\n\n\nApply your knowledge. Or better yet: \nsapply\n it?\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# work_todos and fun_todos have already been defined\n\nwork_todos \n<-\n \nc\n(\n'Schedule call with team'\n,\n \n                \n'Fix error in Recommendation System'\n,\n \n                \n'Respond to Marc from IT'\n)\n\n\nfun_todos \n<-\n \nc\n(\n'Sleep'\n,\n \n'Make arrangements for summer trip'\n)\n\n\n\n# Create a list: todos\n\ntodos \n<-\n \nlist\n(\nwork_todos\n,\n fun_todos\n)\n\ntodos\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n## [[1]]\n## [1] \"Schedule call with team\"           \n## [2] \"Fix error in Recommendation System\"\n## [3] \"Respond to Marc from IT\"           \n## \n## [[2]]\n## [1] \"Sleep\"                             \"Make arrangements for summer trip\"\n\n\n\n\n\n\n1\n2\n# Sort the vectors inside todos alphabetically\n\n\nlapply\n(\ntodos\n,\n \nsort\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n## [[1]]\n## [1] \"Fix error in Recommendation System\"\n## [2] \"Respond to Marc from IT\"           \n## [3] \"Schedule call with team\"           \n## \n## [[2]]\n## [1] \"Make arrangements for summer trip\" \"Sleep\"\n\n\n\n\n\n\nUtilities\n\u00b6\n\n\nMathematical utilities\n\u00b6\n\n\n\n\nabs\n; calculate the absolute value.\n\n\nsum\n; calculate the sum of all the values in a data structure.\n\n\nmean\n; calculate the arithmetic mean.\n\n\nround\n; round the values to 0 decimal places by default. Try out\n\n\n?round\n in the console for variations of \nround\n and ways to change\n\n    the number of digits to round to.\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# The errors vector\n\nerrors \n<-\n \nc\n(\n1.9\n,\n \n-2.6\n,\n \n4.0\n,\n \n-9.5\n,\n \n-3.4\n,\n \n7.3\n)\n\n\n\n# Sum of absolute rounded values of errors\n\n\nsum\n(\nabs\n(\nround\n(\nerrors\n)))\n\n\n\n\n\n\n\n1\n## [1] 29\n\n\n\n\n\n\nFind the error\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n# Vectors\n\nvec1 \n<-\n \nc\n(\n1.5\n,\n \n2.5\n,\n \n8.4\n,\n \n3.7\n,\n \n6.3\n)\n\nvec2 \n<-\n \nrev\n(\nvec1\n)\n\n\n\n# Fix the error\n\n\nmean\n(\nabs\n(\nappend\n(\nvec1\n,\n vec2\n)))\n\n\n\n\n\n\n\n1\n## [1] 4.48\n\n\n\n\n\n\nData utilities\n\u00b6\n\n\n\n\nseq\n; generate sequences, by specifying the from, to and\n\n    by arguments.\n\n\nrep\n; replicate elements of vectors and lists.\n\n\nsort\n; sort a vector in ascending order. Works on numerics, but\n\n    also on character strings and logicals.\n\n\nrev\n; reverse the elements in a data structures for which reversal\n\n    is defined.\n\n\nstr\n; display the structure of any R object. append; Merge vectors\n\n    or lists.\n\n\nis.*\n; check for the class of an R object.\n\n\nas.*\n; convert an R object from one class to another.\n\n\nunlist\n; flatten (possibly embedded) lists to produce a vector.\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n# The linkedin and facebook vectors\n\nlinkedin \n<-\n \nlist\n(\n16\n,\n \n9\n,\n \n13\n,\n \n5\n,\n \n2\n,\n \n17\n,\n \n14\n)\n\nfacebook \n<-\n \nlist\n(\n17\n,\n \n7\n,\n \n5\n,\n \n16\n,\n \n8\n,\n \n13\n,\n \n14\n)\n\n\n\n# Convert linkedin and facebook to a vector: li_vec and fb_vec\n\nli_vec \n<-\n \nunlist\n(\nas.vector\n(\nlinkedin\n))\n\nfb_vec \n<-\n \nunlist\n(\nas.vector\n(\nfacebook\n))\n\n\n\n# Append fb_vec to li_vec: social_vec\n\nsocial_vec \n<-\n \nappend\n(\nli_vec\n,\n fb_vec\n)\n\n\n\n# Sort social_vec\n\n\nsort\n(\nsocial_vec\n,\n decreasing \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n1\n##  [1] 17 17 16 16 14 14 13 13  9  8  7  5  5  2\n\n\n\n\n\n\nFind the error (2)\n\n\n1\n2\n# Fix me\n\n\nround\n(\nsum\n(\nunlist\n(\nlist\n(\n1.1\n,\n \n3\n,\n \n5\n))))\n\n\n\n\n\n\n\n1\n## [1] 9\n\n\n\n\n\n\n1\n2\n# Fix me\n\n\nrep\n(\nseq\n(\n1\n,\n \n7\n,\n by \n=\n \n2\n),\n times \n=\n \n7\n)\n\n\n\n\n\n\n\n1\n##  [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7\n\n\n\n\n\n\n1\nprint\n(\nrep\n(\nseq\n(\n1\n,\n \n7\n,\n by \n=\n \n2\n),\n times \n=\n \n7\n))\n\n\n\n\n\n\n\n1\n##  [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7\n\n\n\n\n\n\nBeat Gauss using R\n\u00b6\n\n\n1\n2\n3\n# Create first sequence: seq1\n\nseq1 \n<-\n \nseq\n(\n1\n,\n500\n,\n by \n=\n \n3\n)\n\n\nprint\n(\nseq1\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n##   [1]   1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49\n##  [18]  52  55  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100\n##  [35] 103 106 109 112 115 118 121 124 127 130 133 136 139 142 145 148 151\n##  [52] 154 157 160 163 166 169 172 175 178 181 184 187 190 193 196 199 202\n##  [69] 205 208 211 214 217 220 223 226 229 232 235 238 241 244 247 250 253\n##  [86] 256 259 262 265 268 271 274 277 280 283 286 289 292 295 298 301 304\n## [103] 307 310 313 316 319 322 325 328 331 334 337 340 343 346 349 352 355\n## [120] 358 361 364 367 370 373 376 379 382 385 388 391 394 397 400 403 406\n## [137] 409 412 415 418 421 424 427 430 433 436 439 442 445 448 451 454 457\n## [154] 460 463 466 469 472 475 478 481 484 487 490 493 496 499\n\n\n\n\n\n\n1\n2\n3\n# Create second sequence: seq2\n\nseq2 \n<-\n \nseq\n(\n1200\n,\n \n900\n,\n by \n=\n \n-7\n)\n\n\nprint\n(\nseq2\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##  [1] 1200 1193 1186 1179 1172 1165 1158 1151 1144 1137 1130 1123 1116 1109\n## [15] 1102 1095 1088 1081 1074 1067 1060 1053 1046 1039 1032 1025 1018 1011\n## [29] 1004  997  990  983  976  969  962  955  948  941  934  927  920  913\n## [43]  906\n\n\n\n\n\n\n1\n2\n# Calculate total sum of the sequences\n\n\nprint\n(\nsum\n(\nappend\n(\nseq1\n,\n seq2\n)))\n\n\n\n\n\n\n\n1\n## [1] 87029\n\n\n\n\n\n\ngrepl\n & \ngrep\n (and the likes)\n\u00b6\n\n\n\n\ngrepl\n; return TRUE when a pattern is found in the corresponding\n\n    character string.\n\n\ngrep\n; return a vector of indices of the character strings that\n\n    contains the pattern.\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# The emails vector has\n\nemails \n<-\n \nc\n(\n'john.doe@ivyleague.edu'\n,\n \n'education@world.gov'\n,\n \n'dalai.lama@peace.org'\n,\n \n            \n'invalid.edu'\n,\n \n'quant@bigdatacollege.edu'\n,\n \n'cookie.monster@sesame.tv'\n)\n\n\n\n# Use grepl() to match for 'edu'\n\n\nprint\n(\ngrepl\n(\npattern \n=\n \n'edu'\n,\n x \n=\n emails\n))\n\n\n\n\n\n\n\n1\n## [1]  TRUE  TRUE FALSE  TRUE  TRUE FALSE\n\n\n\n\n\n\n1\n2\n3\n# Use grep() to match for 'edu', save result to hits\n\nhits \n<-\n \ngrep\n(\npattern \n=\n \n'edu'\n,\n x \n=\n emails\n)\n\nhits\n\n\n\n\n\n\n1\n## [1] 1 2 4 5\n\n\n\n\n\n\n1\n2\n# Subset emails using hits\n\n\nprint\n(\nemails\n[\nhits\n])\n\n\n\n\n\n\n\n1\n2\n## [1] \"john.doe@ivyleague.edu\"   \"education@world.gov\"     \n## [3] \"invalid.edu\"              \"quant@bigdatacollege.edu\"\n\n\n\n\n\n\ngrepl\n & \ngrep\n (2)\n\n\nConsult a regex character chart for more.\n\n\n1\n2\n3\n4\n5\n6\n# The emails vector\n\nemails \n<-\n \nc\n(\n'john.doe@ivyleague.edu'\n,\n \n'education@world.gov'\n,\n \n'dalai.lama@peace.org'\n,\n \n            \n'invalid.edu'\n,\n \n'quant@bigdatacollege.edu'\n,\n \n'cookie.monster@sesame.tv'\n)\n\n\n\n# Use grep() to match for .edu addresses more robustly\n\n\nprint\n(\ngrep\n(\npattern \n=\n \n'@.*\\\\.edu$'\n,\nx \n=\n emails\n))\n\n\n\n\n\n\n\n1\n## [1] 1 5\n\n\n\n\n\n\n1\n2\n3\n# Use grepl() to match for .edu addresses more robustly, save result to hits\n\nhits \n<-\n \ngrepl\n(\npattern \n=\n \n'@.*\\\\.edu$'\n,\nx \n=\n emails\n)\n\nhits\n\n\n\n\n\n\n1\n## [1]  TRUE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n\n\n\n1\n2\n# Subset emails using hits\n\n\nprint\n(\nemails\n[\nhits\n])\n\n\n\n\n\n\n\n1\n## [1] \"john.doe@ivyleague.edu\"   \"quant@bigdatacollege.edu\"\n\n\n\n\n\n\nsub\n & \ngsub\n\n\n1\n2\n3\n4\n5\n6\n# The emails vector\n\nemails \n<-\n \nc\n(\n'john.doe@ivyleague.edu'\n,\n \n'education@world.gov'\n,\n \n'dalai.lama@peace.org'\n,\n \n            \n'invalid.edu'\n,\n \n'quant@bigdatacollege.edu'\n,\n \n'cookie.monster@sesame.tv'\n)\n\n\n\n# Use sub() to convert the email domains to datacamp.edu (attempt 1)\n\n\nprint\n(\nsub\n(\npattern \n=\n \n'@.*\\\\.edu$'\n,\n replacement \n=\n \n'datacamp.edu'\n,\n x \n=\n emails\n))\n\n\n\n\n\n\n\n1\n2\n3\n## [1] \"john.doedatacamp.edu\"     \"education@world.gov\"     \n## [3] \"dalai.lama@peace.org\"     \"invalid.edu\"             \n## [5] \"quantdatacamp.edu\"        \"cookie.monster@sesame.tv\"\n\n\n\n\n\n\n1\n2\n# Use sub() to convert the email domains to datacamp.edu (attempt 2)\n\n\nprint\n(\nsub\n(\npattern \n=\n \n'@.*\\\\.edu$'\n,\n replacement \n=\n \n'@datacamp.edu'\n,\n x \n=\n emails\n))\n\n\n\n\n\n\n\n1\n2\n3\n## [1] \"john.doe@datacamp.edu\"    \"education@world.gov\"     \n## [3] \"dalai.lama@peace.org\"     \"invalid.edu\"             \n## [5] \"quant@datacamp.edu\"       \"cookie.monster@sesame.tv\"\n\n\n\n\n\n\nTime is of the essence\n\u00b6\n\n\nRight here, right now\n\n\n1\n2\n3\n# Get the current date: today\n\ntoday \n<-\n \nSys.Date\n()\n\ntoday\n\n\n\n\n\n\n1\n## [1] \"2017-04-14\"\n\n\n\n\n\n\n1\n2\n# See what today looks like under the hood\n\n\nprint\n(\nunclass\n(\ntoday\n))\n\n\n\n\n\n\n\n1\n## [1] 17270\n\n\n\n\n\n\n1\n2\n3\n# Get the current time: now\n\nnow \n<-\n \nSys.time\n()\n\nnow\n\n\n\n\n\n\n1\n## [1] \"2017-04-14 08:29:36 EDT\"\n\n\n\n\n\n\n1\n2\n# See what now looks like under the hood\n\n\nprint\n(\nunclass\n(\nnow\n))\n\n\n\n\n\n\n\n1\n## [1] 1492172976\n\n\n\n\n\n\nCreate and format dates\n\n\n\n\n\n\n\n\nSymbol\n\n\nMeaning\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n%d\n\n\nday as a number (0-31)\n\n\n31-janv\n\n\n\n\n\n\n%a\n\n\nabbreviated weekday\n\n\nMon\n\n\n\n\n\n\n%A\n\n\nunabbreviated weekday\n\n\nMonday\n\n\n\n\n\n\n%m\n\n\nmonth (00-12)\n\n\n00-12\n\n\n\n\n\n\n%b\n\n\nabbreviated month\n\n\nJan\n\n\n\n\n\n\n%B\n\n\nunabbreviated month\n\n\nJanuary\n\n\n\n\n\n\n%y\n\n\n2-digit year\n\n\n07\n\n\n\n\n\n\n%Y\n\n\n4-digit year\n\n\n2007\n\n\n\n\n\n\n%H\n\n\nhours as a decimal number\n\n\n23\n\n\n\n\n\n\n%M\n\n\nminutes as a decimal number\n\n\n10\n\n\n\n\n\n\n%S\n\n\nseconds as a decimal number\n\n\n53\n\n\n\n\n\n\n%T\n\n\nshorthand notation for the typical format %H:%M:%S\n\n\n23:10:53\n\n\n\n\n\n\n\n\n\nFind out more with \n?strptime\n.\n\n\nR offer default functions for dealing with time and dates. There are\n\nbetter packages: \ndate\n and \nlubridate\n.\n\n\nlubridate\n enhances time-series packages such as \nzoo\n and \nxts\n, and\n\nworks well with \ndplyr\n for data wrangling.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nlibrary\n(\ndate\n)\n\n\n\n# Definition of character strings representing dates\n\nstr1 \n<-\n \n\"May 23, 96\"\n\nstr2 \n<-\n \n\"2012-3-15\"\n\nstr3 \n<-\n \n\"30/January/2006\"\n\n\n\n# Convert the strings to dates: date1, date2, date3\n\ndate1 \n<-\n as.date\n(\nstr1\n,\n order \n=\n \n\"mdy\"\n)\n\ndate1\n\n\n\n\n\n\n1\n## [1] 23May96\n\n\n\n\n\n\n1\n2\ndate1 \n<-\n \nas.POSIXct\n(\ndate1\n,\n format \n=\n \n\"%d %m %y\"\n)\n\ndate1\n\n\n\n\n\n\n1\n## [1] \"1996-05-22 20:00:00 EDT\"\n\n\n\n\n\n\n1\n2\ndate2 \n<-\n as.date\n(\nstr2\n,\n order \n=\n \n\"ymd\"\n)\n\ndate2\n\n\n\n\n\n\n1\n## [1] 15Mar2012\n\n\n\n\n\n\n1\n2\ndate2 \n<-\n \nas.POSIXct\n(\ndate2\n,\n format \n=\n \n\"%d %m %y\"\n)\n\ndate2\n\n\n\n\n\n\n1\n## [1] \"2012-03-14 20:00:00 EDT\"\n\n\n\n\n\n\n1\n2\ndate3 \n<-\n as.date\n(\nstr3\n,\n order \n=\n \n\"dmy\"\n)\n\ndate3\n\n\n\n\n\n\n1\n## [1] 30Jan2006\n\n\n\n\n\n\n1\n2\ndate3 \n<-\n \nas.POSIXct\n(\ndate3\n,\n format \n=\n \n\"%d %m %y\"\n)\n\ndate3\n\n\n\n\n\n\n1\n## [1] \"2006-01-29 19:00:00 EST\"\n\n\n\n\n\n\n1\n2\n# Convert dates to formatted strings\n\n\nformat\n(\ndate1\n,\n \n\"%A\"\n)\n\n\n\n\n\n\n\n1\n## [1] \"mercredi\"\n\n\n\n\n\n\n1\nformat\n(\ndate2\n,\n \n\"%d\"\n)\n\n\n\n\n\n\n\n1\n## [1] \"14\"\n\n\n\n\n\n\n1\nformat\n(\ndate3\n,\n \n\"%b %Y\"\n)\n\n\n\n\n\n\n\n1\n## [1] \"janv. 2006\"\n\n\n\n\n\n\n1\n2\n3\n# convert dates to character data\n\nstrDate2 \n<-\n \nas.character\n(\ndate2\n)\n\nstrDate2\n\n\n\n\n\n\n1\n## [1] \"2012-03-14 20:00:00\"\n\n\n\n\n\n\nCreate and format times\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Definition of character strings representing times\n\nstr1 \n<-\n \n\"2012-3-12 14:23:08\"\n\n\n\n# Convert the strings to POSIXct objects: time1, time2\n\ntime1 \n<-\n \nas.POSIXct\n(\nstr2\n,\n format \n=\n \n\"%Y-%m-%d %H:%M:%S\"\n)\n\n\n\n# Convert times to formatted strings\n\n\n\n# Definition of character strings representing dates\n\n\nformat\n(\ntime1\n,\n \n\"%M\"\n)\n\n\n\n\n\n\n\n1\n## [1] NA\n\n\n\n\n\n\n1\nformat\n(\ntime1\n,\n \n\"%I:%M %p\"\n)\n\n\n\n\n\n\n\n1\n## [1] NA\n\n\n\n\n\n\nCalculations with dates\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# day1, day2, day3, day4 and day5\n\nday1 \n<-\n \nas.Date\n(\n\"2016-11-21\"\n)\n\nday2 \n<-\n \nas.Date\n(\n\"2016-11-16\"\n)\n\nday3 \n<-\n \nas.Date\n(\n\"2016-11-27\"\n)\n\nday4 \n<-\n \nas.Date\n(\n\"2016-11-14\"\n)\n\nday5 \n<-\n \nas.Date\n(\n\"2016-12-02\"\n)\n\n\n\n# Difference between last and first pizza day\n\n\nprint\n(\nday5 \n-\n day1\n)\n\n\n\n\n\n\n\n1\n## Time difference of 11 days\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# Create vector pizza\n\npizza \n<-\n \nc\n(\nday1\n,\n day2\n,\n day3\n,\n day4\n,\n day5\n)\n\n\n\n# Create differences between consecutive pizza days: day_diff\n\nday_diff \n<-\n \ndiff\n(\npizza\n,\n lag \n=\n \n1\n,\n differences \n=\n \n1\n)\n\nday_diff\n\n\n\n\n\n\n1\n2\n## Time differences in days\n## [1]  -5  11 -13  18\n\n\n\n\n\n\n1\n2\n# Average period between two consecutive pizza days\n\n\nprint\n(\nmean\n(\nday_diff\n))\n\n\n\n\n\n\n\n1\n## Time difference of 2.75 days\n\n\n\n\n\n\nCalculus with times\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# login and logout\n\nlogin \n<-\n \nas.POSIXct\n(\nc\n(\n\"2016-11-18 10:18:04 UTC\"\n,\n \n\"2016-11-23 09:14:18 UTC\"\n,\n \n\"2016-11-23 12:21:51 UTC\"\n,\n \n\"2016-11-23 12:37:24 UTC\"\n,\n \n\"2016-11-25 21:37:55 UTC\"\n))\n\n\nlogout \n<-\n \nas.POSIXct\n(\nc\n(\n\"2016-11-18 10:56:29 UTC\"\n,\n \n\"2016-11-23 09:14:52 UTC\"\n,\n \n\"2016-11-23 12:35:48 UTC\"\n,\n \n\"2016-11-23 13:17:22 UTC\"\n,\n \n\"2016-11-25 22:08:47 UTC\"\n))\n\n\n\n# Calculate the difference between login and logout: time_online\n\ntime_online \n<-\n logout \n-\n login\n\n\n# Inspect the variable time_online\n\n\n#class(time_online)\n\ntime_online\n\n\n\n\n\n\n1\n2\n## Time differences in secs\n## [1] 2305   34  837 2398 1852\n\n\n\n\n\n\n1\n2\n# Calculate the total time online\n\n\nprint\n(\nsum\n(\ntime_online\n))\n\n\n\n\n\n\n\n1\n## Time difference of 7426 secs\n\n\n\n\n\n\n1\n2\n# Calculate the average time online\n\n\nprint\n(\nmean\n(\ntime_online\n))\n\n\n\n\n\n\n\n1\n## Time difference of 1485.2 secs",
            "title": "Intermediate R"
        },
        {
            "location": "/Intermediate_R/#equality-or-not",
            "text": "1\n2 # Comparison of logicals  TRUE   ==   FALSE    1 ## [1] FALSE   1\n2 # Comparison of numerics  ( -6   *   14 )   !=   ( 17   -   101 )    1 ## [1] FALSE   1\n2 # Comparison of character strings  'useR'   ==   'user'    1 ## [1] FALSE   1\n2 # Compare a logical with a numeric  TRUE   ==   1    1 ## [1] TRUE   Greater and less than  1\n2 # Comparison of numerics  ( -6 * 5   +   2 )   >=   ( -10   +   1 )    1 ## [1] FALSE   1\n2 # Comparison of character strings  'raining'   <=   'raining dogs'    1 ## [1] TRUE   1\n2 # Comparison of logicals  TRUE   >   FALSE    1 ## [1] TRUE   Compare vectors  1\n2\n3\n4\n5\n6 # The linkedin and facebook vectors \nlinkedin  <-   c ( 16 ,   9 ,   13 ,   5 ,   2 ,   17 ,   14 ) \nfacebook  <-   c ( 17 ,   7 ,   5 ,   16 ,   8 ,   13 ,   14 )  # Popular days \nlinkedin  >   15    1 ## [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE   1\n2 # Quiet days \nlinkedin  <=   5    1 ## [1] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE   1\n2 # LinkedIn more popular than Facebook \nlinkedin  >  facebook   1 ## [1] FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE   Compare matrices  1\n2\n3\n4 views  <-   matrix ( c ( linkedin ,  facebook ),  nrow  =   2 ,  byrow  =   TRUE )  # When does views equal 13? \nviews  ==   13    1\n2\n3 ##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]\n## [1,] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n## [2,] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE   1\n2 # When is views less than or equal to 14? \nviews  <=   14    1\n2\n3 ##       [,1] [,2] [,3]  [,4] [,5]  [,6] [,7]\n## [1,] FALSE TRUE TRUE  TRUE TRUE FALSE TRUE\n## [2,] FALSE TRUE TRUE FALSE TRUE  TRUE TRUE   1\n2 # How often does facebook equal or exceed linkedin times two?  sum ( facebook  >=  linkedin  *   2 )    1 ## [1] 2",
            "title": "Equality (or not)"
        },
        {
            "location": "/Intermediate_R/#and",
            "text": "1\n2\n3\n4\n5\n6 # The linkedin and last variable \nlinkedin  <-   c ( 16 ,   9 ,   13 ,   5 ,   2 ,   17 ,   14 ) \nlast  <-   tail ( linkedin ,   1 )  # Is last under 5 or above 10? \nlast  <   5   |  last  >   10    1 ## [1] TRUE   1\n2 # Is last between 15 (exclusive) and 20 (inclusive)? \nlast  >   15   &  last  <=   20    1 ## [1] FALSE   1\n2 # Is last between 0 and 5 or between 10 and 15?  ( last  >   0   &  last  <   5 )   |   ( last  >   10   &  last  <   15 )    1 ## [1] TRUE   &  and  |  (2)  1\n2 # linkedin exceeds 10 but facebook below 10 \nlinkedin  >   10   &  facebook  <   10    1 ## [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE   1\n2 # When were one or both visited at least 12 times? \nlinkedin  >=   12   |  facebook  >=   12    1 ## [1]  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE   1\n2 # When is views between 11 (exclusive) and 14 (inclusive)? \nviews  >   11   &  views  <=   14    1\n2\n3 ##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6] [,7]\n## [1,] FALSE FALSE  TRUE FALSE FALSE FALSE TRUE\n## [2,] FALSE FALSE FALSE FALSE FALSE  TRUE TRUE   Blend it all together  1\n2\n3\n4\n5\n6\n7\n8 # Select the second column, named day2, from li_df: second \nsecond  <-  li_df $ day2 # Build a logical vector, TRUE if value in second is extreme: extremes \nextremes  <-   ( second  >   25   |  second  <   5 )  # Count the number of TRUEs in extremes  sum ( extremes )    1 ## [1] 16",
            "title": "&amp; and |"
        },
        {
            "location": "/Intermediate_R/#the-if-statement-and-more",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 # Variables related to your last day of recordings \nmedium  <-   'LinkedIn' \nnum_views  <-   14  # Examine the if statement for medium  if   ( medium  ==   'LinkedIn' )   { \n   print ( 'Showing LinkedIn information' )  }    1 ## [1] \"Showing LinkedIn information\"   1\n2\n3\n4 # Write the if statement for num_views  if   ( num_views  >   15 )   { \n     print ( 'You\\'re popular!' )  }    Add an  else   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Variables related to your last day of recordings \nmedium  <-   'LinkedIn' \nnum_views  <-   14  # Control structure for medium  if   ( medium  ==   'LinkedIn' )   { \n   print ( 'Showing LinkedIn information' )  }   else   { \n     print ( 'Unknown medium' )  }    1 ## [1] \"Showing LinkedIn information\"   1\n2\n3\n4\n5\n6 # Control structure for num_views  if   ( num_views  >   15 )   { \n   print ( 'You\\'re popular!' )  }   else   { \n     print ( 'Try to be more visible!' )  }    1 ## [1] \"Try to be more visible!\"   Customize further:  else if   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # Variables related to your last day of recordings \nmedium  <-   'LinkedIn' \nnum_views  <-   14  # Control structure for medium  if   ( medium  ==   'LinkedIn' )   { \n   print ( 'Showing LinkedIn information' )  }   else   if   ( medium  ==   'Facebook' )   { \n   print ( 'Showing Facebook information' )  }   else   { \n   print ( 'Unknown medium' )  }    1 ## [1] \"Showing LinkedIn information\"   1\n2\n3\n4\n5\n6\n7\n8 # Control structure for num_views  if   ( num_views  >   15 )   { \n   print ( 'You\\'re popular!' )  }   else   if   ( num_views  >   10   |  num_views  <=   15 )   { \n   print ( 'Your number of views is average' )  }   else   { \n   print ( 'Try to be more visible!' )  }    1 ## [1] \"Your number of views is average\"   Take control!   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 # Variables related to your last day of recordings \nli  <-   15 \nfb  <-   9  # Code the control-flow construct  if   ( li  >=   15   &  fb  >=   15 )   { \n    sms  <-   2 * ( li  +  fb )  }   else   if   ( li  <   10   &  fb  <   10 )   { \n    sms  <-   ( li  +  fb ) / 2  }   else   { \n    sms  <-  li  +  fb }  # Print the resulting sms to the console \nsms   1 ## [1] 24",
            "title": "The if statement (and more)"
        },
        {
            "location": "/Intermediate_R/#loops",
            "text": "",
            "title": "Loops"
        },
        {
            "location": "/Intermediate_R/#write-a-while-loop",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 # Initialize the speed variable \nspeed  <-   64  # Code the while loop  while   ( speed  >   30 )   { \n   print ( 'Slow down!' ) \n  speed  <-  speed  -   7  }    1\n2\n3\n4\n5 ## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"\n## [1] \"Slow down!\"   1\n2 # Print out the speed variable \nspeed   1 ## [1] 29   Throw in more conditionals   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Initialize the speed variable \nspeed  <-   64  # Extend/adapt the while loop  while   ( speed  >   30 )   { \n   print ( paste ( 'Your speed is ' ,  speed )) \n   if   ( speed  >   48 )   { \n     print ( 'Slow down big time!' ) \n    speed  <-  speed  -   11 \n   }   else   { \n         print ( 'Slow down!' ) \n        speed  <-  speed  -   6 \n   }  }    1\n2\n3\n4\n5\n6\n7\n8 ## [1] \"Your speed is  64\"\n## [1] \"Slow down big time!\"\n## [1] \"Your speed is  53\"\n## [1] \"Slow down big time!\"\n## [1] \"Your speed is  42\"\n## [1] \"Slow down!\"\n## [1] \"Your speed is  36\"\n## [1] \"Slow down!\"   Stop the  while  loop:  break   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 # Initialize the speed variable \nspeed  <-   88  while   ( speed  >   30 )   { \n   print ( paste ( 'Your speed is' , speed )) \n     # Break the while loop when speed exceeds 80 \n   if   ( speed  >   80 )   { \n     break \n   }   else   if   ( speed  >   48 )   { \n     print ( 'Slow down big time!' ) \n    speed  <-  speed  -   11 \n   }   else   { \n     print ( 'Slow down!' ) \n    speed  <-  speed  -   6 \n   }  }    1 ## [1] \"Your speed is 88\"   Build a  while  loop from scratch  strsplit ; split up in a vector that contains separate letters.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Initialize i \ni  <-   1  # Code the while loop  while   ( i  <=   10 )   { \n   print ( i  *   3 ) \n   if   (   ( i  *   3 )   %%   8   ==   0 )   { \n     break \n   } \n  i  <-  i  +   1  }    1\n2\n3\n4\n5\n6\n7\n8 ## [1] 3\n## [1] 6\n## [1] 9\n## [1] 12\n## [1] 15\n## [1] 18\n## [1] 21\n## [1] 24",
            "title": "Write a while loop"
        },
        {
            "location": "/Intermediate_R/#write-a-for-loop",
            "text": "Loop over a vector  1\n2\n3\n4\n5\n6\n7 # The linkedin vector \nlinkedin  <-   c ( 16 ,   9 ,   13 ,   5 ,   2 ,   17 ,   14 )  # Loop version 1  for   ( lin  in  linkedin )   { \n     print ( lin )  }    1\n2\n3\n4\n5\n6\n7 ## [1] 16\n## [1] 9\n## [1] 13\n## [1] 5\n## [1] 2\n## [1] 17\n## [1] 14   1\n2\n3\n4 # Loop version 2  for   ( i  in   1 : length ( linkedin ))   { \n     print ( linkedin [ i ])  }    1\n2\n3\n4\n5\n6\n7 ## [1] 16\n## [1] 9\n## [1] 13\n## [1] 5\n## [1] 2\n## [1] 17\n## [1] 14   Loop over a list  [[]] ; list of list.  1\n2\n3\n4\n5\n6\n7\n8\n9 # The nyc list is already specified \nnyc  <-   list ( pop  =   8405837 ,  \n            boroughs  =   c ( 'Manhattan' ,   'Bronx' ,   'Brooklyn' ,   'Queens' ,   'Staten Island' ),  \n            capital  =   FALSE )  # Loop version 1  for   ( item  in  nyc )   { \n     print ( item )  }    1\n2\n3\n4 ## [1] 8405837\n## [1] \"Manhattan\"     \"Bronx\"         \"Brooklyn\"      \"Queens\"       \n## [5] \"Staten Island\"\n## [1] FALSE   1\n2\n3\n4 # Loop version 2  for   ( i  in   1 : length ( nyc ))   { \n     print ( nyc [[ i ]])  }    1\n2\n3\n4 ## [1] 8405837\n## [1] \"Manhattan\"     \"Bronx\"         \"Brooklyn\"      \"Queens\"       \n## [5] \"Staten Island\"\n## [1] FALSE   Loop over a matrix  1\n2\n3\n4\n5\n6\n7\n8\n9 # The tic-tac-toe matrix has already been defined for you \nttt  <-   matrix ( c ( 'O' ,   NA ,   'X' ,   NA ,   'O' ,   NA ,   'X' ,   'O' ,   'X' ),  nrow  =   3 ,  ncol  =   3 )  # define the double for loop  for   ( i  in   1 : nrow ( ttt ))   { \n     for   ( j  in   1 : ncol ( ttt ))   { \n     print ( paste ( 'On row' ,  i , 'and column' ,  j , 'the board contains ' ,  ttt [ i , j ])) \n     }  }    1\n2\n3\n4\n5\n6\n7\n8\n9 ## [1] \"On row 1 and column 1 the board contains  O\"\n## [1] \"On row 1 and column 2 the board contains  NA\"\n## [1] \"On row 1 and column 3 the board contains  X\"\n## [1] \"On row 2 and column 1 the board contains  NA\"\n## [1] \"On row 2 and column 2 the board contains  O\"\n## [1] \"On row 2 and column 3 the board contains  O\"\n## [1] \"On row 3 and column 1 the board contains  X\"\n## [1] \"On row 3 and column 2 the board contains  NA\"\n## [1] \"On row 3 and column 3 the board contains  X\"",
            "title": "Write a for loop"
        },
        {
            "location": "/Intermediate_R/#mix-up-loops-with-control-flow",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # The linkedin vector \nlinkedin  <-   c ( 16 ,   9 ,   13 ,   5 ,   2 ,   17 ,   14 )  # Code the for loop with conditionals  for   ( i  in   1 : length ( linkedin ))   { \n     if   ( linkedin [ i ]   >   10 )   { \n         print ( 'You\\'re popular!' ) \n     }   else   { \n         print ( 'Be more visible!' ) \n     } \n     print ( linkedin [ i ])  }     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ## [1] \"You're popular!\"\n## [1] 16\n## [1] \"Be more visible!\"\n## [1] 9\n## [1] \"You're popular!\"\n## [1] 13\n## [1] \"Be more visible!\"\n## [1] 5\n## [1] \"Be more visible!\"\n## [1] 2\n## [1] \"You're popular!\"\n## [1] 17\n## [1] \"You're popular!\"\n## [1] 14   Next, you break it   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 # The linkedin vector \nlinkedin  <-   c ( 16 ,   9 ,   13 ,   5 ,   2 ,   17 ,   14 )  # Extend the for loop  for   ( li  in  linkedin )   { \n   if   ( li  >   10 )   { \n     print ( 'You\\'re popular!' ) \n   }   else   { \n     print ( 'Be more visible!' ) \n   } \n     # Add code to conditionally break iteration \n   if   ( li  >   16 )   { \n     print ( 'This is ridiculous, I\\'m outta here!' ) \n     break \n   } \n   # Add code to conditionally skip iteration \n   if   ( li  <   5 )   { \n     print ( 'This is too embarrassing!' ) \n     next \n     } \n   print ( li )  }     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ## [1] \"You're popular!\"\n## [1] 16\n## [1] \"Be more visible!\"\n## [1] 9\n## [1] \"You're popular!\"\n## [1] 13\n## [1] \"Be more visible!\"\n## [1] 5\n## [1] \"Be more visible!\"\n## [1] \"This is too embarrassing!\"\n## [1] \"You're popular!\"\n## [1] \"This is ridiculous, I'm outta here!\"   Build a for loop from scratch   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # Pre-defined variables \nrquote  <-   'R\\'s internals are irrefutably intriguing' \n\nchars  <-   strsplit ( rquote ,  split  =   '' )[[ 1 ]] \nrcount  <-   0  # Your solution here  for   ( i  in   1 : length ( chars ))   { \n     if   ( chars [ i ]   ==   'u' )   { \n     break \n     } \n     if   ( chars [ i ]   ==   'r'   |  chars [ i ]   ==   'R' )   { \n        rcount  <-  rcount  +   1 \n     }  }  # Print the resulting rcount variable to the console  print ( rcount )    1 ## [1] 5",
            "title": "Mix up loops with control flow"
        },
        {
            "location": "/Intermediate_R/#functions",
            "text": "",
            "title": "Functions"
        },
        {
            "location": "/Intermediate_R/#function-documentation",
            "text": "1\n2\n3\n4\n5 # Consult the documentation on the mean() function  ? mean  # Inspect the arguments of the mean() function  args ( mean )",
            "title": "Function documentation"
        },
        {
            "location": "/Intermediate_R/#use-a-function",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # The linkedin and facebook vectors \nlinkedin  <-   c ( 16 ,   9 ,   13 ,   5 ,   2 ,   17 ,   14 ) \nfacebook  <-   c ( 17 ,   7 ,   5 ,   16 ,   8 ,   13 ,   14 )  # Calculate average number of views \navg_li  <-   mean ( linkedin ) \navg_fb  <-   mean ( facebook )  # Inspect avg_li and avg_fb  print ( avg_li )    1 ## [1] 10.85714   1 print ( avg_fb )    1 ## [1] 11.42857   1 avg_li   1 ## [1] 10.85714   1\n2 # Calculate the mean of linkedin minus facebook  print ( mean ( linkedin  -  facebook ))    1 ## [1] -0.5714286   Use a function (2)   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # The linkedin and facebook vectors \nlinkedin  <-   c ( 16 ,   9 ,   13 ,   5 ,   2 ,   17 ,   14 ) \nfacebook  <-   c ( 17 ,   7 ,   5 ,   16 ,   8 ,   13 ,   14 )  # Calculate the mean of the sum \navg_sum  <-   mean ( linkedin  +  facebook )  # Calculate the trimmed mean of the sum \navg_sum_trimmed  <-   mean (( linkedin  +  facebook ),  trim  =   0.2 )  # Inspect both new variables \navg_sum   1 ## [1] 22.28571   1 avg_sum_trimmed   1 ## [1] 22.6   Use a function (3)  1\n2\n3\n4\n5\n6 # The linkedin and facebook vectors \nlinkedin  <-   c ( 16 ,   9 ,   13 ,   5 ,   NA ,   17 ,   14 ) \nfacebook  <-   c ( 17 ,   NA ,   5 ,   16 ,   8 ,   13 ,   14 )  # Basic average of linkedin  print ( mean ( linkedin ))    1 ## [1] NA   1\n2 # Advanced average of facebook  print ( mean ( facebook ,  na.rm  =   TRUE ))    1 ## [1] 12.16667",
            "title": "Use a function"
        },
        {
            "location": "/Intermediate_R/#functions-inside-functions",
            "text": "1\n2\n3\n4\n5\n6 # The linkedin and facebook vectors \nlinkedin  <-   c ( 16 ,   9 ,   13 ,   5 ,   NA ,   17 ,   14 ) \nfacebook  <-   c ( 17 ,   NA ,   5 ,   16 ,   8 ,   13 ,   14 )  # Calculate the mean absolute deviation  mean (( abs ( linkedin  -  facebook )),  na.rm  =   TRUE )    1 ## [1] 4.8",
            "title": "Functions inside functions"
        },
        {
            "location": "/Intermediate_R/#write-your-own-function",
            "text": "1\n2\n3\n4\n5\n6\n7 # Create a function pow_two() \npow_two  <-   function ( arg1 )   { \n    arg1 ^ 2  }  # Use the function  \npow_two ( 12 )    1 ## [1] 144   1\n2\n3\n4\n5\n6\n7 # Create a function sum_abs() \nsum_abs  <-   function ( arg2 , arg3 )   { \n     abs ( arg2 )   +   abs ( arg3 )  }  # Use the function \nsum_abs ( -2 , 3 )    1 ## [1] 5   Write your own function (2)  1\n2\n3\n4\n5\n6\n7\n8 # Define the function hello() \nhello  <-   function ()   { \n     print ( 'Hi there!' ) \n     return ( TRUE )  }  # Call the function hello() \nhello ()    1\n2\n3 ## [1] \"Hi there!\"\n\n## [1] TRUE    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Define the function my_filter() \nmy_filter  <-   function ( arg1 )   { \n     if   ( arg1  >   0 )   { \n         return ( arg1 ) \n     }   else   { \n         return ( NULL ) \n     }  }  # Call the function my_filter() twice \nmy_filter ( 5 )    1 ## [1] 5   1 my_filter ( -5 )    1 ## NULL   Write your own function (3)  Variables inside a function are not in the Global Environment.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Extend the pow_two() function \npow_two  <-   function ( x ,  print_info  =   TRUE )   { \n  y  <-  x  ^   2 \n   if   ( print_info )   { \n     print ( paste ( x ,   'to the power two equals' ,  y )) \n   } \n   return ( y )  }  #pow_two(2) \npow_two ( 2 ,   FALSE )    1 ## [1] 4",
            "title": "Write your own function"
        },
        {
            "location": "/Intermediate_R/#r-you-functional",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 # The linkedin and facebook vectors \nlinkedin  <-   c ( 16 ,   9 ,   13 ,   5 ,   NA ,   17 ,   14 ) \nfacebook  <-   c ( 17 ,   7 ,   5 ,   16 ,   8 ,   13 ,   14 )  # Define the interpret function \ninterpret  <-   function ( arg )   { \n     if   ( arg  >   15 )   { \n         print ( 'You\\'re popular!' ) \n         return ( arg ) \n     }   else   { \n         print ( 'Try to be more visible!' ) \n         return ( 0 ) \n     }  } \n\ninterpret ( linkedin [ 1 ])    1\n2\n3 ## [1] \"You're popular!\"\n\n## [1] 16   1 interpret ( facebook [ 2 ])    1\n2\n3 ## [1] \"Try to be more visible!\"\n\n## [1] 0   R you functional? (2)   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30 # The linkedin and facebook vectors \nlinkedin  <-   c ( 16 ,   9 ,   13 ,   5 ,   2 ,   17 ,   14 ) \nfacebook  <-   c ( 17 ,   7 ,   5 ,   16 ,   8 ,   13 ,   14 )  # The interpret() can be used inside interpret_all() \ninterpret  <-   function ( num_views ){ \n   if   ( num_views  >   15 )   { \n     print ( 'You\\'re popular!' ) \n     return ( num_views ) \n   }   else   { \n     print ( 'Try to be more visible!' ) \n     return ( 0 ) \n   }  }  # Define the interpret_all() function \ninterpret_all  <-   function ( data ,  logi  =   TRUE ){ \n  yy  <-   0 \n   for   ( i  in  data )   { \n    yy  <-  yy  +  interpret ( i ) \n   } \n   if   ( logi )   { \n     return ( yy ) \n   }   else   { \n     return ( NULL ) \n   }  }  # Call the interpret_all() function on both linkedin and facebook \ninterpret_all ( linkedin )    1\n2\n3\n4\n5\n6\n7\n8\n9 ## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n\n## [1] 33   1 interpret_all ( facebook )    1\n2\n3\n4\n5\n6\n7\n8\n9 ## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"You're popular!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n## [1] \"Try to be more visible!\"\n\n## [1] 33",
            "title": "R you functional?"
        },
        {
            "location": "/Intermediate_R/#load-an-r-package",
            "text": "1\n2\n3\n4\n5\n6 # The mtcars vectors have already been prepared for you \nwt  <-  mtcars $ wt\nhp  <-  mtcars $ hp # Request the currently attached packages  search ()    1\n2\n3\n4\n5\n6 ##  [1] \".GlobalEnv\"            \"package:XLConnect\"    \n##  [3] \"package:XLConnectJars\" \"package:stats\"        \n##  [5] \"package:graphics\"      \"package:grDevices\"    \n##  [7] \"package:utils\"         \"package:datasets\"     \n##  [9] \"package:methods\"       \"Autoloads\"            \n## [11] \"package:base\"   1\n2\n3\n4\n5 # Try the qplot() function with wt and hp \nplot ( wt , hp )  # Load the ggplot2 package  library ( 'ggplot2' )     1\n2\n3\n4\n5 # or  require ( 'ggplot2' )  # Retry the qplot() function \nqplot ( wt , hp )     1\n2 # Check out the currently attached packages again  search ()    1\n2\n3\n4\n5\n6 ##  [1] \".GlobalEnv\"            \"package:ggplot2\"      \n##  [3] \"package:XLConnect\"     \"package:XLConnectJars\"\n##  [5] \"package:stats\"         \"package:graphics\"     \n##  [7] \"package:grDevices\"     \"package:utils\"        \n##  [9] \"package:datasets\"      \"package:methods\"      \n## [11] \"Autoloads\"             \"package:base\"",
            "title": "Load an R package"
        },
        {
            "location": "/Intermediate_R/#the-apply-family",
            "text": "",
            "title": "The apply Family"
        },
        {
            "location": "/Intermediate_R/#use-lapply-with-a-built-in-r-function",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # The vector pioneers \npioneers  <-   c ( 'GAUSS:1777' ,   'BAYES:1702' ,   'PASCAL:1623' ,   'PEARSON:1857' )  # Split names from birth year: split_math \nsplit_math  <-   strsplit ( pioneers ,   ':' )  # Convert to lowercase strings: split_low \nsplit_low  <-   lapply ( split_math , tolower )  # Take a look at the structure of split_low \nstr ( split_low )    1\n2\n3\n4\n5 ## List of 4\n##  $ : chr [1:2] \"gauss\" \"1777\"\n##  $ : chr [1:2] \"bayes\" \"1702\"\n##  $ : chr [1:2] \"pascal\" \"1623\"\n##  $ : chr [1:2] \"pearson\" \"1857\"   Use  lapply  with your own function   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Code from previous exercise \npioneers  <-   c ( 'GAUSS:1777' ,   'BAYES:1702' ,   'PASCAL:1623' ,   'PEARSON:1857' ) \n\nsplit  <-   strsplit ( pioneers ,  split  =   ':' ) \nsplit_low  <-   lapply ( split ,   tolower )  # Write function select_first() \nselect_first  <-   function ( x )   { \n     return ( x [ 1 ])  }  # Apply select_first() over split_low: names \nnames  <-   lapply ( split_low ,  select_first )  print ( names )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## [[1]]\n## [1] \"gauss\"\n## \n## [[2]]\n## [1] \"bayes\"\n## \n## [[3]]\n## [1] \"pascal\"\n## \n## [[4]]\n## [1] \"pearson\"   1\n2\n3\n4\n5\n6\n7\n8 # Write function select_second() \nselect_second  <-   function ( x )   { \n     return ( x [ 2 ])  }  # Apply select_second() over split_low: years \nyears  <-   lapply ( split_low ,  select_second )  print ( years )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## [[1]]\n## [1] \"1777\"\n## \n## [[2]]\n## [1] \"1702\"\n## \n## [[3]]\n## [1] \"1623\"\n## \n## [[4]]\n## [1] \"1857\"   lapply  and anonymous functions  Anonymous function == lambda function.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 # Definition of split_low \npioneers  <-   c ( 'GAUSS:1777' ,   'BAYES:1702' ,   'PASCAL:1623' ,   'PEARSON:1857' ) \nsplit  <-   strsplit ( pioneers ,  split  =   ':' ) \nsplit_low  <-   lapply ( split ,   tolower )  #select_first <- function(x) {  #  x[1]  #} \n\nnames  <-   lapply ( split_low ,   function ( x )   {  x [ 1 ]   })  #select_second <- function(x) {  #  x[2]  #} \n\nyears  <-   lapply ( split_low ,   function ( x )   {  x [ 2 ]   })    Use  lapply  with additional arguments   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # Definition of split_low \npioneers  <-   c ( 'GAUSS:1777' ,   'BAYES:1702' ,   'PASCAL:1623' ,   'PEARSON:1857' ) \n\nsplit  <-   strsplit ( pioneers ,  split  =   ':' ) \nsplit_low  <-   lapply ( split ,   tolower )  # Replace the select_*() functions by a single function: select_el \nselect_el  <-   function ( x ,  i )   {  \n  x [ i ]   }  #select_second <- function(x) {   #  x[2]   #}  # Call the select_el() function twice on split_low: names and years \nnames  <-   lapply ( split_low ,  select_el ,  i = 1 ) \nyears  <-   lapply ( split_low ,  select_el ,   2 )",
            "title": "Use lapply (with a built-in R function)"
        },
        {
            "location": "/Intermediate_R/#use-sapply",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 temp1  <-   c ( 3 ,   7 ,   9 ,   6 ,   -1 ) \ntemp2  <-   c ( 6 ,   9 ,   12 ,   13 ,   5 ) \ntemp3  <-   c ( 4 ,   8 ,   3 ,   -1 ,   -3 ) \ntemp4  <-   c ( 1 ,   4 ,   7 ,   2 ,   -2 ) \ntemp5  <-   c ( 5 ,   7 ,   9 ,   4 ,   2 ) \ntemp6  <-   c ( -3 ,   5 ,   8 ,   9 ,   4 ) \ntemp7  <-   c ( 3 ,   6 ,   9 ,   4 ,   1 ) \n\ntemp  <-   list ( temp1 ,  temp2 ,  temp3 ,  temp4 ,  temp5 ,  temp6 ,  temp7 )  # Use lapply() to find each day's minimum temperature  lapply ( temp ,   min )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 ## [[1]]\n## [1] -1\n## \n## [[2]]\n## [1] 5\n## \n## [[3]]\n## [1] -3\n## \n## [[4]]\n## [1] -2\n## \n## [[5]]\n## [1] 2\n## \n## [[6]]\n## [1] -3\n## \n## [[7]]\n## [1] 1   1\n2 # Use sapply() to find each day's minimum temperature  sapply ( temp ,   min )    1 ## [1] -1  5 -3 -2  2 -3  1   1\n2 # Use lapply() to find each day's maximum temperature  lapply ( temp ,   max )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 ## [[1]]\n## [1] 9\n## \n## [[2]]\n## [1] 13\n## \n## [[3]]\n## [1] 8\n## \n## [[4]]\n## [1] 7\n## \n## [[5]]\n## [1] 9\n## \n## [[6]]\n## [1] 9\n## \n## [[7]]\n## [1] 9   1\n2 # Use sapply() to find each day's maximum temperature  sapply ( temp ,   max )    1 ## [1]  9 13  8  7  9  9  9   sapply  with your own function  1\n2\n3\n4\n5\n6\n7\n8\n9 # temp is already defined in the workspace  # Define a function calculates the average of the min and max of a vector: extremes_avg \nextremes_avg  <-   function ( x )   { \n     return (( min ( x )   +   max ( x )) / 2 )  }  # Apply extremes_avg() over temp using sapply()  sapply ( temp ,  extremes_avg )    1 ## [1] 4.0 9.0 2.5 2.5 5.5 3.0 5.0   1\n2 # Apply extremes_avg() over temp using lapply()  lapply ( temp ,  extremes_avg )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 ## [[1]]\n## [1] 4\n## \n## [[2]]\n## [1] 9\n## \n## [[3]]\n## [1] 2.5\n## \n## [[4]]\n## [1] 2.5\n## \n## [[5]]\n## [1] 5.5\n## \n## [[6]]\n## [1] 3\n## \n## [[7]]\n## [1] 5   sapply  with function returning vector  1\n2\n3\n4\n5\n6\n7\n8\n9 # temp is already available in the workspace  # Create a function that returns min and max of a vector: extremes \nextremes  <-   function ( x )   { \n     c ( min ( x ),   max ( x ))  }  # Apply extremes() over temp with sapply()  sapply ( temp ,  extremes )    1\n2\n3 ##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## [1,]   -1    5   -3   -2    2   -3    1\n## [2,]    9   13    8    7    9    9    9   1\n2 # Apply extremes() over temp with lapply()  lapply ( temp ,  extremes )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 ## [[1]]\n## [1] -1  9\n## \n## [[2]]\n## [1]  5 13\n## \n## [[3]]\n## [1] -3  8\n## \n## [[4]]\n## [1] -2  7\n## \n## [[5]]\n## [1] 2 9\n## \n## [[6]]\n## [1] -3  9\n## \n## [[7]]\n## [1] 1 9   sapply  can\u2019t simplify, now what?   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 # temp is already prepared for you in the workspace  # Create a function that returns all values below zero: below_zero \nbelow_zero  <-   function ( x )   { \n    x [ x < 0 ]  }  #below_zero(temp) alone won't work!!!  # Apply below_zero over temp using sapply(): freezing_s \nfreezing_s  <-   sapply ( temp ,  below_zero )  # Apply below_zero over temp using lapply(): freezing_l \nfreezing_l  <-   lapply ( temp ,  below_zero )  # Compare freezing_s to freezing_l using identical()  identical ( freezing_s ,  freezing_l )    1 ## [1] TRUE   sapply  with functions that return NULL  1\n2\n3\n4\n5\n6\n7\n8\n9 # temp is already available in the workspace  # Write a function that 'cat()s' out the average temperatures: print_info \nprint_info  <-   function ( x )   { \n     cat ( 'The average temperature is' ,   mean ( x ),   '\\n' )  }  # Apply print_info() over temp using lapply()  lapply ( temp ,  print_info )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 ## The average temperature is 4.8 \n## The average temperature is 9 \n## The average temperature is 2.2 \n## The average temperature is 2.4 \n## The average temperature is 5.4 \n## The average temperature is 4.6 \n## The average temperature is 4.6\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL   1\n2 # Apply print_info() over temp using sapply()  sapply ( temp ,  print_info )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 ## The average temperature is 4.8 \n## The average temperature is 9 \n## The average temperature is 2.2 \n## The average temperature is 2.4 \n## The average temperature is 5.4 \n## The average temperature is 4.6 \n## The average temperature is 4.6\n\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL",
            "title": "Use sapply"
        },
        {
            "location": "/Intermediate_R/#use-vapply",
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 # temp is already available in the workspace  # Code the basics() function \nbasics  <-   function ( x )   { \n     c ( minimum  =   min ( x ),  average  =   mean ( x ),  maximum  =   max ( x ))  }  # Apply basics() over temp using vapply()  vapply ( temp ,  basics ,   numeric ( 3 ))    1\n2\n3\n4 ##         [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## minimum -1.0    5 -3.0 -2.0  2.0 -3.0  1.0\n## average  4.8    9  2.2  2.4  5.4  4.6  4.6\n## maximum  9.0   13  8.0  7.0  9.0  9.0  9.0   Use  vapply  (2)  1\n2\n3\n4\n5\n6\n7\n8\n9 # temp is already available in the workspace  # Definition of the basics() function \nbasics  <-   function ( x )   { \n   c ( min  =   min ( x ),  mean  =   mean ( x ),  median  =  median ( x ),  max  =   max ( x ))  }  # Fix the error:  vapply ( temp ,  basics ,   numeric ( 4 ))    1\n2\n3\n4\n5 ##        [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n## min    -1.0    5 -3.0 -2.0  2.0 -3.0  1.0\n## mean    4.8    9  2.2  2.4  5.4  4.6  4.6\n## median  6.0    9  3.0  2.0  5.0  5.0  4.0\n## max     9.0   13  8.0  7.0  9.0  9.0  9.0   From  sapply  to  vapply  1\n2\n3\n4 # temp is already defined in the workspace  # Convert to vapply() expression  vapply ( temp ,   max ,   numeric ( 1 ))    1 ## [1]  9 13  8  7  9  9  9   1\n2 # Convert to vapply() expression  vapply ( temp ,   function ( x ,  y )   {   mean ( x )   >  y  },  y  =   5 ,   logical ( 1 ))    1 ## [1] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Definition of get_info (don't change) \nget_info  <-   function ( x ,  y )   {  \n   if   ( mean ( x )   >  y )   { \n     return ( 'Not too cold!' ) \n   }   else   { \n     return ( 'Pretty cold!' ) \n   }  }  # Convert to vapply() expression  vapply ( temp ,  get_info ,  y  =   5 ,   character ( 1 ))    1\n2 ## [1] \"Pretty cold!\"  \"Not too cold!\" \"Pretty cold!\"  \"Pretty cold!\" \n## [5] \"Not too cold!\" \"Pretty cold!\"  \"Pretty cold!\"",
            "title": "Use vapply"
        },
        {
            "location": "/Intermediate_R/#apply-your-knowledge-or-better-yet-sapply-it",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # work_todos and fun_todos have already been defined \nwork_todos  <-   c ( 'Schedule call with team' ,  \n                 'Fix error in Recommendation System' ,  \n                 'Respond to Marc from IT' ) \n\nfun_todos  <-   c ( 'Sleep' ,   'Make arrangements for summer trip' )  # Create a list: todos \ntodos  <-   list ( work_todos ,  fun_todos ) \ntodos   1\n2\n3\n4\n5\n6\n7 ## [[1]]\n## [1] \"Schedule call with team\"           \n## [2] \"Fix error in Recommendation System\"\n## [3] \"Respond to Marc from IT\"           \n## \n## [[2]]\n## [1] \"Sleep\"                             \"Make arrangements for summer trip\"   1\n2 # Sort the vectors inside todos alphabetically  lapply ( todos ,   sort )    1\n2\n3\n4\n5\n6\n7 ## [[1]]\n## [1] \"Fix error in Recommendation System\"\n## [2] \"Respond to Marc from IT\"           \n## [3] \"Schedule call with team\"           \n## \n## [[2]]\n## [1] \"Make arrangements for summer trip\" \"Sleep\"",
            "title": "Apply your knowledge. Or better yet: sapply it?"
        },
        {
            "location": "/Intermediate_R/#utilities",
            "text": "",
            "title": "Utilities"
        },
        {
            "location": "/Intermediate_R/#mathematical-utilities",
            "text": "abs ; calculate the absolute value.  sum ; calculate the sum of all the values in a data structure.  mean ; calculate the arithmetic mean.  round ; round the values to 0 decimal places by default. Try out  ?round  in the console for variations of  round  and ways to change \n    the number of digits to round to.    1\n2\n3\n4\n5 # The errors vector \nerrors  <-   c ( 1.9 ,   -2.6 ,   4.0 ,   -9.5 ,   -3.4 ,   7.3 )  # Sum of absolute rounded values of errors  sum ( abs ( round ( errors )))    1 ## [1] 29",
            "title": "Mathematical utilities"
        },
        {
            "location": "/Intermediate_R/#find-the-error",
            "text": "1\n2\n3\n4\n5\n6 # Vectors \nvec1  <-   c ( 1.5 ,   2.5 ,   8.4 ,   3.7 ,   6.3 ) \nvec2  <-   rev ( vec1 )  # Fix the error  mean ( abs ( append ( vec1 ,  vec2 )))    1 ## [1] 4.48",
            "title": "Find the error"
        },
        {
            "location": "/Intermediate_R/#data-utilities",
            "text": "seq ; generate sequences, by specifying the from, to and \n    by arguments.  rep ; replicate elements of vectors and lists.  sort ; sort a vector in ascending order. Works on numerics, but \n    also on character strings and logicals.  rev ; reverse the elements in a data structures for which reversal \n    is defined.  str ; display the structure of any R object. append; Merge vectors \n    or lists.  is.* ; check for the class of an R object.  as.* ; convert an R object from one class to another.  unlist ; flatten (possibly embedded) lists to produce a vector.     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 # The linkedin and facebook vectors \nlinkedin  <-   list ( 16 ,   9 ,   13 ,   5 ,   2 ,   17 ,   14 ) \nfacebook  <-   list ( 17 ,   7 ,   5 ,   16 ,   8 ,   13 ,   14 )  # Convert linkedin and facebook to a vector: li_vec and fb_vec \nli_vec  <-   unlist ( as.vector ( linkedin )) \nfb_vec  <-   unlist ( as.vector ( facebook ))  # Append fb_vec to li_vec: social_vec \nsocial_vec  <-   append ( li_vec ,  fb_vec )  # Sort social_vec  sort ( social_vec ,  decreasing  =   TRUE )    1 ##  [1] 17 17 16 16 14 14 13 13  9  8  7  5  5  2   Find the error (2)  1\n2 # Fix me  round ( sum ( unlist ( list ( 1.1 ,   3 ,   5 ))))    1 ## [1] 9   1\n2 # Fix me  rep ( seq ( 1 ,   7 ,  by  =   2 ),  times  =   7 )    1 ##  [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7   1 print ( rep ( seq ( 1 ,   7 ,  by  =   2 ),  times  =   7 ))    1 ##  [1] 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7",
            "title": "Data utilities"
        },
        {
            "location": "/Intermediate_R/#beat-gauss-using-r",
            "text": "1\n2\n3 # Create first sequence: seq1 \nseq1  <-   seq ( 1 , 500 ,  by  =   3 )  print ( seq1 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ##   [1]   1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49\n##  [18]  52  55  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100\n##  [35] 103 106 109 112 115 118 121 124 127 130 133 136 139 142 145 148 151\n##  [52] 154 157 160 163 166 169 172 175 178 181 184 187 190 193 196 199 202\n##  [69] 205 208 211 214 217 220 223 226 229 232 235 238 241 244 247 250 253\n##  [86] 256 259 262 265 268 271 274 277 280 283 286 289 292 295 298 301 304\n## [103] 307 310 313 316 319 322 325 328 331 334 337 340 343 346 349 352 355\n## [120] 358 361 364 367 370 373 376 379 382 385 388 391 394 397 400 403 406\n## [137] 409 412 415 418 421 424 427 430 433 436 439 442 445 448 451 454 457\n## [154] 460 463 466 469 472 475 478 481 484 487 490 493 496 499   1\n2\n3 # Create second sequence: seq2 \nseq2  <-   seq ( 1200 ,   900 ,  by  =   -7 )  print ( seq2 )    1\n2\n3\n4 ##  [1] 1200 1193 1186 1179 1172 1165 1158 1151 1144 1137 1130 1123 1116 1109\n## [15] 1102 1095 1088 1081 1074 1067 1060 1053 1046 1039 1032 1025 1018 1011\n## [29] 1004  997  990  983  976  969  962  955  948  941  934  927  920  913\n## [43]  906   1\n2 # Calculate total sum of the sequences  print ( sum ( append ( seq1 ,  seq2 )))    1 ## [1] 87029",
            "title": "Beat Gauss using R"
        },
        {
            "location": "/Intermediate_R/#grepl-grep-and-the-likes",
            "text": "grepl ; return TRUE when a pattern is found in the corresponding \n    character string.  grep ; return a vector of indices of the character strings that \n    contains the pattern.    1\n2\n3\n4\n5\n6 # The emails vector has \nemails  <-   c ( 'john.doe@ivyleague.edu' ,   'education@world.gov' ,   'dalai.lama@peace.org' ,  \n             'invalid.edu' ,   'quant@bigdatacollege.edu' ,   'cookie.monster@sesame.tv' )  # Use grepl() to match for 'edu'  print ( grepl ( pattern  =   'edu' ,  x  =  emails ))    1 ## [1]  TRUE  TRUE FALSE  TRUE  TRUE FALSE   1\n2\n3 # Use grep() to match for 'edu', save result to hits \nhits  <-   grep ( pattern  =   'edu' ,  x  =  emails ) \nhits   1 ## [1] 1 2 4 5   1\n2 # Subset emails using hits  print ( emails [ hits ])    1\n2 ## [1] \"john.doe@ivyleague.edu\"   \"education@world.gov\"     \n## [3] \"invalid.edu\"              \"quant@bigdatacollege.edu\"   grepl  &  grep  (2)  Consult a regex character chart for more.  1\n2\n3\n4\n5\n6 # The emails vector \nemails  <-   c ( 'john.doe@ivyleague.edu' ,   'education@world.gov' ,   'dalai.lama@peace.org' ,  \n             'invalid.edu' ,   'quant@bigdatacollege.edu' ,   'cookie.monster@sesame.tv' )  # Use grep() to match for .edu addresses more robustly  print ( grep ( pattern  =   '@.*\\\\.edu$' , x  =  emails ))    1 ## [1] 1 5   1\n2\n3 # Use grepl() to match for .edu addresses more robustly, save result to hits \nhits  <-   grepl ( pattern  =   '@.*\\\\.edu$' , x  =  emails ) \nhits   1 ## [1]  TRUE FALSE FALSE FALSE  TRUE FALSE   1\n2 # Subset emails using hits  print ( emails [ hits ])    1 ## [1] \"john.doe@ivyleague.edu\"   \"quant@bigdatacollege.edu\"   sub  &  gsub  1\n2\n3\n4\n5\n6 # The emails vector \nemails  <-   c ( 'john.doe@ivyleague.edu' ,   'education@world.gov' ,   'dalai.lama@peace.org' ,  \n             'invalid.edu' ,   'quant@bigdatacollege.edu' ,   'cookie.monster@sesame.tv' )  # Use sub() to convert the email domains to datacamp.edu (attempt 1)  print ( sub ( pattern  =   '@.*\\\\.edu$' ,  replacement  =   'datacamp.edu' ,  x  =  emails ))    1\n2\n3 ## [1] \"john.doedatacamp.edu\"     \"education@world.gov\"     \n## [3] \"dalai.lama@peace.org\"     \"invalid.edu\"             \n## [5] \"quantdatacamp.edu\"        \"cookie.monster@sesame.tv\"   1\n2 # Use sub() to convert the email domains to datacamp.edu (attempt 2)  print ( sub ( pattern  =   '@.*\\\\.edu$' ,  replacement  =   '@datacamp.edu' ,  x  =  emails ))    1\n2\n3 ## [1] \"john.doe@datacamp.edu\"    \"education@world.gov\"     \n## [3] \"dalai.lama@peace.org\"     \"invalid.edu\"             \n## [5] \"quant@datacamp.edu\"       \"cookie.monster@sesame.tv\"",
            "title": "grepl &amp; grep (and the likes)"
        },
        {
            "location": "/Intermediate_R/#time-is-of-the-essence",
            "text": "Right here, right now  1\n2\n3 # Get the current date: today \ntoday  <-   Sys.Date () \ntoday   1 ## [1] \"2017-04-14\"   1\n2 # See what today looks like under the hood  print ( unclass ( today ))    1 ## [1] 17270   1\n2\n3 # Get the current time: now \nnow  <-   Sys.time () \nnow   1 ## [1] \"2017-04-14 08:29:36 EDT\"   1\n2 # See what now looks like under the hood  print ( unclass ( now ))    1 ## [1] 1492172976   Create and format dates     Symbol  Meaning  Example      %d  day as a number (0-31)  31-janv    %a  abbreviated weekday  Mon    %A  unabbreviated weekday  Monday    %m  month (00-12)  00-12    %b  abbreviated month  Jan    %B  unabbreviated month  January    %y  2-digit year  07    %Y  4-digit year  2007    %H  hours as a decimal number  23    %M  minutes as a decimal number  10    %S  seconds as a decimal number  53    %T  shorthand notation for the typical format %H:%M:%S  23:10:53     Find out more with  ?strptime .  R offer default functions for dealing with time and dates. There are \nbetter packages:  date  and  lubridate .  lubridate  enhances time-series packages such as  zoo  and  xts , and \nworks well with  dplyr  for data wrangling.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 library ( date )  # Definition of character strings representing dates \nstr1  <-   \"May 23, 96\" \nstr2  <-   \"2012-3-15\" \nstr3  <-   \"30/January/2006\"  # Convert the strings to dates: date1, date2, date3 \ndate1  <-  as.date ( str1 ,  order  =   \"mdy\" ) \ndate1   1 ## [1] 23May96   1\n2 date1  <-   as.POSIXct ( date1 ,  format  =   \"%d %m %y\" ) \ndate1   1 ## [1] \"1996-05-22 20:00:00 EDT\"   1\n2 date2  <-  as.date ( str2 ,  order  =   \"ymd\" ) \ndate2   1 ## [1] 15Mar2012   1\n2 date2  <-   as.POSIXct ( date2 ,  format  =   \"%d %m %y\" ) \ndate2   1 ## [1] \"2012-03-14 20:00:00 EDT\"   1\n2 date3  <-  as.date ( str3 ,  order  =   \"dmy\" ) \ndate3   1 ## [1] 30Jan2006   1\n2 date3  <-   as.POSIXct ( date3 ,  format  =   \"%d %m %y\" ) \ndate3   1 ## [1] \"2006-01-29 19:00:00 EST\"   1\n2 # Convert dates to formatted strings  format ( date1 ,   \"%A\" )    1 ## [1] \"mercredi\"   1 format ( date2 ,   \"%d\" )    1 ## [1] \"14\"   1 format ( date3 ,   \"%b %Y\" )    1 ## [1] \"janv. 2006\"   1\n2\n3 # convert dates to character data \nstrDate2  <-   as.character ( date2 ) \nstrDate2   1 ## [1] \"2012-03-14 20:00:00\"   Create and format times   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Definition of character strings representing times \nstr1  <-   \"2012-3-12 14:23:08\"  # Convert the strings to POSIXct objects: time1, time2 \ntime1  <-   as.POSIXct ( str2 ,  format  =   \"%Y-%m-%d %H:%M:%S\" )  # Convert times to formatted strings  # Definition of character strings representing dates  format ( time1 ,   \"%M\" )    1 ## [1] NA   1 format ( time1 ,   \"%I:%M %p\" )    1 ## [1] NA   Calculations with dates  1\n2\n3\n4\n5\n6\n7\n8\n9 # day1, day2, day3, day4 and day5 \nday1  <-   as.Date ( \"2016-11-21\" ) \nday2  <-   as.Date ( \"2016-11-16\" ) \nday3  <-   as.Date ( \"2016-11-27\" ) \nday4  <-   as.Date ( \"2016-11-14\" ) \nday5  <-   as.Date ( \"2016-12-02\" )  # Difference between last and first pizza day  print ( day5  -  day1 )    1 ## Time difference of 11 days   1\n2\n3\n4\n5\n6 # Create vector pizza \npizza  <-   c ( day1 ,  day2 ,  day3 ,  day4 ,  day5 )  # Create differences between consecutive pizza days: day_diff \nday_diff  <-   diff ( pizza ,  lag  =   1 ,  differences  =   1 ) \nday_diff   1\n2 ## Time differences in days\n## [1]  -5  11 -13  18   1\n2 # Average period between two consecutive pizza days  print ( mean ( day_diff ))    1 ## Time difference of 2.75 days   Calculus with times   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # login and logout \nlogin  <-   as.POSIXct ( c ( \"2016-11-18 10:18:04 UTC\" ,   \"2016-11-23 09:14:18 UTC\" ,   \"2016-11-23 12:21:51 UTC\" ,   \"2016-11-23 12:37:24 UTC\" ,   \"2016-11-25 21:37:55 UTC\" )) \n\nlogout  <-   as.POSIXct ( c ( \"2016-11-18 10:56:29 UTC\" ,   \"2016-11-23 09:14:52 UTC\" ,   \"2016-11-23 12:35:48 UTC\" ,   \"2016-11-23 13:17:22 UTC\" ,   \"2016-11-25 22:08:47 UTC\" ))  # Calculate the difference between login and logout: time_online \ntime_online  <-  logout  -  login # Inspect the variable time_online  #class(time_online) \ntime_online   1\n2 ## Time differences in secs\n## [1] 2305   34  837 2398 1852   1\n2 # Calculate the total time online  print ( sum ( time_online ))    1 ## Time difference of 7426 secs   1\n2 # Calculate the average time online  print ( mean ( time_online ))    1 ## Time difference of 1485.2 secs",
            "title": "Time is of the essence"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\n\n\nManipulate all data classes.\n\n\nTransform one data class into another class\n\n\nAvoid explicit use of loop constructs and speed up the code.\n\n\nAggregate or subset the data.\n\n\n\n\napply\n to Matrices and Arrays\n\u00b6\n\n\nFlatten a matrix into a vector.\n\n\n1\n2\n3\n# Dataset\n\nX \n<-\n \nmatrix\n(\nrnorm\n(\n30\n),\n nrow \n=\n \n4\n,\n ncol \n=\n \n4\n)\n\nX\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##            [,1]       [,2]       [,3]       [,4]\n## [1,] -0.1902972 -0.3250492  0.1597852 -0.4878550\n## [2,] -0.4195968 -1.8485172 -0.1185904  0.2307813\n## [3,] -0.6387653 -0.7848382 -0.4198662 -0.6314304\n## [4,]  0.7374748 -2.0634558  0.2374686 -1.6572091\n\n\n\n\n\n\n1\n2\n# Sum the values of each column\n\n\napply\n(\nX\n,\n \n2\n,\n \nsum\n)\n\n\n\n\n\n\n\n1\n## [1] -0.5111845 -5.0218604 -0.1412028 -2.5457132\n\n\n\n\n\n\n\n\nX\n is the array or matrix (2D, 3D, etc.).\n\n\nMARGIN=1\n for row, \n2\n for column.\n\n\nFUN=sum\n, \nmean\n, \nmedian\n, \nmin\n, \nmax\n, \nvar\n, \nsd\n, \nrange\n,\n\n\nlength\n, \nskew\n, \nkurtosis\n, \nabs\n, \nround\n, \ntolower\n,\n\n\ntoupper\n, etc.\n\n\nuser-defined function such as\n\n\nFUN = function(x) { c(m = mean(x), s = sd(x)) }\n.\n\n\n\n\n\n\n\n1\n2\n# More examples\n\n\napply\n(\nX\n,\n \n2\n,\n \nmax\n)\n\n\n\n\n\n\n\n1\n## [1]  0.7374748 -0.3250492  0.2374686  0.2307813\n\n\n\n\n\n\n1\napply\n(\nX\n,\n \n2\n,\n \nrange\n)\n\n\n\n\n\n\n\n1\n2\n3\n##            [,1]       [,2]       [,3]       [,4]\n## [1,] -0.6387653 -2.0634558 -0.4198662 -1.6572091\n## [2,]  0.7374748 -0.3250492  0.2374686  0.2307813\n\n\n\n\n\n\n1\napply\n(\nX\n,\n \n2\n,\n \nabs\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##           [,1]      [,2]      [,3]      [,4]\n## [1,] 0.1902972 0.3250492 0.1597852 0.4878550\n## [2,] 0.4195968 1.8485172 0.1185904 0.2307813\n## [3,] 0.6387653 0.7848382 0.4198662 0.6314304\n## [4,] 0.7374748 2.0634558 0.2374686 1.6572091\n\n\n\n\n\n\n1\napply\n(\nX\n,\n \n2\n,\n \nround\n,\n \n2\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##       [,1]  [,2]  [,3]  [,4]\n## [1,] -0.19 -0.33  0.16 -0.49\n## [2,] -0.42 -1.85 -0.12  0.23\n## [3,] -0.64 -0.78 -0.42 -0.63\n## [4,]  0.74 -2.06  0.24 -1.66\n\n\n\n\n\n\nLambda & custom function\n\n\n1\n2\n# Dataset\n\nX\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##            [,1]       [,2]       [,3]       [,4]\n## [1,] -0.1902972 -0.3250492  0.1597852 -0.4878550\n## [2,] -0.4195968 -1.8485172 -0.1185904  0.2307813\n## [3,] -0.6387653 -0.7848382 -0.4198662 -0.6314304\n## [4,]  0.7374748 -2.0634558  0.2374686 -1.6572091\n\n\n\n\n\n\n1\n2\n# Built-in function\n\n\napply\n(\nX\n,\n \n2\n,\n \nmax\n)\n\n\n\n\n\n\n\n1\n## [1]  0.7374748 -0.3250492  0.2374686  0.2307813\n\n\n\n\n\n\n1\n2\n# Lambda function\n\n\napply\n(\nX\n,\n \n2\n,\n \nfunction\n(\nx\n)\n x \n+\n \n10\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##           [,1]     [,2]      [,3]      [,4]\n## [1,]  9.809703 9.674951 10.159785  9.512145\n## [2,]  9.580403 8.151483  9.881410 10.230781\n## [3,]  9.361235 9.215162  9.580134  9.368570\n## [4,] 10.737475 7.936544 10.237469  8.342791\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Custom function\n\nselect_first \n<-\n \nfunction\n(\nx\n)\n \n{\n\n    \nreturn\n(\nx\n[\n1\n])\n\n\n}\n\n\napply\n(\nX\n,\n \n2\n,\n select_first\n)\n\n\n\n\n\n\n\n1\n## [1] -0.1902972 -0.3250492  0.1597852 -0.4878550\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Custom function with two arguments\n\nselect_el \n<-\n \nfunction\n(\nx\n,\n i\n)\n \n{\n \n  x\n[\ni\n]\n \n\n}\n\n\napply\n(\nX\n,\n \n2\n,\n select_el\n,\n i \n=\n \n2\n)\n\n\n\n\n\n\n\n1\n## [1] -0.4195968 -1.8485172 -0.1185904  0.2307813\n\n\n\n\n\n\nStrings\n\n\n1\n2\n3\n# Dataset\n\nY \n<-\n \nmatrix\n(\nc\n(\n'a'\n,\n \n'b'\n,\n \n'c'\n,\n \n'd'\n),\n nrow \n=\n \n2\n,\n ncol \n=\n \n2\n)\n\nY\n\n\n\n\n\n\n1\n2\n3\n##      [,1] [,2]\n## [1,] \"a\"  \"c\" \n## [2,] \"b\"  \"d\"\n\n\n\n\n\n\n1\n2\n# Change the format\n\n\napply\n(\nY\n,\n \n2\n,\n \ntoupper\n)\n\n\n\n\n\n\n\n1\n2\n3\n##      [,1] [,2]\n## [1,] \"A\"  \"C\" \n## [2,] \"B\"  \"D\"\n\n\n\n\n\n\nsweep\n\u00b6\n\n\nSeveral steps\n\n\n1\n2\n3\n# Dataset\n\ndataPoints \n<-\n \nmatrix\n(\n4\n:\n15\n,\n nrow \n=\n \n4\n,\n ncol \n=\n \n3\n)\n\ndataPoints\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##      [,1] [,2] [,3]\n## [1,]    4    8   12\n## [2,]    5    9   13\n## [3,]    6   10   14\n## [4,]    7   11   15\n\n\n\n\n\n\n1\n2\n3\n# Find means (center) per column with `apply()`\n\ndataPoints_means \n<-\n \napply\n(\ndataPoints\n,\n \n2\n,\n \nmean\n)\n\ndataPoints_means\n\n\n\n\n\n\n1\n## [1]  5.5  9.5 13.5\n\n\n\n\n\n\n1\n2\n3\n# Find standard deviation (dispersion) with `apply()`\n\ndataPoints_sdev \n<-\n \napply\n(\ndataPoints\n,\n \n2\n,\n sd\n)\n\ndataPoints_sdev\n\n\n\n\n\n\n1\n## [1] 1.290994 1.290994 1.290994\n\n\n\n\n\n\n1\n2\n3\n# Center the points; shift all the points with respect to their center\n\ndataPoints_Trans1 \n<-\n \nsweep\n(\ndataPoints\n,\n \n2\n,\n dataPoints_means\n,\n\"-\"\n)\n\ndataPoints_Trans1\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##      [,1] [,2] [,3]\n## [1,] -1.5 -1.5 -1.5\n## [2,] -0.5 -0.5 -0.5\n## [3,]  0.5  0.5  0.5\n## [4,]  1.5  1.5  1.5\n\n\n\n\n\n\n1\n2\n3\n# Normalize\n\ndataPoints_Trans2 \n<-\n \nsweep\n(\ndataPoints_Trans1\n,\n \n2\n,\n dataPoints_sdev\n,\n \n\"/\"\n)\n\ndataPoints_Trans2\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##            [,1]       [,2]       [,3]\n## [1,] -1.1618950 -1.1618950 -1.1618950\n## [2,] -0.3872983 -0.3872983 -0.3872983\n## [3,]  0.3872983  0.3872983  0.3872983\n## [4,]  1.1618950  1.1618950  1.1618950\n\n\n\n\n\n\n1\nsweep\n(\ndataPoints\n,\n \n2\n,\n dataPoints_means\n,\n \nsum\n)\n\n\n\n\n\n\n\n1\n## [1] 228\n\n\n\n\n\n\n\n\nX\n.\n\n\nMARGIN=1\n for row, \n2\n for column.\n\n\nSTATS\n for the summary statistics to be swept out: \nsum\n, \nmean\n,\n\n\nmedian\n, \nmin\n, \nmax\n, \nvar\n, \nsd\n, \nrange\n, \nlength\n, \nskew\n,\n\n\nkurtosis\n, \nse\n, etc.\n\n\nFUN=\"-\"\n, \n\"+\"\n, \n\"/\"\n, \n\"*\"\n, \n\"**\"\n, etc.\n\n\nuser-defined function such as\n\n\nFUN = function(x) { c(m = mean(x), s = sd(x)) }\n.\n\n\n\n\nOne step\n\n\n1\n2\n3\n4\n# Normalize the data with a nested call\n\ndataPoints_Trans \n<-\n \nsweep\n(\nsweep\n(\ndataPoints\n,\n \n2\n,\n dataPoints_means\n,\n\"-\"\n),\n \n2\n,\n dataPoints_sdev\n,\n\"/\"\n)\n\n\ndataPoints_Trans\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##            [,1]       [,2]       [,3]\n## [1,] -1.1618950 -1.1618950 -1.1618950\n## [2,] -0.3872983 -0.3872983 -0.3872983\n## [3,]  0.3872983  0.3872983  0.3872983\n## [4,]  1.1618950  1.1618950  1.1618950\n\n\n\n\n\n\nEven simpler\n\n\n1\n2\n# Automatic data scaling\n\n\nscale\n(\ndataPoints\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n##            [,1]       [,2]       [,3]\n## [1,] -1.1618950 -1.1618950 -1.1618950\n## [2,] -0.3872983 -0.3872983 -0.3872983\n## [3,]  0.3872983  0.3872983  0.3872983\n## [4,]  1.1618950  1.1618950  1.1618950\n## attr(,\"scaled:center\")\n## [1]  5.5  9.5 13.5\n## attr(,\"scaled:scale\")\n## [1] 1.290994 1.290994 1.290994\n\n\n\n\n\n\naggregate\n\u00b6\n\n\n1\n2\n# Dataset\n\n\nhead\n(\nMydf\n,\n \n15\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n##     X DepPC DProgr Qty Delivered\n## 1   1    90      1   7     FALSE\n## 2   2    91      2   8      TRUE\n## 3   3    92      3   9     FALSE\n## 4   4    93      4  10      TRUE\n## 5   5    94      5  11      TRUE\n## 6   6    75      6  12     FALSE\n## 7   7    90      7  13      TRUE\n## 8   8    91      8  14      TRUE\n## 9   9    92      9  15     FALSE\n## 10 10    93     10  16     FALSE\n## 11 11    94     11  17      TRUE\n## 12 12    75     12  18     FALSE\n## 13 13    90     13  19     FALSE\n## 14 14    91     14  20      TRUE\n## 15 15    92     15  21      TRUE\n\n\n\n\n\n\n1\n2\n# Show data types for each column\n\n\nsapply\n(\nMydf\n,\n \nclass\n)\n\n\n\n\n\n\n\n1\n2\n##         X     DepPC    DProgr       Qty Delivered \n## \"integer\" \"integer\" \"integer\" \"integer\" \"logical\"\n\n\n\n\n\n\n1\n2\n# Return number of rows and columns\n\n\ndim\n(\nMydf\n)\n\n\n\n\n\n\n\n1\n## [1] 120   5\n\n\n\n\n\n\n1\nnrow\n(\nMydf\n)\n\n\n\n\n\n\n\n1\n## [1] 120\n\n\n\n\n\n\n1\nncol\n(\nMydf\n)\n\n\n\n\n\n\n\n1\n## [1] 5\n\n\n\n\n\n\n1\n2\n# How many departments? \n\n\nunique\n(\nMydf\n$\nDepPC\n)\n\n\n\n\n\n\n\n1\n## [1] 90 91 92 93 94 75\n\n\n\n\n\n\n1\n2\n# Dataset\n\n\nhead\n(\nMydf\n,\n \n5\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##   X DepPC DProgr Qty Delivered\n## 1 1    90      1   7     FALSE\n## 2 2    91      2   8      TRUE\n## 3 3    92      3   9     FALSE\n## 4 4    93      4  10      TRUE\n## 5 5    94      5  11      TRUE\n\n\n\n\n\n\n1\n2\n# Aggregate by a variable (categories) and sum up another variable\n\naggregate\n(\nMydf\n$\nQty\n,\n by \n=\n Mydf\n[\n\"DepPC\"\n],\n FUN \n=\n \nsum\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   DepPC   x\n## 1    75 878\n## 2    90 689\n## 3    91 684\n## 4    92 701\n## 5    93 707\n## 6    94 802\n\n\n\n\n\n\n1\n2\n# Aggregate by a variable (categories) and extract descriptive stats from another variable\n\naggregate\n(\nMydf\n$\nQty\n,\n by \n=\n Mydf\n[\n\"DepPC\"\n],\n FUN \n=\n \nsummary\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   DepPC x.Min. x.1st Qu. x.Median x.Mean x.3rd Qu. x.Max.\n## 1    75   4.00     15.25    23.50  43.90     74.50 124.00\n## 2    90   2.00     11.75    19.50  34.45     27.25 119.00\n## 3    91   3.00      9.00    19.00  34.20     26.25 120.00\n## 4    92   1.00     10.00    20.00  35.05     27.25 121.00\n## 5    93   2.00     11.00    19.00  35.35     27.25 122.00\n## 6    94   3.00     12.00    20.00  40.10     46.50 123.00\n\n\n\n\n\n\nby\n\u00b6\n\n\nAn alternative to \naggregate\n with pros and cons.\n\n\n1\n2\n# Dataset\n\n\nhead\n(\nMydf\n,\n \n5\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##   X DepPC DProgr Qty Delivered\n## 1 1    90      1   7     FALSE\n## 2 2    91      2   8      TRUE\n## 3 3    92      3   9     FALSE\n## 4 4    93      4  10      TRUE\n## 5 5    94      5  11      TRUE\n\n\n\n\n\n\n1\nby\n(\nMydf\n$\nQty\n,\n Mydf\n[\n\"DepPC\"\n],\n FUN \n=\n \nsum\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n## DepPC: 75\n## [1] 878\n## -------------------------------------------------------- \n## DepPC: 90\n## [1] 689\n## -------------------------------------------------------- \n## DepPC: 91\n## [1] 684\n## -------------------------------------------------------- \n## DepPC: 92\n## [1] 701\n## -------------------------------------------------------- \n## DepPC: 93\n## [1] 707\n## -------------------------------------------------------- \n## DepPC: 94\n## [1] 802\n\n\n\n\n\n\n1\nby\n(\nMydf\n$\nQty\n,\n Mydf\n[\n\"DepPC\"\n],\n FUN \n=\n \nsummary\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n## DepPC: 75\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    4.00   15.25   23.50   43.90   74.50  124.00 \n## -------------------------------------------------------- \n## DepPC: 90\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    2.00   11.75   19.50   34.45   27.25  119.00 \n## -------------------------------------------------------- \n## DepPC: 91\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    3.00    9.00   19.00   34.20   26.25  120.00 \n## -------------------------------------------------------- \n## DepPC: 92\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    1.00   10.00   20.00   35.05   27.25  121.00 \n## -------------------------------------------------------- \n## DepPC: 93\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    2.00   11.00   19.00   35.35   27.25  122.00 \n## -------------------------------------------------------- \n## DepPC: 94\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##     3.0    12.0    20.0    40.1    46.5   123.0\n\n\n\n\n\n\nsplit\n\u00b6\n\n\n1\n2\n# Dataset\n\n\nhead\n(\nMydf\n,\n \n5\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##   X DepPC DProgr Qty Delivered\n## 1 1    90      1   7     FALSE\n## 2 2    91      2   8      TRUE\n## 3 3    92      3   9     FALSE\n## 4 4    93      4  10      TRUE\n## 5 5    94      5  11      TRUE\n\n\n\n\n\n\n1\nnrow\n(\nMydf\n)\n\n\n\n\n\n\n\n1\n## [1] 120\n\n\n\n\n\n\n1\n2\n# Split with a variable (categories)\n\n\nsplit\n(\nMydf\n$\nQty\n,\n Mydf\n[\n\"DepPC\"\n])\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n## $`75`\n##  [1]  12  18  24  30  13  19 100 106 112 118 124   7  13  19  25  16  23\n## [18]  29   4  66\n## \n## $`90`\n##  [1]   7  13  19  25  31  14  20 101 107 113 119   2   8  14  20  26  17\n## [18]  24   4   5\n## \n## $`91`\n##  [1]   8  14  20  26   9  15  21 102 108 114 120   3   9  15  21  27  18\n## [18]  25   3   6\n## \n## $`92`\n##  [1]   9  15  21  27  10  16  22 103 109 115 121   4  10  16  22  28  19\n## [18]  26   1   7\n## \n## $`93`\n##  [1]  10  16  22  28  11  17  23 104 110 116 122   5  11  17  23  14  21\n## [18]  27   2   8\n## \n## $`94`\n##  [1]  11  17  23  29  12  18  99 105 111 117 123   6  12  18  24  15  22\n## [18]  28   3   9\n\n\n\n\n\n\n1\n2\n# Split by row\n\n\nunlist\n(\nMydf\n$\nQty\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n##   [1]   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23\n##  [18]  24  25  26  27  28  29  30  31   9  10  11  12  13  14  15  16  17\n##  [35]  18  19  20  21  22  23  99 100 101 102 103 104 105 106 107 108 109\n##  [52] 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124   2   3\n##  [69]   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20\n##  [86]  21  22  23  24  25  26  27  28  14  15  16  17  18  19  21  22  23\n## [103]  24  25  26  27  28  29   4   3   1   2   3   4   5   6   7   8   9\n## [120]  66\n\n\n\n\n\n\n1\nsplit\n(\nMydf\n$\nQty\n,\n \nc\n(\n60\n,\n \n120\n))\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## $`60`\n##  [1]   7   9  11  13  15  17  19  21  23  25  27  29  31  10  12  14  16\n## [18]  18  20  22  99 101 103 105 107 109 111 113 115 117 119 121 123   2\n## [35]   4   6   8  10  12  14  16  18  20  22  24  26  28  15  17  19  22\n## [52]  24  26  28   4   1   3   5   7   9\n## \n## $`120`\n##  [1]   8  10  12  14  16  18  20  22  24  26  28  30   9  11  13  15  17\n## [18]  19  21  23 100 102 104 106 108 110 112 114 116 118 120 122 124   3\n## [35]   5   7   9  11  13  15  17  19  21  23  25  27  14  16  18  21  23\n## [52]  25  27  29   3   2   4   6   8  66\n\n\n\n\n\n\nstrsplit\n\u00b6\n\n\n1\n2\n3\n4\n5\n# The vector pioneers\n\npioneers \n<-\n \nc\n(\n'GAUSS:1777'\n,\n \n'BAYES:1702'\n,\n \n'PASCAL:1623'\n,\n \n'PEARSON:1857'\n)\n\n\n\n# Split names from birth year: split_math\n\nsplit_math \n<-\n \nstrsplit\n(\npioneers\n,\n \n':'\n)\n\n\n\n\n\n\n\nVectorize\n\u00b6\n\n\nVectorize a scalar function.\n\n\n1\n2\n# Scalar function\n\n\nrep\n(\nc\n(\n1\n,\n \n2\n),\n \n2\n)\n\n\n\n\n\n\n\n1\n## [1] 1 2 1 2\n\n\n\n\n\n\n1\n2\n3\n# Vector function\n\nvrep \n<-\n \nVectorize\n(\nrep.int\n)\n\nvrep\n(\nc\n(\n1\n,\n \n2\n),\n \n2\n)\n\n\n\n\n\n\n\n1\n2\n3\n##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    2\n\n\n\n\n\n\n1\n2\n3\n# Scalar function\n\nf \n<-\n \nfunction\n(\nx \n=\n \n1\n:\n3\n,\n y\n)\n \nc\n(\nx\n,\n y\n)\n\nf\n(\n1\n:\n3\n,\n \n1\n:\n3\n)\n\n\n\n\n\n\n\n1\n## [1] 1 2 3 1 2 3\n\n\n\n\n\n\n1\n2\n3\n# Vector function\n\nvf \n<-\n \nVectorize\n(\nf\n,\n SIMPLIFY \n=\n \nFALSE\n)\n\nvf\n(\n1\n:\n3\n,\n \n1\n:\n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [[1]]\n## [1] 1 1\n## \n## [[2]]\n## [1] 2 2\n## \n## [[3]]\n## [1] 3 3\n\n\n\n\n\n\nlapply\n: list-apply\n\u00b6\n\n\nFor lists, vectors, and data frames.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Dataset\n\nA \n<-\n \nmatrix\n(\n1\n:\n9\n,\n nrow \n=\n \n3\n,\n ncol \n=\n \n3\n)\n\nB \n<-\n \nmatrix\n(\n10\n:\n18\n,\n nrow \n=\n \n3\n,\n ncol \n=\n \n3\n)\n\nC \n<-\n \nmatrix\n(\n19\n:\n28\n,\n nrow \n=\n \n3\n,\n ncol \n=\n \n3\n)\n\n\n\n# Create a list of matrices; an array (a 3D matrix)\n\nMyList \n<-\n \nlist\n(\nA\n,\nB\n,\nC\n)\n\nMyList\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27\n\n\n\n\n\n\n1\n2\n# Extract values\n\nMyList\n[[\n1\n]]\n\n\n\n\n\n\n\n1\n2\n3\n4\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n\n\n\n\n\n\n1\nMyList\n[[\n1\n]][\n1\n,]\n\n\n\n\n\n\n\n1\n## [1] 1 4 7\n\n\n\n\n\n\n1\nMyList\n[[\n1\n]][,\n1\n]\n\n\n\n\n\n\n\n1\n## [1] 1 2 3\n\n\n\n\n\n\n1\nMyList\n[[\n1\n]][\n1\n,\n1\n]\n\n\n\n\n\n\n\n1\n## [1] 1\n\n\n\n\n\n\n1\n2\n# Extract the 2nd column from `MyList` with the selection operator `[` with `lapply()`\n\n\nlapply\n(\nMyList\n,\n\"[\"\n,\n \n,\n \n2\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [[1]]\n## [1] 4 5 6\n## \n## [[2]]\n## [1] 13 14 15\n## \n## [[3]]\n## [1] 22 23 24\n\n\n\n\n\n\n1\n2\n# Extract the 1st row from `MyList`\n\n\nlapply\n(\nMyList\n,\n\"[\"\n,\n \n1\n,\n \n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [[1]]\n## [1] 1 4 7\n## \n## [[2]]\n## [1] 10 13 16\n## \n## [[3]]\n## [1] 19 22 25\n\n\n\n\n\n\nLambda & custom function\n\n\n1\n2\n# Dataset\n\nMyList\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27\n\n\n\n\n\n\n1\n2\n# Built-in function\n\n\nlapply\n(\nMyList\n,\n \nmax\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [[1]]\n## [1] 9\n## \n## [[2]]\n## [1] 18\n## \n## [[3]]\n## [1] 27\n\n\n\n\n\n\n1\n2\n# Lambda function\n\n\nlapply\n(\nMyList\n,\n \nfunction\n(\nx\n)\n \nmax\n(\nx\n)\n \n+\n \n10\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [[1]]\n## [1] 19\n## \n## [[2]]\n## [1] 28\n## \n## [[3]]\n## [1] 37\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Custom function\n\nselect_first \n<-\n \nfunction\n(\nx\n)\n \n{\n\n    \nreturn\n(\nx\n[\n1\n])\n\n\n}\n\n\nlapply\n(\nMyList\n,\n select_first\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 10\n## \n## [[3]]\n## [1] 19\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Custom function with two arguments\n\nselect_el \n<-\n \nfunction\n(\nx\n,\n i\n)\n \n{\n \n  x\n[\ni\n]\n \n\n}\n\n\nlapply\n(\nMyList\n,\n select_el\n,\n i \n=\n \n2\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [[1]]\n## [1] 2\n## \n## [[2]]\n## [1] 11\n## \n## [[3]]\n## [1] 20\n\n\n\n\n\n\nStrings\n\n\n1\n2\n# Dataset\n\nY\n\n\n\n\n\n\n1\n2\n3\n##      [,1] [,2]\n## [1,] \"a\"  \"c\" \n## [2,] \"b\"  \"d\"\n\n\n\n\n\n\n1\n2\n# Change the format\n\n\nlapply\n(\nY\n,\n \ntoupper\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## [[1]]\n## [1] \"A\"\n## \n## [[2]]\n## [1] \"B\"\n## \n## [[3]]\n## [1] \"C\"\n## \n## [[4]]\n## [1] \"D\"\n\n\n\n\n\n\nsapply\n: simplify-list-apply\n\u00b6\n\n\nA wrapper that simplifies \nlapply\n.\n\n\n1\n2\n# Dataset\n\nMyList\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27\n\n\n\n\n\n\n1\n2\n# Return a list with `lapply()`\n\n\nlapply\n(\nMyList\n,\n\"[\"\n,\n \n2\n,\n \n1\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [[1]]\n## [1] 2\n## \n## [[2]]\n## [1] 11\n## \n## [[3]]\n## [1] 20\n\n\n\n\n\n\n1\n2\n# Return a vector with `sapply()`\n\n\nsapply\n(\nMyList\n,\n\"[\"\n,\n \n2\n,\n \n1\n)\n \n# same result, but simpler\n\n\n\n\n\n\n\n1\n## [1]  2 11 20\n\n\n\n\n\n\n1\n2\n# Return a list with `sapply()`\n\n\nsapply\n(\nMyList\n,\n\"[\"\n,\n \n2\n,\n \n1\n,\n simplify \n=\n \nT\n)\n \n# by default\n\n\n\n\n\n\n\n1\n## [1]  2 11 20\n\n\n\n\n\n\n1\nsapply\n(\nMyList\n,\n\"[\"\n,\n \n2\n,\n \n1\n,\n simplify \n=\n \nF\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [[1]]\n## [1] 2\n## \n## [[2]]\n## [1] 11\n## \n## [[3]]\n## [1] 20\n\n\n\n\n\n\n1\n2\n# Return a vector with `unlist()`\n\n\nunlist\n(\nlapply\n(\nMyList\n,\n\"[\"\n,\n \n2\n,\n \n1\n))\n \n# similar\n\n\n\n\n\n\n\n1\n## [1]  2 11 20\n\n\n\n\n\n\nLambda & custom function\n\n\n1\n2\n# Dataset\n\nMyList\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27\n\n\n\n\n\n\n1\n2\n# Built-in function\n\n\nsapply\n(\nMyList\n,\n \nmax\n)\n\n\n\n\n\n\n\n1\n## [1]  9 18 27\n\n\n\n\n\n\n1\n2\n# Lambda function\n\n\nlapply\n(\nMyList\n,\n \nfunction\n(\nx\n)\n \nmax\n(\nx\n)\n \n+\n \n10\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [[1]]\n## [1] 19\n## \n## [[2]]\n## [1] 28\n## \n## [[3]]\n## [1] 37\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Custom function\n\nselect_first \n<-\n \nfunction\n(\nx\n)\n \n{\n\n    \nreturn\n(\nx\n[\n1\n])\n\n\n}\n\n\nsapply\n(\nMyList\n,\n select_first\n)\n\n\n\n\n\n\n\n1\n## [1]  1 10 19\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Custom function with two arguments\n\nselect_el \n<-\n \nfunction\n(\nx\n,\n i\n)\n \n{\n \n  x\n[\ni\n]\n \n\n}\n\n\nsapply\n(\nMyList\n,\n select_el\n,\n i \n=\n \n2\n)\n\n\n\n\n\n\n\n1\n## [1]  2 11 20\n\n\n\n\n\n\nStrings\n\n\n1\n2\n# Dataset\n\nY\n\n\n\n\n\n\n1\n2\n3\n##      [,1] [,2]\n## [1,] \"a\"  \"c\" \n## [2,] \"b\"  \"d\"\n\n\n\n\n\n\n1\n2\n# Change the format\n\n\nsapply\n(\nY\n,\n \ntoupper\n)\n\n\n\n\n\n\n\n1\n2\n##   a   b   c   d \n## \"A\" \"B\" \"C\" \"D\"\n\n\n\n\n\n\nvapply\n\u00b6\n\n\nA variant.\n\n\nrapply\n: recursive-list-apply\n\u00b6\n\n\n1\n2\n# Dataset\n\nMyList\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27\n\n\n\n\n\n\n1\n2\n# Build-in function\n\n\nrapply\n(\nMyList\n,\n \nsqrt\n,\n classes \n=\n \n\"ANY\"\n,\n how \n=\n \n\"replace\"\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n## [[1]]\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 2.000000 2.645751\n## [2,] 1.414214 2.236068 2.828427\n## [3,] 1.732051 2.449490 3.000000\n## \n## [[2]]\n##          [,1]     [,2]     [,3]\n## [1,] 3.162278 3.605551 4.000000\n## [2,] 3.316625 3.741657 4.123106\n## [3,] 3.464102 3.872983 4.242641\n## \n## [[3]]\n##          [,1]     [,2]     [,3]\n## [1,] 4.358899 4.690416 5.000000\n## [2,] 4.472136 4.795832 5.099020\n## [3,] 4.582576 4.898979 5.196152\n\n\n\n\n\n\n1\n2\n# Lambda or custom the function\n\n\nrapply\n(\nMyList\n,\n \nfunction\n(\nx\n)\n x\n^\n2\n,\n classes \n=\n \n\"ANY\"\n,\n how \n=\n \n\"replace\"\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1   16   49\n## [2,]    4   25   64\n## [3,]    9   36   81\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]  100  169  256\n## [2,]  121  196  289\n## [3,]  144  225  324\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]  361  484  625\n## [2,]  400  529  676\n## [3,]  441  576  729\n\n\n\n\n\n\n1\n2\n3\n4\nselect_first \n<-\n \nfunction\n(\nx\n)\n \n{\n\n    \nreturn\n(\nx\n[\n1\n])\n\n\n}\n\n\nrapply\n(\nMyList\n,\n select_first\n,\n classes \n=\n \n\"ANY\"\n,\n how \n=\n \n\"replace\"\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 10\n## \n## [[3]]\n## [1] 19\n\n\n\n\n\n\n1\n2\n# Change classes =\n\n\nclass\n(\nMyList\n)\n \n# the object\n\n\n\n\n\n\n\n1\n## [1] \"list\"\n\n\n\n\n\n\n1\nclass\n(\nMyList\n[[\n1\n]][\n1\n])\n \n# each entry\n\n\n\n\n\n\n\n1\n## [1] \"integer\"\n\n\n\n\n\n\n1\nrapply\n(\nMyList\n,\n \nsqrt\n,\n classes \n=\n \n\"list\"\n,\n how \n=\n \n\"replace\"\n)\n \n# not work\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27\n\n\n\n\n\n\n1\nrapply\n(\nMyList\n,\n \nsqrt\n,\n classes \n=\n \n\"ANY\"\n,\n how \n=\n \n\"replace\"\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n## [[1]]\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 2.000000 2.645751\n## [2,] 1.414214 2.236068 2.828427\n## [3,] 1.732051 2.449490 3.000000\n## \n## [[2]]\n##          [,1]     [,2]     [,3]\n## [1,] 3.162278 3.605551 4.000000\n## [2,] 3.316625 3.741657 4.123106\n## [3,] 3.464102 3.872983 4.242641\n## \n## [[3]]\n##          [,1]     [,2]     [,3]\n## [1,] 4.358899 4.690416 5.000000\n## [2,] 4.472136 4.795832 5.099020\n## [3,] 4.582576 4.898979 5.196152\n\n\n\n\n\n\n1\n2\n# Change how = \n\n\nrapply\n(\nMyList\n,\n \nsqrt\n,\n classes \n=\n \n\"ANY\"\n,\n how \n=\n \n\"list\"\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n## [[1]]\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 2.000000 2.645751\n## [2,] 1.414214 2.236068 2.828427\n## [3,] 1.732051 2.449490 3.000000\n## \n## [[2]]\n##          [,1]     [,2]     [,3]\n## [1,] 3.162278 3.605551 4.000000\n## [2,] 3.316625 3.741657 4.123106\n## [3,] 3.464102 3.872983 4.242641\n## \n## [[3]]\n##          [,1]     [,2]     [,3]\n## [1,] 4.358899 4.690416 5.000000\n## [2,] 4.472136 4.795832 5.099020\n## [3,] 4.582576 4.898979 5.196152\n\n\n\n\n\n\n1\nrapply\n(\nMyList\n,\n \nsqrt\n,\n classes \n=\n \n\"ANY\"\n,\n how \n=\n \n\"unlist\"\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751\n##  [8] 2.828427 3.000000 3.162278 3.316625 3.464102 3.605551 3.741657\n## [15] 3.872983 4.000000 4.123106 4.242641 4.358899 4.472136 4.582576\n## [22] 4.690416 4.795832 4.898979 5.000000 5.099020 5.196152\n\n\n\n\n\n\n\n\nBuilt-in or custom \nfunction(x) x\n.\n\n\nclasses\n = \n\"ANY\"\n for any object classes or a given object class.\n\n    Useful when the data is mixed and we want to focus on one\n\n    class only.\n\n\nhow = \"replace\"\n or \n\"list\"\n, \n\"unlist\"\n\n\n\n\nmapply\n: multivariate-apply\n\u00b6\n\n\n1\n2\n3\n# Create a 4x4 matrix\n\nQ1 \n<-\n \nmatrix\n(\nc\n(\nrep\n(\n1\n,\n \n4\n),\n \nrep\n(\n2\n,\n \n4\n),\n \nrep\n(\n3\n,\n \n4\n),\n \nrep\n(\n4\n,\n \n4\n)),\n4\n,\n4\n)\n\nQ1\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    1    2    3    4\n## [3,]    1    2    3    4\n## [4,]    1    2    3    4\n\n\n\n\n\n\n1\n2\n3\n# Or use `mapply()`\n\nQ2 \n<-\n \nmapply\n(\nrep\n,\n \n1\n:\n4\n,\n \n4\n)\n\nQ2\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    1    2    3    4\n## [3,]    1    2    3    4\n## [4,]    1    2    3    4\n\n\n\n\n\n\nVectorize arguments to a function, \nrep\n, that is not usually accepting\n\nvectors as arguments. Applies a function to multiple lists, \n1\n:\n4\n, or\n\nmultiple vector, \nc()\n, arguments.\n\n\nVectorize\n\u00b6\n\n\nVectorize a scaler function.\n\n\n1\n2\n# Scalar function\n\n\nrep\n(\nc\n(\n1\n,\n \n2\n),\n \n2\n)\n\n\n\n\n\n\n\n1\n## [1] 1 2 1 2\n\n\n\n\n\n\n1\n2\n3\n# Vector function\n\nvrep \n<-\n \nVectorize\n(\nrep.int\n)\n\nvrep\n(\nc\n(\n1\n,\n \n2\n),\n \n2\n)\n\n\n\n\n\n\n\n1\n2\n3\n##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    2\n\n\n\n\n\n\n1\n2\n3\n# Scalar function\n\nf \n<-\n \nfunction\n(\nx \n=\n \n1\n:\n3\n,\n y\n)\n \nc\n(\nx\n,\n y\n)\n\nf\n(\n1\n:\n3\n,\n \n1\n:\n3\n)\n\n\n\n\n\n\n\n1\n## [1] 1 2 3 1 2 3\n\n\n\n\n\n\n1\n2\n3\n# Vector function\n\nvf \n<-\n \nVectorize\n(\nf\n,\n SIMPLIFY \n=\n \nFALSE\n)\n\nvf\n(\n1\n:\n3\n,\n \n1\n:\n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## [[1]]\n## [1] 1 1\n## \n## [[2]]\n## [1] 2 2\n## \n## [[3]]\n## [1] 3 3\n\n\n\n\n\n\nAnd more\n\u00b6\n\n\n\n\ntapply\n.\n\n\neapply\n.",
            "title": "Intermediate R - The apply Family"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#sweep",
            "text": "Several steps  1\n2\n3 # Dataset \ndataPoints  <-   matrix ( 4 : 15 ,  nrow  =   4 ,  ncol  =   3 ) \ndataPoints   1\n2\n3\n4\n5 ##      [,1] [,2] [,3]\n## [1,]    4    8   12\n## [2,]    5    9   13\n## [3,]    6   10   14\n## [4,]    7   11   15   1\n2\n3 # Find means (center) per column with `apply()` \ndataPoints_means  <-   apply ( dataPoints ,   2 ,   mean ) \ndataPoints_means   1 ## [1]  5.5  9.5 13.5   1\n2\n3 # Find standard deviation (dispersion) with `apply()` \ndataPoints_sdev  <-   apply ( dataPoints ,   2 ,  sd ) \ndataPoints_sdev   1 ## [1] 1.290994 1.290994 1.290994   1\n2\n3 # Center the points; shift all the points with respect to their center \ndataPoints_Trans1  <-   sweep ( dataPoints ,   2 ,  dataPoints_means , \"-\" ) \ndataPoints_Trans1   1\n2\n3\n4\n5 ##      [,1] [,2] [,3]\n## [1,] -1.5 -1.5 -1.5\n## [2,] -0.5 -0.5 -0.5\n## [3,]  0.5  0.5  0.5\n## [4,]  1.5  1.5  1.5   1\n2\n3 # Normalize \ndataPoints_Trans2  <-   sweep ( dataPoints_Trans1 ,   2 ,  dataPoints_sdev ,   \"/\" ) \ndataPoints_Trans2   1\n2\n3\n4\n5 ##            [,1]       [,2]       [,3]\n## [1,] -1.1618950 -1.1618950 -1.1618950\n## [2,] -0.3872983 -0.3872983 -0.3872983\n## [3,]  0.3872983  0.3872983  0.3872983\n## [4,]  1.1618950  1.1618950  1.1618950   1 sweep ( dataPoints ,   2 ,  dataPoints_means ,   sum )    1 ## [1] 228    X .  MARGIN=1  for row,  2  for column.  STATS  for the summary statistics to be swept out:  sum ,  mean ,  median ,  min ,  max ,  var ,  sd ,  range ,  length ,  skew ,  kurtosis ,  se , etc.  FUN=\"-\" ,  \"+\" ,  \"/\" ,  \"*\" ,  \"**\" , etc.  user-defined function such as  FUN = function(x) { c(m = mean(x), s = sd(x)) } .   One step  1\n2\n3\n4 # Normalize the data with a nested call \ndataPoints_Trans  <-   sweep ( sweep ( dataPoints ,   2 ,  dataPoints_means , \"-\" ),   2 ,  dataPoints_sdev , \"/\" ) \n\ndataPoints_Trans   1\n2\n3\n4\n5 ##            [,1]       [,2]       [,3]\n## [1,] -1.1618950 -1.1618950 -1.1618950\n## [2,] -0.3872983 -0.3872983 -0.3872983\n## [3,]  0.3872983  0.3872983  0.3872983\n## [4,]  1.1618950  1.1618950  1.1618950   Even simpler  1\n2 # Automatic data scaling  scale ( dataPoints )    1\n2\n3\n4\n5\n6\n7\n8\n9 ##            [,1]       [,2]       [,3]\n## [1,] -1.1618950 -1.1618950 -1.1618950\n## [2,] -0.3872983 -0.3872983 -0.3872983\n## [3,]  0.3872983  0.3872983  0.3872983\n## [4,]  1.1618950  1.1618950  1.1618950\n## attr(,\"scaled:center\")\n## [1]  5.5  9.5 13.5\n## attr(,\"scaled:scale\")\n## [1] 1.290994 1.290994 1.290994",
            "title": "sweep"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#aggregate",
            "text": "1\n2 # Dataset  head ( Mydf ,   15 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 ##     X DepPC DProgr Qty Delivered\n## 1   1    90      1   7     FALSE\n## 2   2    91      2   8      TRUE\n## 3   3    92      3   9     FALSE\n## 4   4    93      4  10      TRUE\n## 5   5    94      5  11      TRUE\n## 6   6    75      6  12     FALSE\n## 7   7    90      7  13      TRUE\n## 8   8    91      8  14      TRUE\n## 9   9    92      9  15     FALSE\n## 10 10    93     10  16     FALSE\n## 11 11    94     11  17      TRUE\n## 12 12    75     12  18     FALSE\n## 13 13    90     13  19     FALSE\n## 14 14    91     14  20      TRUE\n## 15 15    92     15  21      TRUE   1\n2 # Show data types for each column  sapply ( Mydf ,   class )    1\n2 ##         X     DepPC    DProgr       Qty Delivered \n## \"integer\" \"integer\" \"integer\" \"integer\" \"logical\"   1\n2 # Return number of rows and columns  dim ( Mydf )    1 ## [1] 120   5   1 nrow ( Mydf )    1 ## [1] 120   1 ncol ( Mydf )    1 ## [1] 5   1\n2 # How many departments?   unique ( Mydf $ DepPC )    1 ## [1] 90 91 92 93 94 75   1\n2 # Dataset  head ( Mydf ,   5 )    1\n2\n3\n4\n5\n6 ##   X DepPC DProgr Qty Delivered\n## 1 1    90      1   7     FALSE\n## 2 2    91      2   8      TRUE\n## 3 3    92      3   9     FALSE\n## 4 4    93      4  10      TRUE\n## 5 5    94      5  11      TRUE   1\n2 # Aggregate by a variable (categories) and sum up another variable \naggregate ( Mydf $ Qty ,  by  =  Mydf [ \"DepPC\" ],  FUN  =   sum )    1\n2\n3\n4\n5\n6\n7 ##   DepPC   x\n## 1    75 878\n## 2    90 689\n## 3    91 684\n## 4    92 701\n## 5    93 707\n## 6    94 802   1\n2 # Aggregate by a variable (categories) and extract descriptive stats from another variable \naggregate ( Mydf $ Qty ,  by  =  Mydf [ \"DepPC\" ],  FUN  =   summary )    1\n2\n3\n4\n5\n6\n7 ##   DepPC x.Min. x.1st Qu. x.Median x.Mean x.3rd Qu. x.Max.\n## 1    75   4.00     15.25    23.50  43.90     74.50 124.00\n## 2    90   2.00     11.75    19.50  34.45     27.25 119.00\n## 3    91   3.00      9.00    19.00  34.20     26.25 120.00\n## 4    92   1.00     10.00    20.00  35.05     27.25 121.00\n## 5    93   2.00     11.00    19.00  35.35     27.25 122.00\n## 6    94   3.00     12.00    20.00  40.10     46.50 123.00",
            "title": "aggregate"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#by",
            "text": "An alternative to  aggregate  with pros and cons.  1\n2 # Dataset  head ( Mydf ,   5 )    1\n2\n3\n4\n5\n6 ##   X DepPC DProgr Qty Delivered\n## 1 1    90      1   7     FALSE\n## 2 2    91      2   8      TRUE\n## 3 3    92      3   9     FALSE\n## 4 4    93      4  10      TRUE\n## 5 5    94      5  11      TRUE   1 by ( Mydf $ Qty ,  Mydf [ \"DepPC\" ],  FUN  =   sum )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ## DepPC: 75\n## [1] 878\n## -------------------------------------------------------- \n## DepPC: 90\n## [1] 689\n## -------------------------------------------------------- \n## DepPC: 91\n## [1] 684\n## -------------------------------------------------------- \n## DepPC: 92\n## [1] 701\n## -------------------------------------------------------- \n## DepPC: 93\n## [1] 707\n## -------------------------------------------------------- \n## DepPC: 94\n## [1] 802   1 by ( Mydf $ Qty ,  Mydf [ \"DepPC\" ],  FUN  =   summary )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 ## DepPC: 75\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    4.00   15.25   23.50   43.90   74.50  124.00 \n## -------------------------------------------------------- \n## DepPC: 90\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    2.00   11.75   19.50   34.45   27.25  119.00 \n## -------------------------------------------------------- \n## DepPC: 91\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    3.00    9.00   19.00   34.20   26.25  120.00 \n## -------------------------------------------------------- \n## DepPC: 92\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    1.00   10.00   20.00   35.05   27.25  121.00 \n## -------------------------------------------------------- \n## DepPC: 93\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    2.00   11.00   19.00   35.35   27.25  122.00 \n## -------------------------------------------------------- \n## DepPC: 94\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##     3.0    12.0    20.0    40.1    46.5   123.0",
            "title": "by"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#split",
            "text": "1\n2 # Dataset  head ( Mydf ,   5 )    1\n2\n3\n4\n5\n6 ##   X DepPC DProgr Qty Delivered\n## 1 1    90      1   7     FALSE\n## 2 2    91      2   8      TRUE\n## 3 3    92      3   9     FALSE\n## 4 4    93      4  10      TRUE\n## 5 5    94      5  11      TRUE   1 nrow ( Mydf )    1 ## [1] 120   1\n2 # Split with a variable (categories)  split ( Mydf $ Qty ,  Mydf [ \"DepPC\" ])     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 ## $`75`\n##  [1]  12  18  24  30  13  19 100 106 112 118 124   7  13  19  25  16  23\n## [18]  29   4  66\n## \n## $`90`\n##  [1]   7  13  19  25  31  14  20 101 107 113 119   2   8  14  20  26  17\n## [18]  24   4   5\n## \n## $`91`\n##  [1]   8  14  20  26   9  15  21 102 108 114 120   3   9  15  21  27  18\n## [18]  25   3   6\n## \n## $`92`\n##  [1]   9  15  21  27  10  16  22 103 109 115 121   4  10  16  22  28  19\n## [18]  26   1   7\n## \n## $`93`\n##  [1]  10  16  22  28  11  17  23 104 110 116 122   5  11  17  23  14  21\n## [18]  27   2   8\n## \n## $`94`\n##  [1]  11  17  23  29  12  18  99 105 111 117 123   6  12  18  24  15  22\n## [18]  28   3   9   1\n2 # Split by row  unlist ( Mydf $ Qty )    1\n2\n3\n4\n5\n6\n7\n8 ##   [1]   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23\n##  [18]  24  25  26  27  28  29  30  31   9  10  11  12  13  14  15  16  17\n##  [35]  18  19  20  21  22  23  99 100 101 102 103 104 105 106 107 108 109\n##  [52] 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124   2   3\n##  [69]   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20\n##  [86]  21  22  23  24  25  26  27  28  14  15  16  17  18  19  21  22  23\n## [103]  24  25  26  27  28  29   4   3   1   2   3   4   5   6   7   8   9\n## [120]  66   1 split ( Mydf $ Qty ,   c ( 60 ,   120 ))     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## $`60`\n##  [1]   7   9  11  13  15  17  19  21  23  25  27  29  31  10  12  14  16\n## [18]  18  20  22  99 101 103 105 107 109 111 113 115 117 119 121 123   2\n## [35]   4   6   8  10  12  14  16  18  20  22  24  26  28  15  17  19  22\n## [52]  24  26  28   4   1   3   5   7   9\n## \n## $`120`\n##  [1]   8  10  12  14  16  18  20  22  24  26  28  30   9  11  13  15  17\n## [18]  19  21  23 100 102 104 106 108 110 112 114 116 118 120 122 124   3\n## [35]   5   7   9  11  13  15  17  19  21  23  25  27  14  16  18  21  23\n## [52]  25  27  29   3   2   4   6   8  66",
            "title": "split"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#strsplit",
            "text": "1\n2\n3\n4\n5 # The vector pioneers \npioneers  <-   c ( 'GAUSS:1777' ,   'BAYES:1702' ,   'PASCAL:1623' ,   'PEARSON:1857' )  # Split names from birth year: split_math \nsplit_math  <-   strsplit ( pioneers ,   ':' )",
            "title": "strsplit"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#vectorize",
            "text": "Vectorize a scalar function.  1\n2 # Scalar function  rep ( c ( 1 ,   2 ),   2 )    1 ## [1] 1 2 1 2   1\n2\n3 # Vector function \nvrep  <-   Vectorize ( rep.int ) \nvrep ( c ( 1 ,   2 ),   2 )    1\n2\n3 ##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    2   1\n2\n3 # Scalar function \nf  <-   function ( x  =   1 : 3 ,  y )   c ( x ,  y ) \nf ( 1 : 3 ,   1 : 3 )    1 ## [1] 1 2 3 1 2 3   1\n2\n3 # Vector function \nvf  <-   Vectorize ( f ,  SIMPLIFY  =   FALSE ) \nvf ( 1 : 3 ,   1 : 3 )    1\n2\n3\n4\n5\n6\n7\n8 ## [[1]]\n## [1] 1 1\n## \n## [[2]]\n## [1] 2 2\n## \n## [[3]]\n## [1] 3 3",
            "title": "Vectorize"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#lapply-list-apply",
            "text": "For lists, vectors, and data frames.  1\n2\n3\n4\n5\n6\n7\n8 # Dataset \nA  <-   matrix ( 1 : 9 ,  nrow  =   3 ,  ncol  =   3 ) \nB  <-   matrix ( 10 : 18 ,  nrow  =   3 ,  ncol  =   3 ) \nC  <-   matrix ( 19 : 28 ,  nrow  =   3 ,  ncol  =   3 )  # Create a list of matrices; an array (a 3D matrix) \nMyList  <-   list ( A , B , C ) \nMyList    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27   1\n2 # Extract values \nMyList [[ 1 ]]    1\n2\n3\n4 ##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9   1 MyList [[ 1 ]][ 1 ,]    1 ## [1] 1 4 7   1 MyList [[ 1 ]][, 1 ]    1 ## [1] 1 2 3   1 MyList [[ 1 ]][ 1 , 1 ]    1 ## [1] 1   1\n2 # Extract the 2nd column from `MyList` with the selection operator `[` with `lapply()`  lapply ( MyList , \"[\" ,   ,   2 )    1\n2\n3\n4\n5\n6\n7\n8 ## [[1]]\n## [1] 4 5 6\n## \n## [[2]]\n## [1] 13 14 15\n## \n## [[3]]\n## [1] 22 23 24   1\n2 # Extract the 1st row from `MyList`  lapply ( MyList , \"[\" ,   1 ,   )    1\n2\n3\n4\n5\n6\n7\n8 ## [[1]]\n## [1] 1 4 7\n## \n## [[2]]\n## [1] 10 13 16\n## \n## [[3]]\n## [1] 19 22 25   Lambda & custom function  1\n2 # Dataset \nMyList    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27   1\n2 # Built-in function  lapply ( MyList ,   max )    1\n2\n3\n4\n5\n6\n7\n8 ## [[1]]\n## [1] 9\n## \n## [[2]]\n## [1] 18\n## \n## [[3]]\n## [1] 27   1\n2 # Lambda function  lapply ( MyList ,   function ( x )   max ( x )   +   10 )    1\n2\n3\n4\n5\n6\n7\n8 ## [[1]]\n## [1] 19\n## \n## [[2]]\n## [1] 28\n## \n## [[3]]\n## [1] 37   1\n2\n3\n4\n5 # Custom function \nselect_first  <-   function ( x )   { \n     return ( x [ 1 ])  }  lapply ( MyList ,  select_first )    1\n2\n3\n4\n5\n6\n7\n8 ## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 10\n## \n## [[3]]\n## [1] 19   1\n2\n3\n4\n5 # Custom function with two arguments \nselect_el  <-   function ( x ,  i )   {  \n  x [ i ]   }  lapply ( MyList ,  select_el ,  i  =   2 )    1\n2\n3\n4\n5\n6\n7\n8 ## [[1]]\n## [1] 2\n## \n## [[2]]\n## [1] 11\n## \n## [[3]]\n## [1] 20   Strings  1\n2 # Dataset \nY   1\n2\n3 ##      [,1] [,2]\n## [1,] \"a\"  \"c\" \n## [2,] \"b\"  \"d\"   1\n2 # Change the format  lapply ( Y ,   toupper )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## [[1]]\n## [1] \"A\"\n## \n## [[2]]\n## [1] \"B\"\n## \n## [[3]]\n## [1] \"C\"\n## \n## [[4]]\n## [1] \"D\"",
            "title": "lapply: list-apply"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#sapply-simplify-list-apply",
            "text": "A wrapper that simplifies  lapply .  1\n2 # Dataset \nMyList    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27   1\n2 # Return a list with `lapply()`  lapply ( MyList , \"[\" ,   2 ,   1 )    1\n2\n3\n4\n5\n6\n7\n8 ## [[1]]\n## [1] 2\n## \n## [[2]]\n## [1] 11\n## \n## [[3]]\n## [1] 20   1\n2 # Return a vector with `sapply()`  sapply ( MyList , \"[\" ,   2 ,   1 )   # same result, but simpler    1 ## [1]  2 11 20   1\n2 # Return a list with `sapply()`  sapply ( MyList , \"[\" ,   2 ,   1 ,  simplify  =   T )   # by default    1 ## [1]  2 11 20   1 sapply ( MyList , \"[\" ,   2 ,   1 ,  simplify  =   F )    1\n2\n3\n4\n5\n6\n7\n8 ## [[1]]\n## [1] 2\n## \n## [[2]]\n## [1] 11\n## \n## [[3]]\n## [1] 20   1\n2 # Return a vector with `unlist()`  unlist ( lapply ( MyList , \"[\" ,   2 ,   1 ))   # similar    1 ## [1]  2 11 20   Lambda & custom function  1\n2 # Dataset \nMyList    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27   1\n2 # Built-in function  sapply ( MyList ,   max )    1 ## [1]  9 18 27   1\n2 # Lambda function  lapply ( MyList ,   function ( x )   max ( x )   +   10 )    1\n2\n3\n4\n5\n6\n7\n8 ## [[1]]\n## [1] 19\n## \n## [[2]]\n## [1] 28\n## \n## [[3]]\n## [1] 37   1\n2\n3\n4\n5 # Custom function \nselect_first  <-   function ( x )   { \n     return ( x [ 1 ])  }  sapply ( MyList ,  select_first )    1 ## [1]  1 10 19   1\n2\n3\n4\n5 # Custom function with two arguments \nselect_el  <-   function ( x ,  i )   {  \n  x [ i ]   }  sapply ( MyList ,  select_el ,  i  =   2 )    1 ## [1]  2 11 20   Strings  1\n2 # Dataset \nY   1\n2\n3 ##      [,1] [,2]\n## [1,] \"a\"  \"c\" \n## [2,] \"b\"  \"d\"   1\n2 # Change the format  sapply ( Y ,   toupper )    1\n2 ##   a   b   c   d \n## \"A\" \"B\" \"C\" \"D\"",
            "title": "sapply: simplify-list-apply"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#vapply",
            "text": "A variant.",
            "title": "vapply"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#rapply-recursive-list-apply",
            "text": "1\n2 # Dataset \nMyList    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27   1\n2 # Build-in function  rapply ( MyList ,   sqrt ,  classes  =   \"ANY\" ,  how  =   \"replace\" )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ## [[1]]\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 2.000000 2.645751\n## [2,] 1.414214 2.236068 2.828427\n## [3,] 1.732051 2.449490 3.000000\n## \n## [[2]]\n##          [,1]     [,2]     [,3]\n## [1,] 3.162278 3.605551 4.000000\n## [2,] 3.316625 3.741657 4.123106\n## [3,] 3.464102 3.872983 4.242641\n## \n## [[3]]\n##          [,1]     [,2]     [,3]\n## [1,] 4.358899 4.690416 5.000000\n## [2,] 4.472136 4.795832 5.099020\n## [3,] 4.582576 4.898979 5.196152   1\n2 # Lambda or custom the function  rapply ( MyList ,   function ( x )  x ^ 2 ,  classes  =   \"ANY\" ,  how  =   \"replace\" )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1   16   49\n## [2,]    4   25   64\n## [3,]    9   36   81\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]  100  169  256\n## [2,]  121  196  289\n## [3,]  144  225  324\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]  361  484  625\n## [2,]  400  529  676\n## [3,]  441  576  729   1\n2\n3\n4 select_first  <-   function ( x )   { \n     return ( x [ 1 ])  }  rapply ( MyList ,  select_first ,  classes  =   \"ANY\" ,  how  =   \"replace\" )    1\n2\n3\n4\n5\n6\n7\n8 ## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 10\n## \n## [[3]]\n## [1] 19   1\n2 # Change classes =  class ( MyList )   # the object    1 ## [1] \"list\"   1 class ( MyList [[ 1 ]][ 1 ])   # each entry    1 ## [1] \"integer\"   1 rapply ( MyList ,   sqrt ,  classes  =   \"list\" ,  how  =   \"replace\" )   # not work     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ## [[1]]\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## [[2]]\n##      [,1] [,2] [,3]\n## [1,]   10   13   16\n## [2,]   11   14   17\n## [3,]   12   15   18\n## \n## [[3]]\n##      [,1] [,2] [,3]\n## [1,]   19   22   25\n## [2,]   20   23   26\n## [3,]   21   24   27   1 rapply ( MyList ,   sqrt ,  classes  =   \"ANY\" ,  how  =   \"replace\" )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ## [[1]]\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 2.000000 2.645751\n## [2,] 1.414214 2.236068 2.828427\n## [3,] 1.732051 2.449490 3.000000\n## \n## [[2]]\n##          [,1]     [,2]     [,3]\n## [1,] 3.162278 3.605551 4.000000\n## [2,] 3.316625 3.741657 4.123106\n## [3,] 3.464102 3.872983 4.242641\n## \n## [[3]]\n##          [,1]     [,2]     [,3]\n## [1,] 4.358899 4.690416 5.000000\n## [2,] 4.472136 4.795832 5.099020\n## [3,] 4.582576 4.898979 5.196152   1\n2 # Change how =   rapply ( MyList ,   sqrt ,  classes  =   \"ANY\" ,  how  =   \"list\" )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ## [[1]]\n##          [,1]     [,2]     [,3]\n## [1,] 1.000000 2.000000 2.645751\n## [2,] 1.414214 2.236068 2.828427\n## [3,] 1.732051 2.449490 3.000000\n## \n## [[2]]\n##          [,1]     [,2]     [,3]\n## [1,] 3.162278 3.605551 4.000000\n## [2,] 3.316625 3.741657 4.123106\n## [3,] 3.464102 3.872983 4.242641\n## \n## [[3]]\n##          [,1]     [,2]     [,3]\n## [1,] 4.358899 4.690416 5.000000\n## [2,] 4.472136 4.795832 5.099020\n## [3,] 4.582576 4.898979 5.196152   1 rapply ( MyList ,   sqrt ,  classes  =   \"ANY\" ,  how  =   \"unlist\" )    1\n2\n3\n4 ##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751\n##  [8] 2.828427 3.000000 3.162278 3.316625 3.464102 3.605551 3.741657\n## [15] 3.872983 4.000000 4.123106 4.242641 4.358899 4.472136 4.582576\n## [22] 4.690416 4.795832 4.898979 5.000000 5.099020 5.196152    Built-in or custom  function(x) x .  classes  =  \"ANY\"  for any object classes or a given object class. \n    Useful when the data is mixed and we want to focus on one \n    class only.  how = \"replace\"  or  \"list\" ,  \"unlist\"",
            "title": "rapply: recursive-list-apply"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#mapply-multivariate-apply",
            "text": "1\n2\n3 # Create a 4x4 matrix \nQ1  <-   matrix ( c ( rep ( 1 ,   4 ),   rep ( 2 ,   4 ),   rep ( 3 ,   4 ),   rep ( 4 ,   4 )), 4 , 4 ) \nQ1   1\n2\n3\n4\n5 ##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    1    2    3    4\n## [3,]    1    2    3    4\n## [4,]    1    2    3    4   1\n2\n3 # Or use `mapply()` \nQ2  <-   mapply ( rep ,   1 : 4 ,   4 ) \nQ2   1\n2\n3\n4\n5 ##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    1    2    3    4\n## [3,]    1    2    3    4\n## [4,]    1    2    3    4   Vectorize arguments to a function,  rep , that is not usually accepting \nvectors as arguments. Applies a function to multiple lists,  1 : 4 , or \nmultiple vector,  c() , arguments.",
            "title": "mapply: multivariate-apply"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#vectorize_1",
            "text": "Vectorize a scaler function.  1\n2 # Scalar function  rep ( c ( 1 ,   2 ),   2 )    1 ## [1] 1 2 1 2   1\n2\n3 # Vector function \nvrep  <-   Vectorize ( rep.int ) \nvrep ( c ( 1 ,   2 ),   2 )    1\n2\n3 ##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    2   1\n2\n3 # Scalar function \nf  <-   function ( x  =   1 : 3 ,  y )   c ( x ,  y ) \nf ( 1 : 3 ,   1 : 3 )    1 ## [1] 1 2 3 1 2 3   1\n2\n3 # Vector function \nvf  <-   Vectorize ( f ,  SIMPLIFY  =   FALSE ) \nvf ( 1 : 3 ,   1 : 3 )    1\n2\n3\n4\n5\n6\n7\n8 ## [[1]]\n## [1] 1 1\n## \n## [[2]]\n## [1] 2 2\n## \n## [[3]]\n## [1] 3 3",
            "title": "Vectorize"
        },
        {
            "location": "/Intermediate_R_-_The_apply_Family/#and-more",
            "text": "tapply .  eapply .",
            "title": "And more"
        },
        {
            "location": "/Code___Plot_Chunk_Options/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nPreliminary Options\n\u00b6\n\n\nUsually, the following code is set to FALSE (not showing in a report). These are the general options. Code and plot chunks overrule the general options.\n\n\n1\nknitr\n::\nopts_chunk\n$\nset\n(\necho\n=\nTRUE\n,\n eval\n=\nTRUE\n,\n fig.height\n=\n3\n,\n fig.width\n=\n3\n)\n\n\n\n\n\n\n\nChunks\n\u00b6\n\n\nNaming a chunk is including it in the document outline. The outline is a navigation tool to jump though the document.\n\n\nInputting Data\n\u00b6\n\n\nThe dataset comes from the \nUS Census Bureau\n. On their website, open Excel file \u2018NST-EST2011-02\u2019 about the annual estimates of the resident population.\n\n\nThe data have become an object: a data frame. Check it out, and add column names:\n\n\n1\nhead\n(\nUSstatePops\n,\n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##        V1      V2\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013\n\n\n\n\n\n\n1\n2\n3\ncolnames\n(\nUSstatePops\n)\n \n<-\n \nc\n(\n'State'\n,\n \n'Pop'\n)\n\n\n\nhead\n(\nUSstatePops\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##     State     Pop\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013\n\n\n\n\n\n\nCheck out the data frame:\n\n\n1\nstr\n(\nUSstatePops\n)\n\n\n\n\n\n\n\n1\n2\n3\n## 'data.frame':    51 obs. of  2 variables:\n##  $ State: Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Pop  : int  4779735 710231 6392013 2915921 37253956 5029196 3574097 897934 601723 18801311 ...\n\n\n\n\n\n\nThe states should be strings, not factors.\n\n\nExtract numbers or strings without any loss from a factor structure:\n\n\n1\n2\n3\n4\n# make a copy for safety\n\nUSstatePops2 \n<-\n USstatePops\n\nUSstatePops2\n$\nState \n<-\n \nas.character\n(\nlevels\n(\nUSstatePops2\n$\nState\n))\n\n\n\n\n\n\n\nCheck out the new data frame:\n\n\n1\nstr\n(\nUSstatePops2\n)\n\n\n\n\n\n\n\n1\n2\n3\n## 'data.frame':    51 obs. of  2 variables:\n##  $ State: chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n##  $ Pop  : int  4779735 710231 6392013 2915921 37253956 5029196 3574097 897934 601723 18801311 ...\n\n\n\n\n\n\n1\nhead\n(\nUSstatePops2\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##     State     Pop\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013\n\n\n\n\n\n\nFormatting Code Chunks\n\u00b6\n\n\neval=TRUE\n; show the results (default).\n\u00b6\n\n\n1\nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n## [1] 6053834\n\n\n\n\n\n\neval=FALSE\n; or no results.\n\u00b6\n\n\n1\nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\nresults='markup'\n; show split code/results/code/results (default).\n\u00b6\n\n\n1\nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n## [1] 6053834\n\n\n\n\n\n\n1\nmedian\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n## [1] 4339362\n\n\n\n\n\n\neval='asis'\n; show \u2018unboxed\u2019 results.\n\u00b6\n\n\n1\nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n[1] 6053834\n\n\n1\nmedian\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n[1] 4339362\n\n\neval='hide'\n; show code only.\n\u00b6\n\n\n1\n2\nmean\n(\nUSstatePops2\n$\nPop\n)\n\nmedian\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\neval='hold'\n; show code block/results block.\n\u00b6\n\n\n1\n2\nmean\n(\nUSstatePops2\n$\nPop\n)\n\nmedian\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n2\n## [1] 6053834\n## [1] 4339362\n\n\n\n\n\n\n\n\necho=TRUE\n; show the code (default).\n\u00b6\n\n\n1\nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n## [1] 6053834\n\n\n\n\n\n\n1\nmedian\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n## [1] 4339362\n\n\n\n\n\n\necho=FALSE\n; or no code.\n\u00b6\n\n\n1\n2\n3\n## [1] 6053834\n\n## [1] 4339362\n\n\n\n\n\n\n\n\nwarning\n, \nerror\n, \nmessage\n are set to TRUE by default. They can be set of FALSE when running a library() code to avoid polluting the report.\n\u00b6\n\n\n1\n{r, warning=TRUE, error=TRUE, message=TRUE}\n\n\n\n\n\n\n\n\ntidy=TRUE/FALSE\n; with the \nformatR\n and \nshiny\n packages (you manage spaces and indents) (FALSE by default).\n\u00b6\n\n\n1\n{r, tidy=TRUE}\n\n\n\n\n\n\n\n\ncache=TRUE/FALSE; cache the results (FALSE by default).\n\u00b6\n\n\nCan be resused in future knits since it \ncreates a subdir (the \u2018cache\u2019)\n with a R workspace, .rdb and .rdx files.\n\n\n1\n{r, cache=TRUE}\n\n\n\n\n\n\nThe \ncache.path='cache/'\n can be changed. See \ncache-comments\n, \ncache.lazy\n, \ncache.vars\n, \nautodep\n, \ndependson\n.\n\n\n\n\ncomment='##'\n; the comments in results (by default).\n\u00b6\n\n\n1\nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n## [1] 6053834\n\n\n\n\n\n\ncomment='#'\n; the new comments.\n\u00b6\n\n\n1\nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n# [1] 6053834\n\n\n\n\n\n\n\n\ncode chunk \n{r}\n.\n\u00b6\n\n\n1\n2\nlist\n \n<-\n \nc\n(\n1\n,\n \n2\n,\n \n3\n)\n\n\nlist\n\n\n\n\n\n\n\n1\n## [1] 1 2 3\n\n\n\n\n\n\ncode chunk \n{code=NULL}\n.\n\u00b6\n\n\n1\n2\nlist\n \n<-\n \nc\n(\n1\n,\n \n2\n,\n \n3\n)\n\n\nlist\n\n\n\n\n\n\n\ncode chunk \n{text}\n.\n\u00b6\n\n\n1\n2\nlist <- c(1, 2, 3)\nlist\n\n\n\n\n\n\ncode chunk \n{python}\n.\n\u00b6\n\n\n1\n2\nlist\n \n=\n \n[\n1\n,\n \n2\n,\n \n3\n]\n\n\nprint\n(\nlist\n)\n\n\n\n\n\n\n\nSet up the new language first.\n\n\n\n\nhightlight=TRUE\n; hightlight the code (default).\n\u00b6\n\n\n1\nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n## [1] 6053834\n\n\n\n\n\n\nhightlight=FALSE\n; or not.\n\u00b6\n\n\n1\nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n## [1] 6053834\n\n\n\n\n\n\nprompt=TRUE\n; add \n>\n before the code.\n\u00b6\n\n\n1\n>\n \nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n## [1] 6053834\n\n\n\n\n\n\nprompt=FALSE\n; or not (default).\n\u00b6\n\n\n1\nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n## [1] 6053834\n\n\n\n\n\n\nstrip.white=TRUE\n; remove white space from the code (default).\n\u00b6\n\n\n1\nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n## [1] 6053834\n\n\n\n\n\n\nstrip.white=FALSE\n; or not.\n\u00b6\n\n\n1\nmean\n(\nUSstatePops2\n$\nPop\n)\n\n\n\n\n\n\n\n1\n## [1] 6053834\n\n\n\n\n\n\nFormatting Plot Chunk\n\u00b6\n\n\nPrints the plots in the .html report and and creates a subdir with the plot files (the references).\n\n\nfig.path='figure/'\n; new file path for this chunk.\n\u00b6\n\n\nOtherwise, the path is set in the general options.\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n \n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\nThe device prints .png files by default.\n\u00b6\n\n\nIt can be changed to other formats.\n\n\ndev='png'\n.\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n \n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\ndev='jpeg'\n.\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n \n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\ndev='pdf'\n; \n'pdf'\n cannot be printed in the .html report, but only included in the subdir.\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\n\n\nfig.width= , fig.height=\n; change the box size (\n=7\n by default).\n\u00b6\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\nfig.width=5, fig.height=5\n.\n\u00b6\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\nfig.height=3\n.\n\u00b6\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\nfig.width=3\n.\n\u00b6\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\nout.height=100, out.width=100\n; in pixels.\n\u00b6\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\n\n\nresize.height=200, resize.width=200\n; resize tike graphics for latex, in pixels.\n\u00b6\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\nsanitize=TRUE\n; sanitize \u2018tike\u2019 graphics for latex.\n\u00b6\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\n\n\nSet the device arguments:\n\u00b6\n\n\ndev.args=list(bg='yellow', pointsize=10)\n.\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\ndev.args=list(pointsize=8), fig.height=3\n.\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\n\n\ndots per inch.\n\u00b6\n\n\ndpi=72\n.\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\ndpi=90\n.\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\ndpi multiplier for .html output on retina screens:\n\u00b6\n\n\nfig.retina=1\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\nfig.retina=2\n; double dpi.\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\n\n\nfig.align='left'\n or \nfig.align='default'\n.\n\u00b6\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\nfig.align='right'\n.\n\u00b6\n\n\nfig.align='center'\n.\n\u00b6\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\n\n\nFigure captions at the bottom of the plot; figure caption in latex:\n\u00b6\n\n\nfig.cap='CAPTION 14'\n.\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\nSee:\n\n\n\n\nfig.lp=''\n; figure caption prefix\n\n\nfig.scap=''\n; short figure caption prefix.\n\n\nfig.subcap=''\n; subcaption.\n\n\nfig.env=''\n; the latex environment for figures.\n\n\n\n\n\n\nVersions\n\u00b6\n\n\nfig.keep='high'\n; merge low-level changes into high-level plots.\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\nSee:\n\n\n\n\nfig.keep='all'\n; keep all plots (low-level changes may produce new plots).\n\n\nfig.keep='first'/'last'\n; keep the first/last plot only.\n\n\nfig.keep='none'\n; discard all plots.\n\n\n\n\n\n\nfig.pos='test'\n; string to be used as the figure position arrangement in latex.\n\u00b6\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n \n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\n\n\nShow\n\u00b6\n\n\nfig.show='asis'\n.\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n10\n,\n main \n=\n \n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\n1\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n \n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\n\n\nfig.show='hold'\n; display the plots at the very end of the chunk.\n\n\n\n\n\n\n\n1\n2\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n10\n,\n main \n=\n \n''\n,\n xlab \n=\n \n'Pop'\n)\n\nhist\n(\nUSstatePops2\n$\nPop\n,\n breaks \n=\n \n20\n,\n main \n=\n \n''\n,\n xlab \n=\n \n'Pop'\n)\n\n\n\n\n\n\n\n\n\n\n\nSee:\n\n\n\n\nfig.show='hide'\n; generate the plots, but not in the final document.\n\n\nfig.show='animate'\n; combine all of the plots created into an animation. Additional packages and settings arerequired.",
            "title": "Code & Plot Chunk Options"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#chunks",
            "text": "Naming a chunk is including it in the document outline. The outline is a navigation tool to jump though the document.",
            "title": "Chunks"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#inputting-data",
            "text": "The dataset comes from the  US Census Bureau . On their website, open Excel file \u2018NST-EST2011-02\u2019 about the annual estimates of the resident population.  The data have become an object: a data frame. Check it out, and add column names:  1 head ( USstatePops , 3 )    1\n2\n3\n4 ##        V1      V2\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013   1\n2\n3 colnames ( USstatePops )   <-   c ( 'State' ,   'Pop' )  head ( USstatePops ,   3 )    1\n2\n3\n4 ##     State     Pop\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013   Check out the data frame:  1 str ( USstatePops )    1\n2\n3 ## 'data.frame':    51 obs. of  2 variables:\n##  $ State: Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Pop  : int  4779735 710231 6392013 2915921 37253956 5029196 3574097 897934 601723 18801311 ...   The states should be strings, not factors.  Extract numbers or strings without any loss from a factor structure:  1\n2\n3\n4 # make a copy for safety \nUSstatePops2  <-  USstatePops\n\nUSstatePops2 $ State  <-   as.character ( levels ( USstatePops2 $ State ))    Check out the new data frame:  1 str ( USstatePops2 )    1\n2\n3 ## 'data.frame':    51 obs. of  2 variables:\n##  $ State: chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n##  $ Pop  : int  4779735 710231 6392013 2915921 37253956 5029196 3574097 897934 601723 18801311 ...   1 head ( USstatePops2 ,   3 )    1\n2\n3\n4 ##     State     Pop\n## 1 Alabama 4779735\n## 2  Alaska  710231\n## 3 Arizona 6392013",
            "title": "Inputting Data"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#formatting-code-chunks",
            "text": "",
            "title": "Formatting Code Chunks"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#evaltrue-show-the-results-default",
            "text": "1 mean ( USstatePops2 $ Pop )    1 ## [1] 6053834",
            "title": "eval=TRUE; show the results (default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#evalfalse-or-no-results",
            "text": "1 mean ( USstatePops2 $ Pop )",
            "title": "eval=FALSE; or no results."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#resultsmarkup-show-split-coderesultscoderesults-default",
            "text": "1 mean ( USstatePops2 $ Pop )    1 ## [1] 6053834   1 median ( USstatePops2 $ Pop )    1 ## [1] 4339362",
            "title": "results=&#39;markup&#39;; show split code/results/code/results (default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#evalasis-show-unboxed-results",
            "text": "1 mean ( USstatePops2 $ Pop )    [1] 6053834  1 median ( USstatePops2 $ Pop )    [1] 4339362",
            "title": "eval=&#39;asis&#39;; show 'unboxed' results."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#evalhide-show-code-only",
            "text": "1\n2 mean ( USstatePops2 $ Pop ) \nmedian ( USstatePops2 $ Pop )",
            "title": "eval=&#39;hide&#39;; show code only."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#evalhold-show-code-blockresults-block",
            "text": "1\n2 mean ( USstatePops2 $ Pop ) \nmedian ( USstatePops2 $ Pop )    1\n2 ## [1] 6053834\n## [1] 4339362",
            "title": "eval=&#39;hold&#39;; show code block/results block."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#echotrue-show-the-code-default",
            "text": "1 mean ( USstatePops2 $ Pop )    1 ## [1] 6053834   1 median ( USstatePops2 $ Pop )    1 ## [1] 4339362",
            "title": "echo=TRUE; show the code (default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#echofalse-or-no-code",
            "text": "1\n2\n3 ## [1] 6053834\n\n## [1] 4339362",
            "title": "echo=FALSE; or no code."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#warning-error-message-are-set-to-true-by-default-they-can-be-set-of-false-when-running-a-library-code-to-avoid-polluting-the-report",
            "text": "1 {r, warning=TRUE, error=TRUE, message=TRUE}",
            "title": "warning, error, message are set to TRUE by default. They can be set of FALSE when running a library() code to avoid polluting the report."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#tidytruefalse-with-the-formatr-and-shiny-packages-you-manage-spaces-and-indents-false-by-default",
            "text": "1 {r, tidy=TRUE}",
            "title": "tidy=TRUE/FALSE; with the formatR and shiny packages (you manage spaces and indents) (FALSE by default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#cachetruefalse-cache-the-results-false-by-default",
            "text": "Can be resused in future knits since it  creates a subdir (the \u2018cache\u2019)  with a R workspace, .rdb and .rdx files.  1 {r, cache=TRUE}   The  cache.path='cache/'  can be changed. See  cache-comments ,  cache.lazy ,  cache.vars ,  autodep ,  dependson .",
            "title": "cache=TRUE/FALSE; cache the results (FALSE by default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#comment-the-comments-in-results-by-default",
            "text": "1 mean ( USstatePops2 $ Pop )    1 ## [1] 6053834",
            "title": "comment=&#39;##&#39;; the comments in results (by default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#comment-the-new-comments",
            "text": "1 mean ( USstatePops2 $ Pop )    1 # [1] 6053834",
            "title": "comment=&#39;#&#39;; the new comments."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#code-chunk-r",
            "text": "1\n2 list   <-   c ( 1 ,   2 ,   3 )  list    1 ## [1] 1 2 3",
            "title": "code chunk {r}."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#code-chunk-codenull",
            "text": "1\n2 list   <-   c ( 1 ,   2 ,   3 )  list",
            "title": "code chunk {code=NULL}."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#code-chunk-text",
            "text": "1\n2 list <- c(1, 2, 3)\nlist",
            "title": "code chunk {text}."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#code-chunk-python",
            "text": "1\n2 list   =   [ 1 ,   2 ,   3 ]  print ( list )    Set up the new language first.",
            "title": "code chunk {python}."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#hightlighttrue-hightlight-the-code-default",
            "text": "1 mean ( USstatePops2 $ Pop )    1 ## [1] 6053834",
            "title": "hightlight=TRUE; hightlight the code (default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#hightlightfalse-or-not",
            "text": "1 mean ( USstatePops2 $ Pop )    1 ## [1] 6053834",
            "title": "hightlight=FALSE; or not."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#prompttrue-add-before-the-code",
            "text": "1 >   mean ( USstatePops2 $ Pop )    1 ## [1] 6053834",
            "title": "prompt=TRUE; add &gt; before the code."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#promptfalse-or-not-default",
            "text": "1 mean ( USstatePops2 $ Pop )    1 ## [1] 6053834",
            "title": "prompt=FALSE; or not (default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#stripwhitetrue-remove-white-space-from-the-code-default",
            "text": "1 mean ( USstatePops2 $ Pop )    1 ## [1] 6053834",
            "title": "strip.white=TRUE; remove white space from the code (default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#stripwhitefalse-or-not",
            "text": "1 mean ( USstatePops2 $ Pop )    1 ## [1] 6053834",
            "title": "strip.white=FALSE; or not."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#formatting-plot-chunk",
            "text": "Prints the plots in the .html report and and creates a subdir with the plot files (the references).",
            "title": "Formatting Plot Chunk"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figpathfigure-new-file-path-for-this-chunk",
            "text": "Otherwise, the path is set in the general options.  1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  =   '' ,  xlab  =   'Pop' )",
            "title": "fig.path=&#39;figure/&#39;; new file path for this chunk."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#the-device-prints-png-files-by-default",
            "text": "It can be changed to other formats.  dev='png' .  1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  =   '' ,  xlab  =   'Pop' )     dev='jpeg' .  1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  =   '' ,  xlab  =   'Pop' )     dev='pdf' ;  'pdf'  cannot be printed in the .html report, but only included in the subdir.  1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "The device prints .png files by default."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figwidth-figheight-change-the-box-size-7-by-default",
            "text": "1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "fig.width= , fig.height=; change the box size (=7 by default)."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figwidth5-figheight5",
            "text": "1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "fig.width=5, fig.height=5."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figheight3",
            "text": "1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "fig.height=3."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figwidth3",
            "text": "1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "fig.width=3."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#outheight100-outwidth100-in-pixels",
            "text": "1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "out.height=100, out.width=100; in pixels."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#resizeheight200-resizewidth200-resize-tike-graphics-for-latex-in-pixels",
            "text": "1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "resize.height=200, resize.width=200; resize tike graphics for latex, in pixels."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#sanitizetrue-sanitize-tike-graphics-for-latex",
            "text": "1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "sanitize=TRUE; sanitize 'tike' graphics for latex."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#set-the-device-arguments",
            "text": "dev.args=list(bg='yellow', pointsize=10) .  1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )     dev.args=list(pointsize=8), fig.height=3 .  1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "Set the device arguments:"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#dots-per-inch",
            "text": "dpi=72 .  1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )     dpi=90 .  1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "dots per inch."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#dpi-multiplier-for-html-output-on-retina-screens",
            "text": "fig.retina=1  1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )     fig.retina=2 ; double dpi.  1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "dpi multiplier for .html output on retina screens:"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figalignleft-or-figaligndefault",
            "text": "1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "fig.align=&#39;left&#39; or fig.align=&#39;default&#39;."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figalignright",
            "text": "",
            "title": "fig.align=&#39;right&#39;."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figaligncenter",
            "text": "1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )",
            "title": "fig.align=&#39;center&#39;."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figure-captions-at-the-bottom-of-the-plot-figure-caption-in-latex",
            "text": "fig.cap='CAPTION 14' .  1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )     See:   fig.lp='' ; figure caption prefix  fig.scap='' ; short figure caption prefix.  fig.subcap='' ; subcaption.  fig.env='' ; the latex environment for figures.",
            "title": "Figure captions at the bottom of the plot; figure caption in latex:"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#versions",
            "text": "fig.keep='high' ; merge low-level changes into high-level plots.  1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  = '' ,  xlab  =   'Pop' )     See:   fig.keep='all' ; keep all plots (low-level changes may produce new plots).  fig.keep='first'/'last' ; keep the first/last plot only.  fig.keep='none' ; discard all plots.",
            "title": "Versions"
        },
        {
            "location": "/Code___Plot_Chunk_Options/#figpostest-string-to-be-used-as-the-figure-position-arrangement-in-latex",
            "text": "1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  =   '' ,  xlab  =   'Pop' )",
            "title": "fig.pos=&#39;test&#39;; string to be used as the figure position arrangement in latex."
        },
        {
            "location": "/Code___Plot_Chunk_Options/#show",
            "text": "fig.show='asis' .  1 hist ( USstatePops2 $ Pop ,  breaks  =   10 ,  main  =   '' ,  xlab  =   'Pop' )     1 hist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  =   '' ,  xlab  =   'Pop' )      fig.show='hold' ; display the plots at the very end of the chunk.    1\n2 hist ( USstatePops2 $ Pop ,  breaks  =   10 ,  main  =   '' ,  xlab  =   'Pop' ) \nhist ( USstatePops2 $ Pop ,  breaks  =   20 ,  main  =   '' ,  xlab  =   'Pop' )      See:   fig.show='hide' ; generate the plots, but not in the final document.  fig.show='animate' ; combine all of the plots created into an animation. Additional packages and settings arerequired.",
            "title": "Show"
        },
        {
            "location": "/Working_with_RStudio_IDE/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nHelp:\n \nAlt\n+\nShift\n+\nK\n or Tools/Keyboard Shortcuts Help.\n\n\nOrientation\n\u00b6\n\n\nCommands\n\n\n\n\nCtrl\n+\nUp\n; command history.\n\n\nTab\n; completion for all.\n\n\nCtrl\n+\nL\n; clear the console.\n\n\nTools/Global Options\u2026 set up RStudio.\n\n\nview(dataframe)\n; open a spreadsheet, show in new window, sort the\n\n    rows, search, filter the rows.\n\n\n\n\n\n\n\n1\n2\ndf \n<-\n \ndata.frame\n(\ncolA \n=\n \nc\n(\n1\n,\n \n2\n),\n colB \n=\n \nc\n(\n3\n,\n \n6\n))\n\ndf\n\n\n\n\n\n\n1\n2\n3\n##   colA colB\n## 1    1    3\n## 2    2    6\n\n\n\n\n\n\n1\nView\n(\ndf\n)\n \n# in a new window\n\n\n\n\n\n\n\n\n\nIDE panes\n\n\n\n\nEnvironment pane; load, save, remove objects, read a dataset,\n\n    import dataset.\n\n\nHistory pane; idem, clear all or one item at the time, copy the\n\n    command in the console pane, reload a command with \nShift\n+\nEnter\n and\n\n    run it.\n\n\nFile pane; working directories, files, add a new folder, rename\n\n\ngetwd()\n; get working directory.\n\n\nsetwd()\n; set working directory.\n\n\nPlot pane; save (extension, size, resolution).\n\n\nPackage pane; update.\n\n\nHelp pane; pages.\n\n\nViewer pane; more than plots!\n\n\n\n\nProgramming\n\u00b6\n\n\nScripting\n\n\n\n\nCtrl\n+\nShift\n+\nM\n; \n%>%\n.\n\n\nAlt\n+\n-\n; \n<-\n.\n\n\nCtrl\n+\nShift\n+\nC\n; add/delete a \n#\n for commenting.\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n%>%\n \n\n\n<-\n \n\n\n# comment\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nCtrl\n+\nAlt\n+\nI\n; new chunk.\n\n\n\n\n\n\nConvert into a function.\n\n\n\n\n\n\n\n\nType code,\n\n\nHighlight it,\n\n\nCode/Extract Function to create a function:\n\n\n\n\nThis:\n\n\n1\nrnorm\n(\n10\n,\n \n0\n,\n \n1\n)\n\n\n\n\n\n\n\nBecomes:\n\n\n1\n2\n3\nrnorm \n<-\n \nfunction\n()\n \n{\n\n  rnorm\n(\n10\n,\n \n0\n,\n \n1\n)\n\n\n}\n\n\n\n\n\n\n\n\n\nCtrl\n+\nAlt\n+\nclick\n; multiple cursors for typing!\n\n\nSwitch between Default, Vim and Emacs modes with\n\n    Tools/Global Options/Code/Keybindings.\n\n\nShift\n+\nAlt\n+\nG\n; go to line.\n\n\nCtrl\n+\nF\n: find/replace in the current document.\n\n\nAlt\n+\nO\n, \nAlt\n+\nShift\n+\nO\n; fold/unfold the code.\n\n\nCtrl\n+\nP\n; jump between symbols like (), {}, [].\n\n\nCtrl\n+\nShift\n+\nEnter\n; run and source the code.\n\n\nCtrl\n+\nEnter\n; run and source the code.\n\n\nCtrl\n+\nShift\n+\nF10\n; restart, refresh the R session.\n\n\n\n\nError handling\n\n\nRStudio traces back the error origin.\n\n\nToggle the show/hide traceback in the console when there is an error or\n\nrerun with the bug, watch the right pane for traceback.\n\n\nInvestigate, highlight the next line of code, click in the traceback\n\nwindow, click the dropdown menu in the Global Environment (upper-right).\n\nStop, continue in the debugger mode, press \nC\n, press \nQ\n.\n\n\nAdd/remove breakpoints, where the line numbers are.\n\n\n\n\ndebugonce\n; automatically call the debugger when the function is\n\n    called, but only once.\n\n\ndebug\n, \nundebug\n; automatically call the debugger when the\n\n    function is called.\n\n\n\n\nAdd \noptions(error=browser)\n or \noptions(error=NULL)\n at the beginning\n\nof the script; R automatically open the debugger mode.\n\n\n\n\nN\n; next line.\n\n\nstep-into\n icon.\n\n\nShift\n+\nF4\n; step into.\n\n\nShift\n+\nF6\n; execute the remainder of the bug.\n\n\n\n\nProject\n\u00b6\n\n\n\n\nCreate a project with a folder and all the files (Global\n\n    Environment, History, etc.): New Directory, Existing Directory,\n\n    Version Control\u2026 Empty Project, R Package (project), Shiny\n\n    Web Application.\n\n\nCreate a Git repository with the new project\n\n\n\n\nCommands\n\n\n\n\nCtrl\n+\nShift\n+\nF\n; find in files.\n\n\nCtrl\n+\nF9\n, \nF10\n; go backwards/forwards.\n\n\n\n\nPackrat\n\n\nPackrat is a dependency management system for R. Use one version of a\n\npackage for one project, another version of a package for another\n\nproject. Associate a project with its own set of packages.\n\n\n\n\nPackrat\n.\n\n\nActivate Packrat when creating a project or\n\n    Tools/Project Options/Packrat.\n\n\nThe library is virtually separate from R library.\n\n\nPerfect for collaborating with GitHub\n\n\n\n\nPackages\n\u00b6\n\n\nIntroduction to R packages\n\n\nAnything that can be automated, should be automated. Do as little as\n\npossible by hand. Do as much as possible with functions.\n\n\nProcess:\n\n\n\n\nWriting R functions.\n\n\nDocumenting functions.\n\n\nWriting tests.\n\n\nChecking compatibility.\n\n\nBuilding the package for sharing and using.\n\n\n\n\nSee the book: \nR Packages\n.\n\n\nCreate a new R package\n\n\nCreate a new directory with new files, folders, and meta-information.\n\n\nTo update or not to update\n\n\nRStudio generates the NAMESPACE content for you automatically.\n\n\nSee the book: \nR Packages\n.\n\n\nImport & load source files\n\n\nMove existing functions from an existing project into the new package.\n\nThe function are moves to a new folder in the project.\n\n\nTest created functions all in once with the Load All command in the\n\nBuild Tab.\n\n\nSimulations:\n\n\n\n\nCtrl\n+\nShift\n+\nL\n\n\nCtrl\n+\nShift\n+\nF10\n; restart a R session.\n\n\n\n\nPackages documentation\n\n\nCreate a help page (.Rd file). Written in HTML.\n\n\nUse the \nroxigen\n \npackage\n to document.\n\n\nTools/Project Optons/Build Tools/Generate documentation. Generate the\n\ndoc, add comments.\n\n\nFirst, create a doc skeleton above the function:\n\n\n\n\nCtrl\n+\nAlt\n+\nShift\n+\nR\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n#' Title\n#'\n#' @param x \n#'\n#' @return\n#' @export\n#'\n#' @examples\ncEnter <- function(x) {\n  x - mean(x)\n}\n\n\n\n\n\n\nFill in the blanks.\n\n\nPackage documentation (2)\n\n\nTitle of the help page. Text on how to use.\n\n\nThe 4 tags (\n@\n) help organizing the doc: \n@export\n (tells R that this\n\nfunction should be made available to people who load your package. ),\n\n\n@params\n, \n@return\n, and \n@examples\n. There are many more advanced\n\ntags.\n\n\nHighlight a section and test it, run it.\n\n\nPackage documentation (3)\n\n\nBuild Tab/build the package.\n\n\n\n\nCtrl\n+\nShift\n+\nD\n\n\n\n\nCompile all. Load all. Open the help page with \n?function\n.\n\n\nLearn the \nroxigen\n workflow to ease the work.\n\n\nTest your package\n\n\nWith the \ntestthat\n (and \ndevtools\n) package.\n\n\nInstall both packages. Run \ndevtools::use_testthat()\n and a now\n\ndirectory with subdir appears in the package. This is where we save\n\ntests.\n\n\nTest your package (2)\n\n\nReady. Write tests. Open a new R script and save it to the tests\n\ndirectory. Create a \ncontext\n function. Add \ntest_that\n functions with\n\narguments.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\ncontext(\"cEnter\")\n\ntest_that(\"cEnter handles integers\", {\n  expect_equal(cEnter(1:3), -1:1)\n  expect_equal(cEnter(-(1:3)), 1:-1)\n})\n\ncontext(\"scale\")\n\ntest_that(\"scale handles integers\", {\n  expect_equal(scale(1:3), 1:3)\n  expect_equal(scale(-(1:3)), -(1:3))\n})\n\ncontext(\"standardize\")\n\ntest_that(\"standardize handles integers\", {\n  expect_equal(standardize(1:3), -1:1)\n  expect_equal(standardize(-(1:3)), 1:-1)\n})\n\n\n\n\n\n\nTest your package (3)\n\n\nRun the tests.\n\n\n\n\nCtrl\n+\nShift\n+\nT\n\n\n\n\nGet a summary (pass or not pass). Test and retest the package.\n\n\nSee the book: \nR Packages\n.\n\n\nTime to test your package\n\n\nRun the test with Build Tab/Test package.\n\n\nCheck your package\n\n\nUpload the package to GitHub. Recreate the package structure in the\n\nrepo. Download and install the package with \ninstall_github()\n from the\n\n\ndevtools\n package. Test the package on GitHub to complete the tests.\n\nType \nR CMD check\n in the terminal. Or Build Tab/Check icon.\n\n\n\n\nCtrl\n+\nShift\n+\nE\n\n\n\n\nBuild your package\n\n\nA package is a tarball or package bundle.\n\n\nBuild Tab/Build & Reload. Install and load the package.\n\n\n\n\nCtrl\n+\nShift\n+\nB\n\n\n\n\nBuild and reload will overwrite the existing package. Run\n\n\ndevtools::dev_mode()\n creates a separate library for development.\n\nRunning the command again cancels it.\n\n\nTwo formats: source package or binary package (more compresses and\n\noptimized).\n\n\nSee the book: \nR Packages\n.\n\n\nWrap-up\n\n\nGet the \ncheat sheet\n\n\nVersion Control\n\u00b6\n\n\nIntroduction to Git\n\n\nUse Git to work in team. Even on R scripts, reports and packages.\n\n\nInstall Git.\n\n\nIn RStudio, Tools/Global Options/Git/SVN + Tools/Project Options/Select\n\nVCS, restart RStudio. RStudio has now a Git pane and a Git icon.\n\n\nStage & commit\n\n\nThe Git Tab is a directory. The real life (local) version of the\n\nproject. The official version of the project as recorded with Git.\n\n\nThe two versions are different. View the differences. When you commit,\n\nyou add thing from the real to the official version.\n\n\nGreen highlighting indicates something you\u2019ve added to the official\n\nversion, while red highlighting indicates lines you have removed.\n\n\n.gitignore\n\n\nInside the project, ther is a .gignore file. Add file that are excluded\n\nfrom the offcial version. The file can be accessed from the Git pane.\n\n\nGit icons\n\n\nAdd, (cancel), commit, (cancel). The icons will changed.\n\n\nCommit history\n\n\nHistory viewer: each commitment. HEAD commit and parent commit. Master\n\nbranch or another.\n\n\nUndo commited changes: checkout\n\n\nCheckout command. Go back to a previous commit. The commit stays in the\n\nhistory (not deleted).\n\n\nRun it in the shell: Tools/shell.\n\n\ngit checkout sha# nameofthefile\n and Git will reverse the commit. The\n\nfile is in the stage area as before the commit.\n\n\nUndo uncommited changes\n\n\nThe file is in the stage area, ready to commit. Cancel the addition.\n\n\nIn the Change window, click on the Revert button.\n\n\nOr, click on Discard chunk.\n\n\nOr, \nCtrl\n+\nZ\n to undo.\n\n\nThen, Save the file.\n\n\nIntroduction to GitHub\n\n\nCentralize, host, track issues, track metrics.\n\n\nInstall a R package From GitHub with \ninstall_github\n from the the\n\n\ndevtools\n packages.\n\n\nPull & Push\n\n\nLocal and GitHub\n\n\nWrap-up\n\n\nGo to help.github.com\n\n\nGo to stackoverflow.com\n\n\nReporting\n\u00b6\n\n\nTools for reporting\n\n\nR Markdown and Shiny (over the web).\n\n\nIntroduction to R Markdown\n\n\nText and code.\n\n\nWeb link: \n<http://www>\n\n\nR Markdown in RStudio\n\n\nReport (HTML, PDF, Word) or presentation(slide).\n\n\nCreate a template. Load in new templates. The \nrticles\n package has\n\ntemplates for academic journals.\n\n\nThe outline (name the chunks).\n\n\nHelp/Markdown Quick Reference\n\n\nAdd a code chunk wih \nCtrl\n+\nAlt\n+\nI\n\n\nRendering R Markdown\n\n\nKnit or preview. For pdf, need \n\\LaTeX\n\\LaTeX\n (install).\n\n\nAdding \nruntime\n:\n \nshiny\n generates a Shiny report. Launch the interactive\n\napp.\n\n\nPublish reports online.\n\n\nCompile notebook\n\n\nFile/Compile Notebook for R script. The script becomes a report (Word,\n\nPDF, HTML).\n\n\nRStudio\u2019s \n\\LaTeX\n\\LaTeX\n editor\n\n\nInstall \n\\LaTeX\n\\LaTeX\n.\n\n\nOpen a .tex file in RStudio. RStudio has limited options to edit \n\\LaTeX\n\\LaTeX\n;\n\nenough to write and compile.\n\n\nThe preview window is linked to the source window. If we click on a\n\ncharacter in the source window, press \nCtrl\n+\nclick\n: the corresponding\n\ncharacter is highlighted in the preview window. Vice-versa.\n\n\nTools/Global options/Sweave to change the \n\\LaTeX\n\\LaTeX\n options.\n\n\nShiny\n\n\nThe server is online.\n\n\nshiny.rstudio.com\n\n\nWhen we create a Shiny app, we create two files: ui.R and server.R\n\n\nRun App (the app) or \nCtrl\n+\nShift\n+\nEnter\n\n\nPublish Shiny apps\n\n\nNeed an account and the \nshinyapps\n package.\n\n\ndevtools::install_github(\"rstudio/shinyapps\")\n\n\nDeploy the local app online. Unique URL. Monitor usage, view logs,\n\narchive or delete the app. Max 5 apps at a time for free. Paid account.",
            "title": "Working with the RStudio IDE"
        },
        {
            "location": "/Working_with_RStudio_IDE/#programming",
            "text": "Scripting   Ctrl + Shift + M ;  %>% .  Alt + - ;  <- .  Ctrl + Shift + C ; add/delete a  #  for commenting.    1\n2\n3\n4\n5 %>%   <-   # comment    Code    Ctrl + Alt + I ; new chunk.    Convert into a function.     Type code,  Highlight it,  Code/Extract Function to create a function:   This:  1 rnorm ( 10 ,   0 ,   1 )    Becomes:  1\n2\n3 rnorm  <-   function ()   { \n  rnorm ( 10 ,   0 ,   1 )  }     Ctrl + Alt + click ; multiple cursors for typing!  Switch between Default, Vim and Emacs modes with \n    Tools/Global Options/Code/Keybindings.  Shift + Alt + G ; go to line.  Ctrl + F : find/replace in the current document.  Alt + O ,  Alt + Shift + O ; fold/unfold the code.  Ctrl + P ; jump between symbols like (), {}, [].  Ctrl + Shift + Enter ; run and source the code.  Ctrl + Enter ; run and source the code.  Ctrl + Shift + F10 ; restart, refresh the R session.   Error handling  RStudio traces back the error origin.  Toggle the show/hide traceback in the console when there is an error or \nrerun with the bug, watch the right pane for traceback.  Investigate, highlight the next line of code, click in the traceback \nwindow, click the dropdown menu in the Global Environment (upper-right). \nStop, continue in the debugger mode, press  C , press  Q .  Add/remove breakpoints, where the line numbers are.   debugonce ; automatically call the debugger when the function is \n    called, but only once.  debug ,  undebug ; automatically call the debugger when the \n    function is called.   Add  options(error=browser)  or  options(error=NULL)  at the beginning \nof the script; R automatically open the debugger mode.   N ; next line.  step-into  icon.  Shift + F4 ; step into.  Shift + F6 ; execute the remainder of the bug.",
            "title": "Programming"
        },
        {
            "location": "/Working_with_RStudio_IDE/#project",
            "text": "Create a project with a folder and all the files (Global \n    Environment, History, etc.): New Directory, Existing Directory, \n    Version Control\u2026 Empty Project, R Package (project), Shiny \n    Web Application.  Create a Git repository with the new project   Commands   Ctrl + Shift + F ; find in files.  Ctrl + F9 ,  F10 ; go backwards/forwards.   Packrat  Packrat is a dependency management system for R. Use one version of a \npackage for one project, another version of a package for another \nproject. Associate a project with its own set of packages.   Packrat .  Activate Packrat when creating a project or \n    Tools/Project Options/Packrat.  The library is virtually separate from R library.  Perfect for collaborating with GitHub",
            "title": "Project"
        },
        {
            "location": "/Working_with_RStudio_IDE/#packages",
            "text": "Introduction to R packages  Anything that can be automated, should be automated. Do as little as \npossible by hand. Do as much as possible with functions.  Process:   Writing R functions.  Documenting functions.  Writing tests.  Checking compatibility.  Building the package for sharing and using.   See the book:  R Packages .  Create a new R package  Create a new directory with new files, folders, and meta-information.  To update or not to update  RStudio generates the NAMESPACE content for you automatically.  See the book:  R Packages .  Import & load source files  Move existing functions from an existing project into the new package. \nThe function are moves to a new folder in the project.  Test created functions all in once with the Load All command in the \nBuild Tab.  Simulations:   Ctrl + Shift + L  Ctrl + Shift + F10 ; restart a R session.   Packages documentation  Create a help page (.Rd file). Written in HTML.  Use the  roxigen   package  to document.  Tools/Project Optons/Build Tools/Generate documentation. Generate the \ndoc, add comments.  First, create a doc skeleton above the function:   Ctrl + Alt + Shift + R     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 #' Title\n#'\n#' @param x \n#'\n#' @return\n#' @export\n#'\n#' @examples\ncEnter <- function(x) {\n  x - mean(x)\n}   Fill in the blanks.  Package documentation (2)  Title of the help page. Text on how to use.  The 4 tags ( @ ) help organizing the doc:  @export  (tells R that this \nfunction should be made available to people who load your package. ),  @params ,  @return , and  @examples . There are many more advanced \ntags.  Highlight a section and test it, run it.  Package documentation (3)  Build Tab/build the package.   Ctrl + Shift + D   Compile all. Load all. Open the help page with  ?function .  Learn the  roxigen  workflow to ease the work.  Test your package  With the  testthat  (and  devtools ) package.  Install both packages. Run  devtools::use_testthat()  and a now \ndirectory with subdir appears in the package. This is where we save \ntests.  Test your package (2)  Ready. Write tests. Open a new R script and save it to the tests \ndirectory. Create a  context  function. Add  test_that  functions with \narguments.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 context(\"cEnter\")\n\ntest_that(\"cEnter handles integers\", {\n  expect_equal(cEnter(1:3), -1:1)\n  expect_equal(cEnter(-(1:3)), 1:-1)\n})\n\ncontext(\"scale\")\n\ntest_that(\"scale handles integers\", {\n  expect_equal(scale(1:3), 1:3)\n  expect_equal(scale(-(1:3)), -(1:3))\n})\n\ncontext(\"standardize\")\n\ntest_that(\"standardize handles integers\", {\n  expect_equal(standardize(1:3), -1:1)\n  expect_equal(standardize(-(1:3)), 1:-1)\n})   Test your package (3)  Run the tests.   Ctrl + Shift + T   Get a summary (pass or not pass). Test and retest the package.  See the book:  R Packages .  Time to test your package  Run the test with Build Tab/Test package.  Check your package  Upload the package to GitHub. Recreate the package structure in the \nrepo. Download and install the package with  install_github()  from the  devtools  package. Test the package on GitHub to complete the tests. \nType  R CMD check  in the terminal. Or Build Tab/Check icon.   Ctrl + Shift + E   Build your package  A package is a tarball or package bundle.  Build Tab/Build & Reload. Install and load the package.   Ctrl + Shift + B   Build and reload will overwrite the existing package. Run  devtools::dev_mode()  creates a separate library for development. \nRunning the command again cancels it.  Two formats: source package or binary package (more compresses and \noptimized).  See the book:  R Packages .  Wrap-up  Get the  cheat sheet",
            "title": "Packages"
        },
        {
            "location": "/Working_with_RStudio_IDE/#version-control",
            "text": "Introduction to Git  Use Git to work in team. Even on R scripts, reports and packages.  Install Git.  In RStudio, Tools/Global Options/Git/SVN + Tools/Project Options/Select \nVCS, restart RStudio. RStudio has now a Git pane and a Git icon.  Stage & commit  The Git Tab is a directory. The real life (local) version of the \nproject. The official version of the project as recorded with Git.  The two versions are different. View the differences. When you commit, \nyou add thing from the real to the official version.  Green highlighting indicates something you\u2019ve added to the official \nversion, while red highlighting indicates lines you have removed.  .gitignore  Inside the project, ther is a .gignore file. Add file that are excluded \nfrom the offcial version. The file can be accessed from the Git pane.  Git icons  Add, (cancel), commit, (cancel). The icons will changed.  Commit history  History viewer: each commitment. HEAD commit and parent commit. Master \nbranch or another.  Undo commited changes: checkout  Checkout command. Go back to a previous commit. The commit stays in the \nhistory (not deleted).  Run it in the shell: Tools/shell.  git checkout sha# nameofthefile  and Git will reverse the commit. The \nfile is in the stage area as before the commit.  Undo uncommited changes  The file is in the stage area, ready to commit. Cancel the addition.  In the Change window, click on the Revert button.  Or, click on Discard chunk.  Or,  Ctrl + Z  to undo.  Then, Save the file.  Introduction to GitHub  Centralize, host, track issues, track metrics.  Install a R package From GitHub with  install_github  from the the  devtools  packages.  Pull & Push  Local and GitHub  Wrap-up  Go to help.github.com  Go to stackoverflow.com",
            "title": "Version Control"
        },
        {
            "location": "/Working_with_RStudio_IDE/#reporting",
            "text": "Tools for reporting  R Markdown and Shiny (over the web).  Introduction to R Markdown  Text and code.  Web link:  <http://www>  R Markdown in RStudio  Report (HTML, PDF, Word) or presentation(slide).  Create a template. Load in new templates. The  rticles  package has \ntemplates for academic journals.  The outline (name the chunks).  Help/Markdown Quick Reference  Add a code chunk wih  Ctrl + Alt + I  Rendering R Markdown  Knit or preview. For pdf, need  \\LaTeX \\LaTeX  (install).  Adding  runtime :   shiny  generates a Shiny report. Launch the interactive \napp.  Publish reports online.  Compile notebook  File/Compile Notebook for R script. The script becomes a report (Word, \nPDF, HTML).  RStudio\u2019s  \\LaTeX \\LaTeX  editor  Install  \\LaTeX \\LaTeX .  Open a .tex file in RStudio. RStudio has limited options to edit  \\LaTeX \\LaTeX ; \nenough to write and compile.  The preview window is linked to the source window. If we click on a \ncharacter in the source window, press  Ctrl + click : the corresponding \ncharacter is highlighted in the preview window. Vice-versa.  Tools/Global options/Sweave to change the  \\LaTeX \\LaTeX  options.  Shiny  The server is online.  shiny.rstudio.com  When we create a Shiny app, we create two files: ui.R and server.R  Run App (the app) or  Ctrl + Shift + Enter  Publish Shiny apps  Need an account and the  shinyapps  package.  devtools::install_github(\"rstudio/shinyapps\")  Deploy the local app online. Unique URL. Monitor usage, view logs, \narchive or delete the app. Max 5 apps at a time for free. Paid account.",
            "title": "Reporting"
        },
        {
            "location": "/IO_snippets___Cleaning/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nDatasets\n\u00b6\n\n\n\n\nR Dataset Packages\n; by default in R.\n\n\nOther dataset can be imported with \ndata(Cars93, package = 'MASS')\n for example.\n\n\ncsv/doc Datasets\n.\n\n\nFree Datasets\n from the World Bank, Gapminder, Kaggle, Quandl, Reddit, and many more websites.\n\n\nDatasets\n to Practice Your Data Mining.\n\n\nHoughton Mifflin Data\n for linear regressions.\n\n\nRegression Datasets\n for Generalized Linear Models (linear, logistic, poisson, multinomial, survival).\n\n\nPublic Datasets on GitHub\n.\n\n\nAwesome Public Datasets\n.\n\n\n\n\n\n\nImporting Data Into R\n\u00b6\n\n\nThe packages:\n\n\n\n\nutils\n.\n\n\nreadr\n.\n\n\ndata.table\n.\n\n\nreadxl\n.\n\n\ngdata\n.\n\n\nXLConnect\n.\n\n\nhaven\n.\n\n\nforeign\n.\n\n\nDBI\n.\n\n\nhttr\n.\n\n\njsonlite\n.\n\n\n\n\nImporting Data from Flat Files\n\u00b6\n\n\nR functions, by default.\n\u00b6\n\n\n\n\nread.csv\n; \nsep = ','\n, \ndec = '.'\n.\n\n\nread.delim\n; .txt, \ndec = '.'\n.\n\n\nread.csv2\n; \nsep = ';'\n, \ndec = ','\n.\n\n\nread.delim2\n; .txt, \ndec = ','\n.\n\n\nNeeds arguments.\n\n\n\n\nread.csv\n for .csv files\n\n\n1\n2\n3\n4\n5\n6\n# List the files in your working directory\n\n\ndir\n()\n\n\n\n# Import swimming_pools.csv: pools\n\n\n# stringAsFactors = FALSE does not import strings as categorical variables\n\npools \n<-\n read.csv\n(\n'swimming_pools.csv'\n,\n stringsAsFactors \n=\n \nFALSE\n)\n\n\n\n\n\n\n\nstringsAsFactors\n\n\n1\n2\n3\n4\n5\n# Import swimming_pools.csv correctly: pools\n\npools \n<-\n read.csv\n(\n'swimming_pools.csv'\n,\n stringsAsFactor \n=\n \nFALSE\n,\n header \n=\n \nTRUE\n,\n sep \n=\n \n','\n)\n\n\n\n# Import swimming_pools.csv with factors: pools_factor\n\npools_factor \n<-\n read.csv\n(\n'swimming_pools.csv'\n,\n header \n=\n \nTRUE\n,\n sep \n=\n \n','\n)\n\n\n\n\n\n\n\nread.delim\n for .txt files\n\n\n1\n2\n3\n4\n5\n# Import hotdogs.txt: hotdogs\n\nhotdogs \n<-\n read.delim\n(\n'hotdogs.txt'\n,\n header \n=\n \nFALSE\n)\n\n\n\n# Name the columns of hotdogs appropriately\n\n\nnames\n(\nhotdogs\n)\n \n<-\n \nc\n(\n'type'\n,\n \n'calories'\n,\n \n'sodium'\n)\n\n\n\n\n\n\n\nArguments.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Load in the hotdogs data set: hotdogs\n\nhotdogs \n<-\n read.delim\n(\n'hotdogs.txt'\n,\n header \n=\n \nFALSE\n,\n sep \n=\n \n'\\t'\n,\n col.names \n=\n \nc\n(\n'type'\n,\n \n'calories'\n,\n \n'sodium'\n))\n\n\n\n# Select the hot dog with the least calories: lily\n\nlily \n<-\n hotdogs\n[\nwhich.min\n(\nhotdogs\n$\ncalories\n),\n \n]\n\n\n# Select the observation with the most sodium: tom\n\n\ntom \n<-\n hotdogs\n[\nwhich.max\n(\nhotdogs\n$\nsodium\n),\n \n]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Previous call to import hotdogs.txt\n\nhotdogs \n<-\n read.delim\n(\n'hotdogs.txt'\n,\n header \n=\n \nFALSE\n,\n col.names \n=\n \nc\n(\n'type'\n,\n \n'calories'\n,\n \n'sodium'\n))\n\n\n\n# Print a vector representing the classes of the columns\n\n\nsapply\n(\nhotdogs\n,\n \nclass\n)\n\n\n\n# Edit the colClasses argument to import the data correctly: hotdogs2\n\nhotdogs2 \n<-\n read.delim\n(\n'hotdogs.txt'\n,\n header \n=\n \nFALSE\n,\n col.names \n=\n \nc\n(\n'type'\n,\n \n'calories'\n,\n \n'sodium'\n),\n colClasses \n=\n \nc\n(\n'factor'\n,\n \n'NULL'\n,\n \n'numeric'\n))\n\n\n\n\n\n\n\nThe \nutils\n package\n\u00b6\n\n\n\n\nread.table\n; \nsep = '\\t'\n, \n= ','\n, \n= ';'\n.\n\n\nRead any tabular as a d.f.\n\n\nNeeds arguments; lots of argument for precision.\n\n\nSlow.\n\n\n\n\n\n\n\n1\nlibrary\n(\nutils\n)\n\n\n\n\n\n\n\nread.table\n .txt files\n\n\n1\n2\n3\n4\n5\n# Create a path to the hotdogs.txt file\n\npath \n<-\n \nfile.path\n(\n'hotdogs'\n,\n \n'hotdogs.txt'\n)\n\n\n\n# Import the hotdogs.txt file: hotdogs\n\nhotdogs \n<-\n read.table\n(\npath\n,\n header \n=\n \nFALSE\n,\n sep \n=\n \n'\\t'\n,\n col.names \n=\n \nc\n(\n'type'\n,\n \n'calories'\n,\n \n'sodium'\n))\n\n\n\n\n\n\n\n\n\n(from Importing Data from the Web)\n\u00b6\n\n\n1\n2\n3\n4\n5\n# https URL to the swimming_pools csv file.\n\nurl_csv \n<-\n \n'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv'\n\n\n\n# Import the file using read.csv(): pools1\n\npools1 \n<-\n read.csv\n(\nurl_csv\n)\n\n\n\n\n\n\n\n\n\nThe \nreadr\n package\n\u00b6\n\n\n\n\nread_delim\n; \ndelim = '\\t'\n, \n= ','\n.\n\n\nread_csv\n; read \n100.000, 200.000\n\n\nread_tsv\n; idem.\n\n\nread_csv2\n; read \n100,000; 200,000\n or European files..\n\n\nread_tsv2\n; idem.\n\n\nread_lines\n.\n\n\nread_file\n.\n\n\nwrite_csv\n.\n\n\nwrite_rds\n.\n\n\ntype_convert\n.\n\n\nparse_factor\n.\n\n\nparse_date\n.\n\n\nparse_number\n.\n\n\nspec_csv\n.\n\n\nspec_delim\n.\n\n\nFast, few arguments.\n\n\nDetect data type.\n\n\n\n\n\n\n\n1\nlibrary\n(\nreadr\n)\n\n\n\n\n\n\n\nread_delim\n .txt files\n\n\n1\n2\n# Import potatoes.txt using read_delim(): potatoes\n\npotatoes \n<-\n read_delim\n(\n'potatoes.txt'\n,\n delim \n=\n \n'\\t'\n)\n\n\n\n\n\n\n\nread_csv\n .csv files\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# Column names\n\nproperties \n<-\n \nc\n(\n'area'\n,\n \n'temp'\n,\n \n'size'\n,\n \n'storage'\n,\n \n'method'\n,\n \n'texture'\n,\n \n'flavor'\n,\n \n'moistness'\n)\n\n\n\n# Import potatoes.csv with read_csv(): potatoes\n\npotatoes \n<-\n read_csv\n(\n'potatoes.csv'\n,\n col_names \n=\n properties\n)\n\n\n\n# Create a copy of potatoes: potatoes2\n\npotatoes2 \n<-\n potatoes\n\n\n# Convert the method column of potatoes2 to a factor\n\npotatoes2\n$\nmethod \n=\n \nfactor\n(\npotatoes2\n$\nmethod\n)\n\n\n\n# or\n\n\npotatoes2\n$\nmethod \n=\n \nas.factor\n(\npotatoes2\n$\nmethod\n)\n\n\n\n\n\n\n\ncol_types\n, \nskip\n and \nn_max\n in .tsv files\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Column names\n\nproperties \n<-\n \nc\n(\n'area'\n,\n \n'temp'\n,\n \n'size'\n,\n \n'storage'\n,\n \n'method'\n,\n \n'texture'\n,\n \n'flavor'\n,\n \n'moistness'\n)\n\n\n\n# Import 5 observations from potatoes.txt: potatoes_fragment\n\n\n# read_tsv or tab-separated values\n\npotatoes_fragment \n<-\n read_tsv\n(\n'potatoes.txt'\n,\n col_names \n=\n properties\n,\n skip \n=\n \n7\n,\n n_max \n=\n \n5\n)\n\n\n\n# Import all data, but force all columns to be character: potatoes_char\n\npotatoes_char \n<-\n read_tsv\n(\n'potatoes.txt'\n,\n col_types \n=\n \n'cccccccc'\n)\n\n\n\n\n\n\n\nSetting column types\n\n\n1\n2\n3\n4\ncols\n(\n\n  weight \n=\n col_integer\n(),\n\n  feed \n=\n col_character\n()\n\n\n)\n\n\n\n\n\n\n\nRemoving NA\n\n\n1\nna \n=\n \nc\n(\n'NA'\n,\n \n'null'\n)\n\n\n\n\n\n\n\ncol_types\n with collectors .tsv files\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Import without col_types\n\nhotdogs \n<-\n read_tsv\n(\n'hotdogs.txt'\n,\n col_names \n=\n \nc\n(\n'type'\n,\n \n'calories'\n,\n \n'sodium'\n))\n\n\n\n# The collectors you will need to import the data\n\nfac \n<-\n col_factor\n(\nlevels \n=\n \nc\n(\n'Beef'\n,\n \n'Meat'\n,\n \n'Poultry'\n))\n\nint \n<-\n col_integer\n()\n\n\n\n# Edit the col_types argument to import the data correctly: hotdogs_factor\n\n\n# Change col_types to the correct vector of collectors; coerce the vector into a list\n\nhotdogs_factor \n<-\n read_tsv\n(\n'hotdogs.txt'\n,\n col_names \n=\n \nc\n(\n'type'\n,\n \n'calories'\n,\n \n'sodium'\n),\n col_types \n=\n \nlist\n(\nfac\n,\n int\n,\n int\n))\n\n\n\n\n\n\n\nSkiping columns\n\n\n1\n2\n3\n4\n5\nsalaries \n<-\n read_tsv\n(\n'Salaries.txt'\n,\n col_names \n=\n \nFALSE\n,\n col_types \n=\n cols\n(\n\n  X2 \n=\n col_skip\n(),\n\n  X3 \n=\n col_skip\n(),\n \n  X4 \n=\n col_skip\n()\n\n\n))\n\n\n\n\n\n\n\nReading an ordinary text file\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# vector of character strings. \n\n\n# Import as a character vector, one item per line: tweets\n\ntweets \n<-\n read_lines\n(\n'tweets.txt'\n)\n\ntweets\n\n\n# returns a length 1 vector of the entire file, with line breaks represented as \\n\n\n\n# Import as a length 1 vector: tweets_all\n\ntweets_all \n<-\n read_file\n(\n'tweets.txt'\n)\n\ntweets_all\n\n\n\n\n\n\nWriting .csv and .tsv files\n\n\n1\n2\n3\n4\n5\n# Save cwts as chickwts.csv\n\nwrite_csv\n(\ncwts\n,\n \n\"chickwts.csv\"\n)\n\n\n\n# Append cwts2 to chickwts.csv\n\nwrite_csv\n(\ncwts2\n,\n \n\"chickwts.csv\"\n,\n append \n=\n \nTRUE\n)\n\n\n\n\n\n\n\nWriting .rds files\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Save trees as trees.rds\n\nwrite_rds\n(\ntrees\n,\n \n'trees.rds'\n)\n\n\n\n# Import trees.rds: trees2\n\ntrees2 \n<-\n read_rds\n(\n'trees.rds'\n)\n\n\n\n# Check whether trees and trees2 are the same\n\n\nidentical\n(\ntrees\n,\n trees2\n)\n\n\n\n\n\n\n\nCoercing columns to different data types\n\n\n1\n2\n# Convert all columns to double\n\ntrees2 \n<-\n type_convert\n(\ntrees\n,\n col_types \n=\n cols\n(\nGirth \n=\n \n'd'\n,\n Height \n=\n \n'd'\n,\n Volume \n=\n \n'd'\n))\n\n\n\n\n\n\n\nCoercing character columns into factors\n\n\n1\n2\n3\n4\n5\n# Parse the title column\n\nsalaries\n$\ntitle \n<-\n parse_factor\n(\nsalaries\n$\ntitle\n,\n levels \n=\n \nc\n(\n'Prof'\n,\n \n'AsstProf'\n,\n \n'AssocProf'\n))\n\n\n\n# Parse the gender column\n\nsalaries\n$\ngender \n<-\n parse_factor\n(\nsalaries\n$\ngender\n,\n levels \n=\n \nc\n(\n'Male'\n,\n \n'Female'\n))\n\n\n\n\n\n\n\nCreating Date objects\n\n\n1\n2\n# Change type of date column\n\nweather\n$\ndate \n<-\n parse_date\n(\nweather\n$\ndate\n,\n format \n=\n \n'%m/%d/%Y'\n)\n\n\n\n\n\n\n\nParsing number formats\n\n\n1\n2\n# Parse amount column as a number\n\ndebt\n$\namount \n<-\n parse_number\n(\ndebt\n$\namount\n)\n\n\n\n\n\n\n\nViewing metadata before importing\n\n\n\n\nspec_csv\n for .csv and .tsv files.\n\n\nspec_delim\n for .txt files (among others).\n\n\n\n\n\n\n\n1\n2\n# Specifications of chickwts\n\nspec_csv\n(\n'chickwts.csv'\n)\n\n\n\n\n\n\n\n\n\n(from Importing Data from the Web)\n\u00b6\n\n\nImport Flat files from the web\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Import the csv file: pools\n\nurl_csv \n<-\n \n'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv'\n\npools \n<-\n read_csv\n(\nurl_csv\n)\n\n\npools\n\n\n# Import the txt file: potatoes\n\nurl_delim \n<-\n \n'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/potatoes.txt'\n\npotatoes \n<-\n read_tsv\n(\nurl_delim\n)\n\n\npotatoes\n\n\n\n\n\n\nSecure importing\n\n\n1\n2\n# Import the file using read_csv(): pools2\n\npools2 \n<-\n read_csv\n(\nurl_csv\n)\n\n\n\n\n\n\n\n\n\nThe \ndata.table\n package\n\u00b6\n\n\n\n\nfread\n == \nread.table\n.\n\n\n.txt files only.\n\n\nFast.\n\n\n\n\n\n\n\n1\nlibrary\n(\ndata.table\n)\n\n\n\n\n\n\n\nfread\n for .txt files\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Import potatoes.txt with fread(): potatoes\n\npotatoes \n<-\n fread\n(\n'potatoes.txt'\n)\n\n\n\n# Print out arranged version of potatoes\n\npotatoes\n[\norder\n(\nmoistness\n),]\n \n\n\n# Import 20 rows of potatoes.txt with fread(): potatoes_part\n\npotatoes_part \n<-\n fread\n(\n'potatoes.txt'\n,\n nrows \n=\n \n20\n)\n\n\n\n\n\n\n\nfread\n: more advanced use\n\n\n1\n2\n3\n4\n5\n# Import columns 6, 7 and 8 of potatoes.txt: potatoes\n\npotatoes \n<-\n fread\n(\n'potatoes.txt'\n,\n select \n=\n \nc\n(\n6\n:\n8\n))\n\n\n\n# Keep only tasty potatoes (flavor > 3): tasty_potatoes\n\ntasty_potatoes \n<-\n \nsubset\n(\npotatoes\n,\n potatoes\n$\nflavor \n>\n \n3\n)\n\n\n\n\n\n\n\nImporting Data from Excel\n\u00b6\n\n\nThe \nreadxl\n package\n\u00b6\n\n\n\n\nexcel_sheets\n; list.\n\n\nread_excel\n; import.\n\n\n.xlsx files only.\n\n\n\n\n\n\n\n1\nlibrary\n(\nreadxl\n)\n\n\n\n\n\n\n\nList the sheets of an Excel file\n\n\n1\n2\n3\n4\n5\n# Find the names of both spreadsheets: sheets\n\n\n# Before, find out what is in the directory with 'dir()'\n\nsheets \n<-\n excel_sheets\n(\n'latitude.xlsx'\n)\n\n\nsheets\n\n\n\n\n\n\nImporting an Excel sheet\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Read the first sheet of latitude.xlsx: latitude_1\n\nlatitude_1 \n<-\n read_excel\n(\n'latitude.xlsx'\n,\n sheet \n=\n \n1\n)\n\n\n\n# Read the second sheet of latitude.xlsx: latitude_2\n\nlatitude_2 \n<-\n read_excel\n(\n'latitude.xlsx'\n,\n sheet \n=\n \n2\n)\n\n\n\n# Put latitude_1 and latitude_2 in a list: lat_list\n\nlat_list \n<-\n \nlist\n(\nlatitude_1\n,\n latitude_2\n)\n\n\n\n\n\n\n\nReading a workbook\n\n\n1\n2\n# Read all Excel sheets with lapply(): lat_list\n\nlat_list \n<-\n \nlapply\n(\nexcel_sheets\n(\n'latitude.xlsx'\n),\n read_excel\n,\n path \n=\n \n'latitude.xlsx'\n)\n\n\n\n\n\n\n\nThe \ncol_names\n argument\n\n\n1\n2\n3\n4\n5\n# Import the the first Excel sheet of latitude_nonames.xlsx (R gives names): latitude_3\n\nlatitude_3 \n<-\n read_excel\n(\n'latitude_nonames.xlsx'\n,\n sheet \n=\n \n1\n,\n col_names \n=\n \nFALSE\n)\n\n\n\n# Import the the second Excel sheet of latitude_nonames.xlsx (specify col_names): latitude_4 \n\nlatitude_4 \n<-\n read_excel\n(\n'latitude_nonames.xlsx'\n,\n sheet \n=\n \n1\n,\n col_names \n=\n \nc\n(\n'country'\n,\n \n'latitude'\n))\n\n\n\n\n\n\n\nThe \nskip\n argument\n\n\n1\n2\n# Import the second sheet of latitude.xlsx, skipping the first 21 rows: latitude_sel\n\nlatitude_sel \n<-\n read_excel\n(\n'latitude.xlsx'\n,\n skip \n=\n \n21\n,\n col_names \n=\n \nFALSE\n)\n\n\n\n\n\n\n\n\n\n(from Importing Data from the Web)\n\u00b6\n\n\nImport Excel files from the web\n\n\n1\n2\n3\n4\n5\n# Download file behind URL, name it local_latitude.xls\n\ndownload.file\n(\nurl_xls\n,\n \n'local_latitude.xls'\n)\n\n\n\n# Import the local .xls file with readxl: excel_readxl\n\nexcel_readxl \n<-\n read_excel\n(\n'local_latitude.xls'\n)\n\n\n\n\n\n\n\nDownloading any file, secure or not\n\n\n1\n2\n3\n4\n5\n# https URL to the wine RData file.\n\nurl_rdata \n<-\n \n'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/wine.RData'\n\n\n\n# Download the wine file to your working directory\n\ndownload.file\n(\nurl_rdata\n,\n \n'wine_local.RData'\n)\n\n\n\n\n\n\n\n\n\nThe \nXLConnect\n package\n\u00b6\n\n\n\n\nloadWorkbook\n.\n\n\ngetSheets\n.\n\n\nreadWorksheet\n.\n\n\nreadWorksheetFromFile\n\n\nreadNamedRegion\n\n\n\n\nreadNamedRegionFromFile\n\n\n\n\n\n\n.xls & .xlsx files.\n\n\n\n\nLike reading a database.\n\n\n\n\n\n\n\n1\n2\nlibrary\n(\nXLConnectJars\n)\n\n\nlibrary\n(\nXLConnect\n)\n\n\n\n\n\n\n\nImport a workbook\n\n\n1\n2\n# Build connection to latitude.xlsx: my_book\n\nmy_book \n<-\n loadWorkbook\n(\n'latitude.xlsx'\n)\n\n\n\n\n\n\n\nList and read Excel sheets\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Build connection to latitude.xlsx\n\nmy_book \n<-\n loadWorkbook\n(\n'latitude.xlsx'\n)\n\n\n\n# List the sheets in latitude.xlsx\n\ngetSheets\n(\nmy_book\n)\n\n\n\n# Import the second sheet in latitude.xlsx\n\nreadWorksheet\n(\nmy_book\n,\n sheet \n=\n \n2\n)\n\n\n\n# Import the second column of the first sheet in latitude.xlsx\n\nreadWorksheet\n(\nmy_book\n,\n sheet \n=\n \n2\n,\n startCol \n=\n \n2\n)\n\n\n\n\n\n\n\nAdd and populate worksheets\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# Build connection to latitude.xlsx\n\nmy_book \n<-\n loadWorkbook\n(\n'latitude.xlsx'\n)\n\n\n\n# Create data frame: summ\n\ndims1 \n<-\n \ndim\n(\nreadWorksheet\n(\nmy_book\n,\n \n1\n))\n\ndims2 \n<-\n \ndim\n(\nreadWorksheet\n(\nmy_book\n,\n \n2\n))\n\nsumm \n<-\n \ndata.frame\n(\nsheets \n=\n getSheets\n(\nmy_book\n),\n \n                   nrows \n=\n \nc\n(\ndims1\n[\n1\n],\n dims2\n[\n1\n]),\n \n                   ncols \n=\n \nc\n(\ndims1\n[\n2\n],\n dims2\n[\n2\n]))\n\n\n\n# Add a worksheet to my_book, named 'data_summary'\n\ncreateSheet\n(\nmy_book\n,\n name \n=\n \n'data_summary'\n)\n\n\n\n# Populate 'data_summary' with summ data frame\n\nwriteWorksheet\n(\nmy_book\n,\n summ\n,\n sheet \n=\n \n'data_summary'\n)\n\n\n# Save workbook as latitude_with_summ.xlsx\n\n\nsaveWorkbook\n(\nmy_book\n,\n \n'latitude_with_summ.xlsx'\n)\n\n\n\n\n\n\n\nOne unique function\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n# Read in the data set and assign to the object\n\nimpact \n<-\n readWorksheetFromFile\n(\n'A Hands-on Introduction to Statistics with R.xls'\n,\n sheet \n=\n \n'impact'\n,\n header \n=\n \nTRUE\n,\n startCol \n=\n \n1\n,\n startRow \n=\n \n1\n)\n\n\n\n# more arguments\n\n\n# endCol = 1\n\n\n# endRow = 1\n\n\n# autofitRow = \n\n\n# autofitCol = \n\n\n# region =\n\n\n# rownames =\n\n\n# colTypes =\n\n\n# forceConversion =\n\n\n# dateTimeFormat =\n\n\n# check.names =\n\n\n# useCachedValues =\n\n\n# keep =\n\n\n# drop =\n\n\n# simplify =\n\n\n# readStrategy =\n\n\n\n\n\n\n\nImporting Data from Other Statistical Software\n\u00b6\n\n\nThe \nhaven\n package\n\u00b6\n\n\n\n\nread_sas\n; sas7bdat & sas7bcat files.\n\n\nread_stata\n; version; dta files.\n\n\nread_dta\n; idem.\n\n\nread_spss\n; sav & por files (and see below).\n\n\nread_por\n.\n\n\nread_sav\n.\n\n\nSimple, few arguments.\n\n\nCreate a d.f.\n\n\n\n\n\n\n\n1\nlibrary\n(\nhaven\n)\n\n\n\n\n\n\n\nImport SAS data with haven\n\n\n1\n2\n# Import sales.sas7bdat: sales\n\nsales \n<-\n read_sas\n(\n'sales.sas7bdat'\n)\n\n\n\n\n\n\n\nImport STATA data with haven\n\n\n1\n2\n# Import the data from the URL: sugar\n\nsugar \n<-\n read_dta\n(\n'http://assets.datacamp.com/course/importing_data_into_r/trade.dta'\n)\n\n\n\n\n\n\n\nImport SPSS data with haven\n\n\n1\n2\n3\n4\n5\n# Specify the file path using file.path(): path\n\npath \n<-\n \nfile.path\n(\n'datasets'\n,\n \n'person.sav'\n)\n\n\n\n# Import person.sav, which is in the datasets folder: traits\n\ntraits \n<-\n read_sav\n(\npath\n)\n\n\n\n\n\n\n\nFactorize, round two\n\n\n1\n2\n# Import SPSS data from the URL: work\n\nwork \n<-\n read_sav\n(\n'http://assets.datacamp.com/course/importing_data_into_r/employee.sav'\n)\n\n\n\n\n\n\n\nThe\nforeign\n package\n\u00b6\n\n\n\n\nCannot import SAS, see the \nsas7bdat\n package.\n\n\nread.dta\n; dta files.\n\n\nread.spss\n; sav & por files.\n\n\nComprehensive.\n\n\n\n\n\n\n\n1\nlibrary\n(\nforeign\n)\n\n\n\n\n\n\n\nImport STATA data with foreign (1)\n\n\n1\n2\n# Import florida.dta and name the resulting data frame florida\n\nflorida \n<-\n read.dta\n(\n'florida.dta'\n)\n\n\n\n\n\n\n\nImport STATA data with foreign (2)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Specify the file path using file.path(): path\n\npath \n<-\n \nfile.path\n(\n'worldbank'\n,\n \n'edequality.dta'\n)\n\n\n\n# Create and print structure of edu_equal_1\n\nedu_equal_1 \n<-\n read.dta\n(\npath\n)\n\n\n\n# Create and print structure of edu_equal_2\n\nedu_equal_2 \n<-\n read.dta\n(\npath\n,\n convert.factors \n=\n \nFALSE\n)\n\n\n\n# Create and print structure of edu_equal_3\n\nedu_equal_3 \n<-\n read.dta\n(\npath\n,\n convert.underscore \n=\n \nTRUE\n)\n \n\n\n\n\n\n\nImport SPSS data with foreign (1)\n\n\n1\n2\n# Import international.sav as a data frame: demo\n\ndemo \n<-\n read.spss\n(\n'international.sav'\n,\n to.data.frame \n=\n \nTRUE\n)\n\n\n\n\n\n\n\nImport SPSS data with foreign (2)\n\n\n1\n2\n3\n4\n5\n# Import international.sav as demo_1\n\ndemo_1 \n<-\n read.spss\n(\n'international.sav'\n,\n to.data.frame \n=\n \nTRUE\n)\n\n\n\n# Import international.sav as demo_2\n\ndemo_2 \n<-\n read.spss\n(\n'international.sav'\n,\n to.data.frame \n=\n \nTRUE\n,\n use.value.labels \n=\n \nFALSE\n)\n\n\n\n\n\n\n\nImporting Data from Relational Data\n\u00b6\n\n\nThe \nDBI\n package\n\u00b6\n\n\n\n\ndbConnect\n.\n\n\ndbReadTable\n.\n\n\ndbGetQuery\n.\n\n\ndbFetch\n.\n\n\ndbDisconnect\n.\n\n\n\n\n\n\n\n1\nlibrary\n(\nDBI\n)\n\n\n\n\n\n\n\nStep 1: Establish a connection\n\n\n1\n2\n3\n4\n# Connect to the MySQL database: con\n\ncon \n<-\n dbConnect\n(\nRMySQL\n::\nMySQL\n(),\n dbname \n=\n \n'tweater'\n,\n host \n=\n \n'courses.csrrinzqubik.us-east-1.rds.amazonaws.com'\n,\n port \n=\n \n3306\n,\n user \n=\n \n'student'\n,\n password \n=\n \n'datacamp'\n)\n \n\ncon\n\n\n\n\n\n\nStep 2: List the database tables\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n# Connect to the MySQL database: con\n\ncon \n<-\n dbConnect\n(\nRMySQL\n::\nMySQL\n(),\n \n                 dbname \n=\n \n'tweater'\n,\n \n                 host \n=\n \n'courses.csrrinzqubik.us-east-1.rds.amazonaws.com'\n,\n \n                 port \n=\n \n3306\n,\n\n                 user \n=\n \n'student'\n,\n\n                 password \n=\n \n'datacamp'\n)\n\n\n\n# Build a vector of table names: tables\n\ntables \n<-\n dbListTables\n(\ncon\n)\n\n\n\n# Display structure of tables\n\nstr\n(\ntables\n)\n\n\n\n\n\n\n\nStep 3: Import data from a table\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n# Connect to the MySQL database: con\n\ncon \n<-\n dbConnect\n(\nRMySQL\n::\nMySQL\n(),\n \n                 dbname \n=\n \n'tweater'\n,\n \n                 host \n=\n \n'courses.csrrinzqubik.us-east-1.rds.amazonaws.com'\n,\n \n                 port \n=\n \n3306\n,\n\n                 user \n=\n \n'student'\n,\n\n                 password \n=\n \n'datacamp'\n)\n\n\n\n# Import the users table from tweater: users\n\nusers \n<-\n dbReadTable\n(\ncon\n,\n \n'users'\n)\n\n\nusers\n\n\n# Import and print the tweats table from tweater: tweats\n\ntweats \n<-\n dbReadTable\n(\ncon\n,\n \n'tweats'\n)\n\n\ntweats\n\n\n# Import and print the comments table from tweater: comments\n\ncomments \n<-\n dbReadTable\n(\ncon\n,\n \n'comments'\n)\n\n\ncomments\n\n\n\n\n\n\nYour very first SQL query\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\ncon \n<-\n dbConnect\n(\nRMySQL\n::\nMySQL\n(),\n \n                 dbname \n=\n \n'tweater'\n,\n \n                 host \n=\n \n'courses.csrrinzqubik.us-east-1.rds.amazonaws.com'\n,\n \n                 port \n=\n \n3306\n,\n\n                 user \n=\n \n'student'\n,\n\n                 password \n=\n \n'datacamp'\n)\n\n\n\n# Import post column of tweats where date is higher than '2015-09-21': latest\n\nlatest \n<-\n dbGetQuery\n(\ncon\n,\n \n'SELECT post FROM tweats WHERE date > \\'2015-09-21\\''\n)\n\n\nlatest\n\n\n# Import tweat_id column of comments where user_id is 1: elisabeth\n\nelisabeth \n<-\n dbGetQuery\n(\ncon\n,\n \n'SELECT tweat_id FROM comments WHERE user_id = 1'\n)\n\n\nelisabeth\n\n\n\n\n\n\nMore advanced SQL queries\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n# Connect to the database\n\ncon \n<-\n dbConnect\n(\nRMySQL\n::\nMySQL\n(),\n\n                 dbname \n=\n \n'tweater'\n,\n \n                 host \n=\n \n'courses.csrrinzqubik.us-east-1.rds.amazonaws.com'\n,\n \n                 port \n=\n \n3306\n,\n\n                 user \n=\n \n'student'\n,\n\n                 password \n=\n \n'datacamp'\n)\n\n\n\n# Create data frame specific\n\nspecific \n<-\n dbGetQuery\n(\ncon\n,\n \n'SELECT message FROM comments WHERE tweat_id = 77 AND user_id > 4'\n)\n\n\nspecific\n\n\n# Create data frame short\n\nshort \n<-\n dbGetQuery\n(\ncon\n,\n \n'SELECT id, name FROM users WHERE CHAR_LENGTH(name) < 5'\n)\n\n\nshort\n\n\n\n\n\n\nSend - Fetch - Clear\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n# Connect to the database\n\ncon \n<-\n dbConnect\n(\nRMySQL\n::\nMySQL\n(),\n \n                 dbname \n=\n \n'tweater'\n,\n \n                 host \n=\n \n'courses.csrrinzqubik.us-east-1.rds.amazonaws.com'\n,\n \n                 port \n=\n \n3306\n,\n\n                 user \n=\n \n'student'\n,\n\n                 password \n=\n \n'datacamp'\n)\n\n\n\n# Send query to the database with dbSendQuery(): res\n\nres \n<-\n dbSendQuery\n(\ncon\n,\n \n'SELECT * FROM comments WHERE user_id > 4'\n)\n\n\n\n# Display information contained in res\n\ndbGetInfo\n(\nres\n)\n\n\n\n# Use dbFetch() twice\n\n\nwhile\n \n(\n!\ndbHasCompleted\n(\nres\n))\n \n{\n\n    chunk \n<-\n dbFetch\n(\nres\n,\n n \n=\n \n2\n)\n\n    chunk2 \n<-\n dbFetch\n(\nres\n)\n\n    \nprint\n(\nchunk\n)\n\n\n}\n\n\n\n# Clear res\n\ndbClearResult\n(\nres\n)\n\n\n\n\n\n\n\nBe polite and \u2026\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# Database specifics\n\ndbname \n<-\n \n'tweater'\n\nhost \n<-\n \n'courses.csrrinzqubik.us-east-1.rds.amazonaws.com'\n\nport \n<-\n \n3306\n\nuser \n<-\n \n'student'\n\npassword \n<-\n \n'datacamp'\n\n\n\n# Connect to the database\n\ncon \n<-\n dbConnect\n(\nRMySQL\n::\nMySQL\n(),\n dbname \n=\n \n'tweater'\n,\n host \n=\n \n'courses.csrrinzqubik.us-east-1.rds.amazonaws.com'\n,\n port \n=\n \n3306\n \n,\n user \n=\n \n'student'\n,\n password \n=\n \n'datacamp'\n)\n\n\n\n# Create the data frame  long_tweats\n\nlong_tweats \n<-\n dbGetQuery\n(\ncon\n,\n \n'SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) > 40'\n)\n\n\n\n# Print long_tweats\n\n\nprint\n(\nlong_tweats\n)\n\n\n\n# Disconnect from the database\n\ndbDisconnect\n(\ncon\n)\n\n\n\n\n\n\n\nOther general packages\n\n\nThe \nRODBC\n package provides access to databases (including Microsoft\n\nAccess and Microsoft SQL Server) through an ODBC interface.\n\n\nThe \nRJDBC\n package provides access to databases through a JDBC\n\ninterface.\n\n\nSpecialized packages\n\n\n\n\nROracle\n provides an interface for Oracle.\n\n\nRMySQL\n provides access to MySQL.\n\n\nRpostgreSQL\n to PostgreSQL.\n\n\nRSQLite\n to SQLite.\n\n\nAnd there are manu more packages for NoSQL databases such\n\n    as MongoDB.\n\n\n\n\nImporting Data from Relational Data \u2013 More\n\u00b6\n\n\nDBI\n\u00b6\n\n\nFirst, change the working directory with \nsetwd\n. Install the \nDBI\n\nlibrary.\n\n\nConnect and read preliminary results\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nlibrary\n(\nDBI\n)\n\n\nlibrary\n(\nsqliter\n)\n\n\n\n# Assign the sqlite database and full path to a variable\n\ndbfile \n=\n \n'chinook.db'\n\n\n\n# Instantiate the dbDriver to a convenient object\n\nsqlite \n=\n dbDriver\n(\n'SQLite'\n)\n\n\n\n# Assign the connection string to a connection object\n\nsqlitedb \n<-\n dbConnect\n(\nRSQLite\n::\nSQLite\n(),\n \n                 dbname \n=\n dbfile\n,\n \n                 host \n=\n \n''\n,\n \n                 port \n=\n \n3306\n,\n\n                 user \n=\n \n''\n,\n\n                 password \n=\n \n''\n)\n\n\n\n# Request a list of tables using the connection object\n\ndbListTables\n(\nsqlitedb\n)\n\n\n\n\n\n\n\nExtract some data\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Assign the results of a SQL query to an object\n\nresults \n=\n dbSendQuery\n(\nsqlitedb\n,\n \n\"SELECT * FROM albums\"\n)\n\n\n\n# Return results from a custom object to a data.frame\n\ndata \n=\n fetch\n(\nresults\n)\n\n\n\n# Print data frame to console\n\n\nhead\n(\ndata\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Clear the results and close the connection\n\ndbClearResult\n(\nresults\n)\n\n\n\n# Disconnect from the database\n\ndbDisconnect\n(\nsqlitedb\n)\n\n\n\n\n\n\n\nRSQLite\n\u00b6\n\n\nFirst, change the working directory with \nsetwd\n. Install the \nRSQLite\n\nlibrary.\n\n\nConnect and read preliminary results\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nlibrary\n(\nRSQLite\n)\n\n\nlibrary\n(\nsqliter\n)\n\n\n\n# Assign the sqlite database and full path to a variable\n\ndbfile \n=\n \n'chinook.db'\n\n\n\n# Instantiate the dbDriver to a convenient object\n\nsqlite \n=\n dbDriver\n(\n'SQLite'\n)\n\n\n\n# Assign the connection string to a connection object\n\nmysqldb \n=\n dbConnect\n(\nsqlite\n,\n dbfile\n)\n\n\n\n# Request a list of tables using the connection object\n\ndbListTables\n(\nsqlitedb\n)\n\n\n\n\n\n\n\nExtract some data\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# Assign the results of a SQL query to an object\n\nresults \n=\n dbSendQuery\n(\nsqlitedb\n,\n \n\"SELECT * FROM albums\"\n)\n\n\n\n# Check the object\n\nresults\ndbGetInfo\n(\nresults\n)\n\n\n\n# Return results from a custom object to a data.frame\n\ndata \n=\n fetch\n(\nresults\n)\n\n\n\n# Print data frame to console\n\n\nhead\n(\ndata\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Clear the results and close the connection\n\ndbClearResult\n(\nresults\n)\n\n\n\n# Disconnect from the database\n\ndbDisconnect\n(\nsqlitedb\n)\n\n\n\n\n\n\n\nMySQL with \nDBI\n or \nRMySQL\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nlibrary\n(\nDBI\n)\n\n\n\n# Assign the sqlite database and full path to a variable\n\ndbfile \n=\n \n'chinook.db'\n\n\n\n# Instantiate the dbDriver to a convenient object\n\nmysql \n=\n dbDriver\n(\n'MySQL'\n)\n\n\n\n# Assign the connection string to a connection object\n\nmysqldb \n<-\n dbConnect\n(\nRMySQL\n::\nMySQL\n(),\n \n                 dbname \n=\n dbfile\n,\n \n                 host \n=\n \n''\n,\n \n                 port \n=\n \n3306\n,\n\n                 user \n=\n \n''\n,\n\n                 password \n=\n \n''\n)\n\n\n\n# Request a list of tables using the connection object\n\ndbListTables\n(\nmysqldb\n)\n\n\n\n# Request a list of tables using the connection object\n\ndbListTables\n(\nmysqldb\n)\n\n\n\n# Disconnect from the database\n\ndbDisconnect\n(\nmysqldb\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nlibrary\n(\nRMySQL\n)\n\n\n\n# Assign the sqlite database and full path to a variable\n\ndbfile \n=\n \n'chinook.db'\n\n\n\n# Instantiate the dbDriver to a convenient object\n\nmysql \n=\n dbDriver\n(\n'MySQL'\n)\n\n\n\n# Assign the connection string to a connection object\n\nmysqldb \n=\n dbConnect\n(\nmysql\n,\n dbfile\n)\n\n\n\n# Request a list of tables using the connection object\n\ndbListTables\n(\nmysqldb\n)\n\n\n\n# Request a list of tables using the connection object\n\ndbListTables\n(\nmysqldb\n)\n\n\n\n# Disconnect from the database\n\ndbDisconnect\n(\nmysqldb\n)\n\n\n\n\n\n\n\nPosgreSQL with \nDBI\n or \nRPostgreSQL\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nlibrary\n(\nDBI\n)\n\n\n\n# Assign the sqlite database and full path to a variable\n\ndbfile \n=\n \n'chinook.db'\n\n\n\n# Instantiate the dbDriver to a convenient object\n\npostgresql \n=\n dbDriver\n(\n'PostgreSQL'\n)\n\n\n\n# Assign the connection string to a connection object\n\npostgresqldb \n<-\n dbConnect\n(\nRPostgreSQL\n::\nPostgreSQL\n(),\n \n                 dbname \n=\n dbfile\n,\n \n                 host \n=\n \n''\n,\n \n                 port \n=\n \n3306\n,\n\n                 user \n=\n \n''\n,\n\n                 password \n=\n \n''\n)\n\n\n\n# Request a list of tables using the connection object\n\ndbListTables\n(\npostgresqldb\n)\n\n\n\n# Request a list of tables using the connection object\n\ndbListTables\n(\npostgresqldb\n)\n\n\n\n# Disconnect from the database\n\ndbDisconnect\n(\npostgresqldb\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nlibrary\n(\nRPostgreSQL\n)\n\n\n\n# Assign the sqlite database and full path to a variable\n\ndbfile \n=\n \n'chinook.db'\n\n\n\n# Instantiate the dbDriver to a convenient object\n\npostgresql \n=\n dbDriver\n(\n'PostgreSQL'\n)\n\n\n\n# Assign the connection string to a connection object\n\npostgresqldb \n=\n dbConnect\n(\npostgresql\n,\n dbfile\n)\n\n\n\n# Request a list of tables using the connection object\n\ndbListTables\n(\npostgresqldb\n)\n\n\n\n# Request a list of tables using the connection object\n\ndbListTables\n(\npostgresqldb\n)\n\n\n\n# Disconnect from the database\n\ndbDisconnect\n(\npostgresqldb\n)\n\n\n\n\n\n\n\nImporting Data from the Web\n\u00b6\n\n\nThe other package above can download files from the web. The next\n\npackages are web-oriented.\n\n\nThe \nhttr\n package\n\u00b6\n\n\n\n\nGET\n pages and files from the web.\n\n\nConcise.\n\n\nParse JSON files.\n\n\nCommunicate with APIs.\n\n\n\n\n\n\n\n1\nlibrary\n(\nhttr\n)\n\n\n\n\n\n\n\nHTTP? \nhttr\n!\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Get the url, save response to resp\n\nurl \n<-\n \n'http://docs.datacamp.com/teach/'\n\nresp \n<-\n GET\n(\nurl\n)\n\n\nresp\n\n\n# Get the raw content of resp\n\nraw_content \n<-\n content\n(\nresp\n,\n as \n=\n \n'raw'\n)\n\n\n\n# Print the head of content\n\n\nhead\n(\nraw_content\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n# Get the url\n\nurl \n<-\n \n'https://www.omdbapi.com/?t=Annie+Hall&y=&plot=short&r=json'\n\n\nresp \n<-\n GET\n(\nurl\n)\n\n\n\n# Print resp\n\nresp\n\n\n# Print content of resp as text\n\ncontent\n(\nresp\n,\n as \n=\n \n'text'\n)\n\n\n\n# Print content of resp\n\ncontent\n(\nresp\n)\n\n\n\n\n\n\n\nThe \njsonlite\n package\n\u00b6\n\n\n\n\nRobust.\n\n\nImprove the imported data.\n\n\nfromJSON\n.\n\n\nfrom an R object to \ntoJSON\n\n\nprettify\n.\n\n\nminify\n.\n\n\n\n\n\n\n\n1\nlibrary\n(\njsonlite\n)\n\n\n\n\n\n\n\nFrom \nJSON\n to R\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Convert wine_json to a list: wine\n\nwine_json \n<-\n \n'{'\nname\n':'\nChateau Migraine\n', '\nyear\n':1997, '\nalcohol_pct\n':12.4, '\ncolor\n':'\nred\n', '\nawarded\n':false}'\n\nwine \n<-\n fromJSON\n(\nwine_json\n)\n\n\nstr\n(\nwine\n)\n\n\n\n# Import Quandl data: quandl_data\n\nquandl_url \n<-\n \n'http://www.quandl.com/api/v1/datasets/IWS/INTERNET_INDIA.json?auth_token=i83asDsiWUUyfoypkgMz'\n\nquandl_data \n<-\n fromJSON\n(\nquandl_url\n)\n\n\nstr\n(\nquandl_data\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# Experiment 1\n\njson1 \n<-\n \n'[1, 2, 3, 4, 5, 6]'\n\nfromJSON\n(\njson1\n)\n\n\n\n# Experiment 2\n\njson2 \n<-\n \n'{'\na\n': [1, 2, 3], '\nb\n': [4, 5, 6]}'\n\nfromJSON\n(\njson2\n)\n\n\n\n# Experiment 3\n\njson3 \n<-\n \n'[[1, 2], [3, 4]]'\n\nfromJSON\n(\njson3\n)\n\n\n\n# Experiment 4\n\njson4 \n<-\n \n'[{'\na\n': 1, '\nb\n': 2}, {'\na\n': 3, '\nb\n': 4}, {'\na\n': 5, '\nb\n': 6}]'\n\nfromJSON\n(\njson4\n)\n\n\n\n\n\n\n\nAsk OMDb\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Definition of the URLs\n\nurl_sw4 \n<-\n \n'http://www.omdbapi.com/?i=tt0076759&r=json'\n\nurl_sw3 \n<-\n \n'http://www.omdbapi.com/?i=tt0121766&r=json'\n\n\n\n# Import two URLs with fromJSON(): sw4 and sw3\n\nsw4 \n<-\n fromJSON\n(\nurl_sw4\n)\n\nsw3 \n<-\n fromJSON\n(\nurl_sw3\n)\n\n\n\n# Print out the Title element of both lists\n\nsw4\n$\nTitle\nsw3\n$\nTitle\n\n\n# Is the release year of sw4 later than sw3\n\nsw4\n$\nYear \n>\n sw3\n$\nYear\n\n\n\n\n\n\nFrom R to \nJSON\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n# URL pointing to the .csv file\n\nurl_csv \n<-\n \n'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/water.csv'\n\n\n\n# Import the .csv file located at url_csv\n\nwater \n<-\n read.csv\n(\nurl_csv\n,\n stringsAsFactors \n=\n \nFALSE\n)\n\n\n\n# Generate a summary of water\n\n\nsummary\n(\nwater\n)\n\n\n\n# Convert the data file according to the requirements\n\nwater_json \n<-\n toJSON\n(\nwater\n)\n\n\nwater_json\n\n\n\n\n\n\nMinify\n and \nprettify\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Convert mtcars to a pretty JSON: pretty_json\n\npretty_json \n<-\n toJSON\n(\nmtcars\n,\n pretty \n=\n \nTRUE\n)\n\n\n\n# Print pretty_json\n\npretty_json\n\n\n# Minify pretty_json: mini_json\n\nmini_json \n<-\n minify\n(\npretty_json\n)\n\n\n\n# Print mini_json\n\nmini_json\n\n\n\n\n\n\nKeyboard Inputting\n\u00b6\n\n\nCoding\n\n\n1\n2\n3\n4\n5\n# create a data frame from scratch\n\nage \n<-\n \nc\n(\n25\n,\n \n30\n,\n \n56\n)\n\ngender \n<-\n \nc\n(\n\"male\"\n,\n \n\"female\"\n,\n \n\"male\"\n)\n\nweight \n<-\n \nc\n(\n160\n,\n \n110\n,\n \n220\n)\n\nmydata \n<-\n \ndata.frame\n(\nage\n,\ngender\n,\nweight\n)\n\n\n\n\n\n\n\nSpreadsheet-like\n\n\n1\n2\n3\n4\n5\n# enter data using editor\n\nmydata \n<-\n \ndata.frame\n(\nage \n=\n \nnumeric\n(\n0\n),\n gender \n=\n \ncharacter\n(\n0\n),\n weight \n=\n \nnumeric\n(\n0\n))\n\n\nmydata \n<-\n edit\n(\nmydata\n)\n\n\n# note that without the assignment in the line above, the edits are not saved! \n\n\n\n\n\n\n\nExporting Data\n\u00b6\n\n\nTo a Tab-Delimited Text File\n\u00b6\n\n\n1\nwrite.table\n(\nmydata\n,\n \n'c:/mydata.txt'\n,\n sep \n=\n \n\"\\t\"\n)\n\n\n\n\n\n\n\nTo an Excel Spreadsheet\n\u00b6\n\n\n1\n2\n3\nlibrary\n(\nxlsx\n)\n\n\nwrite.xlsx\n(\nmydata\n,\n \n\"c:/mydata.xlsx\"\n)\n\n\n\n\n\n\n\nWorksheet\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nlibrary\n(\nXLConnect\n)\n\n\n# xls or xlsx\n\n\n\n# write a worksheet in steps\n\nwb \n<-\n loadWorkbook\n(\n'XLConnectExample1.xls'\n,\n create \n=\n \nTRUE\n)\n\ncreateSheet\n(\nwb\n,\n name \n=\n \n'chickSheet'\n)\n\nwriteWorksheet\n(\nwb\n,\n ChickWeight\n,\n sheet \n=\n \n'chickSheet'\n,\n startRow \n=\n \n3\n,\n startCol \n=\n \n4\n)\n\nsaveWorkbook\n(\nwb\n)\n\n\n\n# write a worksheet all in one step\n\nChickWeight \n<-\n \n1\n\n\nwriteWorksheetToFile\n(\n'XLConnectExample2.xlsx'\n,\n data \n=\n ChickWeight\n,\n sheet \n=\n \n'chickSheet'\n,\n startRow \n=\n \n3\n,\n startCol \n=\n \n4\n)\n\n\n\n\n\n\n\nField\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# write a field in steps\n\nwb \n=\n loadWorkbook\n(\n'XLConnectExample3.xlsx'\n,\n create \n=\n \nTRUE\n)\n\ncreateSheet\n(\nwb\n,\n name \n=\n \n'womenData'\n)\n\ncreateName\n(\nwb\n,\n name \n=\n \n'womenName'\n,\n formula \n=\n \n'womenData!$C$5'\n,\n overwrite \n=\n \nTRUE\n)\n\nwriteNamedRegion\n(\nwb\n,\n women\n,\n name \n=\n \n\"womenName\"\n)\n\nsaveWorkbook\n(\nwb\n)\n\n\n\n# write a field all in one step\n\nwriteNamedRegionToFile\n(\n\"XLConnectExample4.xlsx\"\n,\n women\n,\n name \n=\n \n\"womenName\"\n,\n formula \n=\n \n\"womenData!$C$5\"\n)\n\n\n\n\n\n\n\nI/O\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# Build connection to latitude.xlsx\n\nmy_book \n<-\n loadWorkbook\n(\n'latitude.xlsx'\n)\n\n\n\n# Create data frame: summ\n\ndims1 \n<-\n \ndim\n(\nreadWorksheet\n(\nmy_book\n,\n \n1\n))\n\ndims2 \n<-\n \ndim\n(\nreadWorksheet\n(\nmy_book\n,\n \n2\n))\n\nsumm \n<-\n \ndata.frame\n(\nsheets \n=\n getSheets\n(\nmy_book\n),\n \n                   nrows \n=\n \nc\n(\ndims1\n[\n1\n],\n dims2\n[\n1\n]),\n \n                   ncols \n=\n \nc\n(\ndims1\n[\n2\n],\n dims2\n[\n2\n]))\n\n\n\n# Add a worksheet to my_book, named 'data_summary'\n\ncreateSheet\n(\nmy_book\n,\n name \n=\n \n'data_summary'\n)\n\n\n\n# Populate 'data_summary' with summ data frame\n\nwriteWorksheet\n(\nmy_book\n,\n summ\n,\n sheet \n=\n \n'data_summary'\n)\n\n\n# Save workbook as latitude_with_summ.xlsx\n\n\nsaveWorkbook\n(\nmy_book\n,\n \n'latitude_with_summ.xlsx'\n)\n\n\n\n\n\n\n\nTo SPSS\n\u00b6\n\n\n1\n2\n3\nlibrary\n(\nforeign\n)\n\n\nwrite.foreign\n(\nmydata\n,\n \n\"c:/mydata.txt\"\n,\n \n\"c:/mydata.sps\"\n,\n package \n=\n \n\"SPSS\"\n)\n\n\n\n\n\n\n\nTo SAS\n\u00b6\n\n\n1\n2\n3\nlibrary\n(\nforeign\n)\n\n\nwrite.foreign\n(\nmydata\n,\n \n\"c:/mydata.txt\"\n,\n \n\"c:/mydata.sas\"\n,\n package \n=\n \n\"SAS\"\n)\n \n\n\n\n\n\n\nTo Stata\n\u00b6\n\n\n1\n2\n3\nlibrary\n(\nforeign\n)\n\n\nwrite.dta\n(\nmydata\n,\n \n\"c:/mydata.dta\"\n)\n \n\n\n\n\n\n\nInspecting Data - Missing Data\n\u00b6\n\n\nInspecting\n\u00b6\n\n\n\n\nls(object)\n.\n\n\nnames(object)\n.\n\n\nstr(object)\n.\n\n\nlevels(object$v1)\n.\n\n\ndim(object)\n.\n\n\nclass\n(\nobject\n)\n.\n\n\nprint(object)\n.\n\n\nhead(object, 10)\n.\n\n\ntail(object, 20)\n.\n\n\n\n\nTesting for Missing Values\n\n\n1\n2\ny \n<-\n \nc\n(\n1\n,\n \n2\n,\n \n3\n,\n \nNA\n)\n \n# returns TRUE of x is missing\n\n\nis.na\n(\ny\n)\n \n# returns a vector (F F F T) \n\n\n\n\n\n\n\nRecoding Values to Missing\n\n\n1\n2\n3\n# recode 99 to missing for variable v1\n\n\n# select rows where v1 is 99 and recode column v1\n\nmydata\n$\nv1\n[\nmydata\n$\nv1 \n==\n \n99\n]\n \n<-\n \nNA\n \n\n\n\n\n\n\nExcluding Missing Values from Analyses\n\n\n1\n2\n3\n4\nx \n<-\n \nc\n(\n1\n,\n \n2\n,\n \nNA\n,\n \n3\n)\n\n\n\nmean\n(\nx\n)\n \n# returns NA\n\n\nmean\n(\nx\n,\n na.rm \n=\n \nTRUE\n)\n \n# returns 2 \n\n\n\n\n\n\n\n1\n2\n# list rows of data that have missing values\n\nmydata\n[\n!\ncomplete.cases\n(\nmydata\n),]\n\n\n\n\n\n\n\n1\n2\n# create new dataset without missing data\n\nnewdata \n<-\n na.omit\n(\nmydata\n)\n \n\n\n\n\n\n\nThe \ndplyr\n package\n\u00b6\n\n\n1\n2\n3\n4\n5\nlibrary\n(\ndplyr\n)\n\n\ntbl_df\n(\niris\n)\n \n# almost like head/tail\n\nglimpse\n(\niris\n)\n \n# almost like str\n\nView\n(\niris\n)\n \n# open a spreadsheet\n\n\n\n\n\n\n\nFor thorough cleaning\n\u00b6\n\n\n\n\nThe \nAmelia II\n software.\n\n\nThe \nmitools\n package.\n\n\n\n\nLabels & Levels\n\u00b6\n\n\nBasic\n\n\n1\n2\n3\n4\n5\n# variable v1 is coded 1, 2 or 3\n\n\n# we want to attach value labels 1=red, 2=blue, 3=green\n\nmydata\n$\nv1 \n<-\n \nfactor\n(\nmydata\n$\nv1\n,\n \n                    levels \n=\n \nc\n(\n1\n,\n2\n,\n3\n),\n\n                    labels \n=\n \nc\n(\n\"red\"\n,\n \n\"blue\"\n,\n \n\"green\"\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# variable y is coded 1, 3 or 5\n\n\n# we want to attach value labels 1=Low, 3=Medium, 5=High\n\nmydata\n$\nv1 \n<-\n \nordered\n(\nmydata\n$\ny\n,\n\n                     levels \n=\n \nc\n(\n1\n,\n3\n,\n \n5\n),\n\n                     labels \n=\n \nc\n(\n\"Low\"\n,\n \n\"Medium\"\n,\n \n\"High\"\n))\n \n\n\n\n\n\n\nOrder\n\n\n1\n2\n3\n4\n5\n# Create a vector of temperature observations\n\ntemperature_vector \n<-\n \nc\n(\n'High'\n,\n \n'Low'\n,\n \n'High'\n,\n \n'Low'\n,\n \n'Medium'\n)\n\n\n\n# Specify that they are ordinal variables with the given levels\n\nfactor_temperature_vector \n<-\n \nfactor\n(\ntemperature_vector\n,\n order \n=\n \nTRUE\n,\n levels \n=\n \nc\n(\n'Low'\n,\n \n'Medium'\n,\n \n'High'\n))\n\n\n\n\n\n\n\nAdd comments to an object\n\n\n1\n2\n3\n4\nnames\n(\niris\n)[\n5\n]\n \n<-\n \n\"This is the label for variable 5\"\n\n\n\nnames\n(\niris\n)[\n5\n]\n \n# the comment\n\niris\n[\n5\n]\n \n# the data\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# labeling the variables\n\n\nlibrary\n(\nHmisc\n)\n\n\nlabel\n(\niris\n$\nSpecies\n)\n \n<-\n \n\"Variable label for variable myvar\"\n\n\ndescribe\n(\niris\n$\nSpecies\n)\n \n# commented\n\n\n#vs\n\ndescribe\n(\niris\n$\nSepal.Length\n)\n \n# not commented\n\n\n\n\n\n\n\n\n\nHow to work with Quandl in R\n\u00b6\n\n\nImporting Quandl Datasets\n\u00b6\n\n\nQuandl\n delivers financial, economic and\n\nalternative data to the world\u2019s top hedge funds, asset managers and\n\ninvestment banks in several formats:\n\n\n\n\nExcel.\n\n\nR.\n\n\nPython.\n\n\nAPI.\n\n\nDB.\n\n\n\n\nThe packages used:\n\n\n\n\nQuandl\n.\n\n\nquantmod\n for plotting.\n\n\n\n\nQuandl - A first date\n\n\n1\n2\n3\n4\n5\n# Load in the Quandl package\n\n\nlibrary\n(\nQuandl\n)\n\n\n\n# Assign your first dataset to the variable:\n\nmydata \n<-\n Quandl\n(\n'NSE/OIL'\n)\n\n\n\n\n\n\n\nIdentifying a dataset with its ID\n\n\n1\n2\n# Assign the Prague Stock Exchange to:\n\nPragueStockExchange \n<-\n Quandl\n(\n'PRAGUESE/PX'\n)\n\n\n\n\n\n\n\nPlotting a stock chart\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# The quantmod package\n\n\nlibrary\n(\nquantmod\n)\n\n\n\n# Load the Facebook data with the help of Quandl\n\nFacebook \n<-\n Quandl\n(\n'GOOG/NASDAQ_FB'\n,\n type \n=\n \n'xts'\n)\n\n\n\n# Plot the chart with the help of candleChart()\n\ncandleChart\n(\nFacebook\n)\n\n\n\n\n\n\n\nSearching a Quandl dataset in R\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Look up the first 3 results for 'Bitcoin' within the Quandl database:\n\nresults \n<-\n Quandl.search\n(\nquery \n=\n \n'Bitcoin'\n,\n silent \n=\n \nFALSE\n)\n\n\n\n# Print out the results\n\nstr\n(\nresults\n)\n\n\n\n# Assign the data set with code BCHAIN/TOTBC\n\nBitCoin \n<-\n Quandl\n(\n'BCHAIN/TOTBC'\n)\n\n\n\n\n\n\n\nManipulating Quandl Datasets\n\u00b6\n\n\nManipulating data\n\n\n1\n2\n# Assign to the variable Exchange\n\nExchange \n<-\n Quandl\n(\n'BNP/USDEUR'\n,\n start_date \n=\n \n'2013-01-01'\n,\n end_date \n=\n \n'2013-12-01'\n)\n\n\n\n\n\n\n\nTransforming your Quandl dataset\n\n\n1\n2\n3\n4\n5\n6\n# API transformation\n\n\n# The result:\n\nGDP_Change \n<-\n Quandl\n(\n'FRED/CANRGDPR'\n,\n transformation \n=\n \n'rdiff'\n)\n\n\nhead\n(\nGDP_Change\n)\n\nGDP_Chang \n<-\n Quandl\n(\n'FRED/CANRGDPR'\n)\n\n\nhead\n(\nGDP_Chang\n)\n\n\n\n\n\n\n\nThe magic of frequency collapsing\n\n\n1\n2\n# The result:\n\neiaQuarterly \n<-\n Quandl\n(\n'DOE/RWTC'\n,\n collapse \n=\n \n'quarterly'\n)\n\n\n\n\n\n\n\nTruncation and sort\n\n\n1\n2\n3\n4\n5\n# Assign to TruSo the first 5 observations of the crude oil prices\n\nTruSo \n<-\n Quandl\n(\n'DOE/RWTC'\n,\n sort \n=\n \n'asc'\n,\n rows \n=\n \n5\n)\n\n\n\n# Print the result\n\nTruSo\n\n\n\n\n\n\nA complex example\n\n\n1\n2\n# Here you should place the return:\n\nFinal \n<-\n Quandl\n(\n'DOE/RWTC'\n,\n collapse \n=\n \n'daily'\n,\n transformation \n=\n \n'rdiff'\n,\n start_date \n=\n \n'2005-01-01'\n,\n end_date \n=\n \n'2010-03-01'\n,\n sort \n=\n \n'asc'\n)\n\n\n\n\n\n\n\n\n\nCleaning Data in R\n\u00b6\n\n\nThe packages used:\n\n\n\n\ndplyr\n & \ntidyr\n for data wrangling.\n\n\nstringr\n for regex.\n\n\nlubridate\n for time and date.\n\n\n\n\nIntroduction and Exploring Raw Data\n\u00b6\n\n\nHere\u2019s what messy data look like\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# View the first 6 rows of data\n\n\nhead\n(\nweather\n)\n\n\n\n# View the last 6 rows of data\n\n\ntail\n(\nweather\n)\n\n\n\n# View a condensed summary of the data\n\nstr\n(\nweather\n)\n\n\n\n\n\n\n\nGetting a feel for your data\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Check the class of bmi\n\n\nclass\n(\nbmi\n)\n\n\n\n# Check the dimensions of bmi\n\n\ndim\n(\nbmi\n)\n\n\n\n# View the column names of bmi\n\n\nnames\n(\nbmi\n)\n\n\n\n\n\n\n\nViewing the structure of your data\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Check the structure of bmi\n\nstr\n(\nbmi\n)\n\n\n\n# Load dplyr\n\n\nlibrary\n(\ndplyr\n)\n\n\n\n# Check the structure of bmi, the dplyr way\n\nglimpse\n(\nbmi\n)\n\n\n\n# View a summary of bmi\n\n\nsummary\n(\nbmi\n)\n\n\n\n\n\n\n\nLooking at your data\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Print bmi to the console\n\n\nprint\n(\nbmi\n)\n\n\n\n# View the first 6 rows\n\n\nhead\n(\nbmi\n,\n \n6\n)\n\n\n\n# View the first 15 rows\n\n\nhead\n(\nbmi\n,\n \n15\n)\n\n\n\n# View the last 6 rows\n\n\ntail\n(\nbmi\n,\n \n6\n)\n\n\n\n# View the last 10 rows\n\n\ntail\n(\nbmi\n,\n \n10\n)\n\n\n\n\n\n\n\nVisualizing your data\n\n\n1\n2\n3\n4\n5\n# Histogram of BMIs from 2008\n\nhist\n(\nbmi\n$\nY2008\n)\n\n\n\n# Scatter plot comparing BMIs from 1980 to those from 2008\n\nplot\n(\nbmi\n$\nY1980\n,\n bmi\n$\nY2008\n)\n\n\n\n\n\n\n\nTidying Data\n\u00b6\n\n\nGathering columns into key-value pairs\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Load tidyr\n\n\nlibrary\n(\ntidyr\n)\n\n\n\n# Apply gather() to bmi and save the result as bmi_long\n\nbmi_long \n<-\n gather\n(\nbmi\n,\n year\n,\n bmi_val\n,\n \n-\nCountry\n)\n\n\n\n# View the first 20 rows of the result\n\n\nhead\n(\nbmi_long\n,\n \n20\n)\n\n\n\n\n\n\n\nSpreading key-value pairs into columns\n\n\n1\n2\n3\n4\n5\n# Apply spread() to bmi_long\n\nbmi_wide \n<-\n spread\n(\nbmi_long\n,\n year\n,\n bmi_val\n)\n\n\n\n# View the head of bmi_wide\n\n\nhead\n(\nbmi_wide\n)\n\n\n\n\n\n\n\nSeparating columns\n\n\n1\n2\n3\n4\n5\n# Apply separate() to bmi_cc\n\nbmi_cc_clean \n<-\n separate\n(\nbmi_cc\n,\n col \n=\n Country_ISO\n,\n into \n=\n \nc\n(\n'Country'\n,\n \n'ISO'\n),\n sep \n=\n \n'/'\n)\n\n\n\n# Print the head of the result\n\n\nhead\n(\nbmi_cc_clean\n)\n\n\n\n\n\n\n\nUniting columns\n\n\n1\n2\n3\n4\n5\n# Apply unite() to bmi_cc_clean\n\nbmi_cc \n<-\n unite\n(\nbmi_cc_clean\n,\n Country_ISO\n,\n Country\n,\n ISO\n,\n sep \n=\n \n'-'\n)\n\n\n\n# View the head of the result\n\n\nhead\n(\nbmi_cc\n)\n\n\n\n\n\n\n\nColumn headers are values, not variable names\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# View the head of census\n\n\nhead\n(\ncensus\n)\n\n\n\n# Gather the month columns\n\ncensus2 \n<-\n gather\n(\ncensus\n,\n month\n,\n amount\n,\n JAN\n,\n FEB\n,\n MAR\n,\n APR\n,\n MAY\n,\n JUN\n,\n JUL\n,\n AUG\n,\n SEP\n,\n OCT\n,\n NOV\n,\n DEC\n)\n\n\n\n# Arrange rows by YEAR using dplyr's arrange\n\ncensus2 \n<-\n arrange\n(\ncensus2\n,\n YEAR\n)\n\n\n\n# View first 20 rows of census2\n\n\nhead\n(\ncensus2\n,\n \n20\n)\n\n\n\n\n\n\n\nVariables are stored in both rows and columns\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# View first 50 rows of census_long\n\n\nhead\n(\ncensus_long\n,\n \n50\n)\n\n\n\n# Spread the type column\n\ncensus_long2 \n<-\n spread\n(\ncensus_long\n,\n type\n,\n amount\n)\n\n\n\n# View first 20 rows of census_long2\n\n\nhead\n(\ncensus_long2\n,\n \n20\n)\n\n\n\n\n\n\n\nMultiple values are stored in one column\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# View the head of census_long3\n\n\nhead\n(\ncensus_long3\n)\n\n\n\n# Separate the yr_month column into two\n\ncensus_long4 \n<-\n separate\n(\ncensus_long3\n,\n yr_month\n,\n \nc\n(\n'year'\n,\n \n'month'\n),\n \n'_'\n)\n\n\n\n# View the first 6 rows of the result\n\n\nhead\n(\ncensus_long4\n,\n \n6\n)\n\n\n\n\n\n\n\nPreparing Data for Analysis\n\u00b6\n\n\nTypes of variables in R\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Make this evaluate to character\n\n\nclass\n(\n'true'\n)\n\n\n\n# Make this evaluate to numeric\n\n\nclass\n(\n8484.00\n)\n\n\n\n# Make this evaluate to integer\n\n\nclass\n(\n99L\n)\n\n\n\n# Make this evaluate to factor\n\n\nclass\n(\nfactor\n(\n'factor'\n))\n\n\n\n# Make this evaluate to logical\n\n\nclass\n(\nFALSE\n)\n\n\n\n\n\n\n\nCommon type conversions\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Preview students with str()\n\nstr\n(\nstudents\n)\n\n\n\n# Coerce Grades to character\n\nstudents\n$\nGrades \n<-\n \nas.character\n(\nstudents\n$\nGrades\n)\n\n\n\n# Coerce Medu to factor\n\nstudents\n$\nMedu \n<-\n \nas.factor\n(\nstudents\n$\nMedu\n)\n\n\n\n# Coerce Fedu to factor\n\nstudents\n$\nFedu \n<-\n \nas.factor\n(\nstudents\n$\nFedu\n)\n\n\n \n# Look at students once more with str()\n\nstr\n(\nstudents\n)\n\n\n\n\n\n\n\nWorking with dates\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n# Preview students2 with str()\n\nstr\n(\nstudents2\n)\n\n\n\n# Load the lubridate package\n\n\nlibrary\n(\nlubridate\n)\n\n\n\n# Parse as date\n\nymd\n(\n'2015-Sep-17'\n)\n\n\n\n# Parse as date and time (with no seconds!)\n\nymd_hm\n(\n'2012-July-15, 12.56'\n)\n\n\n\n# Coerce dob to a date (with no time)\n\nstudents2\n$\ndob \n<-\n ymd\n(\nstudents2\n$\ndob\n)\n\n\n\n# Coerce nurse_visit to a date and time\n\nstudents2\n$\nnurse_visit \n<-\n ymd_hms\n(\nstudents2\n$\nnurse_visit\n)\n\n\n\n# Look at students2 once more with str()\n\nstr\n(\nstudents2\n)\n\n\n\n\n\n\n\nTrimming and padding strings\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Load the stringr package\n\n\nlibrary\n(\nstringr\n)\n\n\n\n# Trim all leading and trailing whitespace\n\nstr_trim\n(\nc\n(\n'   Filip '\n,\n \n'Nick  '\n,\n \n' Jonathan'\n))\n\n\n\n# Pad these strings with leading zeros\n\nstr_pad\n(\nc\n(\n'23485W'\n,\n \n'8823453Q'\n,\n \n'994Z'\n),\n width \n=\n \n9\n,\n side \n=\n \n'left'\n,\n pad \n=\n \n'0'\n)\n\n\n\n\n\n\n\nUpper and lower case\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Print state abbreviations\n\nstates\n\n\n# Make states all uppercase and save result to states_upper\n\nstates_upper \n<-\n \ntoupper\n(\nstates\n)\n\nstates_upper\n\n\n# Make states_upper all lowercase again\n\n\ntolower\n(\nstates_upper\n)\n\n\n\n\n\n\n\nFinding and replacing strings\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# stringr has been loaded for you\n\n\n# Look at the head of students2\n\n\nhead\n(\nstudents2\n)\n\n\n\n# Detect all dates of birth (dob) in 1997\n\nstr_detect\n(\nstudents2\n$\ndob\n,\n \n'1997'\n)\n\n\n\n# In the sex column, replace 'F' with 'Female'...\n\nstudents2\n$\nsex \n<-\n str_replace\n(\nstudents2\n$\nsex\n,\n \n'F'\n,\n \n'Female'\n)\n\n\n\n# ...And 'M' with 'Male'\n\nstudents2\n$\nsex \n<-\n str_replace\n(\nstudents2\n$\nsex\n,\n \n'M'\n,\n \n'Male'\n)\n\n\n\n# View the head of students2\n\n\nhead\n(\nstudents2\n)\n\n\n\n\n\n\n\nFinding missing values\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# Call is.na() on the full social_df to spot all NAs\n\n\nis.na\n(\nsocial_df\n)\n\n\n\n# Use the any() function to ask whether there are any NAs in the data\n\n\nany\n(\nis.na\n(\nsocial_df\n))\n\n\nsum\n(\nis.na\n(\nsocial_df\n))\n\n\n\n# View a summary() of the dataset\n\n\nsummary\n(\nsocial_df\n)\n\n\n\n# Call table() on the status column\n\n\ntable\n(\nsocial_df\n$\nstatus\n)\n\n\n\n\n\n\n\nDealing with missing values\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Use str_replace() to replace all missing strings in status with NA\n\nsocial_df\n$\nstatus \n<-\n str_replace\n(\nsocial_df\n$\nstatus\n,\n \n'^$'\n,\n \nNA\n)\n\n\n\n# Print social_df to the console\n\nsocial_df\n\n\n# Use complete.cases() to see which rows have no missing values\n\ncomplete.cases\n(\nsocial_df\n)\n\n\n\n# Use na.omit() to remove all rows with any missing values\n\nna.omit\n(\nsocial_df\n)\n\n\n\n\n\n\n\nDealing with outliers and obvious errors\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Look at a summary() of students3\n\n\nsummary\n(\nstudents3\n)\n\n\n\n# View a histogram of the age variable\n\nhist\n(\nstudents3\n$\nage\n,\n breaks \n=\n \n20\n)\n\n\n\n# View a histogram of the absences variable\n\nhist\n(\nstudents3\n$\nabsences\n,\n breaks \n=\n \n20\n)\n\n\n\n# View a histogram of absences, but force zeros to be bucketed to the right of zero\n\nhist\n(\nstudents3\n$\nabsences\n,\n breaks \n=\n \n20\n,\n right \n=\n \nFALSE\n)\n\n\n\n\n\n\n\nAnother look at strange values\n\n\n1\n2\n3\n4\n5\n# View a boxplot of age\n\nboxplot\n(\nstudents3\n$\nage\n)\n\n\n\n# View a boxplot of absences\n\nboxplot\n(\nstudents3\n$\nabsences\n)\n\n\n\n\n\n\n\nPutting it All Together\n\u00b6\n\n\nGet a feel for the data\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Verify that weather is a data.frame\n\n\nclass\n(\nweather\n)\n\n\n\n# Check the dimensions\n\n\ndim\n(\nweather\n)\n\n\n\n# View the column names\n\n\nnames\n(\nweather\n)\n\n\n\n\n\n\n\nSummarize the data\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# View the structure of the data\n\nstr\n(\nweather\n)\n\n\n\n# Load dplyr package\n\n\nlibrary\n(\ndplyr\n)\n\n\n\n# Look at the structure using dplyr's glimpse()\n\nglimpse\n(\nweather\n)\n\n\n\n# View a summary of the data\n\n\nsummary\n(\nweather\n)\n\n\n\n\n\n\n\nTake a closer look\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# View first 6 rows\n\n\nhead\n(\nweather\n,\n \n6\n)\n\n\n\n# View first 15 rows\n\n\nhead\n(\nweather\n,\n \n15\n)\n\n\n\n# View the last 6 rows\n\n\ntail\n(\nweather\n,\n \n6\n)\n\n\n\n# View the last 10 rows\n\n\ntail\n(\nweather\n,\n \n10\n)\n\n\n\n\n\n\n\nColumn names are values\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Load the tidyr package\n\n\nlibrary\n(\ntidyr\n)\n\n\n\n# Gather the columns\n\nweather2 \n<-\n gather\n(\nweather\n,\n day\n,\n value\n,\n X1\n:\nX31\n,\n na.rm \n=\n \nTRUE\n)\n\n\n\n# View the head\n\n\nhead\n(\nweather2\n)\n\n\n\n\n\n\n\nValues are variable names\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# First remove column of row names\n\nweather2 \n<-\n weather2\n[,\n \n-1\n]\n\n\n\n# Spread the data\n\nweather3 \n<-\n spread\n(\nweather2\n,\n measure\n,\n value\n)\n\n\n\n# View the head\n\n\nhead\n(\nweather3\n)\n\n\n\n\n\n\n\nClean up dates\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n# Load the stringr and lubridate packages\n\n\nlibrary\n(\nstringr\n)\n\n\nlibrary\n(\nlubridate\n)\n\n\n\n# Remove X's from day column\n\n\n# weather3$day <- str_pad(str_replace(weather3$day, 'X', ''), width = 2, side = 'left', pad = '0')\n\nweather3\n$\nday \n<-\n str_replace\n(\nweather3\n$\nday\n,\n \n'X'\n,\n \n''\n)\n\n\n\n# Unite the year, month, and day columns\n\nweather4 \n<-\n unite\n(\nweather3\n,\n \ndate\n,\n year\n,\n month\n,\n day\n,\n sep \n=\n \n'-'\n)\n\n\n\n# Convert date column to proper date format using stringr's ymd()\n\nweather4\n$\ndate \n<-\n ymd\n(\nweather4\n$\ndate\n)\n\n\n\n# Rearrange columns using dplyr's select()\n\nweather5 \n<-\n select\n(\nweather4\n,\n \ndate\n,\n Events\n,\n CloudCover\n:\nWindDirDegrees\n)\n\n\n\n# View the head\n\n\nhead\n(\nweather5\n)\n\n\n\n\n\n\n\nA closer look at column types\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# View the structure of weather5\n\nstr\n(\nweather5\n)\n\n\n\n# Examine the first 20 rows of weather5. Are most of the characters numeric?\n\n\nhead\n(\nweather5\n,\n \n20\n)\n\n\n\n# See what happens if we try to convert PrecipitationIn to numeric\n\n\nas.numeric\n(\nweather5\n$\nPrecipitationIn\n)\n\n\n\n\n\n\n\nColumn type conversions\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# The dplyr package is already loaded\n\n\n# Replace T with 0 (T = trace)\n\nweather5\n$\nPrecipitationIn \n<-\n str_replace\n(\nweather5\n$\nPrecipitationIn\n,\n \n'T'\n,\n \n'0'\n)\n\n\n\n# Convert characters to numerics\n\nweather6 \n<-\n mutate_each\n(\nweather5\n,\n funs\n(\nas.numeric\n),\n CloudCover\n:\nWindDirDegrees\n)\n\n\n\n# Look at result\n\nstr\n(\nweather6\n)\n\n\n\n\n\n\n\nFind missing values\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# Count missing values\n\n\nsum\n(\nis.na\n(\nweather6\n))\n\n\n\n# Find missing values\n\n\nsummary\n(\nweather6\n)\n\n\n\n# Find indices of NAs in Max.Gust.SpeedMPH\n\nind \n<-\n \nwhich\n(\nis.na\n(\nweather6\n$\nMax.Gust.SpeedMPH\n))\n\nind\n\n\n# Look at the full rows for records missing Max.Gust.SpeedMPH\n\nweather6\n[\nind\n,\n \n]\n\n\n\n\n\n\n\nAn obvious error\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Review distibutions for all variables\n\n\nsummary\n(\nweather6\n)\n\n\n\n# Find row with Max.Humidity of 1000\n\nind \n<-\n \nwhich\n(\nweather6\n$\nMax.Humidity \n==\n \n1000\n)\n\n\n\n# Look at the data for that day\n\nweather6\n[\nind\n,\n \n]\n\n\n\n# Change 1000 to 100\n\nweather6\n$\nMax.Humidity\n[\nind\n]\n \n<-\n \n100\n\n\n\n\n\n\n\nAnother obvious error\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Look at summary of Mean.VisibilityMiles\n\n\nsummary\n(\nweather6\n$\nMean.VisibilityMiles\n)\n\n\n\n# Get index of row with -1 value\n\nind \n<-\n \nwhich\n(\nweather6\n$\nMean.VisibilityMiles \n==\n \n-1\n)\n\n\n\n# Look at full row\n\nweather6\n[\nind\n,\n \n]\n\n\n\n# Set Mean.VisibilityMiles to the appropriate value\n\nweather6\n$\nMean.VisibilityMiles\n[\nind\n]\n \n<-\n \n10\n\n\n\n\n\n\n\nCheck other extreme values\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Review summary of full data once more\n\n\nsummary\n(\nweather6\n)\n\n\n\n# Look at histogram for MeanDew.PointF\n\nhist\n(\nweather6\n$\nMeanDew.PointF\n)\n\n\n\n# Look at histogram for Min.TemperatureF\n\nhist\n(\nweather6\n$\nMin.TemperatureF\n)\n\n\n\n# Compare to histogram for Mean.TemperatureF\n\nhist\n(\nweather6\n$\nMean.TemperatureF\n)\n\n\n\n\n\n\n\nFinishing touches\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Clean up column names\n\n\nnames\n(\nweather6\n)\n \n<-\n new_colnames\n\n\n# Replace empty cells in Events column\n\nweather6\n$\nevents\n[\nweather6\n$\nevents \n==\n \n''\n]\n \n<-\n \n'None'\n\n\n\n# Print the first 6 rows of weather6\n\n\nhead\n(\nweather6\n,\n \n6\n)",
            "title": "I/O Snippets & Cleaning"
        },
        {
            "location": "/IO_snippets___Cleaning/#importing-data-into-r",
            "text": "The packages:   utils .  readr .  data.table .  readxl .  gdata .  XLConnect .  haven .  foreign .  DBI .  httr .  jsonlite .",
            "title": "Importing Data Into R"
        },
        {
            "location": "/IO_snippets___Cleaning/#importing-data-from-flat-files",
            "text": "",
            "title": "Importing Data from Flat Files"
        },
        {
            "location": "/IO_snippets___Cleaning/#r-functions-by-default",
            "text": "read.csv ;  sep = ',' ,  dec = '.' .  read.delim ; .txt,  dec = '.' .  read.csv2 ;  sep = ';' ,  dec = ',' .  read.delim2 ; .txt,  dec = ',' .  Needs arguments.   read.csv  for .csv files  1\n2\n3\n4\n5\n6 # List the files in your working directory  dir ()  # Import swimming_pools.csv: pools  # stringAsFactors = FALSE does not import strings as categorical variables \npools  <-  read.csv ( 'swimming_pools.csv' ,  stringsAsFactors  =   FALSE )    stringsAsFactors  1\n2\n3\n4\n5 # Import swimming_pools.csv correctly: pools \npools  <-  read.csv ( 'swimming_pools.csv' ,  stringsAsFactor  =   FALSE ,  header  =   TRUE ,  sep  =   ',' )  # Import swimming_pools.csv with factors: pools_factor \npools_factor  <-  read.csv ( 'swimming_pools.csv' ,  header  =   TRUE ,  sep  =   ',' )    read.delim  for .txt files  1\n2\n3\n4\n5 # Import hotdogs.txt: hotdogs \nhotdogs  <-  read.delim ( 'hotdogs.txt' ,  header  =   FALSE )  # Name the columns of hotdogs appropriately  names ( hotdogs )   <-   c ( 'type' ,   'calories' ,   'sodium' )    Arguments.  1\n2\n3\n4\n5\n6\n7\n8 # Load in the hotdogs data set: hotdogs \nhotdogs  <-  read.delim ( 'hotdogs.txt' ,  header  =   FALSE ,  sep  =   '\\t' ,  col.names  =   c ( 'type' ,   'calories' ,   'sodium' ))  # Select the hot dog with the least calories: lily \nlily  <-  hotdogs [ which.min ( hotdogs $ calories ),   ]  # Select the observation with the most sodium: tom \n\ntom  <-  hotdogs [ which.max ( hotdogs $ sodium ),   ]    1\n2\n3\n4\n5\n6\n7\n8 # Previous call to import hotdogs.txt \nhotdogs  <-  read.delim ( 'hotdogs.txt' ,  header  =   FALSE ,  col.names  =   c ( 'type' ,   'calories' ,   'sodium' ))  # Print a vector representing the classes of the columns  sapply ( hotdogs ,   class )  # Edit the colClasses argument to import the data correctly: hotdogs2 \nhotdogs2  <-  read.delim ( 'hotdogs.txt' ,  header  =   FALSE ,  col.names  =   c ( 'type' ,   'calories' ,   'sodium' ),  colClasses  =   c ( 'factor' ,   'NULL' ,   'numeric' ))",
            "title": "R functions, by default."
        },
        {
            "location": "/IO_snippets___Cleaning/#the-utils-package",
            "text": "read.table ;  sep = '\\t' ,  = ',' ,  = ';' .  Read any tabular as a d.f.  Needs arguments; lots of argument for precision.  Slow.    1 library ( utils )    read.table  .txt files  1\n2\n3\n4\n5 # Create a path to the hotdogs.txt file \npath  <-   file.path ( 'hotdogs' ,   'hotdogs.txt' )  # Import the hotdogs.txt file: hotdogs \nhotdogs  <-  read.table ( path ,  header  =   FALSE ,  sep  =   '\\t' ,  col.names  =   c ( 'type' ,   'calories' ,   'sodium' ))",
            "title": "The utils package"
        },
        {
            "location": "/IO_snippets___Cleaning/#from-importing-data-from-the-web",
            "text": "1\n2\n3\n4\n5 # https URL to the swimming_pools csv file. \nurl_csv  <-   'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv'  # Import the file using read.csv(): pools1 \npools1  <-  read.csv ( url_csv )",
            "title": "(from Importing Data from the Web)"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-readr-package",
            "text": "read_delim ;  delim = '\\t' ,  = ',' .  read_csv ; read  100.000, 200.000  read_tsv ; idem.  read_csv2 ; read  100,000; 200,000  or European files..  read_tsv2 ; idem.  read_lines .  read_file .  write_csv .  write_rds .  type_convert .  parse_factor .  parse_date .  parse_number .  spec_csv .  spec_delim .  Fast, few arguments.  Detect data type.    1 library ( readr )    read_delim  .txt files  1\n2 # Import potatoes.txt using read_delim(): potatoes \npotatoes  <-  read_delim ( 'potatoes.txt' ,  delim  =   '\\t' )    read_csv  .csv files   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 # Column names \nproperties  <-   c ( 'area' ,   'temp' ,   'size' ,   'storage' ,   'method' ,   'texture' ,   'flavor' ,   'moistness' )  # Import potatoes.csv with read_csv(): potatoes \npotatoes  <-  read_csv ( 'potatoes.csv' ,  col_names  =  properties )  # Create a copy of potatoes: potatoes2 \npotatoes2  <-  potatoes # Convert the method column of potatoes2 to a factor \npotatoes2 $ method  =   factor ( potatoes2 $ method )  # or \n\npotatoes2 $ method  =   as.factor ( potatoes2 $ method )    col_types ,  skip  and  n_max  in .tsv files  1\n2\n3\n4\n5\n6\n7\n8\n9 # Column names \nproperties  <-   c ( 'area' ,   'temp' ,   'size' ,   'storage' ,   'method' ,   'texture' ,   'flavor' ,   'moistness' )  # Import 5 observations from potatoes.txt: potatoes_fragment  # read_tsv or tab-separated values \npotatoes_fragment  <-  read_tsv ( 'potatoes.txt' ,  col_names  =  properties ,  skip  =   7 ,  n_max  =   5 )  # Import all data, but force all columns to be character: potatoes_char \npotatoes_char  <-  read_tsv ( 'potatoes.txt' ,  col_types  =   'cccccccc' )    Setting column types  1\n2\n3\n4 cols ( \n  weight  =  col_integer (), \n  feed  =  col_character ()  )    Removing NA  1 na  =   c ( 'NA' ,   'null' )    col_types  with collectors .tsv files   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Import without col_types \nhotdogs  <-  read_tsv ( 'hotdogs.txt' ,  col_names  =   c ( 'type' ,   'calories' ,   'sodium' ))  # The collectors you will need to import the data \nfac  <-  col_factor ( levels  =   c ( 'Beef' ,   'Meat' ,   'Poultry' )) \nint  <-  col_integer ()  # Edit the col_types argument to import the data correctly: hotdogs_factor  # Change col_types to the correct vector of collectors; coerce the vector into a list \nhotdogs_factor  <-  read_tsv ( 'hotdogs.txt' ,  col_names  =   c ( 'type' ,   'calories' ,   'sodium' ),  col_types  =   list ( fac ,  int ,  int ))    Skiping columns  1\n2\n3\n4\n5 salaries  <-  read_tsv ( 'Salaries.txt' ,  col_names  =   FALSE ,  col_types  =  cols ( \n  X2  =  col_skip (), \n  X3  =  col_skip (),  \n  X4  =  col_skip ()  ))    Reading an ordinary text file  1\n2\n3\n4\n5\n6\n7\n8\n9 # vector of character strings.   # Import as a character vector, one item per line: tweets \ntweets  <-  read_lines ( 'tweets.txt' ) \ntweets # returns a length 1 vector of the entire file, with line breaks represented as \\n  # Import as a length 1 vector: tweets_all \ntweets_all  <-  read_file ( 'tweets.txt' ) \ntweets_all   Writing .csv and .tsv files  1\n2\n3\n4\n5 # Save cwts as chickwts.csv \nwrite_csv ( cwts ,   \"chickwts.csv\" )  # Append cwts2 to chickwts.csv \nwrite_csv ( cwts2 ,   \"chickwts.csv\" ,  append  =   TRUE )    Writing .rds files  1\n2\n3\n4\n5\n6\n7\n8 # Save trees as trees.rds \nwrite_rds ( trees ,   'trees.rds' )  # Import trees.rds: trees2 \ntrees2  <-  read_rds ( 'trees.rds' )  # Check whether trees and trees2 are the same  identical ( trees ,  trees2 )    Coercing columns to different data types  1\n2 # Convert all columns to double \ntrees2  <-  type_convert ( trees ,  col_types  =  cols ( Girth  =   'd' ,  Height  =   'd' ,  Volume  =   'd' ))    Coercing character columns into factors  1\n2\n3\n4\n5 # Parse the title column \nsalaries $ title  <-  parse_factor ( salaries $ title ,  levels  =   c ( 'Prof' ,   'AsstProf' ,   'AssocProf' ))  # Parse the gender column \nsalaries $ gender  <-  parse_factor ( salaries $ gender ,  levels  =   c ( 'Male' ,   'Female' ))    Creating Date objects  1\n2 # Change type of date column \nweather $ date  <-  parse_date ( weather $ date ,  format  =   '%m/%d/%Y' )    Parsing number formats  1\n2 # Parse amount column as a number \ndebt $ amount  <-  parse_number ( debt $ amount )    Viewing metadata before importing   spec_csv  for .csv and .tsv files.  spec_delim  for .txt files (among others).    1\n2 # Specifications of chickwts \nspec_csv ( 'chickwts.csv' )",
            "title": "The readr package"
        },
        {
            "location": "/IO_snippets___Cleaning/#from-importing-data-from-the-web_1",
            "text": "Import Flat files from the web   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Import the csv file: pools \nurl_csv  <-   'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/swimming_pools.csv' \npools  <-  read_csv ( url_csv ) \n\npools # Import the txt file: potatoes \nurl_delim  <-   'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/potatoes.txt' \npotatoes  <-  read_tsv ( url_delim ) \n\npotatoes   Secure importing  1\n2 # Import the file using read_csv(): pools2 \npools2  <-  read_csv ( url_csv )",
            "title": "(from Importing Data from the Web)"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-datatable-package",
            "text": "fread  ==  read.table .  .txt files only.  Fast.    1 library ( data.table )    fread  for .txt files  1\n2\n3\n4\n5\n6\n7\n8 # Import potatoes.txt with fread(): potatoes \npotatoes  <-  fread ( 'potatoes.txt' )  # Print out arranged version of potatoes \npotatoes [ order ( moistness ),]   # Import 20 rows of potatoes.txt with fread(): potatoes_part \npotatoes_part  <-  fread ( 'potatoes.txt' ,  nrows  =   20 )    fread : more advanced use  1\n2\n3\n4\n5 # Import columns 6, 7 and 8 of potatoes.txt: potatoes \npotatoes  <-  fread ( 'potatoes.txt' ,  select  =   c ( 6 : 8 ))  # Keep only tasty potatoes (flavor > 3): tasty_potatoes \ntasty_potatoes  <-   subset ( potatoes ,  potatoes $ flavor  >   3 )",
            "title": "The data.table package"
        },
        {
            "location": "/IO_snippets___Cleaning/#importing-data-from-excel",
            "text": "",
            "title": "Importing Data from Excel"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-readxl-package",
            "text": "excel_sheets ; list.  read_excel ; import.  .xlsx files only.    1 library ( readxl )    List the sheets of an Excel file  1\n2\n3\n4\n5 # Find the names of both spreadsheets: sheets  # Before, find out what is in the directory with 'dir()' \nsheets  <-  excel_sheets ( 'latitude.xlsx' ) \n\nsheets   Importing an Excel sheet  1\n2\n3\n4\n5\n6\n7\n8 # Read the first sheet of latitude.xlsx: latitude_1 \nlatitude_1  <-  read_excel ( 'latitude.xlsx' ,  sheet  =   1 )  # Read the second sheet of latitude.xlsx: latitude_2 \nlatitude_2  <-  read_excel ( 'latitude.xlsx' ,  sheet  =   2 )  # Put latitude_1 and latitude_2 in a list: lat_list \nlat_list  <-   list ( latitude_1 ,  latitude_2 )    Reading a workbook  1\n2 # Read all Excel sheets with lapply(): lat_list \nlat_list  <-   lapply ( excel_sheets ( 'latitude.xlsx' ),  read_excel ,  path  =   'latitude.xlsx' )    The  col_names  argument  1\n2\n3\n4\n5 # Import the the first Excel sheet of latitude_nonames.xlsx (R gives names): latitude_3 \nlatitude_3  <-  read_excel ( 'latitude_nonames.xlsx' ,  sheet  =   1 ,  col_names  =   FALSE )  # Import the the second Excel sheet of latitude_nonames.xlsx (specify col_names): latitude_4  \nlatitude_4  <-  read_excel ( 'latitude_nonames.xlsx' ,  sheet  =   1 ,  col_names  =   c ( 'country' ,   'latitude' ))    The  skip  argument  1\n2 # Import the second sheet of latitude.xlsx, skipping the first 21 rows: latitude_sel \nlatitude_sel  <-  read_excel ( 'latitude.xlsx' ,  skip  =   21 ,  col_names  =   FALSE )",
            "title": "The readxl package"
        },
        {
            "location": "/IO_snippets___Cleaning/#from-importing-data-from-the-web_2",
            "text": "Import Excel files from the web  1\n2\n3\n4\n5 # Download file behind URL, name it local_latitude.xls \ndownload.file ( url_xls ,   'local_latitude.xls' )  # Import the local .xls file with readxl: excel_readxl \nexcel_readxl  <-  read_excel ( 'local_latitude.xls' )    Downloading any file, secure or not  1\n2\n3\n4\n5 # https URL to the wine RData file. \nurl_rdata  <-   'https://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/wine.RData'  # Download the wine file to your working directory \ndownload.file ( url_rdata ,   'wine_local.RData' )",
            "title": "(from Importing Data from the Web)"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-xlconnect-package",
            "text": "loadWorkbook .  getSheets .  readWorksheet .  readWorksheetFromFile  readNamedRegion   readNamedRegionFromFile    .xls & .xlsx files.   Like reading a database.    1\n2 library ( XLConnectJars )  library ( XLConnect )    Import a workbook  1\n2 # Build connection to latitude.xlsx: my_book \nmy_book  <-  loadWorkbook ( 'latitude.xlsx' )    List and read Excel sheets   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Build connection to latitude.xlsx \nmy_book  <-  loadWorkbook ( 'latitude.xlsx' )  # List the sheets in latitude.xlsx \ngetSheets ( my_book )  # Import the second sheet in latitude.xlsx \nreadWorksheet ( my_book ,  sheet  =   2 )  # Import the second column of the first sheet in latitude.xlsx \nreadWorksheet ( my_book ,  sheet  =   2 ,  startCol  =   2 )    Add and populate worksheets   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # Build connection to latitude.xlsx \nmy_book  <-  loadWorkbook ( 'latitude.xlsx' )  # Create data frame: summ \ndims1  <-   dim ( readWorksheet ( my_book ,   1 )) \ndims2  <-   dim ( readWorksheet ( my_book ,   2 )) \nsumm  <-   data.frame ( sheets  =  getSheets ( my_book ),  \n                   nrows  =   c ( dims1 [ 1 ],  dims2 [ 1 ]),  \n                   ncols  =   c ( dims1 [ 2 ],  dims2 [ 2 ]))  # Add a worksheet to my_book, named 'data_summary' \ncreateSheet ( my_book ,  name  =   'data_summary' )  # Populate 'data_summary' with summ data frame \nwriteWorksheet ( my_book ,  summ ,  sheet  =   'data_summary' )  # Save workbook as latitude_with_summ.xlsx \n\nsaveWorkbook ( my_book ,   'latitude_with_summ.xlsx' )    One unique function   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 # Read in the data set and assign to the object \nimpact  <-  readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' ,  sheet  =   'impact' ,  header  =   TRUE ,  startCol  =   1 ,  startRow  =   1 )  # more arguments  # endCol = 1  # endRow = 1  # autofitRow =   # autofitCol =   # region =  # rownames =  # colTypes =  # forceConversion =  # dateTimeFormat =  # check.names =  # useCachedValues =  # keep =  # drop =  # simplify =  # readStrategy =",
            "title": "The XLConnect package"
        },
        {
            "location": "/IO_snippets___Cleaning/#importing-data-from-other-statistical-software",
            "text": "",
            "title": "Importing Data from Other Statistical Software"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-haven-package",
            "text": "read_sas ; sas7bdat & sas7bcat files.  read_stata ; version; dta files.  read_dta ; idem.  read_spss ; sav & por files (and see below).  read_por .  read_sav .  Simple, few arguments.  Create a d.f.    1 library ( haven )    Import SAS data with haven  1\n2 # Import sales.sas7bdat: sales \nsales  <-  read_sas ( 'sales.sas7bdat' )    Import STATA data with haven  1\n2 # Import the data from the URL: sugar \nsugar  <-  read_dta ( 'http://assets.datacamp.com/course/importing_data_into_r/trade.dta' )    Import SPSS data with haven  1\n2\n3\n4\n5 # Specify the file path using file.path(): path \npath  <-   file.path ( 'datasets' ,   'person.sav' )  # Import person.sav, which is in the datasets folder: traits \ntraits  <-  read_sav ( path )    Factorize, round two  1\n2 # Import SPSS data from the URL: work \nwork  <-  read_sav ( 'http://assets.datacamp.com/course/importing_data_into_r/employee.sav' )",
            "title": "The haven package"
        },
        {
            "location": "/IO_snippets___Cleaning/#theforeign-package",
            "text": "Cannot import SAS, see the  sas7bdat  package.  read.dta ; dta files.  read.spss ; sav & por files.  Comprehensive.    1 library ( foreign )    Import STATA data with foreign (1)  1\n2 # Import florida.dta and name the resulting data frame florida \nflorida  <-  read.dta ( 'florida.dta' )    Import STATA data with foreign (2)   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Specify the file path using file.path(): path \npath  <-   file.path ( 'worldbank' ,   'edequality.dta' )  # Create and print structure of edu_equal_1 \nedu_equal_1  <-  read.dta ( path )  # Create and print structure of edu_equal_2 \nedu_equal_2  <-  read.dta ( path ,  convert.factors  =   FALSE )  # Create and print structure of edu_equal_3 \nedu_equal_3  <-  read.dta ( path ,  convert.underscore  =   TRUE )     Import SPSS data with foreign (1)  1\n2 # Import international.sav as a data frame: demo \ndemo  <-  read.spss ( 'international.sav' ,  to.data.frame  =   TRUE )    Import SPSS data with foreign (2)  1\n2\n3\n4\n5 # Import international.sav as demo_1 \ndemo_1  <-  read.spss ( 'international.sav' ,  to.data.frame  =   TRUE )  # Import international.sav as demo_2 \ndemo_2  <-  read.spss ( 'international.sav' ,  to.data.frame  =   TRUE ,  use.value.labels  =   FALSE )",
            "title": "Theforeign package"
        },
        {
            "location": "/IO_snippets___Cleaning/#importing-data-from-relational-data",
            "text": "",
            "title": "Importing Data from Relational Data"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-dbi-package",
            "text": "dbConnect .  dbReadTable .  dbGetQuery .  dbFetch .  dbDisconnect .    1 library ( DBI )    Step 1: Establish a connection  1\n2\n3\n4 # Connect to the MySQL database: con \ncon  <-  dbConnect ( RMySQL :: MySQL (),  dbname  =   'tweater' ,  host  =   'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' ,  port  =   3306 ,  user  =   'student' ,  password  =   'datacamp' )  \n\ncon   Step 2: List the database tables   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 # Connect to the MySQL database: con \ncon  <-  dbConnect ( RMySQL :: MySQL (),  \n                 dbname  =   'tweater' ,  \n                 host  =   'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' ,  \n                 port  =   3306 , \n                 user  =   'student' , \n                 password  =   'datacamp' )  # Build a vector of table names: tables \ntables  <-  dbListTables ( con )  # Display structure of tables \nstr ( tables )    Step 3: Import data from a table   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 # Connect to the MySQL database: con \ncon  <-  dbConnect ( RMySQL :: MySQL (),  \n                 dbname  =   'tweater' ,  \n                 host  =   'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' ,  \n                 port  =   3306 , \n                 user  =   'student' , \n                 password  =   'datacamp' )  # Import the users table from tweater: users \nusers  <-  dbReadTable ( con ,   'users' ) \n\nusers # Import and print the tweats table from tweater: tweats \ntweats  <-  dbReadTable ( con ,   'tweats' ) \n\ntweats # Import and print the comments table from tweater: comments \ncomments  <-  dbReadTable ( con ,   'comments' ) \n\ncomments   Your very first SQL query   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 con  <-  dbConnect ( RMySQL :: MySQL (),  \n                 dbname  =   'tweater' ,  \n                 host  =   'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' ,  \n                 port  =   3306 , \n                 user  =   'student' , \n                 password  =   'datacamp' )  # Import post column of tweats where date is higher than '2015-09-21': latest \nlatest  <-  dbGetQuery ( con ,   'SELECT post FROM tweats WHERE date > \\'2015-09-21\\'' ) \n\nlatest # Import tweat_id column of comments where user_id is 1: elisabeth \nelisabeth  <-  dbGetQuery ( con ,   'SELECT tweat_id FROM comments WHERE user_id = 1' ) \n\nelisabeth   More advanced SQL queries   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 # Connect to the database \ncon  <-  dbConnect ( RMySQL :: MySQL (), \n                 dbname  =   'tweater' ,  \n                 host  =   'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' ,  \n                 port  =   3306 , \n                 user  =   'student' , \n                 password  =   'datacamp' )  # Create data frame specific \nspecific  <-  dbGetQuery ( con ,   'SELECT message FROM comments WHERE tweat_id = 77 AND user_id > 4' ) \n\nspecific # Create data frame short \nshort  <-  dbGetQuery ( con ,   'SELECT id, name FROM users WHERE CHAR_LENGTH(name) < 5' ) \n\nshort   Send - Fetch - Clear   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 # Connect to the database \ncon  <-  dbConnect ( RMySQL :: MySQL (),  \n                 dbname  =   'tweater' ,  \n                 host  =   'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' ,  \n                 port  =   3306 , \n                 user  =   'student' , \n                 password  =   'datacamp' )  # Send query to the database with dbSendQuery(): res \nres  <-  dbSendQuery ( con ,   'SELECT * FROM comments WHERE user_id > 4' )  # Display information contained in res \ndbGetInfo ( res )  # Use dbFetch() twice  while   ( ! dbHasCompleted ( res ))   { \n    chunk  <-  dbFetch ( res ,  n  =   2 ) \n    chunk2  <-  dbFetch ( res ) \n     print ( chunk )  }  # Clear res \ndbClearResult ( res )    Be polite and \u2026   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # Database specifics \ndbname  <-   'tweater' \nhost  <-   'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' \nport  <-   3306 \nuser  <-   'student' \npassword  <-   'datacamp'  # Connect to the database \ncon  <-  dbConnect ( RMySQL :: MySQL (),  dbname  =   'tweater' ,  host  =   'courses.csrrinzqubik.us-east-1.rds.amazonaws.com' ,  port  =   3306   ,  user  =   'student' ,  password  =   'datacamp' )  # Create the data frame  long_tweats \nlong_tweats  <-  dbGetQuery ( con ,   'SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) > 40' )  # Print long_tweats  print ( long_tweats )  # Disconnect from the database \ndbDisconnect ( con )    Other general packages  The  RODBC  package provides access to databases (including Microsoft \nAccess and Microsoft SQL Server) through an ODBC interface.  The  RJDBC  package provides access to databases through a JDBC \ninterface.  Specialized packages   ROracle  provides an interface for Oracle.  RMySQL  provides access to MySQL.  RpostgreSQL  to PostgreSQL.  RSQLite  to SQLite.  And there are manu more packages for NoSQL databases such \n    as MongoDB.",
            "title": "The DBI package"
        },
        {
            "location": "/IO_snippets___Cleaning/#importing-data-from-relational-data-more",
            "text": "",
            "title": "Importing Data from Relational Data -- More"
        },
        {
            "location": "/IO_snippets___Cleaning/#dbi",
            "text": "First, change the working directory with  setwd . Install the  DBI \nlibrary.  Connect and read preliminary results   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 library ( DBI )  library ( sqliter )  # Assign the sqlite database and full path to a variable \ndbfile  =   'chinook.db'  # Instantiate the dbDriver to a convenient object \nsqlite  =  dbDriver ( 'SQLite' )  # Assign the connection string to a connection object \nsqlitedb  <-  dbConnect ( RSQLite :: SQLite (),  \n                 dbname  =  dbfile ,  \n                 host  =   '' ,  \n                 port  =   3306 , \n                 user  =   '' , \n                 password  =   '' )  # Request a list of tables using the connection object \ndbListTables ( sqlitedb )    Extract some data  1\n2\n3\n4\n5\n6\n7\n8 # Assign the results of a SQL query to an object \nresults  =  dbSendQuery ( sqlitedb ,   \"SELECT * FROM albums\" )  # Return results from a custom object to a data.frame \ndata  =  fetch ( results )  # Print data frame to console  head ( data )    1\n2\n3\n4\n5 # Clear the results and close the connection \ndbClearResult ( results )  # Disconnect from the database \ndbDisconnect ( sqlitedb )",
            "title": "DBI"
        },
        {
            "location": "/IO_snippets___Cleaning/#rsqlite",
            "text": "First, change the working directory with  setwd . Install the  RSQLite \nlibrary.  Connect and read preliminary results   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 library ( RSQLite )  library ( sqliter )  # Assign the sqlite database and full path to a variable \ndbfile  =   'chinook.db'  # Instantiate the dbDriver to a convenient object \nsqlite  =  dbDriver ( 'SQLite' )  # Assign the connection string to a connection object \nmysqldb  =  dbConnect ( sqlite ,  dbfile )  # Request a list of tables using the connection object \ndbListTables ( sqlitedb )    Extract some data   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # Assign the results of a SQL query to an object \nresults  =  dbSendQuery ( sqlitedb ,   \"SELECT * FROM albums\" )  # Check the object \nresults\ndbGetInfo ( results )  # Return results from a custom object to a data.frame \ndata  =  fetch ( results )  # Print data frame to console  head ( data )    1\n2\n3\n4\n5 # Clear the results and close the connection \ndbClearResult ( results )  # Disconnect from the database \ndbDisconnect ( sqlitedb )",
            "title": "RSQLite"
        },
        {
            "location": "/IO_snippets___Cleaning/#mysql-with-dbi-or-rmysql",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 library ( DBI )  # Assign the sqlite database and full path to a variable \ndbfile  =   'chinook.db'  # Instantiate the dbDriver to a convenient object \nmysql  =  dbDriver ( 'MySQL' )  # Assign the connection string to a connection object \nmysqldb  <-  dbConnect ( RMySQL :: MySQL (),  \n                 dbname  =  dbfile ,  \n                 host  =   '' ,  \n                 port  =   3306 , \n                 user  =   '' , \n                 password  =   '' )  # Request a list of tables using the connection object \ndbListTables ( mysqldb )  # Request a list of tables using the connection object \ndbListTables ( mysqldb )  # Disconnect from the database \ndbDisconnect ( mysqldb )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 library ( RMySQL )  # Assign the sqlite database and full path to a variable \ndbfile  =   'chinook.db'  # Instantiate the dbDriver to a convenient object \nmysql  =  dbDriver ( 'MySQL' )  # Assign the connection string to a connection object \nmysqldb  =  dbConnect ( mysql ,  dbfile )  # Request a list of tables using the connection object \ndbListTables ( mysqldb )  # Request a list of tables using the connection object \ndbListTables ( mysqldb )  # Disconnect from the database \ndbDisconnect ( mysqldb )",
            "title": "MySQL with DBI or RMySQL"
        },
        {
            "location": "/IO_snippets___Cleaning/#posgresql-with-dbi-or-rpostgresql",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 library ( DBI )  # Assign the sqlite database and full path to a variable \ndbfile  =   'chinook.db'  # Instantiate the dbDriver to a convenient object \npostgresql  =  dbDriver ( 'PostgreSQL' )  # Assign the connection string to a connection object \npostgresqldb  <-  dbConnect ( RPostgreSQL :: PostgreSQL (),  \n                 dbname  =  dbfile ,  \n                 host  =   '' ,  \n                 port  =   3306 , \n                 user  =   '' , \n                 password  =   '' )  # Request a list of tables using the connection object \ndbListTables ( postgresqldb )  # Request a list of tables using the connection object \ndbListTables ( postgresqldb )  # Disconnect from the database \ndbDisconnect ( postgresqldb )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 library ( RPostgreSQL )  # Assign the sqlite database and full path to a variable \ndbfile  =   'chinook.db'  # Instantiate the dbDriver to a convenient object \npostgresql  =  dbDriver ( 'PostgreSQL' )  # Assign the connection string to a connection object \npostgresqldb  =  dbConnect ( postgresql ,  dbfile )  # Request a list of tables using the connection object \ndbListTables ( postgresqldb )  # Request a list of tables using the connection object \ndbListTables ( postgresqldb )  # Disconnect from the database \ndbDisconnect ( postgresqldb )",
            "title": "PosgreSQL with DBI or RPostgreSQL"
        },
        {
            "location": "/IO_snippets___Cleaning/#importing-data-from-the-web",
            "text": "The other package above can download files from the web. The next \npackages are web-oriented.",
            "title": "Importing Data from the Web"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-httr-package",
            "text": "GET  pages and files from the web.  Concise.  Parse JSON files.  Communicate with APIs.    1 library ( httr )    HTTP?  httr !   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Get the url, save response to resp \nurl  <-   'http://docs.datacamp.com/teach/' \nresp  <-  GET ( url ) \n\nresp # Get the raw content of resp \nraw_content  <-  content ( resp ,  as  =   'raw' )  # Print the head of content  head ( raw_content )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 # Get the url \nurl  <-   'https://www.omdbapi.com/?t=Annie+Hall&y=&plot=short&r=json' \n\nresp  <-  GET ( url )  # Print resp \nresp # Print content of resp as text \ncontent ( resp ,  as  =   'text' )  # Print content of resp \ncontent ( resp )",
            "title": "The httr package"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-jsonlite-package",
            "text": "Robust.  Improve the imported data.  fromJSON .  from an R object to  toJSON  prettify .  minify .    1 library ( jsonlite )    From  JSON  to R   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Convert wine_json to a list: wine \nwine_json  <-   '{' name ':' Chateau Migraine ', ' year ':1997, ' alcohol_pct ':12.4, ' color ':' red ', ' awarded ':false}' \nwine  <-  fromJSON ( wine_json ) \n\nstr ( wine )  # Import Quandl data: quandl_data \nquandl_url  <-   'http://www.quandl.com/api/v1/datasets/IWS/INTERNET_INDIA.json?auth_token=i83asDsiWUUyfoypkgMz' \nquandl_data  <-  fromJSON ( quandl_url ) \n\nstr ( quandl_data )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 # Experiment 1 \njson1  <-   '[1, 2, 3, 4, 5, 6]' \nfromJSON ( json1 )  # Experiment 2 \njson2  <-   '{' a ': [1, 2, 3], ' b ': [4, 5, 6]}' \nfromJSON ( json2 )  # Experiment 3 \njson3  <-   '[[1, 2], [3, 4]]' \nfromJSON ( json3 )  # Experiment 4 \njson4  <-   '[{' a ': 1, ' b ': 2}, {' a ': 3, ' b ': 4}, {' a ': 5, ' b ': 6}]' \nfromJSON ( json4 )    Ask OMDb   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Definition of the URLs \nurl_sw4  <-   'http://www.omdbapi.com/?i=tt0076759&r=json' \nurl_sw3  <-   'http://www.omdbapi.com/?i=tt0121766&r=json'  # Import two URLs with fromJSON(): sw4 and sw3 \nsw4  <-  fromJSON ( url_sw4 ) \nsw3  <-  fromJSON ( url_sw3 )  # Print out the Title element of both lists \nsw4 $ Title\nsw3 $ Title # Is the release year of sw4 later than sw3 \nsw4 $ Year  >  sw3 $ Year   From R to  JSON   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 # URL pointing to the .csv file \nurl_csv  <-   'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/water.csv'  # Import the .csv file located at url_csv \nwater  <-  read.csv ( url_csv ,  stringsAsFactors  =   FALSE )  # Generate a summary of water  summary ( water )  # Convert the data file according to the requirements \nwater_json  <-  toJSON ( water ) \n\nwater_json   Minify  and  prettify   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Convert mtcars to a pretty JSON: pretty_json \npretty_json  <-  toJSON ( mtcars ,  pretty  =   TRUE )  # Print pretty_json \npretty_json # Minify pretty_json: mini_json \nmini_json  <-  minify ( pretty_json )  # Print mini_json \nmini_json",
            "title": "The jsonlite package"
        },
        {
            "location": "/IO_snippets___Cleaning/#keyboard-inputting",
            "text": "Coding  1\n2\n3\n4\n5 # create a data frame from scratch \nage  <-   c ( 25 ,   30 ,   56 ) \ngender  <-   c ( \"male\" ,   \"female\" ,   \"male\" ) \nweight  <-   c ( 160 ,   110 ,   220 ) \nmydata  <-   data.frame ( age , gender , weight )    Spreadsheet-like  1\n2\n3\n4\n5 # enter data using editor \nmydata  <-   data.frame ( age  =   numeric ( 0 ),  gender  =   character ( 0 ),  weight  =   numeric ( 0 )) \n\nmydata  <-  edit ( mydata )  # note that without the assignment in the line above, the edits are not saved!",
            "title": "Keyboard Inputting"
        },
        {
            "location": "/IO_snippets___Cleaning/#exporting-data",
            "text": "",
            "title": "Exporting Data"
        },
        {
            "location": "/IO_snippets___Cleaning/#to-a-tab-delimited-text-file",
            "text": "1 write.table ( mydata ,   'c:/mydata.txt' ,  sep  =   \"\\t\" )",
            "title": "To a Tab-Delimited Text File"
        },
        {
            "location": "/IO_snippets___Cleaning/#to-an-excel-spreadsheet",
            "text": "1\n2\n3 library ( xlsx ) \n\nwrite.xlsx ( mydata ,   \"c:/mydata.xlsx\" )    Worksheet   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 library ( XLConnect )  # xls or xlsx  # write a worksheet in steps \nwb  <-  loadWorkbook ( 'XLConnectExample1.xls' ,  create  =   TRUE ) \ncreateSheet ( wb ,  name  =   'chickSheet' ) \nwriteWorksheet ( wb ,  ChickWeight ,  sheet  =   'chickSheet' ,  startRow  =   3 ,  startCol  =   4 ) \nsaveWorkbook ( wb )  # write a worksheet all in one step \nChickWeight  <-   1 \n\nwriteWorksheetToFile ( 'XLConnectExample2.xlsx' ,  data  =  ChickWeight ,  sheet  =   'chickSheet' ,  startRow  =   3 ,  startCol  =   4 )    Field  1\n2\n3\n4\n5\n6\n7\n8\n9 # write a field in steps \nwb  =  loadWorkbook ( 'XLConnectExample3.xlsx' ,  create  =   TRUE ) \ncreateSheet ( wb ,  name  =   'womenData' ) \ncreateName ( wb ,  name  =   'womenName' ,  formula  =   'womenData!$C$5' ,  overwrite  =   TRUE ) \nwriteNamedRegion ( wb ,  women ,  name  =   \"womenName\" ) \nsaveWorkbook ( wb )  # write a field all in one step \nwriteNamedRegionToFile ( \"XLConnectExample4.xlsx\" ,  women ,  name  =   \"womenName\" ,  formula  =   \"womenData!$C$5\" )    I/O   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # Build connection to latitude.xlsx \nmy_book  <-  loadWorkbook ( 'latitude.xlsx' )  # Create data frame: summ \ndims1  <-   dim ( readWorksheet ( my_book ,   1 )) \ndims2  <-   dim ( readWorksheet ( my_book ,   2 )) \nsumm  <-   data.frame ( sheets  =  getSheets ( my_book ),  \n                   nrows  =   c ( dims1 [ 1 ],  dims2 [ 1 ]),  \n                   ncols  =   c ( dims1 [ 2 ],  dims2 [ 2 ]))  # Add a worksheet to my_book, named 'data_summary' \ncreateSheet ( my_book ,  name  =   'data_summary' )  # Populate 'data_summary' with summ data frame \nwriteWorksheet ( my_book ,  summ ,  sheet  =   'data_summary' )  # Save workbook as latitude_with_summ.xlsx \n\nsaveWorkbook ( my_book ,   'latitude_with_summ.xlsx' )",
            "title": "To an Excel Spreadsheet"
        },
        {
            "location": "/IO_snippets___Cleaning/#to-spss",
            "text": "1\n2\n3 library ( foreign ) \n\nwrite.foreign ( mydata ,   \"c:/mydata.txt\" ,   \"c:/mydata.sps\" ,  package  =   \"SPSS\" )",
            "title": "To SPSS"
        },
        {
            "location": "/IO_snippets___Cleaning/#to-sas",
            "text": "1\n2\n3 library ( foreign ) \n\nwrite.foreign ( mydata ,   \"c:/mydata.txt\" ,   \"c:/mydata.sas\" ,  package  =   \"SAS\" )",
            "title": "To SAS"
        },
        {
            "location": "/IO_snippets___Cleaning/#to-stata",
            "text": "1\n2\n3 library ( foreign ) \n\nwrite.dta ( mydata ,   \"c:/mydata.dta\" )",
            "title": "To Stata"
        },
        {
            "location": "/IO_snippets___Cleaning/#inspecting-data-missing-data",
            "text": "",
            "title": "Inspecting Data - Missing Data"
        },
        {
            "location": "/IO_snippets___Cleaning/#inspecting",
            "text": "ls(object) .  names(object) .  str(object) .  levels(object$v1) .  dim(object) .  class ( object ) .  print(object) .  head(object, 10) .  tail(object, 20) .   Testing for Missing Values  1\n2 y  <-   c ( 1 ,   2 ,   3 ,   NA )   # returns TRUE of x is missing  is.na ( y )   # returns a vector (F F F T)     Recoding Values to Missing  1\n2\n3 # recode 99 to missing for variable v1  # select rows where v1 is 99 and recode column v1 \nmydata $ v1 [ mydata $ v1  ==   99 ]   <-   NA     Excluding Missing Values from Analyses  1\n2\n3\n4 x  <-   c ( 1 ,   2 ,   NA ,   3 )  mean ( x )   # returns NA  mean ( x ,  na.rm  =   TRUE )   # returns 2     1\n2 # list rows of data that have missing values \nmydata [ ! complete.cases ( mydata ),]    1\n2 # create new dataset without missing data \nnewdata  <-  na.omit ( mydata )",
            "title": "Inspecting"
        },
        {
            "location": "/IO_snippets___Cleaning/#the-dplyr-package",
            "text": "1\n2\n3\n4\n5 library ( dplyr ) \n\ntbl_df ( iris )   # almost like head/tail \nglimpse ( iris )   # almost like str \nView ( iris )   # open a spreadsheet",
            "title": "The dplyr package"
        },
        {
            "location": "/IO_snippets___Cleaning/#for-thorough-cleaning",
            "text": "The  Amelia II  software.  The  mitools  package.",
            "title": "For thorough cleaning"
        },
        {
            "location": "/IO_snippets___Cleaning/#labels-levels",
            "text": "Basic  1\n2\n3\n4\n5 # variable v1 is coded 1, 2 or 3  # we want to attach value labels 1=red, 2=blue, 3=green \nmydata $ v1  <-   factor ( mydata $ v1 ,  \n                    levels  =   c ( 1 , 2 , 3 ), \n                    labels  =   c ( \"red\" ,   \"blue\" ,   \"green\" ))    1\n2\n3\n4\n5 # variable y is coded 1, 3 or 5  # we want to attach value labels 1=Low, 3=Medium, 5=High \nmydata $ v1  <-   ordered ( mydata $ y , \n                     levels  =   c ( 1 , 3 ,   5 ), \n                     labels  =   c ( \"Low\" ,   \"Medium\" ,   \"High\" ))     Order  1\n2\n3\n4\n5 # Create a vector of temperature observations \ntemperature_vector  <-   c ( 'High' ,   'Low' ,   'High' ,   'Low' ,   'Medium' )  # Specify that they are ordinal variables with the given levels \nfactor_temperature_vector  <-   factor ( temperature_vector ,  order  =   TRUE ,  levels  =   c ( 'Low' ,   'Medium' ,   'High' ))    Add comments to an object  1\n2\n3\n4 names ( iris )[ 5 ]   <-   \"This is the label for variable 5\"  names ( iris )[ 5 ]   # the comment \niris [ 5 ]   # the data    1\n2\n3\n4\n5\n6\n7\n8 # labeling the variables  library ( Hmisc ) \n\nlabel ( iris $ Species )   <-   \"Variable label for variable myvar\" \n\ndescribe ( iris $ Species )   # commented  #vs \ndescribe ( iris $ Sepal.Length )   # not commented",
            "title": "Labels &amp; Levels"
        },
        {
            "location": "/IO_snippets___Cleaning/#how-to-work-with-quandl-in-r",
            "text": "",
            "title": "How to work with Quandl in R"
        },
        {
            "location": "/IO_snippets___Cleaning/#importing-quandl-datasets",
            "text": "Quandl  delivers financial, economic and \nalternative data to the world\u2019s top hedge funds, asset managers and \ninvestment banks in several formats:   Excel.  R.  Python.  API.  DB.   The packages used:   Quandl .  quantmod  for plotting.   Quandl - A first date  1\n2\n3\n4\n5 # Load in the Quandl package  library ( Quandl )  # Assign your first dataset to the variable: \nmydata  <-  Quandl ( 'NSE/OIL' )    Identifying a dataset with its ID  1\n2 # Assign the Prague Stock Exchange to: \nPragueStockExchange  <-  Quandl ( 'PRAGUESE/PX' )    Plotting a stock chart  1\n2\n3\n4\n5\n6\n7\n8 # The quantmod package  library ( quantmod )  # Load the Facebook data with the help of Quandl \nFacebook  <-  Quandl ( 'GOOG/NASDAQ_FB' ,  type  =   'xts' )  # Plot the chart with the help of candleChart() \ncandleChart ( Facebook )    Searching a Quandl dataset in R  1\n2\n3\n4\n5\n6\n7\n8 # Look up the first 3 results for 'Bitcoin' within the Quandl database: \nresults  <-  Quandl.search ( query  =   'Bitcoin' ,  silent  =   FALSE )  # Print out the results \nstr ( results )  # Assign the data set with code BCHAIN/TOTBC \nBitCoin  <-  Quandl ( 'BCHAIN/TOTBC' )",
            "title": "Importing Quandl Datasets"
        },
        {
            "location": "/IO_snippets___Cleaning/#manipulating-quandl-datasets",
            "text": "Manipulating data  1\n2 # Assign to the variable Exchange \nExchange  <-  Quandl ( 'BNP/USDEUR' ,  start_date  =   '2013-01-01' ,  end_date  =   '2013-12-01' )    Transforming your Quandl dataset  1\n2\n3\n4\n5\n6 # API transformation  # The result: \nGDP_Change  <-  Quandl ( 'FRED/CANRGDPR' ,  transformation  =   'rdiff' )  head ( GDP_Change ) \nGDP_Chang  <-  Quandl ( 'FRED/CANRGDPR' )  head ( GDP_Chang )    The magic of frequency collapsing  1\n2 # The result: \neiaQuarterly  <-  Quandl ( 'DOE/RWTC' ,  collapse  =   'quarterly' )    Truncation and sort  1\n2\n3\n4\n5 # Assign to TruSo the first 5 observations of the crude oil prices \nTruSo  <-  Quandl ( 'DOE/RWTC' ,  sort  =   'asc' ,  rows  =   5 )  # Print the result \nTruSo   A complex example  1\n2 # Here you should place the return: \nFinal  <-  Quandl ( 'DOE/RWTC' ,  collapse  =   'daily' ,  transformation  =   'rdiff' ,  start_date  =   '2005-01-01' ,  end_date  =   '2010-03-01' ,  sort  =   'asc' )",
            "title": "Manipulating Quandl Datasets"
        },
        {
            "location": "/IO_snippets___Cleaning/#cleaning-data-in-r",
            "text": "The packages used:   dplyr  &  tidyr  for data wrangling.  stringr  for regex.  lubridate  for time and date.",
            "title": "Cleaning Data in R"
        },
        {
            "location": "/IO_snippets___Cleaning/#introduction-and-exploring-raw-data",
            "text": "Here\u2019s what messy data look like  1\n2\n3\n4\n5\n6\n7\n8 # View the first 6 rows of data  head ( weather )  # View the last 6 rows of data  tail ( weather )  # View a condensed summary of the data \nstr ( weather )    Getting a feel for your data  1\n2\n3\n4\n5\n6\n7\n8 # Check the class of bmi  class ( bmi )  # Check the dimensions of bmi  dim ( bmi )  # View the column names of bmi  names ( bmi )    Viewing the structure of your data   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Check the structure of bmi \nstr ( bmi )  # Load dplyr  library ( dplyr )  # Check the structure of bmi, the dplyr way \nglimpse ( bmi )  # View a summary of bmi  summary ( bmi )    Looking at your data   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Print bmi to the console  print ( bmi )  # View the first 6 rows  head ( bmi ,   6 )  # View the first 15 rows  head ( bmi ,   15 )  # View the last 6 rows  tail ( bmi ,   6 )  # View the last 10 rows  tail ( bmi ,   10 )    Visualizing your data  1\n2\n3\n4\n5 # Histogram of BMIs from 2008 \nhist ( bmi $ Y2008 )  # Scatter plot comparing BMIs from 1980 to those from 2008 \nplot ( bmi $ Y1980 ,  bmi $ Y2008 )",
            "title": "Introduction and Exploring Raw Data"
        },
        {
            "location": "/IO_snippets___Cleaning/#tidying-data",
            "text": "Gathering columns into key-value pairs  1\n2\n3\n4\n5\n6\n7\n8 # Load tidyr  library ( tidyr )  # Apply gather() to bmi and save the result as bmi_long \nbmi_long  <-  gather ( bmi ,  year ,  bmi_val ,   - Country )  # View the first 20 rows of the result  head ( bmi_long ,   20 )    Spreading key-value pairs into columns  1\n2\n3\n4\n5 # Apply spread() to bmi_long \nbmi_wide  <-  spread ( bmi_long ,  year ,  bmi_val )  # View the head of bmi_wide  head ( bmi_wide )    Separating columns  1\n2\n3\n4\n5 # Apply separate() to bmi_cc \nbmi_cc_clean  <-  separate ( bmi_cc ,  col  =  Country_ISO ,  into  =   c ( 'Country' ,   'ISO' ),  sep  =   '/' )  # Print the head of the result  head ( bmi_cc_clean )    Uniting columns  1\n2\n3\n4\n5 # Apply unite() to bmi_cc_clean \nbmi_cc  <-  unite ( bmi_cc_clean ,  Country_ISO ,  Country ,  ISO ,  sep  =   '-' )  # View the head of the result  head ( bmi_cc )    Column headers are values, not variable names   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # View the head of census  head ( census )  # Gather the month columns \ncensus2  <-  gather ( census ,  month ,  amount ,  JAN ,  FEB ,  MAR ,  APR ,  MAY ,  JUN ,  JUL ,  AUG ,  SEP ,  OCT ,  NOV ,  DEC )  # Arrange rows by YEAR using dplyr's arrange \ncensus2  <-  arrange ( census2 ,  YEAR )  # View first 20 rows of census2  head ( census2 ,   20 )    Variables are stored in both rows and columns  1\n2\n3\n4\n5\n6\n7\n8 # View first 50 rows of census_long  head ( census_long ,   50 )  # Spread the type column \ncensus_long2  <-  spread ( census_long ,  type ,  amount )  # View first 20 rows of census_long2  head ( census_long2 ,   20 )    Multiple values are stored in one column  1\n2\n3\n4\n5\n6\n7\n8 # View the head of census_long3  head ( census_long3 )  # Separate the yr_month column into two \ncensus_long4  <-  separate ( census_long3 ,  yr_month ,   c ( 'year' ,   'month' ),   '_' )  # View the first 6 rows of the result  head ( census_long4 ,   6 )",
            "title": "Tidying Data"
        },
        {
            "location": "/IO_snippets___Cleaning/#preparing-data-for-analysis",
            "text": "Types of variables in R   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Make this evaluate to character  class ( 'true' )  # Make this evaluate to numeric  class ( 8484.00 )  # Make this evaluate to integer  class ( 99L )  # Make this evaluate to factor  class ( factor ( 'factor' ))  # Make this evaluate to logical  class ( FALSE )    Common type conversions   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Preview students with str() \nstr ( students )  # Coerce Grades to character \nstudents $ Grades  <-   as.character ( students $ Grades )  # Coerce Medu to factor \nstudents $ Medu  <-   as.factor ( students $ Medu )  # Coerce Fedu to factor \nstudents $ Fedu  <-   as.factor ( students $ Fedu ) \n\n  # Look at students once more with str() \nstr ( students )    Working with dates   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 # Preview students2 with str() \nstr ( students2 )  # Load the lubridate package  library ( lubridate )  # Parse as date \nymd ( '2015-Sep-17' )  # Parse as date and time (with no seconds!) \nymd_hm ( '2012-July-15, 12.56' )  # Coerce dob to a date (with no time) \nstudents2 $ dob  <-  ymd ( students2 $ dob )  # Coerce nurse_visit to a date and time \nstudents2 $ nurse_visit  <-  ymd_hms ( students2 $ nurse_visit )  # Look at students2 once more with str() \nstr ( students2 )    Trimming and padding strings  1\n2\n3\n4\n5\n6\n7\n8 # Load the stringr package  library ( stringr )  # Trim all leading and trailing whitespace \nstr_trim ( c ( '   Filip ' ,   'Nick  ' ,   ' Jonathan' ))  # Pad these strings with leading zeros \nstr_pad ( c ( '23485W' ,   '8823453Q' ,   '994Z' ),  width  =   9 ,  side  =   'left' ,  pad  =   '0' )    Upper and lower case  1\n2\n3\n4\n5\n6\n7\n8\n9 # Print state abbreviations \nstates # Make states all uppercase and save result to states_upper \nstates_upper  <-   toupper ( states ) \nstates_upper # Make states_upper all lowercase again  tolower ( states_upper )    Finding and replacing strings   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 # stringr has been loaded for you  # Look at the head of students2  head ( students2 )  # Detect all dates of birth (dob) in 1997 \nstr_detect ( students2 $ dob ,   '1997' )  # In the sex column, replace 'F' with 'Female'... \nstudents2 $ sex  <-  str_replace ( students2 $ sex ,   'F' ,   'Female' )  # ...And 'M' with 'Male' \nstudents2 $ sex  <-  str_replace ( students2 $ sex ,   'M' ,   'Male' )  # View the head of students2  head ( students2 )    Finding missing values   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # Call is.na() on the full social_df to spot all NAs  is.na ( social_df )  # Use the any() function to ask whether there are any NAs in the data  any ( is.na ( social_df ))  sum ( is.na ( social_df ))  # View a summary() of the dataset  summary ( social_df )  # Call table() on the status column  table ( social_df $ status )    Dealing with missing values   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Use str_replace() to replace all missing strings in status with NA \nsocial_df $ status  <-  str_replace ( social_df $ status ,   '^$' ,   NA )  # Print social_df to the console \nsocial_df # Use complete.cases() to see which rows have no missing values \ncomplete.cases ( social_df )  # Use na.omit() to remove all rows with any missing values \nna.omit ( social_df )    Dealing with outliers and obvious errors   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Look at a summary() of students3  summary ( students3 )  # View a histogram of the age variable \nhist ( students3 $ age ,  breaks  =   20 )  # View a histogram of the absences variable \nhist ( students3 $ absences ,  breaks  =   20 )  # View a histogram of absences, but force zeros to be bucketed to the right of zero \nhist ( students3 $ absences ,  breaks  =   20 ,  right  =   FALSE )    Another look at strange values  1\n2\n3\n4\n5 # View a boxplot of age \nboxplot ( students3 $ age )  # View a boxplot of absences \nboxplot ( students3 $ absences )",
            "title": "Preparing Data for Analysis"
        },
        {
            "location": "/IO_snippets___Cleaning/#putting-it-all-together",
            "text": "Get a feel for the data  1\n2\n3\n4\n5\n6\n7\n8 # Verify that weather is a data.frame  class ( weather )  # Check the dimensions  dim ( weather )  # View the column names  names ( weather )    Summarize the data   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # View the structure of the data \nstr ( weather )  # Load dplyr package  library ( dplyr )  # Look at the structure using dplyr's glimpse() \nglimpse ( weather )  # View a summary of the data  summary ( weather )    Take a closer look   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # View first 6 rows  head ( weather ,   6 )  # View first 15 rows  head ( weather ,   15 )  # View the last 6 rows  tail ( weather ,   6 )  # View the last 10 rows  tail ( weather ,   10 )    Column names are values  1\n2\n3\n4\n5\n6\n7\n8 # Load the tidyr package  library ( tidyr )  # Gather the columns \nweather2  <-  gather ( weather ,  day ,  value ,  X1 : X31 ,  na.rm  =   TRUE )  # View the head  head ( weather2 )    Values are variable names  1\n2\n3\n4\n5\n6\n7\n8 # First remove column of row names \nweather2  <-  weather2 [,   -1 ]  # Spread the data \nweather3  <-  spread ( weather2 ,  measure ,  value )  # View the head  head ( weather3 )    Clean up dates   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 # Load the stringr and lubridate packages  library ( stringr )  library ( lubridate )  # Remove X's from day column  # weather3$day <- str_pad(str_replace(weather3$day, 'X', ''), width = 2, side = 'left', pad = '0') \nweather3 $ day  <-  str_replace ( weather3 $ day ,   'X' ,   '' )  # Unite the year, month, and day columns \nweather4  <-  unite ( weather3 ,   date ,  year ,  month ,  day ,  sep  =   '-' )  # Convert date column to proper date format using stringr's ymd() \nweather4 $ date  <-  ymd ( weather4 $ date )  # Rearrange columns using dplyr's select() \nweather5  <-  select ( weather4 ,   date ,  Events ,  CloudCover : WindDirDegrees )  # View the head  head ( weather5 )    A closer look at column types  1\n2\n3\n4\n5\n6\n7\n8 # View the structure of weather5 \nstr ( weather5 )  # Examine the first 20 rows of weather5. Are most of the characters numeric?  head ( weather5 ,   20 )  # See what happens if we try to convert PrecipitationIn to numeric  as.numeric ( weather5 $ PrecipitationIn )    Column type conversions  1\n2\n3\n4\n5\n6\n7\n8\n9 # The dplyr package is already loaded  # Replace T with 0 (T = trace) \nweather5 $ PrecipitationIn  <-  str_replace ( weather5 $ PrecipitationIn ,   'T' ,   '0' )  # Convert characters to numerics \nweather6  <-  mutate_each ( weather5 ,  funs ( as.numeric ),  CloudCover : WindDirDegrees )  # Look at result \nstr ( weather6 )    Find missing values   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # Count missing values  sum ( is.na ( weather6 ))  # Find missing values  summary ( weather6 )  # Find indices of NAs in Max.Gust.SpeedMPH \nind  <-   which ( is.na ( weather6 $ Max.Gust.SpeedMPH )) \nind # Look at the full rows for records missing Max.Gust.SpeedMPH \nweather6 [ ind ,   ]    An obvious error   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Review distibutions for all variables  summary ( weather6 )  # Find row with Max.Humidity of 1000 \nind  <-   which ( weather6 $ Max.Humidity  ==   1000 )  # Look at the data for that day \nweather6 [ ind ,   ]  # Change 1000 to 100 \nweather6 $ Max.Humidity [ ind ]   <-   100    Another obvious error   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Look at summary of Mean.VisibilityMiles  summary ( weather6 $ Mean.VisibilityMiles )  # Get index of row with -1 value \nind  <-   which ( weather6 $ Mean.VisibilityMiles  ==   -1 )  # Look at full row \nweather6 [ ind ,   ]  # Set Mean.VisibilityMiles to the appropriate value \nweather6 $ Mean.VisibilityMiles [ ind ]   <-   10    Check other extreme values   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Review summary of full data once more  summary ( weather6 )  # Look at histogram for MeanDew.PointF \nhist ( weather6 $ MeanDew.PointF )  # Look at histogram for Min.TemperatureF \nhist ( weather6 $ Min.TemperatureF )  # Compare to histogram for Mean.TemperatureF \nhist ( weather6 $ Mean.TemperatureF )    Finishing touches  1\n2\n3\n4\n5\n6\n7\n8 # Clean up column names  names ( weather6 )   <-  new_colnames # Replace empty cells in Events column \nweather6 $ events [ weather6 $ events  ==   '' ]   <-   'None'  # Print the first 6 rows of weather6  head ( weather6 ,   6 )",
            "title": "Putting it All Together"
        },
        {
            "location": "/Reading_Data_into_R_with_readr/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nImporting data with \nreadr\n\u00b6\n\n\nReading a .csv file\n\n\nYour first task will be to master the use of the \nread_csv()\n function. There are many arguments available, but the only required argument is \nfile\n, a path to a CSV file on your computer (or the web).\n\n\nOne big advantage that \nread_csv()\n has over \nread.csv()\n is that it doesn\u2019t convert strings into factors by default.\n\n\nread_csv()\n recognizes 8 different data types (integer, logical, etc.) and leaves anything else as characters. That means you don\u2019t have to set \nstringsAsFactors = FALSE\n every time you import a CSV file with character strings!\n\n\n1\n2\n3\n4\n#install.packages('readr')\n\n\nlibrary\n(\nreadr\n)\n\n\n\ngetwd\n()\n\n\n\n\n\n\n\n1\n## [1] \"D:/.../Rprojects/Data Wrangling\"\n\n\n\n\n\n\n1\nsetwd\n(\n\"D:/.../Rprojects/Data Wrangling\"\n)\n\n\n\n\n\n\n\nImport .csv (only \u2018,\u2019).\n\n\n1\n2\n3\n4\n5\n# Import chickwts.csv: cwts\n\ncwts \n<-\n read_csv\n(\n'chickwts.csv'\n)\n\n\n\n# View the head of cwts\n\n\nhead\n(\ncwts\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## # A tibble: 6 \u00d7 2\n##   weight      feed\n##    <int>     <chr>\n## 1    179 horsebean\n## 2    160 horsebean\n## 3    136 horsebean\n## 4    227 horsebean\n## 5    217 horsebean\n## 6    168 horsebean\n\n\n\n\n\n\nReading a (.txt) .tsv file\n\n\nSkipping columns with \ncol_skip()\n.\n\n\nCode only:\n\n\n\n\nSetting the column type.\n\n\n\n\n\n\n\n1\n2\n3\n4\ncols\n(\n\n  weight \n=\n col_integer\n(),\n\n  feed \n=\n col_character\n()\n\n\n)\n\n\n\n\n\n\n\n\n\nSetting the column names.\n\n\n\n\n\n\n\n1\ncol_names \n=\n \nc\n(\n'name'\n,\n \n'state'\n,\n \n'phone'\n)\n\n\n\n\n\n\n\n\n\nRemoving NA.\n\n\n\n\n\n\n\n1\nna \n=\n \nc\n(\n'NA'\n,\n \n'null'\n)\n\n\n\n\n\n\n\nIn practice.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Import data\n\nsalaries \n<-\n read_tsv\n(\n'Salaries.txt'\n,\n col_names \n=\n \nFALSE\n,\n col_types \n=\n cols\n(\n\n  X2 \n=\n col_skip\n(),\n\n  X3 \n=\n col_skip\n(),\n \n  X4 \n=\n col_skip\n()\n\n\n))\n\n\n\n# View first six rows of salaries\n\n\nhead\n(\nsalaries\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## # A tibble: 6 \u00d7 3\n##          X1    X5     X6\n##       <chr> <chr>  <int>\n## 1      Prof  Male 139750\n## 2      Prof  Male 173200\n## 3  AsstProf  Male  79750\n## 4      Prof  Male 115000\n## 5      Prof  Male 141500\n## 6 AssocProf  Male  97000\n\n\n\n\n\n\nReading a European .csv\n\n\nIn most of Europe, commas (rather than periods) are used as  decimal points.\n\n\n1\n2\n3\n4\n5\n# Import data with read_csv2(): trees\n\ntrees \n<-\n read_csv2\n(\n'trees.csv'\n)\n\n\n\n# View dimensions and head of trees\n\n\ndim\n(\ntrees\n)\n\n\n\n\n\n\n\n1\n## [1] 9 3\n\n\n\n\n\n\n1\nhead\n(\ntrees\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## # A tibble: 6 \u00d7 3\n##   Girth Height Volume\n##   <dbl>  <int>  <dbl>\n## 1    83     70    103\n## 2    86     65    103\n## 3    88     63    102\n## 4   105     72    164\n## 5   107     81    188\n## 6   108     83    197\n\n\n\n\n\n\nRead a fixed-width file\n\n\nFiles containing columns of data that are separated by whitespace and all line up on one side.\n\n\nCode only:\n\n\n1\n2\n# Import names.txt: names\n\nnames \n<-\n read_table\n(\n'names.txt'\n,\n col_names \n=\n \nc\n(\n'name'\n,\n \n'state'\n,\n \n'phone'\n),\n na \n=\n \nc\n(\n'NA'\n,\n \n'null'\n))\n\n\n\n\n\n\n\nReading a text file\n\n\nImport ordinary text files.\n\n\n1\n2\n3\n4\n# vector of character strings. \n\n\n# Import as a character vector, one item per line: tweets\n\ntweets \n<-\n read_lines\n(\n'tweets.txt'\n)\n\ntweets\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n## [1] \"carrots  can be eat by most people\"                                                                                          \n## [2] \"On predisents day we honor the big US man himself: Aberham Liclon.   Tall, skinny, dry, and cruncy - he was america's carrot\"\n## [3] \"knock knoc who is there? yup: carosot   ( joke )\"                                                                            \n## [4] \"it is 2016 time for a carot emoji   please!\"                                                                                 \n## [5] \"when life give you lemnos ,  have a carrot\"                                                                                  \n## [6] \"If you squent your eyes real hard a football  look like  a dry brown carrot   Honestly\"\n\n\n\n\n\n\n1\n2\n3\n4\n# returns a length 1 vector of the entire file, with line breaks represented as \\n\n\n\n# Import as a length 1 vector: tweets_all\n\ntweets_all \n<-\n read_file\n(\n'tweets.txt'\n)\n\ntweets_all\n\n\n\n\n\n\n1\n## [1] \"carrots  can be eat by most people\\r\\nOn predisents day we honor the big US man himself: Aberham Liclon.   Tall, skinny, dry, and cruncy - he was america's carrot\\r\\nknock knoc who is there? yup: carosot   ( joke )\\r\\nit is 2016 time for a carot emoji   please!\\r\\nwhen life give you lemnos ,  have a carrot\\r\\nIf you squent your eyes real hard a football  look like  a dry brown carrot   Honestly\"\n\n\n\n\n\n\nWriting .csv and .tsv files\n\n\nCode only:\n\n\n1\n2\n3\n4\n5\n# Save cwts as chickwts.csv\n\nwrite_csv\n(\ncwts\n,\n \n\"chickwts.csv\"\n)\n\n\n\n# Append cwts2 to chickwts.csv\n\nwrite_csv\n(\ncwts2\n,\n \n\"chickwts.csv\"\n,\n append \n=\n \nTRUE\n)\n\n\n\n\n\n\n\nWriting .rds files\n\n\nIf the R object you\u2019re working with has metadata associated with it, saving to a CSV will cause that information to be lost.\n\n\nExports an entire R object (metadata and all).\n\n\nCode only:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Save trees as trees.rds\n\nwrite_rds\n(\ntrees\n,\n \n'trees.rds'\n)\n\n\n\n# Import trees.rds: trees2\n\ntrees2 \n<-\n read_rds\n(\n'trees.rds'\n)\n\n\n\n# Check whether trees and trees2 are the same\n\n\nidentical\n(\ntrees\n,\n trees2\n)\n\n\n\n\n\n\n\nParsing Data with \nreadr\n\u00b6\n\n\nCoercing columns to different data types\n\n\nreadr\n functions are quite good at guessing the correct data type for each column in a dataset. Of course, they aren\u2019t perfect, so sometimes you will need to change the type of a column after importing.\n\n\nCode only:\n\n\n1\n2\n# Convert all columns to double\n\ntrees2 \n<-\n type_convert\n(\ntrees\n,\n col_types \n=\n cols\n(\nGirth \n=\n \n'd'\n,\n Height \n=\n \n'd'\n,\n Volume \n=\n \n'd'\n))\n\n\n\n\n\n\n\nCoercing character columns into factors\n\n\nreadr\n import functions is that they don\u2019t automatically convert strings into factors like \nread.csv\n does.\n\n\nCode only:\n\n\n1\n2\n3\n4\n5\n# Parse the title column\n\nsalaries\n$\ntitle \n<-\n parse_factor\n(\nsalaries\n$\ntitle\n,\n levels \n=\n \nc\n(\n'Prof'\n,\n \n'AsstProf'\n,\n \n'AssocProf'\n))\n\n\n\n# Parse the gender column\n\nsalaries\n$\ngender \n<-\n parse_factor\n(\nsalaries\n$\ngender\n,\n levels \n=\n \nc\n(\n'Male'\n,\n \n'Female'\n))\n\n\n\n\n\n\n\nCreating Date objects\n\n\nThe \nreadr\n import functions can automatically recognize dates in standard ISO 8601 format (YYYY-MM-DD) and parse columns accordingly. If you want to import a dataset with dates in other formats, you can use \nparse_date\n.\n\n\nCode only:\n\n\n1\n2\n# Change type of date column\n\nweather\n$\ndate \n<-\n parse_date\n(\nweather\n$\ndate\n,\n format \n=\n \n'%m/%d/%Y'\n)\n\n\n\n\n\n\n\nParsing number formats\n\n\nThe \nreadr\n importing functions can sometimes run into trouble parsing a column as numbers when it contains non-numeric symbols in addition to numerals.\n\n\nCode only:\n\n\n1\n2\n# Parse amount column as a number\n\ndebt\n$\namount \n<-\n parse_number\n(\ndebt\n$\namount\n)\n\n\n\n\n\n\n\nViewing metadata before importing\n\n\nIn some cases, it may be easier to get an idea of how \nreadr\n plans to parse a dataset before you actually import it. When you see the planned column specification, you might decide to change the type of one or more columns, for example.\n\n\n\n\nspec_csv\n for .csv and .tsv files.\n\n\nspec_delim\n for .txt files (among others).\n\n\n\n\n\n\n\n1\n2\n# Specifications of chickwts\n\nspec_csv\n(\n'chickwts.csv'\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n## cols(\n##   weight = col_integer(),\n##   feed = col_character()\n## )",
            "title": "Reading Data into R with readr"
        },
        {
            "location": "/Reading_Data_into_R_with_readr/#parsing-data-with-readr",
            "text": "Coercing columns to different data types  readr  functions are quite good at guessing the correct data type for each column in a dataset. Of course, they aren\u2019t perfect, so sometimes you will need to change the type of a column after importing.  Code only:  1\n2 # Convert all columns to double \ntrees2  <-  type_convert ( trees ,  col_types  =  cols ( Girth  =   'd' ,  Height  =   'd' ,  Volume  =   'd' ))    Coercing character columns into factors  readr  import functions is that they don\u2019t automatically convert strings into factors like  read.csv  does.  Code only:  1\n2\n3\n4\n5 # Parse the title column \nsalaries $ title  <-  parse_factor ( salaries $ title ,  levels  =   c ( 'Prof' ,   'AsstProf' ,   'AssocProf' ))  # Parse the gender column \nsalaries $ gender  <-  parse_factor ( salaries $ gender ,  levels  =   c ( 'Male' ,   'Female' ))    Creating Date objects  The  readr  import functions can automatically recognize dates in standard ISO 8601 format (YYYY-MM-DD) and parse columns accordingly. If you want to import a dataset with dates in other formats, you can use  parse_date .  Code only:  1\n2 # Change type of date column \nweather $ date  <-  parse_date ( weather $ date ,  format  =   '%m/%d/%Y' )    Parsing number formats  The  readr  importing functions can sometimes run into trouble parsing a column as numbers when it contains non-numeric symbols in addition to numerals.  Code only:  1\n2 # Parse amount column as a number \ndebt $ amount  <-  parse_number ( debt $ amount )    Viewing metadata before importing  In some cases, it may be easier to get an idea of how  readr  plans to parse a dataset before you actually import it. When you see the planned column specification, you might decide to change the type of one or more columns, for example.   spec_csv  for .csv and .tsv files.  spec_delim  for .txt files (among others).    1\n2 # Specifications of chickwts \nspec_csv ( 'chickwts.csv' )    1\n2\n3\n4 ## cols(\n##   weight = col_integer(),\n##   feed = col_character()\n## )",
            "title": "Parsing Data with readr"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nDocumentation\n\u00b6\n\n\ndata.table\n\n\n\n\nExtension of \ndata.frame\n.\n\n\nFast aggregation of large data (e.g. 100GB in RAM), fast ordered     joins, fast add/modify/delete of columns by group using no copies at     all, list columns, a fast friendly file reader and parallel file writer. Offers a natural and flexible syntax, for faster development.\n\n\n\n\ndplyr\n\n\n\n\nA fast, consistent tool for working with data frame like objects, both in memory and out of memory.\n\n\nPipelines.\n\n\n\n\ntidyr\n\n\n\n\nAn evolution of \u2018reshape2\u2019. It\u2019s designed specifically for data tidying (not general reshaping or aggregating) and works well with dplyr data pipelines.\n\n\n\n\n\n\n\n\n\n\npackage\n\n\n'narrower'\n\n\n'wider'\n\n\n\n\n\n\n\n\n\n\ntidyr\n\n\ngather\n\n\nspread\n\n\n\n\n\n\nreshape2\n\n\nmelt\n\n\ncast\n\n\n\n\n\n\nspreadsheets\n\n\nunpivot\n\n\npivot\n\n\n\n\n\n\ndatabases\n\n\nfold\n\n\nunfold\n\n\n\n\n\n\n\n\n\ndata.table\n novice\n\u00b6\n\n\nFind out more with \n?data.table\n.\n\n\nCreate and subset a \ndata.table\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# The data.table package\n\n\nlibrary\n(\ndata.table\n)\n\n\n\n# Create my_first_data_table\n\nmy_first_data_table \n<-\n data.table\n(\nx \n=\n \nc\n(\n'a'\n,\n \n'b'\n,\n \n'c'\n,\n \n'd'\n,\n \n'e'\n),\n y \n=\n \nc\n(\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n))\n\n\n\n# Create a data.table using recycling\n\nDT \n<-\n data.table\n(\na \n=\n \n1\n:\n2\n,\n b \n=\n \nc\n(\n'A'\n,\n \n'B'\n,\n \n'C'\n,\n \n'D'\n))\n\n\n\n# Print the third row to the console\n\nDT\n[\n3\n,]\n\n\n\n\n\n\n\n1\n2\n##    a b\n## 1: 1 C\n\n\n\n\n\n\n1\n2\n# Print the second and third row to the console, but do not commas\n\nDT\n[\n2\n:\n3\n]\n\n\n\n\n\n\n\n1\n2\n3\n##    a b\n## 1: 2 B\n## 2: 1 C\n\n\n\n\n\n\nGetting to know a \ndata.table\n\u00b6\n\n\nLike \nhead\n, \ntail\n.\n\n\n1\n2\n# Print the penultimate row of DT using .N\n\nDT\n[\n.\nN \n-\n \n1\n]\n\n\n\n\n\n\n\n1\n2\n##    a b\n## 1: 1 C\n\n\n\n\n\n\n1\n2\n# Print the column names of DT, and number of rows and number of columns\n\n\ncolnames\n(\nDT\n)\n\n\n\n\n\n\n\n1\n## [1] \"a\" \"b\"\n\n\n\n\n\n\n1\ndim\n(\nDT\n)\n\n\n\n\n\n\n\n1\n## [1] 4 2\n\n\n\n\n\n\n1\n2\n# Select row 2 twice and row 3, returning a data.table with three rows where row 2 is a duplicate of row 1.\n\nDT\n[\nc\n(\n2\n,\n \n2\n,\n \n3\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n##    a b\n## 1: 2 B\n## 2: 2 B\n## 3: 1 C\n\n\n\n\n\n\nDT\n is a data.table/data.frame, but \nDT[ , B]\n is a vector; \nDT[ , .(B)]\n is a subsetted data.table.\n\n\nSubsetting data tables\n\u00b6\n\n\nDT[i, j, by]\n means take \nDT\n, subset rows using \ni\n, then calculate \nj\n grouped by \nby\n. You can wrap \nj\n with \n.()\n.\n\n\n1\n2\n3\n4\n5\n6\n7\nA \n<-\n \nc\n(\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n)\n\nB \n<-\n \nc\n(\n'a'\n,\n \n'b'\n,\n \n'c'\n,\n \n'd'\n,\n \n'e'\n)\n\nC \n<-\n \nc\n(\n6\n,\n \n7\n,\n \n8\n,\n \n9\n,\n \n10\n)\n\nDT \n<-\n data.table\n(\nA\n,\n B\n,\n C\n)\n\n\n\n# Subset rows 1 and 3, and columns B and C\n\nDT\n[\nc\n(\n1\n,\n3\n)\n \n,\n.\n(\nB\n,\n C\n)]\n\n\n\n\n\n\n\n1\n2\n3\n##    B C\n## 1: a 6\n## 2: c 8\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# Assign to ans the correct value\n\nans \n<-\n data.table\n(\nDT\n[,\n \n.\n(\nB\n,\n val \n=\n A \n*\n C\n)])\n\n\n\n# Fill in the blanks such that ans2 equals target\n\n\n#target <- data.table(B = c('a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e'),  val = as.integer(c(6:10, 1:5)))\n\nans2 \n<-\n data.table\n(\nDT\n[,\n \n.\n(\nB\n,\n val \n=\n \nas.integer\n(\nc\n(\n6\n:\n10\n,\n \n1\n:\n5\n)))])\n\n\n\n\n\n\n\nThe \nby\n basics\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n# iris and iris3 are already available in the workspace\n\n\n\n# Convert iris to a data.table: DT\n\nDT \n<-\n as.data.table\n(\niris\n)\n\n\n\n# For each Species, print the mean Sepal.Length\n\nDT\n[,\n \n.\n(\nmean\n(\nSepal.Length\n)),\n by \n=\n \n.\n(\nSpecies\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n##       Species    V1\n## 1:     setosa 5.006\n## 2: versicolor 5.936\n## 3:  virginica 6.588\n\n\n\n\n\n\n1\n2\n# Print mean Sepal.Length, grouping by first letter of Species\n\nDT\n[,\n \n.\n(\nmean\n(\nSepal.Length\n)),\n by \n=\n \n.\n(\nsubstr\n(\nSpecies\n,\n \n1\n,\n1\n))]\n\n\n\n\n\n\n\n1\n2\n3\n##    substr    V1\n## 1:      s 5.006\n## 2:      v 6.262\n\n\n\n\n\n\nUsing \n.N\n and \nby\n\u00b6\n\n\n.N\n, number, in row or column.\n\n\n1\n2\n3\n4\n5\n# data.table version of iris: DT\n\nDT \n<-\n as.data.table\n(\niris\n)\n\n\n\n# Group the specimens by Sepal area (to the nearest 10 cm2) and count how many occur in each group.\n\nDT\n[,\n \n.\nN\n,\n by \n=\n \n10\n \n*\n \nround\n(\nSepal.Length \n*\n Sepal.Width \n/\n \n10\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n##    round   N\n## 1:    20 117\n## 2:    10  29\n## 3:    30   4\n\n\n\n\n\n\n1\n2\n# Now name the output columns `Area` and `Count`\n\nDT\n[,\n \n.\n(\nCount \n=\n \n.\nN\n),\n by \n=\n \n.\n(\nArea \n=\n \n10\n \n*\n \nround\n(\nSepal.Length \n*\n Sepal.Width \n/\n \n10\n))]\n\n\n\n\n\n\n\n1\n2\n3\n4\n##    Area Count\n## 1:   20   117\n## 2:   10    29\n## 3:   30     4\n\n\n\n\n\n\nReturn multiple numbers in \nj\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Create the data.table DT\n\n\nset.seed\n(\n1L\n)\n\nDT \n<-\n data.table\n(\nA \n=\n \nrep\n(\nletters\n[\n2\n:\n1\n],\n each \n=\n \n4L\n),\n B \n=\n \nrep\n(\n1\n:\n4\n,\n each \n=\n \n2L\n),\n C \n=\n \nsample\n(\n8\n))\n\n\n\n# Create the new data.table, DT2\n\nDT2 \n<-\n DT\n[,\n \n.\n(\nC \n=\n \ncumsum\n(\nC\n)),\n by\n=\n.\n(\nA\n,\nB\n)]\n\n\n\n# Select from DT2 the last two values from C while you group by A\n\nDT2\n[,\n \n.\n(\nC \n=\n \ntail\n(\nC\n,\n2\n)),\n by\n=\nA\n]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##    A C\n## 1: b 4\n## 2: b 9\n## 3: a 2\n## 4: a 8\n\n\n\n\n\n\ndata.table\n yeoman\n\u00b6\n\n\nChaining, the basics\n\u00b6\n\n\n1\n2\n3\n4\n# Build DT\n\n\nset.seed\n(\n1L\n)\n\nDT \n<-\n data.table\n(\nA \n=\n \nrep\n(\nletters\n[\n2\n:\n1\n],\n each \n=\n \n4L\n),\n B \n=\n \nrep\n(\n1\n:\n4\n,\n each \n=\n \n2L\n),\n C \n=\n \nsample\n(\n8\n))\n \nDT\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n##    A B C\n## 1: b 1 3\n## 2: b 1 8\n## 3: b 2 4\n## 4: b 2 5\n## 5: a 3 1\n## 6: a 3 7\n## 7: a 4 2\n## 8: a 4 6\n\n\n\n\n\n\n1\n2\n3\n# Use chaining\n\n\n# Cumsum of C while grouping by A and B, and then select last two values of C while grouping by A\n\nDT\n[,\n \n.\n(\nC \n=\n \ncumsum\n(\nC\n)),\n by \n=\n \n.\n(\nA\n,\nB\n)][,\n \n.\n(\nC \n=\n \ntail\n(\nC\n,\n2\n)),\n by \n=\n \n.\n(\nA\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##    A C\n## 1: b 4\n## 2: b 9\n## 3: a 2\n## 4: a 8\n\n\n\n\n\n\nChaining your \niris\n dataset\n\n\n1\n2\n3\n4\nDT \n<-\n data.table\n(\niris\n)\n\n\n\n# Perform chained operations on DT\n\nDT\n[,\n \n.\n(\nSepal.Length \n=\n median\n(\nSepal.Length\n),\n Sepal.Width \n=\n median\n(\nSepal.Width\n),\n Petal.Length \n=\n median\n(\nPetal.Length\n),\n Petal.Width \n=\n median\n(\nPetal.Width\n)),\n by \n=\n \n.\n(\nSpecies\n)][\norder\n(\nSpecies\n,\n decreasing \n=\n \nTRUE\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n##       Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1:  virginica          6.5         3.0         5.55         2.0\n## 2: versicolor          5.9         2.8         4.35         1.3\n## 3:     setosa          5.0         3.4         1.50         0.2\n\n\n\n\n\n\n1\nDT\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica\n\n\n\n\n\n\nProgramming time vs readability\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\nx \n<-\n \nc\n(\n2\n,\n \n1\n,\n \n2\n,\n \n1\n,\n \n2\n,\n \n2\n,\n \n1\n)\n\ny \n<-\n \nc\n(\n1\n,\n \n3\n,\n \n5\n,\n \n7\n,\n \n9\n,\n \n11\n,\n \n13\n)\n\nz \n<-\n \nc\n(\n2\n,\n \n4\n,\n \n6\n,\n \n8\n,\n \n10\n,\n \n12\n,\n \n14\n)\n\nDT \n<-\n data.table\n(\nx\n,\n y\n,\n z\n)\n\n\n\n# Mean of columns\n\nDT\n[,\n \nlapply\n(\n.\nSD\n,\n \nmean\n),\n by \n=\n \n.\n(\nx\n)]\n \n\n\n\n\n\n\n1\n2\n3\n##    x        y        z\n## 1: 2 6.500000 7.500000\n## 2: 1 7.666667 8.666667\n\n\n\n\n\n\n1\n2\n# Median of columns\n\nDT\n[,\n \nlapply\n(\n.\nSD\n,\n median\n),\n by \n=\n \n.\n(\nx\n)]\n\n\n\n\n\n\n\n1\n2\n3\n##    x y z\n## 1: 2 7 8\n## 2: 1 7 8\n\n\n\n\n\n\nIntroducing \n.SDcols\n\u00b6\n\n\n.SDcols\n specifies the columns of \nDT\n that are included in \n.SD\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\ngrp \n<-\n \nc\n(\n6\n,\n \n6\n,\n \n8\n,\n \n8\n,\n \n8\n)\n\nQ1 \n<-\n \nc\n(\n4\n,\n \n3\n,\n \n3\n,\n \n5\n,\n \n3\n)\n\nQ2 \n<-\n \nc\n(\n1\n,\n \n4\n,\n \n1\n,\n \n4\n,\n \n4\n)\n\nQ3 \n<-\n \nc\n(\n3\n,\n \n1\n,\n \n5\n,\n \n5\n,\n \n2\n)\n\nH1 \n<-\n \nc\n(\n1\n,\n \n2\n,\n \n3\n,\n \n2\n,\n \n4\n)\n\nH2 \n<-\n \nc\n(\n1\n,\n \n4\n,\n \n3\n,\n \n4\n,\n \n3\n)\n\nDT \n<-\n data.table\n(\ngrp\n,\n Q1\n,\n Q2\n,\n Q3\n,\n H1\n,\n H2\n)\n\n\n\n# Calculate the sum of the Q columns\n\nDT\n[,\n \nlapply\n(\n.\nSD\n,\n \nsum\n),\n \n.\nSDcols \n=\n \n2\n:\n4\n]\n\n\n\n\n\n\n\n1\n2\n##    Q1 Q2 Q3\n## 1: 18 14 16\n\n\n\n\n\n\n1\n2\n# Calculate the sum of columns H1 and H2 \n\nDT\n[,\n \nlapply\n(\n.\nSD\n,\n \nsum\n),\n \n.\nSDcols \n=\n \n5\n:\n6\n]\n\n\n\n\n\n\n\n1\n2\n##    H1 H2\n## 1: 12 15\n\n\n\n\n\n\n1\n2\n# Select all but the first row of groups 1 and 2, returning only the grp column and the Q columns. \n\nDT\n[,\n \n.\nSD\n[\n-1\n],\n \n.\nSDcols \n=\n \n2\n:\n4\n,\n by \n=\n \n.\n(\ngrp\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n##    grp Q1 Q2 Q3\n## 1:   6  3  4  1\n## 2:   8  5  4  5\n## 3:   8  3  4  2\n\n\n\n\n\n\nMixing it together: \nlapply\n, \n.SD\n, \nSDcols\n and \n.N\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nx \n<-\n \nc\n(\n2\n,\n \n1\n,\n \n2\n,\n \n1\n,\n \n2\n,\n \n2\n,\n \n1\n)\n\ny \n<-\n \nc\n(\n1\n,\n \n3\n,\n \n5\n,\n \n7\n,\n \n9\n,\n \n11\n,\n \n13\n)\n\nz \n<-\n \nc\n(\n2\n,\n \n4\n,\n \n6\n,\n \n8\n,\n \n10\n,\n \n12\n,\n \n14\n)\n\nDT \n<-\n data.table\n(\nx\n,\n y\n,\n z\n)\n\n\n\n# Sum of all columns and the number of rows\n\n\n# For the first part, you need to combine the returned list from lapply, .SD and .SDcols and the integer vector of .N. You have to this because the result of the two together has to be a list again, with all values put together.\n\nDT\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n##    x  y  z\n## 1: 2  1  2\n## 2: 1  3  4\n## 3: 2  5  6\n## 4: 1  7  8\n## 5: 2  9 10\n## 6: 2 11 12\n## 7: 1 13 14\n\n\n\n\n\n\n1\nDT\n[,\nc\n(\nlapply\n(\n.\nSD\n,\n \nsum\n),\n \n.\nN\n),\n \n.\nSDcols \n=\n \n1\n:\n3\n,\n by \n=\n x\n]\n\n\n\n\n\n\n\n1\n2\n3\n##    x x  y  z N\n## 1: 2 8 26 30 4\n## 2: 1 3 23 26 3\n\n\n\n\n\n\n1\n2\n# Cumulative sum of column x and y while grouping by x and z > 8\n\nDT\n[,\nlapply\n(\n.\nSD\n,\n \ncumsum\n),\n \n.\nSDcols \n=\n \n1\n:\n2\n,\n by \n=\n \n.\n(\nby1 \n=\n x\n,\n by2 \n=\n z \n>\n \n8\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n##    by1   by2 x  y\n## 1:   2 FALSE 2  1\n## 2:   2 FALSE 4  6\n## 3:   1 FALSE 1  3\n## 4:   1 FALSE 2 10\n## 5:   2  TRUE 2  9\n## 6:   2  TRUE 4 20\n## 7:   1  TRUE 1 13\n\n\n\n\n\n\n1\n2\n# Chaining\n\nDT\n[,\nlapply\n(\n.\nSD\n,\n \ncumsum\n),\n \n.\nSDcols \n=\n \n1\n:\n2\n,\n by \n=\n \n.\n(\nby1 \n=\n x\n,\n by2 \n=\n z \n>\n \n8\n)][,\nlapply\n(\n.\nSD\n,\n \nmax\n),\n \n.\nSDcols \n=\n \n3\n:\n4\n,\n by \n=\n by1\n]\n\n\n\n\n\n\n\n1\n2\n3\n##    by1 x  y\n## 1:   2 4 20\n## 2:   1 2 13\n\n\n\n\n\n\nAdding, updating, and removing columns\n\u00b6\n\n\n:=\n is defined for use in \nj\n only.\n\n\n1\n2\n3\n# The data.table DT\n\nDT \n<-\n data.table\n(\nA \n=\n \nletters\n[\nc\n(\n1\n,\n \n1\n,\n \n1\n,\n \n2\n,\n \n2\n)],\n B \n=\n \n1\n:\n5\n)\n\nDT\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##    A B\n## 1: a 1\n## 2: a 2\n## 3: a 3\n## 4: b 4\n## 5: b 5\n\n\n\n\n\n\n1\n2\n# Add column by reference: Total\n\nDT\n[,\n \n(\n'Total'\n)\n \n:=\n \nsum\n(\nB\n),\n by \n=\n \n.\n(\nA\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##    A B Total\n## 1: a 1     6\n## 2: a 2     6\n## 3: a 3     6\n## 4: b 4     9\n## 5: b 5     9\n\n\n\n\n\n\n1\n2\n# Add 1 to column B\n\nDT\n[\nc\n(\n2\n,\n4\n),\n \n(\n'B'\n)\n \n:=\n \nas.integer\n(\n1\n \n+\n B\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##    A B Total\n## 1: a 1     6\n## 2: a 3     6\n## 3: a 3     6\n## 4: b 5     9\n## 5: b 5     9\n\n\n\n\n\n\n1\n2\n# Add a new column Total2\n\nDT\n[\n2\n:\n4\n,\n \n':='\n(\nTotal2 \n=\n \nsum\n(\nB\n)),\n by \n=\n \n.\n(\nA\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##    A B Total Total2\n## 1: a 1     6     NA\n## 2: a 3     6      6\n## 3: a 3     6      6\n## 4: b 5     9      5\n## 5: b 5     9     NA\n\n\n\n\n\n\n1\n2\n# Remove the Total column\n\nDT\n[,\n Total \n:=\n \nNULL\n]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##    A B Total2\n## 1: a 1     NA\n## 2: a 3      6\n## 3: a 3      6\n## 4: b 5      5\n## 5: b 5     NA\n\n\n\n\n\n\n1\n2\n# Select the third column using `[[`\n\nDT\n[[\n3\n]]\n\n\n\n\n\n\n\n1\n## [1] NA  6  6  5 NA\n\n\n\n\n\n\nThe functional form\n\u00b6\n\n\n1\n2\n3\n4\n5\n# A data.table DT\n\nDT \n<-\n data.table\n(\nA \n=\n \nc\n(\n1\n,\n \n1\n,\n \n1\n,\n \n2\n,\n \n2\n),\n B \n=\n \n1\n:\n5\n)\n\n\n\n# Update B, add C and D\n\nDT\n[,\n \n`:=`\n(\nB \n=\n B \n+\n \n1\n,\n  C \n=\n A \n+\n B\n,\n D \n=\n \n2\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##    A B C D\n## 1: 1 2 2 2\n## 2: 1 3 3 2\n## 3: 1 4 4 2\n## 4: 2 5 6 2\n## 5: 2 6 7 2\n\n\n\n\n\n\n1\n2\n3\n# Delete my_cols\n\nmy_cols \n<-\n \nc\n(\n'B'\n,\n \n'C'\n)\n\nDT\n[,\n \n(\nmy_cols\n)\n \n:=\n \nNULL\n]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##    A D\n## 1: 1 2\n## 2: 1 2\n## 3: 1 2\n## 4: 2 2\n## 5: 2 2\n\n\n\n\n\n\n1\n2\n# Delete column 2 by number\n\nDT\n[,\n \n2\n \n:=\n \nNULL\n]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##    A\n## 1: 1\n## 2: 1\n## 3: 1\n## 4: 2\n## 5: 2\n\n\n\n\n\n\nReady, \nset\n, go!\n\u00b6\n\n\nThe \nset\n function is used to repeatedly update a data.table by reference. You can think of the \nset\n function as a loopable.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nA \n<-\n \nc\n(\n2\n,\n \n2\n,\n \n3\n,\n \n5\n,\n \n2\n,\n \n5\n,\n \n5\n,\n \n4\n,\n \n4\n,\n \n1\n)\n\nB \n<-\n \nc\n(\n2\n,\n \n1\n,\n \n4\n,\n \n2\n,\n \n4\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n2\n,\n \n4\n)\n\nC \n<-\n \nc\n(\n5\n,\n \n2\n,\n \n4\n,\n \n1\n,\n \n2\n,\n \n2\n,\n \n1\n,\n \n2\n,\n \n5\n,\n \n2\n)\n\nD \n<-\n \nc\n(\n3\n,\n \n3\n,\n \n3\n,\n \n1\n,\n \n5\n,\n \n4\n,\n \n4\n,\n \n1\n,\n \n4\n,\n \n3\n)\n\nDT \n<-\n data.table\n(\nA\n,\n B\n,\n C\n,\n D\n)\n\n\n\n# Set the seed\n\n\nset.seed\n(\n1\n)\n\n\n\n# Check the DT\n\nDT\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n##     A B C D\n##  1: 2 2 5 3\n##  2: 2 1 2 3\n##  3: 3 4 4 3\n##  4: 5 2 1 1\n##  5: 2 4 2 5\n##  6: 5 3 2 4\n##  7: 5 4 1 4\n##  8: 4 5 2 1\n##  9: 4 2 5 4\n## 10: 1 4 2 3\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# For loop with set\n\n\nfor\n \n(\nl \nin\n \n2\n:\n4\n)\n set\n(\nDT\n,\n \nsample\n(\n10\n,\n3\n),\n l\n,\n \nNA\n)\n\n\n\n# Change the column names to lowercase\n\nsetnames\n(\nDT\n,\nc\n(\n'A'\n,\n'B'\n,\n'C'\n,\n'D'\n),\n \nc\n(\n'a'\n,\n'b'\n,\n'c'\n,\n'd'\n))\n\n\n\n# Print the resulting DT to the console\n\nDT\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n##     a  b  c  d\n##  1: 2  2  5  3\n##  2: 2  1 NA  3\n##  3: 3 NA  4  3\n##  4: 5 NA  1  1\n##  5: 2 NA  2  5\n##  6: 5  3  2 NA\n##  7: 5  4  1  4\n##  8: 4  5 NA  1\n##  9: 4  2  5 NA\n## 10: 1  4 NA NA\n\n\n\n\n\n\nThe \nset\n family\n\n\n1\n2\n3\n# Define DT\n\nDT \n<-\n data.table\n(\na \n=\n \nletters\n[\nc\n(\n1\n,\n \n1\n,\n \n1\n,\n \n2\n,\n \n2\n)],\n b \n=\n \n1\n)\n\nDT\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##    a b\n## 1: a 1\n## 2: a 1\n## 3: a 1\n## 4: b 1\n## 5: b 1\n\n\n\n\n\n\n1\n2\n3\n# Add a postfix '_2' to all column names\n\nsetnames\n(\nDT\n,\n \nc\n(\n1\n:\n2\n),\n \npaste0\n(\nc\n(\n'a'\n,\n'b'\n),\n \n'_2'\n))\n\nDT\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##    a_2 b_2\n## 1:   a   1\n## 2:   a   1\n## 3:   a   1\n## 4:   b   1\n## 5:   b   1\n\n\n\n\n\n\n1\n2\n3\n# Change column name 'a_2' to 'A2'\n\nsetnames\n(\nDT\n,\n \n'a_2'\n,\n \n'A2'\n)\n\nDT\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##    A2 b_2\n## 1:  a   1\n## 2:  a   1\n## 3:  a   1\n## 4:  b   1\n## 5:  b   1\n\n\n\n\n\n\n1\n2\n# Reverse the order of the columns\n\nsetcolorder\n(\nDT\n,\n \nc\n(\n'b_2'\n,\n'A2'\n))\n\n\n\n\n\n\n\ndata.table\n expert\n\u00b6\n\n\nSelecting rows the \ndata.table\n way\n\u00b6\n\n\n1\n2\n3\n# Convert iris to a data.table\n\niris \n<-\n data.table\n(\n'Sepal.Length'\n \n=\n iris\n$\nSepal.Length\n,\n \n'Sepal.Width'\n \n=\n iris\n$\nSepal.Width\n,\n \n'Petal.Length'\n \n=\n iris\n$\nPetal.Length\n,\n \n'Petal.Width'\n \n=\n iris\n$\nPetal.Width\n,\n \n'Species'\n \n=\n iris\n$\nSpecies\n)\n\niris\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica\n\n\n\n\n\n\n1\n2\n# Species is 'virginica'\n\n\nhead\n(\niris\n[\nSpecies \n==\n \n'virginica'\n],\n \n20\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##  1:          6.3         3.3          6.0         2.5 virginica\n##  2:          5.8         2.7          5.1         1.9 virginica\n##  3:          7.1         3.0          5.9         2.1 virginica\n##  4:          6.3         2.9          5.6         1.8 virginica\n##  5:          6.5         3.0          5.8         2.2 virginica\n##  6:          7.6         3.0          6.6         2.1 virginica\n##  7:          4.9         2.5          4.5         1.7 virginica\n##  8:          7.3         2.9          6.3         1.8 virginica\n##  9:          6.7         2.5          5.8         1.8 virginica\n## 10:          7.2         3.6          6.1         2.5 virginica\n## 11:          6.5         3.2          5.1         2.0 virginica\n## 12:          6.4         2.7          5.3         1.9 virginica\n## 13:          6.8         3.0          5.5         2.1 virginica\n## 14:          5.7         2.5          5.0         2.0 virginica\n## 15:          5.8         2.8          5.1         2.4 virginica\n## 16:          6.4         3.2          5.3         2.3 virginica\n## 17:          6.5         3.0          5.5         1.8 virginica\n## 18:          7.7         3.8          6.7         2.2 virginica\n## 19:          7.7         2.6          6.9         2.3 virginica\n## 20:          6.0         2.2          5.0         1.5 virginica\n\n\n\n\n\n\n1\n2\n# Species is either 'virginica' or 'versicolor'\n\n\nhead\n(\niris\n[\nSpecies \n%in%\n \nc\n(\n'virginica'\n,\n \n'versicolor'\n)],\n \n20\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n##  1:          7.0         3.2          4.7         1.4 versicolor\n##  2:          6.4         3.2          4.5         1.5 versicolor\n##  3:          6.9         3.1          4.9         1.5 versicolor\n##  4:          5.5         2.3          4.0         1.3 versicolor\n##  5:          6.5         2.8          4.6         1.5 versicolor\n##  6:          5.7         2.8          4.5         1.3 versicolor\n##  7:          6.3         3.3          4.7         1.6 versicolor\n##  8:          4.9         2.4          3.3         1.0 versicolor\n##  9:          6.6         2.9          4.6         1.3 versicolor\n## 10:          5.2         2.7          3.9         1.4 versicolor\n## 11:          5.0         2.0          3.5         1.0 versicolor\n## 12:          5.9         3.0          4.2         1.5 versicolor\n## 13:          6.0         2.2          4.0         1.0 versicolor\n## 14:          6.1         2.9          4.7         1.4 versicolor\n## 15:          5.6         2.9          3.6         1.3 versicolor\n## 16:          6.7         3.1          4.4         1.4 versicolor\n## 17:          5.6         3.0          4.5         1.5 versicolor\n## 18:          5.8         2.7          4.1         1.0 versicolor\n## 19:          6.2         2.2          4.5         1.5 versicolor\n## 20:          5.6         2.5          3.9         1.1 versicolor\n\n\n\n\n\n\nRemoving columns and adapting your column names\n\u00b6\n\n\nRefer to a regex cheat sheet for metacharacter.\n\n\n1\n2\n3\n# iris as a data.table\n\niris \n<-\n as.data.table\n(\niris\n)\n\niris\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# Remove the 'Sepal.' prefix\n\n\n#gsub('([ab])', '\\\\1_\\\\1_', 'abc and ABC') = pattern, replacement, x\n\nsetnames\n(\niris\n,\n \nc\n(\n'Sepal.Length'\n,\n \n'Sepal.Width'\n),\n \nc\n(\n'Length'\n,\n'Width'\n))\n \n\n#gsub('^Sepal\\\\.','', iris)\n\n\n\n# Remove the two columns starting with 'Petal'\n\niris\n[,\n \nc\n(\n'Petal.Length'\n,\n \n'Petal.Width'\n)\n \n:=\n \nNULL\n]\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n##      Length Width   Species\n##   1:    5.1   3.5    setosa\n##   2:    4.9   3.0    setosa\n##   3:    4.7   3.2    setosa\n##   4:    4.6   3.1    setosa\n##   5:    5.0   3.6    setosa\n##  ---                       \n## 146:    6.7   3.0 virginica\n## 147:    6.3   2.5 virginica\n## 148:    6.5   3.0 virginica\n## 149:    6.2   3.4 virginica\n## 150:    5.9   3.0 virginica\n\n\n\n\n\n\nUnderstanding automatic indexing\n\u00b6\n\n\n1\n2\n3\n# Cleaned up iris data.table\n\niris2 \n<-\n \ndata.frame\n(\nLength \n=\n iris\n$\nSepal.Length\n,\n Width \n=\n iris\n$\nSepal.Width\n,\n Species \n=\n iris\n$\nSpecies\n)\n\niris2 \n<-\n as.data.table\n(\niris2\n)\n\n\n\n\n\n\n\n1\n2\n# Area is greater than 20 square centimeters\n\niris2\n[\n Width \n*\n Length \n>\n \n20\n \n],\n \n20\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n   Length Width    Species is_large\n 1:    5.4   3.9     setosa    FALSE\n 2:    5.8   4.0     setosa    FALSE\n 3:    5.7   4.4     setosa     TRUE\n 4:    5.4   3.9     setosa    FALSE\n 5:    5.7   3.8     setosa    FALSE\n 6:    5.2   4.1     setosa    FALSE\n 7:    5.5   4.2     setosa    FALSE\n 8:    7.0   3.2 versicolor    FALSE\n 9:    6.4   3.2 versicolor    FALSE\n10:    6.9   3.1 versicolor    FALSE\n11:    6.3   3.3 versicolor    FALSE\n12:    6.7   3.1 versicolor    FALSE\n13:    6.7   3.0 versicolor    FALSE\n14:    6.0   3.4 versicolor    FALSE\n15:    6.7   3.1 versicolor    FALSE\n16:    6.3   3.3  virginica    FALSE\n17:    7.1   3.0  virginica    FALSE\n18:    7.6   3.0  virginica    FALSE\n19:    7.3   2.9  virginica    FALSE\n20:    7.2   3.6  virginica     TRUE\n...\n\n\n\n\n\n\n1\n2\n# Add new boolean column\n\niris2\n[,\n is_large \n:=\n Width \n*\n Length \n>\n \n25\n]\n\n\n\n\n\n\n\n1\n2\n# Now large observations with is_large\n\niris2\n[\nis_large \n==\n \nTRUE\n]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n   Length Width   Species is_large\n1:    5.7   4.4    setosa     TRUE\n2:    7.2   3.6 virginica     TRUE\n3:    7.7   3.8 virginica     TRUE\n4:    7.9   3.8 virginica     TRUE\n\n\n\n\n\n\n1\niris2\n[(\nis_large\n)]\n \n# Also OK\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n   Length Width   Species is_large\n1:    5.7   4.4    setosa     TRUE\n2:    7.2   3.6 virginica     TRUE\n3:    7.7   3.8 virginica     TRUE\n4:    7.9   3.8 virginica     TRUE\n\n\n\n\n\n\nSelecting groups or parts of groups\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# The 'keyed' data.table DT\n\nDT \n<-\n data.table\n(\nA \n=\n \nletters\n[\nc\n(\n2\n,\n \n1\n,\n \n2\n,\n \n3\n,\n \n1\n,\n \n2\n,\n \n3\n)],\n\n                 B \n=\n \nc\n(\n5\n,\n \n4\n,\n \n1\n,\n \n9\n,\n8\n \n,\n8\n,\n \n6\n),\n \n                 C \n=\n \n6\n:\n12\n)\n\nsetkey\n(\nDT\n,\n A\n,\n B\n)\n\n\n\n# Select the 'b' group\n\nDT\n[\n'b'\n]\n\n\n\n\n\n\n\n1\n2\n3\n4\n##    A B  C\n## 1: b 1  8\n## 2: b 5  6\n## 3: b 8 11\n\n\n\n\n\n\n1\n2\n# 'b' and 'c' groups\n\nDT\n[\nc\n(\n'b'\n,\n \n'c'\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##    A B  C\n## 1: b 1  8\n## 2: b 5  6\n## 3: b 8 11\n## 4: c 6 12\n## 5: c 9  9\n\n\n\n\n\n\n1\n2\n# The first row of the 'b' and 'c' groups\n\nDT\n[\nc\n(\n'b'\n,\n \n'c'\n),\n mult \n=\n \n'first'\n]\n\n\n\n\n\n\n\n1\n2\n3\n##    A B  C\n## 1: b 1  8\n## 2: c 6 12\n\n\n\n\n\n\n1\n2\n# First and last row of the 'b' and 'c' groups\n\nDT\n[\nc\n(\n'b'\n,\n \n'c'\n),\n \n.\nSD\n[\nc\n(\n1\n,\n \n.\nN\n)],\n by \n=\n \n.\nEACHI\n]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##    A B  C\n## 1: b 1  8\n## 2: b 8 11\n## 3: c 6 12\n## 4: c 9  9\n\n\n\n\n\n\n1\n2\n# Copy and extend code for instruction 4: add printout\n\nDT\n[\nc\n(\n'b'\n,\n \n'c'\n),\n \n{\n \nprint\n(\n.\nSD\n);\n \n.\nSD\n[\nc\n(\n1\n,\n \n.\nN\n)]\n \n},\n by \n=\n \n.\nEACHI\n]\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n##    B  C\n## 1: 1  8\n## 2: 5  6\n## 3: 8 11\n##    B  C\n## 1: 6 12\n## 2: 9  9\n\n##    A B  C\n## 1: b 1  8\n## 2: b 8 11\n## 3: c 6 12\n## 4: c 9  9\n\n\n\n\n\n\nRolling joins\n\u00b6\n\n\nRolling joins \u2013 part one\n\n\n1\n2\n3\n4\n5\n6\n7\n# Keyed data.table DT\n\nDT \n<-\n data.table\n(\nA \n=\n \nletters\n[\nc\n(\n2\n,\n \n1\n,\n \n2\n,\n \n3\n,\n \n1\n,\n \n2\n,\n \n3\n)],\n                  B \n=\n \nc\n(\n5\n,\n \n4\n,\n \n1\n,\n \n9\n,\n \n8\n,\n \n8\n,\n \n6\n),\n \n                 C \n=\n \n6\n:\n12\n,\n \n                 key \n=\n \n'A,B'\n)\n\n\n\n# Get the key of DT\n\nkey\n(\nDT\n)\n\n\n\n\n\n\n\n1\n## [1] \"A\" \"B\"\n\n\n\n\n\n\n1\n2\n3\n# Row where A == 'b' & B == 6\n\nsetkey\n(\nDT\n,\n A\n,\n B\n)\n\nDT\n[\n.\n(\n'b'\n,\n \n6\n)]\n\n\n\n\n\n\n\n1\n2\n##    A B  C\n## 1: b 6 NA\n\n\n\n\n\n\n1\n2\n# Return the prevailing row\n\nDT\n[\n.\n(\n'b'\n,\n6\n),\n roll \n=\n \nTRUE\n]\n\n\n\n\n\n\n\n1\n2\n##    A B C\n## 1: b 6 6\n\n\n\n\n\n\n1\n2\n# Return the nearest row\n\nDT\n[\n.\n(\n'b'\n,\n6\n),\n roll \n=+\n \nInf\n]\n\n\n\n\n\n\n\n1\n2\n##    A B C\n## 1: b 6 6\n\n\n\n\n\n\nRolling joins \u2013 part two\n\n\n1\n2\n3\n4\n5\n6\n7\n# Keyed data.table DT\n\nDT \n<-\n data.table\n(\nA \n=\n \nletters\n[\nc\n(\n2\n,\n \n1\n,\n \n2\n,\n \n3\n,\n \n1\n,\n \n2\n,\n \n3\n)],\n                  B \n=\n \nc\n(\n5\n,\n \n4\n,\n \n1\n,\n \n9\n,\n \n8\n,\n \n8\n,\n \n6\n),\n \n                 C \n=\n \n6\n:\n12\n,\n \n                 key \n=\n \n'A,B'\n)\n\n\n\n# Look at the sequence (-2):10 for the 'b' group\n\nDT\n[\n.\n(\n'b'\n,\n \n(\n-2\n)\n:\n10\n)]\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n##     A  B  C\n##  1: b -2 NA\n##  2: b -1 NA\n##  3: b  0 NA\n##  4: b  1  8\n##  5: b  2 NA\n##  6: b  3 NA\n##  7: b  4 NA\n##  8: b  5  6\n##  9: b  6 NA\n## 10: b  7 NA\n## 11: b  8 11\n## 12: b  9 NA\n## 13: b 10 NA\n\n\n\n\n\n\n1\n2\n# Add code: carry the prevailing values forwards\n\nDT\n[\n.\n(\n'b'\n,\n \n(\n-2\n)\n:\n10\n),\n roll \n=\n \nTRUE\n]\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n##     A  B  C\n##  1: b -2 NA\n##  2: b -1 NA\n##  3: b  0 NA\n##  4: b  1  8\n##  5: b  2  8\n##  6: b  3  8\n##  7: b  4  8\n##  8: b  5  6\n##  9: b  6  6\n## 10: b  7  6\n## 11: b  8 11\n## 12: b  9 11\n## 13: b 10 11\n\n\n\n\n\n\n1\n2\n# Add code: carry the first observation backwards\n\nDT\n[\n.\n(\n'b'\n,\n \n(\n-2\n)\n:\n10\n),\n roll \n=\n \nTRUE\n,\n rollends \n=\n \nTRUE\n]\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n##     A  B  C\n##  1: b -2  8\n##  2: b -1  8\n##  3: b  0  8\n##  4: b  1  8\n##  5: b  2  8\n##  6: b  3  8\n##  7: b  4  8\n##  8: b  5  6\n##  9: b  6  6\n## 10: b  7  6\n## 11: b  8 11\n## 12: b  9 11\n## 13: b 10 11",
            "title": "Data Analysis in R, the data.table Way"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#datatable-novice",
            "text": "Find out more with  ?data.table .",
            "title": "data.table novice"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#create-and-subset-a-datatable",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # The data.table package  library ( data.table )  # Create my_first_data_table \nmy_first_data_table  <-  data.table ( x  =   c ( 'a' ,   'b' ,   'c' ,   'd' ,   'e' ),  y  =   c ( 1 ,   2 ,   3 ,   4 ,   5 ))  # Create a data.table using recycling \nDT  <-  data.table ( a  =   1 : 2 ,  b  =   c ( 'A' ,   'B' ,   'C' ,   'D' ))  # Print the third row to the console \nDT [ 3 ,]    1\n2 ##    a b\n## 1: 1 C   1\n2 # Print the second and third row to the console, but do not commas \nDT [ 2 : 3 ]    1\n2\n3 ##    a b\n## 1: 2 B\n## 2: 1 C",
            "title": "Create and subset a data.table"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#getting-to-know-a-datatable",
            "text": "Like  head ,  tail .  1\n2 # Print the penultimate row of DT using .N \nDT [ . N  -   1 ]    1\n2 ##    a b\n## 1: 1 C   1\n2 # Print the column names of DT, and number of rows and number of columns  colnames ( DT )    1 ## [1] \"a\" \"b\"   1 dim ( DT )    1 ## [1] 4 2   1\n2 # Select row 2 twice and row 3, returning a data.table with three rows where row 2 is a duplicate of row 1. \nDT [ c ( 2 ,   2 ,   3 )]    1\n2\n3\n4 ##    a b\n## 1: 2 B\n## 2: 2 B\n## 3: 1 C   DT  is a data.table/data.frame, but  DT[ , B]  is a vector;  DT[ , .(B)]  is a subsetted data.table.",
            "title": "Getting to know a data.table"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#subsetting-data-tables",
            "text": "DT[i, j, by]  means take  DT , subset rows using  i , then calculate  j  grouped by  by . You can wrap  j  with  .() .  1\n2\n3\n4\n5\n6\n7 A  <-   c ( 1 ,   2 ,   3 ,   4 ,   5 ) \nB  <-   c ( 'a' ,   'b' ,   'c' ,   'd' ,   'e' ) \nC  <-   c ( 6 ,   7 ,   8 ,   9 ,   10 ) \nDT  <-  data.table ( A ,  B ,  C )  # Subset rows 1 and 3, and columns B and C \nDT [ c ( 1 , 3 )   , . ( B ,  C )]    1\n2\n3 ##    B C\n## 1: a 6\n## 2: c 8   1\n2\n3\n4\n5\n6 # Assign to ans the correct value \nans  <-  data.table ( DT [,   . ( B ,  val  =  A  *  C )])  # Fill in the blanks such that ans2 equals target  #target <- data.table(B = c('a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e'),  val = as.integer(c(6:10, 1:5))) \nans2  <-  data.table ( DT [,   . ( B ,  val  =   as.integer ( c ( 6 : 10 ,   1 : 5 )))])",
            "title": "Subsetting data tables"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#the-by-basics",
            "text": "1\n2\n3\n4\n5\n6\n7 # iris and iris3 are already available in the workspace  # Convert iris to a data.table: DT \nDT  <-  as.data.table ( iris )  # For each Species, print the mean Sepal.Length \nDT [,   . ( mean ( Sepal.Length )),  by  =   . ( Species )]    1\n2\n3\n4 ##       Species    V1\n## 1:     setosa 5.006\n## 2: versicolor 5.936\n## 3:  virginica 6.588   1\n2 # Print mean Sepal.Length, grouping by first letter of Species \nDT [,   . ( mean ( Sepal.Length )),  by  =   . ( substr ( Species ,   1 , 1 ))]    1\n2\n3 ##    substr    V1\n## 1:      s 5.006\n## 2:      v 6.262",
            "title": "The by basics"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#using-n-and-by",
            "text": ".N , number, in row or column.  1\n2\n3\n4\n5 # data.table version of iris: DT \nDT  <-  as.data.table ( iris )  # Group the specimens by Sepal area (to the nearest 10 cm2) and count how many occur in each group. \nDT [,   . N ,  by  =   10   *   round ( Sepal.Length  *  Sepal.Width  /   10 )]    1\n2\n3\n4 ##    round   N\n## 1:    20 117\n## 2:    10  29\n## 3:    30   4   1\n2 # Now name the output columns `Area` and `Count` \nDT [,   . ( Count  =   . N ),  by  =   . ( Area  =   10   *   round ( Sepal.Length  *  Sepal.Width  /   10 ))]    1\n2\n3\n4 ##    Area Count\n## 1:   20   117\n## 2:   10    29\n## 3:   30     4",
            "title": "Using .N and by"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#return-multiple-numbers-in-j",
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 # Create the data.table DT  set.seed ( 1L ) \nDT  <-  data.table ( A  =   rep ( letters [ 2 : 1 ],  each  =   4L ),  B  =   rep ( 1 : 4 ,  each  =   2L ),  C  =   sample ( 8 ))  # Create the new data.table, DT2 \nDT2  <-  DT [,   . ( C  =   cumsum ( C )),  by = . ( A , B )]  # Select from DT2 the last two values from C while you group by A \nDT2 [,   . ( C  =   tail ( C , 2 )),  by = A ]    1\n2\n3\n4\n5 ##    A C\n## 1: b 4\n## 2: b 9\n## 3: a 2\n## 4: a 8",
            "title": "Return multiple numbers in j"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#datatable-yeoman",
            "text": "",
            "title": "data.table yeoman"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#chaining-the-basics",
            "text": "1\n2\n3\n4 # Build DT  set.seed ( 1L ) \nDT  <-  data.table ( A  =   rep ( letters [ 2 : 1 ],  each  =   4L ),  B  =   rep ( 1 : 4 ,  each  =   2L ),  C  =   sample ( 8 ))  \nDT   1\n2\n3\n4\n5\n6\n7\n8\n9 ##    A B C\n## 1: b 1 3\n## 2: b 1 8\n## 3: b 2 4\n## 4: b 2 5\n## 5: a 3 1\n## 6: a 3 7\n## 7: a 4 2\n## 8: a 4 6   1\n2\n3 # Use chaining  # Cumsum of C while grouping by A and B, and then select last two values of C while grouping by A \nDT [,   . ( C  =   cumsum ( C )),  by  =   . ( A , B )][,   . ( C  =   tail ( C , 2 )),  by  =   . ( A )]    1\n2\n3\n4\n5 ##    A C\n## 1: b 4\n## 2: b 9\n## 3: a 2\n## 4: a 8   Chaining your  iris  dataset  1\n2\n3\n4 DT  <-  data.table ( iris )  # Perform chained operations on DT \nDT [,   . ( Sepal.Length  =  median ( Sepal.Length ),  Sepal.Width  =  median ( Sepal.Width ),  Petal.Length  =  median ( Petal.Length ),  Petal.Width  =  median ( Petal.Width )),  by  =   . ( Species )][ order ( Species ,  decreasing  =   TRUE )]    1\n2\n3\n4 ##       Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n## 1:  virginica          6.5         3.0         5.55         2.0\n## 2: versicolor          5.9         2.8         4.35         1.3\n## 3:     setosa          5.0         3.4         1.50         0.2   1 DT    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica",
            "title": "Chaining, the basics"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#programming-time-vs-readability",
            "text": "1\n2\n3\n4\n5\n6\n7 x  <-   c ( 2 ,   1 ,   2 ,   1 ,   2 ,   2 ,   1 ) \ny  <-   c ( 1 ,   3 ,   5 ,   7 ,   9 ,   11 ,   13 ) \nz  <-   c ( 2 ,   4 ,   6 ,   8 ,   10 ,   12 ,   14 ) \nDT  <-  data.table ( x ,  y ,  z )  # Mean of columns \nDT [,   lapply ( . SD ,   mean ),  by  =   . ( x )]     1\n2\n3 ##    x        y        z\n## 1: 2 6.500000 7.500000\n## 2: 1 7.666667 8.666667   1\n2 # Median of columns \nDT [,   lapply ( . SD ,  median ),  by  =   . ( x )]    1\n2\n3 ##    x y z\n## 1: 2 7 8\n## 2: 1 7 8",
            "title": "Programming time vs readability"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#introducing-sdcols",
            "text": ".SDcols  specifies the columns of  DT  that are included in  .SD .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 grp  <-   c ( 6 ,   6 ,   8 ,   8 ,   8 ) \nQ1  <-   c ( 4 ,   3 ,   3 ,   5 ,   3 ) \nQ2  <-   c ( 1 ,   4 ,   1 ,   4 ,   4 ) \nQ3  <-   c ( 3 ,   1 ,   5 ,   5 ,   2 ) \nH1  <-   c ( 1 ,   2 ,   3 ,   2 ,   4 ) \nH2  <-   c ( 1 ,   4 ,   3 ,   4 ,   3 ) \nDT  <-  data.table ( grp ,  Q1 ,  Q2 ,  Q3 ,  H1 ,  H2 )  # Calculate the sum of the Q columns \nDT [,   lapply ( . SD ,   sum ),   . SDcols  =   2 : 4 ]    1\n2 ##    Q1 Q2 Q3\n## 1: 18 14 16   1\n2 # Calculate the sum of columns H1 and H2  \nDT [,   lapply ( . SD ,   sum ),   . SDcols  =   5 : 6 ]    1\n2 ##    H1 H2\n## 1: 12 15   1\n2 # Select all but the first row of groups 1 and 2, returning only the grp column and the Q columns.  \nDT [,   . SD [ -1 ],   . SDcols  =   2 : 4 ,  by  =   . ( grp )]    1\n2\n3\n4 ##    grp Q1 Q2 Q3\n## 1:   6  3  4  1\n## 2:   8  5  4  5\n## 3:   8  3  4  2",
            "title": "Introducing .SDcols"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#mixing-it-together-lapply-sd-sdcols-and-n",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 x  <-   c ( 2 ,   1 ,   2 ,   1 ,   2 ,   2 ,   1 ) \ny  <-   c ( 1 ,   3 ,   5 ,   7 ,   9 ,   11 ,   13 ) \nz  <-   c ( 2 ,   4 ,   6 ,   8 ,   10 ,   12 ,   14 ) \nDT  <-  data.table ( x ,  y ,  z )  # Sum of all columns and the number of rows  # For the first part, you need to combine the returned list from lapply, .SD and .SDcols and the integer vector of .N. You have to this because the result of the two together has to be a list again, with all values put together. \nDT   1\n2\n3\n4\n5\n6\n7\n8 ##    x  y  z\n## 1: 2  1  2\n## 2: 1  3  4\n## 3: 2  5  6\n## 4: 1  7  8\n## 5: 2  9 10\n## 6: 2 11 12\n## 7: 1 13 14   1 DT [, c ( lapply ( . SD ,   sum ),   . N ),   . SDcols  =   1 : 3 ,  by  =  x ]    1\n2\n3 ##    x x  y  z N\n## 1: 2 8 26 30 4\n## 2: 1 3 23 26 3   1\n2 # Cumulative sum of column x and y while grouping by x and z > 8 \nDT [, lapply ( . SD ,   cumsum ),   . SDcols  =   1 : 2 ,  by  =   . ( by1  =  x ,  by2  =  z  >   8 )]    1\n2\n3\n4\n5\n6\n7\n8 ##    by1   by2 x  y\n## 1:   2 FALSE 2  1\n## 2:   2 FALSE 4  6\n## 3:   1 FALSE 1  3\n## 4:   1 FALSE 2 10\n## 5:   2  TRUE 2  9\n## 6:   2  TRUE 4 20\n## 7:   1  TRUE 1 13   1\n2 # Chaining \nDT [, lapply ( . SD ,   cumsum ),   . SDcols  =   1 : 2 ,  by  =   . ( by1  =  x ,  by2  =  z  >   8 )][, lapply ( . SD ,   max ),   . SDcols  =   3 : 4 ,  by  =  by1 ]    1\n2\n3 ##    by1 x  y\n## 1:   2 4 20\n## 2:   1 2 13",
            "title": "Mixing it together: lapply, .SD, SDcols and .N"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#adding-updating-and-removing-columns",
            "text": ":=  is defined for use in  j  only.  1\n2\n3 # The data.table DT \nDT  <-  data.table ( A  =   letters [ c ( 1 ,   1 ,   1 ,   2 ,   2 )],  B  =   1 : 5 ) \nDT   1\n2\n3\n4\n5\n6 ##    A B\n## 1: a 1\n## 2: a 2\n## 3: a 3\n## 4: b 4\n## 5: b 5   1\n2 # Add column by reference: Total \nDT [,   ( 'Total' )   :=   sum ( B ),  by  =   . ( A )]    1\n2\n3\n4\n5\n6 ##    A B Total\n## 1: a 1     6\n## 2: a 2     6\n## 3: a 3     6\n## 4: b 4     9\n## 5: b 5     9   1\n2 # Add 1 to column B \nDT [ c ( 2 , 4 ),   ( 'B' )   :=   as.integer ( 1   +  B )]    1\n2\n3\n4\n5\n6 ##    A B Total\n## 1: a 1     6\n## 2: a 3     6\n## 3: a 3     6\n## 4: b 5     9\n## 5: b 5     9   1\n2 # Add a new column Total2 \nDT [ 2 : 4 ,   ':=' ( Total2  =   sum ( B )),  by  =   . ( A )]    1\n2\n3\n4\n5\n6 ##    A B Total Total2\n## 1: a 1     6     NA\n## 2: a 3     6      6\n## 3: a 3     6      6\n## 4: b 5     9      5\n## 5: b 5     9     NA   1\n2 # Remove the Total column \nDT [,  Total  :=   NULL ]    1\n2\n3\n4\n5\n6 ##    A B Total2\n## 1: a 1     NA\n## 2: a 3      6\n## 3: a 3      6\n## 4: b 5      5\n## 5: b 5     NA   1\n2 # Select the third column using `[[` \nDT [[ 3 ]]    1 ## [1] NA  6  6  5 NA",
            "title": "Adding, updating, and removing columns"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#the-functional-form",
            "text": "1\n2\n3\n4\n5 # A data.table DT \nDT  <-  data.table ( A  =   c ( 1 ,   1 ,   1 ,   2 ,   2 ),  B  =   1 : 5 )  # Update B, add C and D \nDT [,   `:=` ( B  =  B  +   1 ,   C  =  A  +  B ,  D  =   2 )]    1\n2\n3\n4\n5\n6 ##    A B C D\n## 1: 1 2 2 2\n## 2: 1 3 3 2\n## 3: 1 4 4 2\n## 4: 2 5 6 2\n## 5: 2 6 7 2   1\n2\n3 # Delete my_cols \nmy_cols  <-   c ( 'B' ,   'C' ) \nDT [,   ( my_cols )   :=   NULL ]    1\n2\n3\n4\n5\n6 ##    A D\n## 1: 1 2\n## 2: 1 2\n## 3: 1 2\n## 4: 2 2\n## 5: 2 2   1\n2 # Delete column 2 by number \nDT [,   2   :=   NULL ]    1\n2\n3\n4\n5\n6 ##    A\n## 1: 1\n## 2: 1\n## 3: 1\n## 4: 2\n## 5: 2",
            "title": "The functional form"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#ready-set-go",
            "text": "The  set  function is used to repeatedly update a data.table by reference. You can think of the  set  function as a loopable.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 A  <-   c ( 2 ,   2 ,   3 ,   5 ,   2 ,   5 ,   5 ,   4 ,   4 ,   1 ) \nB  <-   c ( 2 ,   1 ,   4 ,   2 ,   4 ,   3 ,   4 ,   5 ,   2 ,   4 ) \nC  <-   c ( 5 ,   2 ,   4 ,   1 ,   2 ,   2 ,   1 ,   2 ,   5 ,   2 ) \nD  <-   c ( 3 ,   3 ,   3 ,   1 ,   5 ,   4 ,   4 ,   1 ,   4 ,   3 ) \nDT  <-  data.table ( A ,  B ,  C ,  D )  # Set the seed  set.seed ( 1 )  # Check the DT \nDT    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ##     A B C D\n##  1: 2 2 5 3\n##  2: 2 1 2 3\n##  3: 3 4 4 3\n##  4: 5 2 1 1\n##  5: 2 4 2 5\n##  6: 5 3 2 4\n##  7: 5 4 1 4\n##  8: 4 5 2 1\n##  9: 4 2 5 4\n## 10: 1 4 2 3   1\n2\n3\n4\n5\n6\n7\n8 # For loop with set  for   ( l  in   2 : 4 )  set ( DT ,   sample ( 10 , 3 ),  l ,   NA )  # Change the column names to lowercase \nsetnames ( DT , c ( 'A' , 'B' , 'C' , 'D' ),   c ( 'a' , 'b' , 'c' , 'd' ))  # Print the resulting DT to the console \nDT    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ##     a  b  c  d\n##  1: 2  2  5  3\n##  2: 2  1 NA  3\n##  3: 3 NA  4  3\n##  4: 5 NA  1  1\n##  5: 2 NA  2  5\n##  6: 5  3  2 NA\n##  7: 5  4  1  4\n##  8: 4  5 NA  1\n##  9: 4  2  5 NA\n## 10: 1  4 NA NA   The  set  family  1\n2\n3 # Define DT \nDT  <-  data.table ( a  =   letters [ c ( 1 ,   1 ,   1 ,   2 ,   2 )],  b  =   1 ) \nDT   1\n2\n3\n4\n5\n6 ##    a b\n## 1: a 1\n## 2: a 1\n## 3: a 1\n## 4: b 1\n## 5: b 1   1\n2\n3 # Add a postfix '_2' to all column names \nsetnames ( DT ,   c ( 1 : 2 ),   paste0 ( c ( 'a' , 'b' ),   '_2' )) \nDT   1\n2\n3\n4\n5\n6 ##    a_2 b_2\n## 1:   a   1\n## 2:   a   1\n## 3:   a   1\n## 4:   b   1\n## 5:   b   1   1\n2\n3 # Change column name 'a_2' to 'A2' \nsetnames ( DT ,   'a_2' ,   'A2' ) \nDT   1\n2\n3\n4\n5\n6 ##    A2 b_2\n## 1:  a   1\n## 2:  a   1\n## 3:  a   1\n## 4:  b   1\n## 5:  b   1   1\n2 # Reverse the order of the columns \nsetcolorder ( DT ,   c ( 'b_2' , 'A2' ))",
            "title": "Ready, set, go!"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#datatable-expert",
            "text": "",
            "title": "data.table expert"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#selecting-rows-the-datatable-way",
            "text": "1\n2\n3 # Convert iris to a data.table \niris  <-  data.table ( 'Sepal.Length'   =  iris $ Sepal.Length ,   'Sepal.Width'   =  iris $ Sepal.Width ,   'Petal.Length'   =  iris $ Petal.Length ,   'Petal.Width'   =  iris $ Petal.Width ,   'Species'   =  iris $ Species ) \niris    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica   1\n2 # Species is 'virginica'  head ( iris [ Species  ==   'virginica' ],   20 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 ##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##  1:          6.3         3.3          6.0         2.5 virginica\n##  2:          5.8         2.7          5.1         1.9 virginica\n##  3:          7.1         3.0          5.9         2.1 virginica\n##  4:          6.3         2.9          5.6         1.8 virginica\n##  5:          6.5         3.0          5.8         2.2 virginica\n##  6:          7.6         3.0          6.6         2.1 virginica\n##  7:          4.9         2.5          4.5         1.7 virginica\n##  8:          7.3         2.9          6.3         1.8 virginica\n##  9:          6.7         2.5          5.8         1.8 virginica\n## 10:          7.2         3.6          6.1         2.5 virginica\n## 11:          6.5         3.2          5.1         2.0 virginica\n## 12:          6.4         2.7          5.3         1.9 virginica\n## 13:          6.8         3.0          5.5         2.1 virginica\n## 14:          5.7         2.5          5.0         2.0 virginica\n## 15:          5.8         2.8          5.1         2.4 virginica\n## 16:          6.4         3.2          5.3         2.3 virginica\n## 17:          6.5         3.0          5.5         1.8 virginica\n## 18:          7.7         3.8          6.7         2.2 virginica\n## 19:          7.7         2.6          6.9         2.3 virginica\n## 20:          6.0         2.2          5.0         1.5 virginica   1\n2 # Species is either 'virginica' or 'versicolor'  head ( iris [ Species  %in%   c ( 'virginica' ,   'versicolor' )],   20 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 ##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n##  1:          7.0         3.2          4.7         1.4 versicolor\n##  2:          6.4         3.2          4.5         1.5 versicolor\n##  3:          6.9         3.1          4.9         1.5 versicolor\n##  4:          5.5         2.3          4.0         1.3 versicolor\n##  5:          6.5         2.8          4.6         1.5 versicolor\n##  6:          5.7         2.8          4.5         1.3 versicolor\n##  7:          6.3         3.3          4.7         1.6 versicolor\n##  8:          4.9         2.4          3.3         1.0 versicolor\n##  9:          6.6         2.9          4.6         1.3 versicolor\n## 10:          5.2         2.7          3.9         1.4 versicolor\n## 11:          5.0         2.0          3.5         1.0 versicolor\n## 12:          5.9         3.0          4.2         1.5 versicolor\n## 13:          6.0         2.2          4.0         1.0 versicolor\n## 14:          6.1         2.9          4.7         1.4 versicolor\n## 15:          5.6         2.9          3.6         1.3 versicolor\n## 16:          6.7         3.1          4.4         1.4 versicolor\n## 17:          5.6         3.0          4.5         1.5 versicolor\n## 18:          5.8         2.7          4.1         1.0 versicolor\n## 19:          6.2         2.2          4.5         1.5 versicolor\n## 20:          5.6         2.5          3.9         1.1 versicolor",
            "title": "Selecting rows the data.table way"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#removing-columns-and-adapting-your-column-names",
            "text": "Refer to a regex cheat sheet for metacharacter.  1\n2\n3 # iris as a data.table \niris  <-  as.data.table ( iris ) \niris    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ##      Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n##   1:          5.1         3.5          1.4         0.2    setosa\n##   2:          4.9         3.0          1.4         0.2    setosa\n##   3:          4.7         3.2          1.3         0.2    setosa\n##   4:          4.6         3.1          1.5         0.2    setosa\n##   5:          5.0         3.6          1.4         0.2    setosa\n##  ---                                                            \n## 146:          6.7         3.0          5.2         2.3 virginica\n## 147:          6.3         2.5          5.0         1.9 virginica\n## 148:          6.5         3.0          5.2         2.0 virginica\n## 149:          6.2         3.4          5.4         2.3 virginica\n## 150:          5.9         3.0          5.1         1.8 virginica   1\n2\n3\n4\n5\n6\n7 # Remove the 'Sepal.' prefix  #gsub('([ab])', '\\\\1_\\\\1_', 'abc and ABC') = pattern, replacement, x \nsetnames ( iris ,   c ( 'Sepal.Length' ,   'Sepal.Width' ),   c ( 'Length' , 'Width' ))   #gsub('^Sepal\\\\.','', iris)  # Remove the two columns starting with 'Petal' \niris [,   c ( 'Petal.Length' ,   'Petal.Width' )   :=   NULL ]     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ##      Length Width   Species\n##   1:    5.1   3.5    setosa\n##   2:    4.9   3.0    setosa\n##   3:    4.7   3.2    setosa\n##   4:    4.6   3.1    setosa\n##   5:    5.0   3.6    setosa\n##  ---                       \n## 146:    6.7   3.0 virginica\n## 147:    6.3   2.5 virginica\n## 148:    6.5   3.0 virginica\n## 149:    6.2   3.4 virginica\n## 150:    5.9   3.0 virginica",
            "title": "Removing columns and adapting your column names"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#understanding-automatic-indexing",
            "text": "1\n2\n3 # Cleaned up iris data.table \niris2  <-   data.frame ( Length  =  iris $ Sepal.Length ,  Width  =  iris $ Sepal.Width ,  Species  =  iris $ Species ) \niris2  <-  as.data.table ( iris2 )    1\n2 # Area is greater than 20 square centimeters \niris2 [  Width  *  Length  >   20   ],   20     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22    Length Width    Species is_large\n 1:    5.4   3.9     setosa    FALSE\n 2:    5.8   4.0     setosa    FALSE\n 3:    5.7   4.4     setosa     TRUE\n 4:    5.4   3.9     setosa    FALSE\n 5:    5.7   3.8     setosa    FALSE\n 6:    5.2   4.1     setosa    FALSE\n 7:    5.5   4.2     setosa    FALSE\n 8:    7.0   3.2 versicolor    FALSE\n 9:    6.4   3.2 versicolor    FALSE\n10:    6.9   3.1 versicolor    FALSE\n11:    6.3   3.3 versicolor    FALSE\n12:    6.7   3.1 versicolor    FALSE\n13:    6.7   3.0 versicolor    FALSE\n14:    6.0   3.4 versicolor    FALSE\n15:    6.7   3.1 versicolor    FALSE\n16:    6.3   3.3  virginica    FALSE\n17:    7.1   3.0  virginica    FALSE\n18:    7.6   3.0  virginica    FALSE\n19:    7.3   2.9  virginica    FALSE\n20:    7.2   3.6  virginica     TRUE\n...   1\n2 # Add new boolean column \niris2 [,  is_large  :=  Width  *  Length  >   25 ]    1\n2 # Now large observations with is_large \niris2 [ is_large  ==   TRUE ]    1\n2\n3\n4\n5    Length Width   Species is_large\n1:    5.7   4.4    setosa     TRUE\n2:    7.2   3.6 virginica     TRUE\n3:    7.7   3.8 virginica     TRUE\n4:    7.9   3.8 virginica     TRUE   1 iris2 [( is_large )]   # Also OK    1\n2\n3\n4\n5    Length Width   Species is_large\n1:    5.7   4.4    setosa     TRUE\n2:    7.2   3.6 virginica     TRUE\n3:    7.7   3.8 virginica     TRUE\n4:    7.9   3.8 virginica     TRUE",
            "title": "Understanding automatic indexing"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#selecting-groups-or-parts-of-groups",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 # The 'keyed' data.table DT \nDT  <-  data.table ( A  =   letters [ c ( 2 ,   1 ,   2 ,   3 ,   1 ,   2 ,   3 )], \n                 B  =   c ( 5 ,   4 ,   1 ,   9 , 8   , 8 ,   6 ),  \n                 C  =   6 : 12 ) \nsetkey ( DT ,  A ,  B )  # Select the 'b' group \nDT [ 'b' ]    1\n2\n3\n4 ##    A B  C\n## 1: b 1  8\n## 2: b 5  6\n## 3: b 8 11   1\n2 # 'b' and 'c' groups \nDT [ c ( 'b' ,   'c' )]    1\n2\n3\n4\n5\n6 ##    A B  C\n## 1: b 1  8\n## 2: b 5  6\n## 3: b 8 11\n## 4: c 6 12\n## 5: c 9  9   1\n2 # The first row of the 'b' and 'c' groups \nDT [ c ( 'b' ,   'c' ),  mult  =   'first' ]    1\n2\n3 ##    A B  C\n## 1: b 1  8\n## 2: c 6 12   1\n2 # First and last row of the 'b' and 'c' groups \nDT [ c ( 'b' ,   'c' ),   . SD [ c ( 1 ,   . N )],  by  =   . EACHI ]    1\n2\n3\n4\n5 ##    A B  C\n## 1: b 1  8\n## 2: b 8 11\n## 3: c 6 12\n## 4: c 9  9   1\n2 # Copy and extend code for instruction 4: add printout \nDT [ c ( 'b' ,   'c' ),   {   print ( . SD );   . SD [ c ( 1 ,   . N )]   },  by  =   . EACHI ]     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 ##    B  C\n## 1: 1  8\n## 2: 5  6\n## 3: 8 11\n##    B  C\n## 1: 6 12\n## 2: 9  9\n\n##    A B  C\n## 1: b 1  8\n## 2: b 8 11\n## 3: c 6 12\n## 4: c 9  9",
            "title": "Selecting groups or parts of groups"
        },
        {
            "location": "/Data_Analysis_in_R,_the_data.table_Way/#rolling-joins",
            "text": "Rolling joins \u2013 part one  1\n2\n3\n4\n5\n6\n7 # Keyed data.table DT \nDT  <-  data.table ( A  =   letters [ c ( 2 ,   1 ,   2 ,   3 ,   1 ,   2 ,   3 )],                   B  =   c ( 5 ,   4 ,   1 ,   9 ,   8 ,   8 ,   6 ),  \n                 C  =   6 : 12 ,  \n                 key  =   'A,B' )  # Get the key of DT \nkey ( DT )    1 ## [1] \"A\" \"B\"   1\n2\n3 # Row where A == 'b' & B == 6 \nsetkey ( DT ,  A ,  B ) \nDT [ . ( 'b' ,   6 )]    1\n2 ##    A B  C\n## 1: b 6 NA   1\n2 # Return the prevailing row \nDT [ . ( 'b' , 6 ),  roll  =   TRUE ]    1\n2 ##    A B C\n## 1: b 6 6   1\n2 # Return the nearest row \nDT [ . ( 'b' , 6 ),  roll  =+   Inf ]    1\n2 ##    A B C\n## 1: b 6 6   Rolling joins \u2013 part two  1\n2\n3\n4\n5\n6\n7 # Keyed data.table DT \nDT  <-  data.table ( A  =   letters [ c ( 2 ,   1 ,   2 ,   3 ,   1 ,   2 ,   3 )],                   B  =   c ( 5 ,   4 ,   1 ,   9 ,   8 ,   8 ,   6 ),  \n                 C  =   6 : 12 ,  \n                 key  =   'A,B' )  # Look at the sequence (-2):10 for the 'b' group \nDT [ . ( 'b' ,   ( -2 ) : 10 )]     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ##     A  B  C\n##  1: b -2 NA\n##  2: b -1 NA\n##  3: b  0 NA\n##  4: b  1  8\n##  5: b  2 NA\n##  6: b  3 NA\n##  7: b  4 NA\n##  8: b  5  6\n##  9: b  6 NA\n## 10: b  7 NA\n## 11: b  8 11\n## 12: b  9 NA\n## 13: b 10 NA   1\n2 # Add code: carry the prevailing values forwards \nDT [ . ( 'b' ,   ( -2 ) : 10 ),  roll  =   TRUE ]     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ##     A  B  C\n##  1: b -2 NA\n##  2: b -1 NA\n##  3: b  0 NA\n##  4: b  1  8\n##  5: b  2  8\n##  6: b  3  8\n##  7: b  4  8\n##  8: b  5  6\n##  9: b  6  6\n## 10: b  7  6\n## 11: b  8 11\n## 12: b  9 11\n## 13: b 10 11   1\n2 # Add code: carry the first observation backwards \nDT [ . ( 'b' ,   ( -2 ) : 10 ),  roll  =   TRUE ,  rollends  =   TRUE ]     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ##     A  B  C\n##  1: b -2  8\n##  2: b -1  8\n##  3: b  0  8\n##  4: b  1  8\n##  5: b  2  8\n##  6: b  3  8\n##  7: b  4  8\n##  8: b  5  6\n##  9: b  6  6\n## 10: b  7  6\n## 11: b  8 11\n## 12: b  9 11\n## 13: b 10 11",
            "title": "Rolling joins"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nDocumentation\n\u00b6\n\n\ndata.table\n\n\n\n\nextension of \ndata.frame\n.\n\n\nFast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, a fast friendly file reader and parallel file writer. Offers a natural and flexible syntax, for faster development.\n\n\n\n\ndplyr\n\n\n\n\nA fast, consistent tool for working with data frame like objects, both in memory and out of memory.\n\n\nPipelines.\n\n\n\n\ntidyr\n\n\n\n\nAn evolution of \u2018reshape2\u2019. It\u2019s designed specifically for data tidying (not general reshaping or aggregating) and works well with dplyr data pipelines.\n\n\n\n\n\n\n\n\n\n\npackage\n\n\n'narrower'\n\n\n'wider'\n\n\n\n\n\n\n\n\n\n\ntidyr\n\n\ngather\n\n\nspread\n\n\n\n\n\n\nreshape2\n\n\nmelt\n\n\ncast\n\n\n\n\n\n\nspreadsheets\n\n\nunpivot\n\n\npivot\n\n\n\n\n\n\ndatabases\n\n\nfold\n\n\nunfold\n\n\n\n\n\n\n\n\n\nIntroduction to \ndplyr\n\u00b6\n\n\nLoad the \ndplyr\n and \nhflights\n package\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Load the dplyr package\n\n\nlibrary\n(\ndplyr\n)\n\n\nlibrary\n(\ndtplyr\n)\n\n\n\n# Load the hflights package\n\n\n# A data only package containing commercial domestic flights that departed Houston (IAH and HOU) in 2011\n\n\nlibrary\n(\nhflights\n)\n\n\n\n# Call both head() and summary() on hflights\n\n\nhead\n(\nhflights\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n##      Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## 5424 2011     1          1         6    1400    1500            AA\n## 5425 2011     1          2         7    1401    1501            AA\n## 5426 2011     1          3         1    1352    1502            AA\n## 5427 2011     1          4         2    1403    1513            AA\n## 5428 2011     1          5         3    1405    1507            AA\n## 5429 2011     1          6         4    1359    1503            AA\n##      FlightNum TailNum ActualElapsedTime AirTime ArrDelay DepDelay Origin\n## 5424       428  N576AA                60      40      -10        0    IAH\n## 5425       428  N557AA                60      45       -9        1    IAH\n## 5426       428  N541AA                70      48       -8       -8    IAH\n## 5427       428  N403AA                70      39        3        3    IAH\n## 5428       428  N492AA                62      44       -3        5    IAH\n## 5429       428  N262AA                64      45       -7       -1    IAH\n##      Dest Distance TaxiIn TaxiOut Cancelled CancellationCode Diverted\n## 5424  DFW      224      7      13         0                         0\n## 5425  DFW      224      6       9         0                         0\n## 5426  DFW      224      5      17         0                         0\n## 5427  DFW      224      9      22         0                         0\n## 5428  DFW      224      9       9         0                         0\n## 5429  DFW      224      6      13         0                         0\n\n\n\n\n\n\n1\nsummary\n(\nhflights\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n##       Year          Month          DayofMonth      DayOfWeek    \n\n\n##  Min.   :2011   Min.   : 1.000   Min.   : 1.00   Min.   :1.000  \n\n\n##  1st Qu.:2011   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.:2.000  \n\n\n##  Median :2011   Median : 7.000   Median :16.00   Median :4.000  \n\n\n##  Mean   :2011   Mean   : 6.514   Mean   :15.74   Mean   :3.948  \n\n\n##  3rd Qu.:2011   3rd Qu.: 9.000   3rd Qu.:23.00   3rd Qu.:6.000  \n\n\n##  Max.   :2011   Max.   :12.000   Max.   :31.00   Max.   :7.000  \n\n\n##                                                                 \n\n\n##     DepTime        ArrTime     UniqueCarrier        FlightNum   \n\n\n##  Min.   :   1   Min.   :   1   Length:227496      Min.   :   1  \n\n\n##  1st Qu.:1021   1st Qu.:1215   Class :character   1st Qu.: 855  \n\n\n##  Median :1416   Median :1617   Mode  :character   Median :1696  \n\n\n##  Mean   :1396   Mean   :1578                      Mean   :1962  \n\n\n##  3rd Qu.:1801   3rd Qu.:1953                      3rd Qu.:2755  \n\n\n##  Max.   :2400   Max.   :2400                      Max.   :7290  \n\n\n##  NA's   :2905   NA's   :3066                                    \n\n\n##    TailNum          ActualElapsedTime    AirTime         ArrDelay      \n\n\n##  Length:227496      Min.   : 34.0     Min.   : 11.0   Min.   :-70.000  \n\n\n##  Class :character   1st Qu.: 77.0     1st Qu.: 58.0   1st Qu.: -8.000  \n\n\n##  Mode  :character   Median :128.0     Median :107.0   Median :  0.000  \n\n\n##                     Mean   :129.3     Mean   :108.1   Mean   :  7.094  \n\n\n##                     3rd Qu.:165.0     3rd Qu.:141.0   3rd Qu.: 11.000  \n\n\n##                     Max.   :575.0     Max.   :549.0   Max.   :978.000  \n\n\n##                     NA's   :3622      NA's   :3622    NA's   :3622     \n\n\n##     DepDelay          Origin              Dest              Distance     \n\n\n##  Min.   :-33.000   Length:227496      Length:227496      Min.   :  79.0  \n\n\n##  1st Qu.: -3.000   Class :character   Class :character   1st Qu.: 376.0  \n\n\n##  Median :  0.000   Mode  :character   Mode  :character   Median : 809.0  \n\n\n##  Mean   :  9.445                                         Mean   : 787.8  \n\n\n##  3rd Qu.:  9.000                                         3rd Qu.:1042.0  \n\n\n##  Max.   :981.000                                         Max.   :3904.0  \n\n\n##  NA's   :2905                                                            \n\n\n##      TaxiIn           TaxiOut         Cancelled       CancellationCode  \n\n\n##  Min.   :  1.000   Min.   :  1.00   Min.   :0.00000   Length:227496     \n\n\n##  1st Qu.:  4.000   1st Qu.: 10.00   1st Qu.:0.00000   Class :character  \n\n\n##  Median :  5.000   Median : 14.00   Median :0.00000   Mode  :character  \n\n\n##  Mean   :  6.099   Mean   : 15.09   Mean   :0.01307                     \n\n\n##  3rd Qu.:  7.000   3rd Qu.: 18.00   3rd Qu.:0.00000                     \n\n\n##  Max.   :165.000   Max.   :163.00   Max.   :1.00000                     \n\n\n##  NA's   :3066      NA's   :2947                                         \n\n\n##     Diverted       \n\n\n##  Min.   :0.000000  \n\n\n##  1st Qu.:0.000000  \n\n\n##  Median :0.000000  \n\n\n##  Mean   :0.002853  \n\n\n##  3rd Qu.:0.000000  \n\n\n##  Max.   :1.000000  \n\n\n##\n\n\n\n\n\n\n\nConvert \ndata.frame\n to table\n\u00b6\n\n\n1\n2\n3\n4\n5\n# Convert the hflights data.frame into a hflights tbl\n\nhflights \n<-\n tbl_df\n(\nhflights\n)\n\n\n\n# Display the hflights tbl\n\nhflights\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## *  <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6    1400    1500            AA\n## 2   2011     1          2         7    1401    1501            AA\n## 3   2011     1          3         1    1352    1502            AA\n## 4   2011     1          4         2    1403    1513            AA\n## 5   2011     1          5         3    1405    1507            AA\n## 6   2011     1          6         4    1359    1503            AA\n## 7   2011     1          7         5    1359    1509            AA\n## 8   2011     1          8         6    1355    1454            AA\n## 9   2011     1          9         7    1443    1554            AA\n## 10  2011     1         10         1    1443    1553            AA\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\n1\n2\n# Create the object carriers, containing only the UniqueCarrier variable of hflights\n\ncarriers \n<-\n hflights\n$\nUniqueCarrier\n\n\n\n\n\n\nChanging labels of \nhflights\n\u00b6\n\n\npart 1 of 2\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# add\n\nlut \n<-\n \nc\n(\n'AA'\n \n=\n \n'American'\n,\n \n'AS'\n \n=\n \n'Alaska'\n,\n \n'B6'\n \n=\n \n'JetBlue'\n,\n \n'CO'\n \n=\n \n'Continental'\n,\n \n         \n'DL'\n \n=\n \n'Delta'\n,\n \n'OO'\n \n=\n \n'SkyWest'\n,\n \n'UA'\n \n=\n \n'United'\n,\n \n'US'\n \n=\n \n'US_Airways'\n,\n \n         \n'WN'\n \n=\n \n'Southwest'\n,\n \n'EV'\n \n=\n \n'Atlantic_Southeast'\n,\n \n'F9'\n \n=\n \n'Frontier'\n,\n \n         \n'FL'\n \n=\n \n'AirTran'\n,\n \n'MQ'\n \n=\n \n'American_Eagle'\n,\n \n'XE'\n \n=\n \n'ExpressJet'\n,\n \n'YV'\n \n=\n \n'Mesa'\n)\n\n\n\n# Use lut to translate the UniqueCarrier column of hflights\n\nhflights\n$\nUniqueCarrier \n<-\n lut\n[\nhflights\n$\nUniqueCarrier\n]\n\n\n\n# Inspect the resulting raw values of your variables\n\nglimpse\n(\nhflights\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n## Observations: 227,496\n## Variables: 21\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n\n\n\n\n\n\npart 2 of 2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Build the lookup table: lut\n\nlut \n<-\n \nc\n(\n\"A\"\n \n=\n \n\"carrier\"\n,\n \n\"B\"\n \n=\n \n\"weather\"\n \n,\n\"C\"\n \n=\n \n\"FFA\"\n \n,\n\"D\"\n \n=\n \n\"security\"\n,\n \n\"E\"\n \n=\n \n\"not cancelled\"\n)\n\n\n\n# Add the Code column\n\nhflights\n$\nCode \n<-\n lut\n[\nhflights\n$\nCancellationCode\n]\n\n\n\n# Glimpse at hflights\n\nglimpse\n(\nhflights\n)\n\n\n\n\n\n\n\nResult.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nObservations: 227,496\nVariables: 22\n$ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011,...\n$ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n$ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...\n$ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,...\n$ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359,...\n$ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503,...\n$ UniqueCarrier     <chr> \"American\", \"American\", \"American\",...\n$ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, ...\n$ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403...\n$ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71,...\n$ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41,...\n$ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44...\n$ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43,...\n$ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", ...\n$ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", ...\n$ Distance          <int> 224, 224, 224, 224, 224, 224, 224, ...\n$ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4...\n$ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 1...\n$ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",...\n$ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ Code              <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA,...\n\n\n\n\n\n\nselect\n and \nmutate\n\u00b6\n\n\nThe five verbs and their meaning\n\u00b6\n\n\n\n\nselect\n; which returns a subset of the columns.\n\n\nfilter\n; that is able to return a subset of the rows.\n\n\narrange\n; that reorders the rows according to single or multiple variables.\n\n\nmutate\n; used to add columns from existing data.\n\n\nsummarise\n; which reduces each group to a single row by calculating aggregate measures.\n\n\n\n\nThe \nselect\n verb\n\u00b6\n\n\n1\n2\n# Print out a tbl with the four columns of hflights related to delay\n\nselect\n(\nhflights\n,\n ActualElapsedTime\n,\n AirTime\n,\n ArrDelay\n,\n DepDelay\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n## # A tibble: 227,496 \u00d7 4\n##    ActualElapsedTime AirTime ArrDelay DepDelay\n## *              <int>   <int>    <int>    <int>\n## 1                 60      40      -10        0\n## 2                 60      45       -9        1\n## 3                 70      48       -8       -8\n## 4                 70      39        3        3\n## 5                 62      44       -3        5\n## 6                 64      45       -7       -1\n## 7                 70      43       -1       -1\n## 8                 59      40      -16       -5\n## 9                 71      41       44       43\n## 10                70      45       43       43\n## # ... with 227,486 more rows\n\n\n\n\n\n\n1\n2\n# Print out hflights, nothing has changed!\n\nhflights\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## *  <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6    1400    1500      American\n## 2   2011     1          2         7    1401    1501      American\n## 3   2011     1          3         1    1352    1502      American\n## 4   2011     1          4         2    1403    1513      American\n## 5   2011     1          5         3    1405    1507      American\n## 6   2011     1          6         4    1359    1503      American\n## 7   2011     1          7         5    1359    1509      American\n## 8   2011     1          8         6    1355    1454      American\n## 9   2011     1          9         7    1443    1554      American\n## 10  2011     1         10         1    1443    1553      American\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\n1\n2\n# Print out the columns Origin up to Cancelled of hflights\n\nselect\n(\nhflights\n,\n \n14\n:\n19\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n## # A tibble: 227,496 \u00d7 6\n##    Origin  Dest Distance TaxiIn TaxiOut Cancelled\n## *   <chr> <chr>    <int>  <int>   <int>     <int>\n## 1     IAH   DFW      224      7      13         0\n## 2     IAH   DFW      224      6       9         0\n## 3     IAH   DFW      224      5      17         0\n## 4     IAH   DFW      224      9      22         0\n## 5     IAH   DFW      224      9       9         0\n## 6     IAH   DFW      224      6      13         0\n## 7     IAH   DFW      224     12      15         0\n## 8     IAH   DFW      224      7      12         0\n## 9     IAH   DFW      224      8      22         0\n## 10    IAH   DFW      224      6      19         0\n## # ... with 227,486 more rows\n\n\n\n\n\n\n1\n2\n# Answer to last question: be concise!\n\nselect\n(\nhflights\n,\n \n1\n:\n4\n,\n \n12\n:\n21\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n## # A tibble: 227,496 \u00d7 14\n##     Year Month DayofMonth DayOfWeek ArrDelay DepDelay Origin  Dest\n## *  <int> <int>      <int>     <int>    <int>    <int>  <chr> <chr>\n## 1   2011     1          1         6      -10        0    IAH   DFW\n## 2   2011     1          2         7       -9        1    IAH   DFW\n## 3   2011     1          3         1       -8       -8    IAH   DFW\n## 4   2011     1          4         2        3        3    IAH   DFW\n## 5   2011     1          5         3       -3        5    IAH   DFW\n## 6   2011     1          6         4       -7       -1    IAH   DFW\n## 7   2011     1          7         5       -1       -1    IAH   DFW\n## 8   2011     1          8         6      -16       -5    IAH   DFW\n## 9   2011     1          9         7       44       43    IAH   DFW\n## 10  2011     1         10         1       43       43    IAH   DFW\n## # ... with 227,486 more rows, and 6 more variables: Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\nHelper functions for variable selection\n\u00b6\n\n\nselect\n:\n\n\n\n\nstarts_with(\"X\")\n; every name that starts with \n\"X\"\n,\n\n\nends_with(\"X\")\n; every name that ends with \n\"X\"\n,\n\n\ncontains(\"X\")\n; every name that contains \n\"X\"\n,\n\n\nmatches(\"X\")\n; every name that matches \n\"X\"\n, where \n\"X\"\n can be a regular expression,\n\n\nnum_range(\"x\", 1:5)\n; the variables named \nx01\n, \nx02\n, \nx03\n, \nx04\n and \nx05\n,\n\n\none_of(x)\n; every name that appears in \nx\n, which should be a character vector.\n\n\n\n\n\n\n\n1\n2\n# Print out a tbl containing just ArrDelay and DepDelay\n\nselect\n(\nhflights\n,\n ArrDelay\n,\n DepDelay\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n## # A tibble: 227,496 \u00d7 2\n##    ArrDelay DepDelay\n## *     <int>    <int>\n## 1       -10        0\n## 2        -9        1\n## 3        -8       -8\n## 4         3        3\n## 5        -3        5\n## 6        -7       -1\n## 7        -1       -1\n## 8       -16       -5\n## 9        44       43\n## 10       43       43\n## # ... with 227,486 more rows\n\n\n\n\n\n\n1\n2\n# Print out a tbl as described in the second instruction, using both helper functions and variable names\n\nselect\n(\nhflights\n,\n UniqueCarrier\n,\n ends_with\n(\n'Num'\n),\n starts_with\n(\n'Cancel'\n))\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n## # A tibble: 227,496 \u00d7 5\n##    UniqueCarrier FlightNum TailNum Cancelled CancellationCode\n## *          <chr>     <int>   <chr>     <int>            <chr>\n## 1       American       428  N576AA         0                 \n## 2       American       428  N557AA         0                 \n## 3       American       428  N541AA         0                 \n## 4       American       428  N403AA         0                 \n## 5       American       428  N492AA         0                 \n## 6       American       428  N262AA         0                 \n## 7       American       428  N493AA         0                 \n## 8       American       428  N477AA         0                 \n## 9       American       428  N476AA         0                 \n## 10      American       428  N504AA         0                 \n## # ... with 227,486 more rows\n\n\n\n\n\n\n1\n2\n# Print out a tbl as described in the third instruction, using only helper functions.\n\nselect\n(\nhflights\n,\n ends_with\n(\n'Time'\n),\n ends_with\n(\n'Delay'\n))\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n## # A tibble: 227,496 \u00d7 6\n##    DepTime ArrTime ActualElapsedTime AirTime ArrDelay DepDelay\n## *    <int>   <int>             <int>   <int>    <int>    <int>\n## 1     1400    1500                60      40      -10        0\n## 2     1401    1501                60      45       -9        1\n## 3     1352    1502                70      48       -8       -8\n## 4     1403    1513                70      39        3        3\n## 5     1405    1507                62      44       -3        5\n## 6     1359    1503                64      45       -7       -1\n## 7     1359    1509                70      43       -1       -1\n## 8     1355    1454                59      40      -16       -5\n## 9     1443    1554                71      41       44       43\n## 10    1443    1553                70      45       43       43\n## # ... with 227,486 more rows\n\n\n\n\n\n\nComparison to basic R\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# add\n\nex1r \n<-\n hflights\n[\nc\n(\n'TaxiIn'\n,\n'TaxiOut'\n,\n'Distance'\n)]\n\n\nex1d \n<-\n select\n(\nhflights\n,\n starts_with\n(\n'Taxi'\n),\n Distance\n)\n\n\nex2r \n<-\n hflights\n[\nc\n(\n'Year'\n,\n'Month'\n,\n'DayOfWeek'\n,\n'DepTime'\n,\n'ArrTime'\n)]\n\n\nex2d \n<-\n select\n(\nhflights\n,\n Year\n,\n Month\n,\n DayOfWeek\n,\n DepTime\n,\n ArrTime\n)\n\n\nex3r \n<-\n hflights\n[\nc\n(\n'TailNum'\n,\n'TaxiIn'\n,\n'TaxiOut'\n)]\n\n\nex3d \n<-\n select\n(\nhflights\n,\n TailNum\n,\n starts_with\n(\n'Taxi'\n))\n\n\n\n\n\n\n\nmutate\n is creating\n\u00b6\n\n\n1\n2\n3\n# Add the new variable ActualGroundTime to a copy of hflights and save the result as g1\n\ng1 \n<-\n mutate\n(\nhflights\n,\n ActualGroundTime \n=\n ActualElapsedTime \n-\n AirTime\n)\n\nglimpse\n(\nhflights\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n## Observations: 227,496\n## Variables: 21\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n\n\n\n\n\n\n1\nglimpse\n(\ng1\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n## Observations: 227,496\n## Variables: 22\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ ActualGroundTime  <int> 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n\n\n\n\n\n\n1\n2\n3\n4\n# Add the new variable GroundTime to a g1; save the result as g2\n\ng2 \n<-\n mutate\n(\ng1\n,\n GroundTime \n=\n TaxiIn \n+\n TaxiOut\n)\n\n\n\nhead\n(\ng1\n$\nActualGroundTime \n==\n g2\n$\nGroundTime\n,\n \n20\n)\n\n\n\n\n\n\n\n1\n2\n##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n## [15] TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n\n\n\n\n1\n2\n3\n# Add the new variable AverageSpeed to g2; save the result as g3\n\ng3 \n<-\n mutate\n(\ng2\n,\n AverageSpeed \n=\n Distance \n/\n AirTime \n*\n \n60\n)\n\ng3\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n## # A tibble: 227,496 \u00d7 24\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6    1400    1500      American\n## 2   2011     1          2         7    1401    1501      American\n## 3   2011     1          3         1    1352    1502      American\n## 4   2011     1          4         2    1403    1513      American\n## 5   2011     1          5         3    1405    1507      American\n## 6   2011     1          6         4    1359    1503      American\n## 7   2011     1          7         5    1359    1509      American\n## 8   2011     1          8         6    1355    1454      American\n## 9   2011     1          9         7    1443    1554      American\n## 10  2011     1         10         1    1443    1553      American\n## # ... with 227,486 more rows, and 17 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>, ActualGroundTime <int>, GroundTime <int>,\n## #   AverageSpeed <dbl>\n\n\n\n\n\n\nAdd multiple variables using \nmutate\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Add a second variable loss_percent to the dataset: m1\n\nm1 \n<-\n mutate\n(\nhflights\n,\n loss \n=\n ArrDelay \n-\n DepDelay\n,\n loss_percent \n=\n \n(\nArrDelay \n-\n DepDelay\n)\n/\nDepDelay\n*\n100\n)\n\n\n\n# Copy and adapt the previous command to reduce redendancy: m2\n\nm2 \n<-\n mutate\n(\nhflights\n,\n loss \n=\n ArrDelay \n-\n DepDelay\n,\n loss_percent \n=\n loss\n/\nDepDelay \n*\n \n100\n)\n\n\n\n# Add the three variables as described in the third instruction: m3\n\nm3 \n<-\n mutate\n(\nhflights\n,\n TotalTaxi \n=\n TaxiIn \n+\n TaxiOut\n,\n ActualGroundTime \n=\n ActualElapsedTime \n-\n AirTime\n,\n Diff \n=\n TotalTaxi \n-\n ActualGroundTime\n)\n\nglimpse\n(\nm3\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n## Observations: 227,496\n## Variables: 24\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ TotalTaxi         <int> 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n## $ ActualGroundTime  <int> 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n## $ Diff              <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n\n\n\n\n\n\nfilter\n and \narrange\n\u00b6\n\n\nLogical operators\n\u00b6\n\n\nfilter\n:\n\n\n\n\nx < y\n; \nTRUE\n if \nx\n is less than \ny\n.\n\n\nx <= y\n; \nTRUE\n if \nx\n is less than or equal to \ny\n.\n\n\nx == y\n; \nTRUE\n if \nx\n equals \ny\n.\n\n\nx != y\n; \nTRUE\n if \nx\n does not equal \ny\n.\n\n\nx >= y\n; \nTRUE\n if \nx\n is greater than or equal to \ny\n.\n\n\nx > y\n; \nTRUE\n if \nx\n is greater than \ny\n.\n\n\nx %in% c(a, b, c)\n; \nTRUE\n if \nx\n is in the vector \nc(a, b, c)\n.\n\n\n\n\n\n\n\n1\n2\n# All flights that traveled 3000 miles or more\n\nfilter\n(\nhflights\n,\n Distance \n>=\n \n3000\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 527 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1         31         1     924    1413   Continental\n## 2   2011     1         30         7     925    1410   Continental\n## 3   2011     1         29         6    1045    1445   Continental\n## 4   2011     1         28         5    1516    1916   Continental\n## 5   2011     1         27         4     950    1344   Continental\n## 6   2011     1         26         3     944    1350   Continental\n## 7   2011     1         25         2     924    1337   Continental\n## 8   2011     1         24         1    1144    1605   Continental\n## 9   2011     1         23         7     926    1335   Continental\n## 10  2011     1         22         6     942    1340   Continental\n## # ... with 517 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\n1\n2\n# All flights flown by one of JetBlue, Southwest, or Delta\n\nfilter\n(\nhflights\n,\n UniqueCarrier \n%in%\n \nc\n(\n'JetBlue'\n,\n'Southwest'\n,\n'Delta'\n))\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 48,679 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6     654    1124       JetBlue\n## 2   2011     1          1         6    1639    2110       JetBlue\n## 3   2011     1          2         7     703    1113       JetBlue\n## 4   2011     1          2         7    1604    2040       JetBlue\n## 5   2011     1          3         1     659    1100       JetBlue\n## 6   2011     1          3         1    1801    2200       JetBlue\n## 7   2011     1          4         2     654    1103       JetBlue\n## 8   2011     1          4         2    1608    2034       JetBlue\n## 9   2011     1          5         3     700    1103       JetBlue\n## 10  2011     1          5         3    1544    1954       JetBlue\n## # ... with 48,669 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\n1\n2\n# All flights where taxiing took longer than flying\n\nfilter\n(\nhflights\n,\n \n(\nTaxiIn \n+\n TaxiOut\n)\n \n>\n AirTime\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 1,389 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1         24         1     731     904      American\n## 2   2011     1         30         7    1959    2132      American\n## 3   2011     1         24         1    1621    1749      American\n## 4   2011     1         10         1     941    1113      American\n## 5   2011     1         31         1    1301    1356   Continental\n## 6   2011     1         31         1    2113    2215   Continental\n## 7   2011     1         31         1    1434    1539   Continental\n## 8   2011     1         31         1     900    1006   Continental\n## 9   2011     1         30         7    1304    1408   Continental\n## 10  2011     1         30         7    2004    2128   Continental\n## # ... with 1,379 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\nCombining tests using boolean operators\n\u00b6\n\n\n1\n2\n# All flights that departed before 5am or arrived after 10pm\n\nfilter\n(\nhflights\n,\n DepTime \n<\n \n500\n \n|\n ArrTime \n>\n \n2200\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 27,799 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          4         2    2100    2207      American\n## 2   2011     1         14         5    2119    2229      American\n## 3   2011     1         10         1    1934    2235      American\n## 4   2011     1         26         3    1905    2211      American\n## 5   2011     1         30         7    1856    2209      American\n## 6   2011     1          9         7    1938    2228        Alaska\n## 7   2011     1         31         1    1919    2231   Continental\n## 8   2011     1         31         1    2116    2344   Continental\n## 9   2011     1         31         1    1850    2211   Continental\n## 10  2011     1         31         1    2102    2216   Continental\n## # ... with 27,789 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\n1\n2\n# All flights that departed late but arrived ahead of schedule\n\nfilter\n(\nhflights\n,\n DepDelay \n>\n \n0\n \n&\n ArrDelay \n<\n \n0\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 27,712 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          2         7    1401    1501      American\n## 2   2011     1          5         3    1405    1507      American\n## 3   2011     1         18         2    1408    1508      American\n## 4   2011     1         18         2     721     827      American\n## 5   2011     1         12         3    2015    2113      American\n## 6   2011     1         13         4    2020    2116      American\n## 7   2011     1         26         3    2009    2103      American\n## 8   2011     1          1         6    1631    1736      American\n## 9   2011     1         10         1    1639    1740      American\n## 10  2011     1         12         3    1631    1739      American\n## # ... with 27,702 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\n1\n2\n# All cancelled weekend flights\n\nfilter\n(\nhflights\n,\n DayOfWeek \n%in%\n \nc\n(\n6\n,\n7\n)\n \n&\n Cancelled \n==\n \n1\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 585 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime      UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>              <chr>\n## 1   2011     1          9         7      NA      NA           American\n## 2   2011     1         29         6      NA      NA        Continental\n## 3   2011     1          9         7      NA      NA        Continental\n## 4   2011     1          9         7      NA      NA              Delta\n## 5   2011     1          9         7      NA      NA            SkyWest\n## 6   2011     1          2         7      NA      NA          Southwest\n## 7   2011     1         29         6      NA      NA              Delta\n## 8   2011     1          9         7      NA      NA Atlantic_Southeast\n## 9   2011     1          1         6      NA      NA            AirTran\n## 10  2011     1          9         7      NA      NA            AirTran\n## # ... with 575 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\n1\n2\n# All flights that were cancelled after being delayed\n\nfilter\n(\nhflights\n,\n DepDelay \n>\n \n0\n \n&\n Cancelled \n==\n \n1\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 40 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1         26         3    1926      NA   Continental\n## 2   2011     1         11         2    1100      NA    US_Airways\n## 3   2011     1         19         3    1811      NA    ExpressJet\n## 4   2011     1          7         5    2028      NA    ExpressJet\n## 5   2011     2          4         5    1638      NA      American\n## 6   2011     2          8         2    1057      NA   Continental\n## 7   2011     2          2         3     802      NA    ExpressJet\n## 8   2011     2          9         3     904      NA    ExpressJet\n## 9   2011     2          1         2    1508      NA       SkyWest\n## 10  2011     3         31         4    1016      NA   Continental\n## # ... with 30 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\nBlend together\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Select the flights that had JFK as their destination: c1\n\nc1 \n<-\n filter\n(\nhflights\n,\n Dest \n==\n \n'JFK'\n)\n\n\n\n# Combine the Year, Month and DayofMonth variables to create a Date column: c2\n\nc2 \n<-\n mutate\n(\nc1\n,\n Date \n=\n \npaste\n(\nYear\n,\n Month\n,\n DayofMonth\n,\n sep \n=\n \n'-'\n))\n\n\n\n# Print out a selection of columns of c2\n\nselect\n(\nc2\n,\n Date\n,\n DepTime\n,\n ArrTime\n,\n TailNum\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n## # A tibble: 695 \u00d7 4\n##        Date DepTime ArrTime TailNum\n##       <chr>   <int>   <int>   <chr>\n## 1  2011-1-1     654    1124  N324JB\n## 2  2011-1-1    1639    2110  N324JB\n## 3  2011-1-2     703    1113  N324JB\n## 4  2011-1-2    1604    2040  N324JB\n## 5  2011-1-3     659    1100  N229JB\n## 6  2011-1-3    1801    2200  N206JB\n## 7  2011-1-4     654    1103  N267JB\n## 8  2011-1-4    1608    2034  N267JB\n## 9  2011-1-5     700    1103  N708JB\n## 10 2011-1-5    1544    1954  N644JB\n## # ... with 685 more rows\n\n\n\n\n\n\nArranging your data\n\u00b6\n\n\n1\n2\n3\n4\n5\n# Definition of dtc\n\ndtc \n<-\n filter\n(\nhflights\n,\n Cancelled \n==\n \n1\n,\n \n!\nis.na\n(\nDepDelay\n))\n\n\n\n# Arrange dtc by departure delays\n\narrange\n(\ndtc\n,\n DepDelay\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>          <chr>\n## 1   2011     7         23         6     605      NA       Frontier\n## 2   2011     1         17         1     916      NA     ExpressJet\n## 3   2011    12          1         4     541      NA     US_Airways\n## 4   2011    10         12         3    2022      NA American_Eagle\n## 5   2011     7         29         5    1424      NA    Continental\n## 6   2011     9         29         4    1639      NA        SkyWest\n## 7   2011     2          9         3     555      NA American_Eagle\n## 8   2011     5          9         1     715      NA        SkyWest\n## 9   2011     1         20         4    1413      NA         United\n## 10  2011     1         17         1     831      NA      Southwest\n## # ... with 58 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\n1\n2\n# Arrange dtc so that cancellation reasons are grouped\n\narrange\n(\ndtc\n,\n CancellationCode\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>          <chr>\n## 1   2011     1         20         4    1413      NA         United\n## 2   2011     1          7         5    2028      NA     ExpressJet\n## 3   2011     2          4         5    1638      NA       American\n## 4   2011     2          8         2    1057      NA    Continental\n## 5   2011     2          1         2    1508      NA        SkyWest\n## 6   2011     2         21         1    2257      NA        SkyWest\n## 7   2011     2          9         3     555      NA American_Eagle\n## 8   2011     3         18         5     727      NA         United\n## 9   2011     4          4         1    1632      NA          Delta\n## 10  2011     4          8         5    1608      NA      Southwest\n## # ... with 58 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\n1\n2\n# Arrange dtc according to carrier and departure delays\n\narrange\n(\ndtc\n,\n UniqueCarrier\n,\n DepDelay\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime      UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>              <chr>\n## 1   2011     6         11         6    1649      NA            AirTran\n## 2   2011     8         18         4    1808      NA           American\n## 3   2011     2          4         5    1638      NA           American\n## 4   2011    10         12         3    2022      NA     American_Eagle\n## 5   2011     2          9         3     555      NA     American_Eagle\n## 6   2011     7         17         7    1917      NA     American_Eagle\n## 7   2011     4         30         6     612      NA Atlantic_Southeast\n## 8   2011     4         10         7    1147      NA Atlantic_Southeast\n## 9   2011     5         23         1     657      NA Atlantic_Southeast\n## 10  2011     9         29         4     723      NA Atlantic_Southeast\n## # ... with 58 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\nReverse the order of arranging\n\u00b6\n\n\n1\n2\n# Arrange according to carrier and decreasing departure delays\n\narrange\n(\nhflights\n,\n UniqueCarrier\n,\n desc\n(\nDepDelay\n))\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     2         19         6    1902    2143       AirTran\n## 2   2011     3         14         1    2024    2309       AirTran\n## 3   2011     2         16         3    2349     227       AirTran\n## 4   2011    11         13         7    2312     213       AirTran\n## 5   2011     5         26         4    2353     305       AirTran\n## 6   2011     5         26         4    1922    2229       AirTran\n## 7   2011     4         28         4    1045    1328       AirTran\n## 8   2011     6          5         7    2207      52       AirTran\n## 9   2011     5          7         6    1009    1256       AirTran\n## 10  2011     7         25         1    2107      14       AirTran\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\n1\n2\n# Arrange flights by total delay (normal order).\n\narrange\n(\nhflights\n,\n \n(\nArrDelay\n+\nDepDelay\n))\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     7          3         7    1914    2039    ExpressJet\n## 2   2011     8         31         3     934    1039       SkyWest\n## 3   2011     8         21         7     935    1039       SkyWest\n## 4   2011     8         28         7    2059    2206       SkyWest\n## 5   2011     8         29         1     935    1041       SkyWest\n## 6   2011    12         25         7     741     926       SkyWest\n## 7   2011     1         30         7     620     812       SkyWest\n## 8   2011     8          3         3    1741    1810    ExpressJet\n## 9   2011     8          4         4     930    1041       SkyWest\n## 10  2011     8         18         4     939    1043       SkyWest\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\n1\n2\n# Keep flights leaving to DFW before 8am and arrange according to decreasing AirTime \n\narrange\n(\nfilter\n(\nhflights\n,\n Dest \n==\n \n'DFW'\n \n&\n DepTime\n<\n800\n),\n desc\n(\nAirTime\n))\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 799 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>          <chr>\n## 1   2011    11         22         2     635     825       American\n## 2   2011     8         25         4     602     758 American_Eagle\n## 3   2011    10         12         3     559     738 American_Eagle\n## 4   2011     5          2         1     716     854       American\n## 5   2011     4          4         1     741     949       American\n## 6   2011     4          4         1     627     742 American_Eagle\n## 7   2011     6         21         2     726     848     ExpressJet\n## 8   2011     9          1         4     715     844       American\n## 9   2011     3         14         1     729     917    Continental\n## 10  2011    12          5         1     724     847    Continental\n## # ... with 789 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>\n\n\n\n\n\n\nsummarise\n and the Pipe Operator\n\u00b6\n\n\nThe syntax of \nsummarise\n\u00b6\n\n\n1\n2\n# Print out a summary with variables min_dist and max_dist\n\nsummarise\n(\nhflights\n,\n min_dist \n=\n \nmin\n(\nDistance\n),\n max_dist \n=\n \nmax\n(\nDistance\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n## # A tibble: 1 \u00d7 2\n##   min_dist max_dist\n##      <int>    <int>\n## 1       79     3904\n\n\n\n\n\n\n1\n2\n# Print out a summary with variable max_div\n\nsummarise\n(\nfilter\n(\nhflights\n,\n Diverted \n==\n \n1\n),\n max_div \n=\n \nmax\n(\nDistance\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n## # A tibble: 1 \u00d7 1\n##   max_div\n##     <int>\n## 1    3904\n\n\n\n\n\n\nAggregate functions\n\u00b6\n\n\nsummarise\n:\n\n\n\n\nmin(x)\n; minimum value of vector x.\n\n\nmax(x)\n; maximum value of vector x.\n\n\nmean(x)\n; mean value of vector x.\n\n\nmedian(x)\n; median value of vector x.\n\n\nquantile(x, p)\n; pth quantile of vector x.\n\n\nsd(x)\n; standard deviation of vector x.\n\n\nvar(x)\n; variance of vector x.\n\n\nIQR(x)\n; Inter Quartile Range (IQR) of vector x.\n\n\ndiff(range(x))\n; total range of vector x.\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Remove rows that have NA ArrDelay: temp1\n\ntemp1 \n<-\n filter\n(\nhflights\n,\n \n!\nis.na\n(\nArrDelay\n))\n\n\n\n# Generate summary about ArrDelay column of temp1\n\nsummarise\n(\ntemp1\n,\n earliest \n=\n \nmin\n(\nArrDelay\n),\n average \n=\n \nmean\n(\nArrDelay\n),\n latest \n=\n \nmax\n(\nArrDelay\n),\n sd \n=\n sd\n(\nArrDelay\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n## # A tibble: 1 \u00d7 4\n##   earliest  average latest       sd\n##      <int>    <dbl>  <int>    <dbl>\n## 1      -70 7.094334    978 30.70852\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Keep rows that have no NA TaxiIn and no NA TaxiOut: temp2\n\ntemp2 \n<-\n filter\n(\nhflights\n,\n \n!\nis.na\n(\nTaxiIn\n),\n \n!\nis.na\n(\nTaxiOut\n))\n\n\n\n# Print the maximum taxiing difference of temp2 with summarise()\n\nsummarise\n(\ntemp2\n,\n max_taxi_diff \n=\n \nmax\n(\nabs\n(\nTaxiIn \n-\n TaxiOut\n)))\n\n\n\n\n\n\n\n1\n2\n3\n4\n## # A tibble: 1 \u00d7 1\n##   max_taxi_diff\n##           <int>\n## 1           160\n\n\n\n\n\n\ndplyr\n aggregate functions\n\u00b6\n\n\n\n\nfirst(x)\n; the first element of vector \nx\n.\n\n\nlast(x)\n; the last element of vector \nx\n.\n\n\nnth(x, n)\n; The \nn\nth element of vector \nx\n.\n\n\nn()\n; The number of rows in the data.frame or group of observations that \nsummarise()\n describes.\n\n\nn_distinct(x)\n; The number of unique values in vector \nx\n.\n\n\n\n\n\n\n\n1\n2\n# Generate summarizing statistics for hflights\n\nsummarise\n(\nhflights\n,\n n_obs \n=\n n\n(),\n n_carrier \n=\n n_distinct\n(\nUniqueCarrier\n),\n n_dest \n=\n n_distinct\n(\nDest\n),\n dest100 \n=\n nth\n(\nDest\n,\n \n100\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n## # A tibble: 1 \u00d7 4\n##    n_obs n_carrier n_dest dest100\n##    <int>     <int>  <int>   <chr>\n## 1 227496        15    116     DFW\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Filter hflights to keep all American Airline flights: aa\n\naa \n<-\n filter\n(\nhflights\n,\n UniqueCarrier \n==\n \n'American'\n)\n\n\n\n# Generate summarizing statistics for aa \n\nsummarise\n(\naa\n,\n n_flights \n=\n n\n(),\n n_canc \n=\n \nsum\n(\nCancelled\n),\n p_canc \n=\n n_canc\n/\nn_flights\n*\n100\n,\n avg_delay \n=\n \nmean\n(\nArrDelay\n,\n na.rm \n=\n \nTRUE\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n## # A tibble: 1 \u00d7 4\n##   n_flights n_canc   p_canc avg_delay\n##       <int>  <int>    <dbl>     <dbl>\n## 1      3244     60 1.849568 0.8917558\n\n\n\n\n\n\nOverview of syntax\n\n\n1\n2\n3\n4\n5\n# Write the 'piped' version of the English sentences\n\nhflights \n%>%\n\n    mutate\n(\ndiff \n=\n TaxiOut \n-\n TaxiIn\n)\n \n%>%\n\n    filter\n(\n!\nis.na\n(\ndiff\n))\n \n%>%\n\n    summarise\n(\n avg \n=\n \nmean\n(\ndiff\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n## # A tibble: 1 \u00d7 1\n##        avg\n##      <dbl>\n## 1 8.992064\n\n\n\n\n\n\nDrive or fly? Part 1 of 2\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# Part 1, concerning the selection and creation of columns\n\nd \n<-\n hflights \n%>%\n\n  select\n(\nDest\n,\n UniqueCarrier\n,\n Distance\n,\n ActualElapsedTime\n)\n \n%>%\n  \n  mutate\n(\nRealTime \n=\n ActualElapsedTime \n+\n \n100\n,\n mph \n=\n Distance \n/\n RealTime \n*\n \n60\n)\n\n\n\n# Part 2, concerning flights that had an actual average speed of < 70 mph.\n\nd \n%>%\n\n  filter\n(\n!\nis.na\n(\nmph\n),\n mph \n<\n \n70\n)\n \n%>%\n\n  summarise\n(\n n_less \n=\n n\n(),\n \n             n_dest \n=\n n_distinct\n(\nDest\n),\n \n             min_dist \n=\n \nmin\n(\nDistance\n),\n \n             max_dist \n=\n \nmax\n(\nDistance\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n## # A tibble: 1 \u00d7 4\n##   n_less n_dest min_dist max_dist\n##    <int>  <int>    <int>    <int>\n## 1   6726     13       79      305\n\n\n\n\n\n\nDrive or fly? Part 2 of 2\n\n\n1\n2\n3\n4\n5\n# Solve the exercise using a combination of dplyr verbs and %>%\n\nhflights \n%>%\n\n    \n#summarise(all_flights = n()) %>%\n\n    filter\n(((\nDistance \n/\n \n(\nActualElapsedTime \n+\n \n100\n)\n \n*\n \n60\n)\n \n<\n \n105\n)\n \n|\n Cancelled \n==\n \n1\n \n|\n Diverted \n==\n \n1\n)\n \n%>%\n\n    summarise\n(\nn_non \n=\n n\n(),\n p_non \n=\n n_non \n/\n \n22751\n \n*\n100\n,\n n_dest \n=\n n_distinct\n(\nDest\n),\n min_dist \n=\n \nmin\n(\nDistance\n),\n max_dist \n=\n \nmax\n(\nDistance\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n## # A tibble: 1 \u00d7 5\n##   n_non    p_non n_dest min_dist max_dist\n##   <int>    <dbl>  <int>    <int>    <int>\n## 1 42400 186.3654    113       79     3904\n\n\n\n\n\n\nAdvanced piping exercise\n\n\n1\n2\n3\n4\n# Count the number of overnight flights\n\nhflights \n%>%\n\n    filter\n(\nArrTime \n<\n DepTime \n&\n \n!\nis.na\n(\nDepTime\n)\n \n&\n \n!\nis.na\n(\nArrTime\n))\n \n%>%\n\n    summarise\n(\nn \n=\n n\n())\n\n\n\n\n\n\n\n1\n2\n3\n4\n## # A tibble: 1 \u00d7 1\n##       n\n##   <int>\n## 1  2718\n\n\n\n\n\n\ngroup_by\n and working with data\n\u00b6\n\n\nUnite and conquer using \ngroup_by\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Make an ordered per-carrier summary of hflights\n\nhflights \n%>%\n\n   group_by\n(\nUniqueCarrier\n)\n \n%>%\n\n   summarise\n(\nn_flights \n=\n n\n(),\n \n             n_canc \n=\n \nsum\n(\nCancelled \n==\n \n1\n),\n \n             p_canc \n=\n \nmean\n(\nCancelled \n==\n \n1\n)\n \n*\n \n100\n,\n \n             avg_delay \n=\n \nmean\n(\nArrDelay\n,\n na.rm \n=\n \nTRUE\n))\n \n%>%\n\n   arrange\n(\navg_delay\n,\n p_canc\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 15 \u00d7 5\n##         UniqueCarrier n_flights n_canc    p_canc  avg_delay\n##                 <chr>     <int>  <int>     <dbl>      <dbl>\n## 1          US_Airways      4082     46 1.1268986 -0.6307692\n## 2            American      3244     60 1.8495684  0.8917558\n## 3             AirTran      2139     21 0.9817672  1.8536239\n## 4              Alaska       365      0 0.0000000  3.1923077\n## 5                Mesa        79      1 1.2658228  4.0128205\n## 6               Delta      2641     42 1.5903067  6.0841374\n## 7         Continental     70032    475 0.6782614  6.0986983\n## 8      American_Eagle      4648    135 2.9044750  7.1529751\n## 9  Atlantic_Southeast      2204     76 3.4482759  7.2569543\n## 10          Southwest     45343    703 1.5504047  7.5871430\n## 11           Frontier       838      6 0.7159905  7.6682692\n## 12         ExpressJet     73053   1132 1.5495599  8.1865242\n## 13            SkyWest     16061    224 1.3946828  8.6934922\n## 14            JetBlue       695     18 2.5899281  9.8588410\n## 15             United      2072     34 1.6409266 10.4628628\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Make an ordered per-day summary of hflights\n\nhflights \n%>%\n \n   group_by\n(\nDayOfWeek\n)\n \n%>%\n\n   summarise\n(\navg_taxi \n=\n \nmean\n(\nTaxiIn \n+\n TaxiOut\n,\n na.rm\n=\nTRUE\n))\n \n%>%\n\n   arrange\n(\ndesc\n(\navg_taxi\n))\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n## # A tibble: 7 \u00d7 2\n##   DayOfWeek avg_taxi\n##       <int>    <dbl>\n## 1         1 21.77027\n## 2         2 21.43505\n## 3         4 21.26076\n## 4         3 21.19055\n## 5         5 21.15805\n## 6         7 20.93726\n## 7         6 20.43061\n\n\n\n\n\n\nCombine \ngroup_by\n with \nmutate\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n# Solution to first instruction\n\nhflights \n%>%\n\n    filter\n(\n!\nis.na\n(\nArrDelay\n))\n \n%>%\n\n    group_by\n(\nUniqueCarrier\n)\n \n%>%\n\n    summarise\n(\np_delay \n=\n \nsum\n(\nArrDelay \n>\n \n0\n)\n \n/\n n\n())\n \n%>%\n\n    mutate\n(\nrank \n=\n \nrank\n(\np_delay\n))\n \n%>%\n\n    arrange\n(\nrank\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 15 \u00d7 3\n##         UniqueCarrier   p_delay  rank\n##                 <chr>     <dbl> <dbl>\n## 1            American 0.3030208     1\n## 2             AirTran 0.3112269     2\n## 3          US_Airways 0.3267990     3\n## 4  Atlantic_Southeast 0.3677511     4\n## 5      American_Eagle 0.3696714     5\n## 6               Delta 0.3871092     6\n## 7             JetBlue 0.3952452     7\n## 8              Alaska 0.4368132     8\n## 9           Southwest 0.4644557     9\n## 10               Mesa 0.4743590    10\n## 11        Continental 0.4907385    11\n## 12         ExpressJet 0.4943420    12\n## 13             United 0.4963109    13\n## 14            SkyWest 0.5350105    14\n## 15           Frontier 0.5564904    15\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# Solution to second instruction\n\nhflights \n%>%\n\n    filter\n(\n!\nis.na\n(\nArrDelay\n),\n ArrDelay \n>\n \n0\n)\n \n%>%\n\n    group_by\n(\nUniqueCarrier\n)\n \n%>%\n\n    summarise\n(\navg \n=\n \nmean\n(\nArrDelay\n))\n \n%>%\n\n    mutate\n(\nrank \n=\n \nrank\n(\navg\n))\n \n%>%\n\n    arrange\n(\nrank\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## # A tibble: 15 \u00d7 3\n##         UniqueCarrier      avg  rank\n##                 <chr>    <dbl> <dbl>\n## 1                Mesa 18.67568     1\n## 2            Frontier 18.68683     2\n## 3          US_Airways 20.70235     3\n## 4         Continental 22.13374     4\n## 5              Alaska 22.91195     5\n## 6             SkyWest 24.14663     6\n## 7          ExpressJet 24.19337     7\n## 8           Southwest 25.27750     8\n## 9             AirTran 27.85693     9\n## 10           American 28.49740    10\n## 11              Delta 32.12463    11\n## 12             United 32.48067    12\n## 13     American_Eagle 38.75135    13\n## 14 Atlantic_Southeast 40.24231    14\n## 15            JetBlue 45.47744    15\n\n\n\n\n\n\nAdvanced \ngroup_by\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n# Which plane (by tail number) flew out of Houston the most times? How many times? adv1\n\nadv1 \n<-\n hflights \n%>%\n\n          group_by\n(\nTailNum\n)\n \n%>%\n\n          summarise\n(\nn \n=\n n\n())\n \n%>%\n\n          filter\n(\nn \n==\n \nmax\n(\nn\n))\n\n\n\n# How many airplanes only flew to one destination from Houston? adv2\n\nadv2 \n<-\n hflights \n%>%\n\n          group_by\n(\nTailNum\n)\n \n%>%\n\n          summarise\n(\nndest \n=\n n_distinct\n(\nDest\n))\n \n%>%\n\n          filter\n(\nndest \n==\n \n1\n)\n \n%>%\n\n          summarise\n(\nnplanes \n=\n n\n())\n\n\n\n# Find the most visited destination for each carrier: adv3\n\nadv3 \n<-\n hflights \n%>%\n \n          group_by\n(\nUniqueCarrier\n,\n Dest\n)\n \n%>%\n\n          summarise\n(\nn \n=\n n\n())\n \n%>%\n\n          mutate\n(\nrank \n=\n \nrank\n(\ndesc\n(\nn\n)))\n \n%>%\n\n          filter\n(\nrank \n==\n \n1\n)\n\n\n\n# Find the carrier that travels to each destination the most: adv4\n\nadv4 \n<-\n hflights \n%>%\n \n          group_by\n(\nDest\n,\n UniqueCarrier\n)\n \n%>%\n\n          summarise\n(\nn \n=\n n\n())\n \n%>%\n\n          mutate\n(\nrank \n=\n \nrank\n(\ndesc\n(\nn\n)))\n \n%>%\n\n          filter\n(\nrank \n==\n \n1\n)\n\n\n\n\n\n\n\ndplyr\n deals with different types\n\u00b6\n\n\n1\n2\n3\n# Use summarise to calculate n_carrier\n\ns2 \n<-\n hflights \n%>%\n\n    summarise\n(\nn_carrier \n=\n n_distinct\n(\nUniqueCarrier\n))\n\n\n\n\n\n\n\ndplyr\n and mySQL databases\n\u00b6\n\n\nCode only.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# set up a src that connects to the mysql database (src_mysql is provided by dplyr)\n\nmy_db \n<-\n src_mysql\n(\ndbname \n=\n \n'dplyr'\n,\n \n                  host \n=\n \n'dplyr.csrrinzqubik.us-east-1.rds.amazonaws.com'\n,\n \n                  port \n=\n \n3306\n,\n\n                  user \n=\n \n'dplyr'\n,\n\n                  password \n=\n \n'dplyr'\n)\n\n\n\n# and reference a table within that src: nycflights is now available as an R object that references to the remote nycflights table\n\nnycflights \n<-\n tbl\n(\nmy_db\n,\n \n'dplyr'\n)\n\n\n\n# glimpse at nycflights\n\nglimpse\n(\nnycflights\n)\n\n\n\n# Calculate the grouped summaries detailed in the instructions\n\nnycflights \n%>%\n\n   group_by\n(\ncarrier\n)\n \n%>%\n\n   summarise\n(\nn_flights \n=\n n\n(),\n avg_delay \n=\n \nmean\n(\narr_delay\n))\n \n%>%\n\n   arrange\n(\navg_delay\n)\n\n\n\n\n\n\n\nAdding \ntidyr\n Functions\n\u00b6\n\n\n\n\ncomplete\n.\n\n\ndrop_na\n.\n\n\nexpand\n.\n\n\nextract\n.\n\n\nextract_numeric\n.\n\n\ncomplete\n.\n\n\nfill\n.\n\n\nfull_seq\n.\n\n\ngather\n.\n\n\nnest\n.\n\n\nreplace_na\n.\n\n\nseparate\n.\n\n\nseparate_rows\n.\n\n\nseparate_rows_\n.\n\n\nsmiths\n.\n\n\nspread\n.\n\n\ntable1\n.\n\n\nunite\n.\n\n\nunnest\n.\n\n\nwho\n.\n\n\n\n\n\n\nJoining Data in R with \ndplyr\n\u00b6\n\n\nIn development\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## 1, Mutating Joins\n\n## 2, Filtering Joins and Set Operations\n\n## 3, Assembling Data\n\n## 4, Advanced Joining\n\n## 5, Case Study",
            "title": "Data Manipulation in R with dplyr"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#introduction-to-dplyr",
            "text": "",
            "title": "Introduction to dplyr"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#load-the-dplyr-and-hflights-package",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Load the dplyr package  library ( dplyr )  library ( dtplyr )  # Load the hflights package  # A data only package containing commercial domestic flights that departed Houston (IAH and HOU) in 2011  library ( hflights )  # Call both head() and summary() on hflights  head ( hflights )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 ##      Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## 5424 2011     1          1         6    1400    1500            AA\n## 5425 2011     1          2         7    1401    1501            AA\n## 5426 2011     1          3         1    1352    1502            AA\n## 5427 2011     1          4         2    1403    1513            AA\n## 5428 2011     1          5         3    1405    1507            AA\n## 5429 2011     1          6         4    1359    1503            AA\n##      FlightNum TailNum ActualElapsedTime AirTime ArrDelay DepDelay Origin\n## 5424       428  N576AA                60      40      -10        0    IAH\n## 5425       428  N557AA                60      45       -9        1    IAH\n## 5426       428  N541AA                70      48       -8       -8    IAH\n## 5427       428  N403AA                70      39        3        3    IAH\n## 5428       428  N492AA                62      44       -3        5    IAH\n## 5429       428  N262AA                64      45       -7       -1    IAH\n##      Dest Distance TaxiIn TaxiOut Cancelled CancellationCode Diverted\n## 5424  DFW      224      7      13         0                         0\n## 5425  DFW      224      6       9         0                         0\n## 5426  DFW      224      5      17         0                         0\n## 5427  DFW      224      9      22         0                         0\n## 5428  DFW      224      9       9         0                         0\n## 5429  DFW      224      6      13         0                         0   1 summary ( hflights )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48 ##       Year          Month          DayofMonth      DayOfWeek      ##  Min.   :2011   Min.   : 1.000   Min.   : 1.00   Min.   :1.000    ##  1st Qu.:2011   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.:2.000    ##  Median :2011   Median : 7.000   Median :16.00   Median :4.000    ##  Mean   :2011   Mean   : 6.514   Mean   :15.74   Mean   :3.948    ##  3rd Qu.:2011   3rd Qu.: 9.000   3rd Qu.:23.00   3rd Qu.:6.000    ##  Max.   :2011   Max.   :12.000   Max.   :31.00   Max.   :7.000    ##                                                                   ##     DepTime        ArrTime     UniqueCarrier        FlightNum     ##  Min.   :   1   Min.   :   1   Length:227496      Min.   :   1    ##  1st Qu.:1021   1st Qu.:1215   Class :character   1st Qu.: 855    ##  Median :1416   Median :1617   Mode  :character   Median :1696    ##  Mean   :1396   Mean   :1578                      Mean   :1962    ##  3rd Qu.:1801   3rd Qu.:1953                      3rd Qu.:2755    ##  Max.   :2400   Max.   :2400                      Max.   :7290    ##  NA's   :2905   NA's   :3066                                      ##    TailNum          ActualElapsedTime    AirTime         ArrDelay        ##  Length:227496      Min.   : 34.0     Min.   : 11.0   Min.   :-70.000    ##  Class :character   1st Qu.: 77.0     1st Qu.: 58.0   1st Qu.: -8.000    ##  Mode  :character   Median :128.0     Median :107.0   Median :  0.000    ##                     Mean   :129.3     Mean   :108.1   Mean   :  7.094    ##                     3rd Qu.:165.0     3rd Qu.:141.0   3rd Qu.: 11.000    ##                     Max.   :575.0     Max.   :549.0   Max.   :978.000    ##                     NA's   :3622      NA's   :3622    NA's   :3622       ##     DepDelay          Origin              Dest              Distance       ##  Min.   :-33.000   Length:227496      Length:227496      Min.   :  79.0    ##  1st Qu.: -3.000   Class :character   Class :character   1st Qu.: 376.0    ##  Median :  0.000   Mode  :character   Mode  :character   Median : 809.0    ##  Mean   :  9.445                                         Mean   : 787.8    ##  3rd Qu.:  9.000                                         3rd Qu.:1042.0    ##  Max.   :981.000                                         Max.   :3904.0    ##  NA's   :2905                                                              ##      TaxiIn           TaxiOut         Cancelled       CancellationCode    ##  Min.   :  1.000   Min.   :  1.00   Min.   :0.00000   Length:227496       ##  1st Qu.:  4.000   1st Qu.: 10.00   1st Qu.:0.00000   Class :character    ##  Median :  5.000   Median : 14.00   Median :0.00000   Mode  :character    ##  Mean   :  6.099   Mean   : 15.09   Mean   :0.01307                       ##  3rd Qu.:  7.000   3rd Qu.: 18.00   3rd Qu.:0.00000                       ##  Max.   :165.000   Max.   :163.00   Max.   :1.00000                       ##  NA's   :3066      NA's   :2947                                           ##     Diverted         ##  Min.   :0.000000    ##  1st Qu.:0.000000    ##  Median :0.000000    ##  Mean   :0.002853    ##  3rd Qu.:0.000000    ##  Max.   :1.000000    ##",
            "title": "Load the dplyr and hflights package"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#convert-dataframe-to-table",
            "text": "1\n2\n3\n4\n5 # Convert the hflights data.frame into a hflights tbl \nhflights  <-  tbl_df ( hflights )  # Display the hflights tbl \nhflights    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## *  <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6    1400    1500            AA\n## 2   2011     1          2         7    1401    1501            AA\n## 3   2011     1          3         1    1352    1502            AA\n## 4   2011     1          4         2    1403    1513            AA\n## 5   2011     1          5         3    1405    1507            AA\n## 6   2011     1          6         4    1359    1503            AA\n## 7   2011     1          7         5    1359    1509            AA\n## 8   2011     1          8         6    1355    1454            AA\n## 9   2011     1          9         7    1443    1554            AA\n## 10  2011     1         10         1    1443    1553            AA\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>   1\n2 # Create the object carriers, containing only the UniqueCarrier variable of hflights \ncarriers  <-  hflights $ UniqueCarrier",
            "title": "Convert data.frame to table"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#changing-labels-of-hflights",
            "text": "part 1 of 2   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # add \nlut  <-   c ( 'AA'   =   'American' ,   'AS'   =   'Alaska' ,   'B6'   =   'JetBlue' ,   'CO'   =   'Continental' ,  \n          'DL'   =   'Delta' ,   'OO'   =   'SkyWest' ,   'UA'   =   'United' ,   'US'   =   'US_Airways' ,  \n          'WN'   =   'Southwest' ,   'EV'   =   'Atlantic_Southeast' ,   'F9'   =   'Frontier' ,  \n          'FL'   =   'AirTran' ,   'MQ'   =   'American_Eagle' ,   'XE'   =   'ExpressJet' ,   'YV'   =   'Mesa' )  # Use lut to translate the UniqueCarrier column of hflights \nhflights $ UniqueCarrier  <-  lut [ hflights $ UniqueCarrier ]  # Inspect the resulting raw values of your variables \nglimpse ( hflights )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 ## Observations: 227,496\n## Variables: 21\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   part 2 of 2  1\n2\n3\n4\n5\n6\n7\n8 # Build the lookup table: lut \nlut  <-   c ( \"A\"   =   \"carrier\" ,   \"B\"   =   \"weather\"   , \"C\"   =   \"FFA\"   , \"D\"   =   \"security\" ,   \"E\"   =   \"not cancelled\" )  # Add the Code column \nhflights $ Code  <-  lut [ hflights $ CancellationCode ]  # Glimpse at hflights \nglimpse ( hflights )    Result.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 Observations: 227,496\nVariables: 22\n$ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011,...\n$ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n$ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...\n$ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,...\n$ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359,...\n$ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503,...\n$ UniqueCarrier     <chr> \"American\", \"American\", \"American\",...\n$ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, ...\n$ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403...\n$ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71,...\n$ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41,...\n$ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44...\n$ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43,...\n$ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", ...\n$ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", ...\n$ Distance          <int> 224, 224, 224, 224, 224, 224, 224, ...\n$ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4...\n$ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 1...\n$ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",...\n$ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n$ Code              <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA,...",
            "title": "Changing labels of hflights"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#select-and-mutate",
            "text": "",
            "title": "select and mutate"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#the-five-verbs-and-their-meaning",
            "text": "select ; which returns a subset of the columns.  filter ; that is able to return a subset of the rows.  arrange ; that reorders the rows according to single or multiple variables.  mutate ; used to add columns from existing data.  summarise ; which reduces each group to a single row by calculating aggregate measures.",
            "title": "The five verbs and their meaning"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#the-select-verb",
            "text": "1\n2 # Print out a tbl with the four columns of hflights related to delay \nselect ( hflights ,  ActualElapsedTime ,  AirTime ,  ArrDelay ,  DepDelay )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ## # A tibble: 227,496 \u00d7 4\n##    ActualElapsedTime AirTime ArrDelay DepDelay\n## *              <int>   <int>    <int>    <int>\n## 1                 60      40      -10        0\n## 2                 60      45       -9        1\n## 3                 70      48       -8       -8\n## 4                 70      39        3        3\n## 5                 62      44       -3        5\n## 6                 64      45       -7       -1\n## 7                 70      43       -1       -1\n## 8                 59      40      -16       -5\n## 9                 71      41       44       43\n## 10                70      45       43       43\n## # ... with 227,486 more rows   1\n2 # Print out hflights, nothing has changed! \nhflights    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n## *  <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6    1400    1500      American\n## 2   2011     1          2         7    1401    1501      American\n## 3   2011     1          3         1    1352    1502      American\n## 4   2011     1          4         2    1403    1513      American\n## 5   2011     1          5         3    1405    1507      American\n## 6   2011     1          6         4    1359    1503      American\n## 7   2011     1          7         5    1359    1509      American\n## 8   2011     1          8         6    1355    1454      American\n## 9   2011     1          9         7    1443    1554      American\n## 10  2011     1         10         1    1443    1553      American\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>   1\n2 # Print out the columns Origin up to Cancelled of hflights \nselect ( hflights ,   14 : 19 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ## # A tibble: 227,496 \u00d7 6\n##    Origin  Dest Distance TaxiIn TaxiOut Cancelled\n## *   <chr> <chr>    <int>  <int>   <int>     <int>\n## 1     IAH   DFW      224      7      13         0\n## 2     IAH   DFW      224      6       9         0\n## 3     IAH   DFW      224      5      17         0\n## 4     IAH   DFW      224      9      22         0\n## 5     IAH   DFW      224      9       9         0\n## 6     IAH   DFW      224      6      13         0\n## 7     IAH   DFW      224     12      15         0\n## 8     IAH   DFW      224      7      12         0\n## 9     IAH   DFW      224      8      22         0\n## 10    IAH   DFW      224      6      19         0\n## # ... with 227,486 more rows   1\n2 # Answer to last question: be concise! \nselect ( hflights ,   1 : 4 ,   12 : 21 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 ## # A tibble: 227,496 \u00d7 14\n##     Year Month DayofMonth DayOfWeek ArrDelay DepDelay Origin  Dest\n## *  <int> <int>      <int>     <int>    <int>    <int>  <chr> <chr>\n## 1   2011     1          1         6      -10        0    IAH   DFW\n## 2   2011     1          2         7       -9        1    IAH   DFW\n## 3   2011     1          3         1       -8       -8    IAH   DFW\n## 4   2011     1          4         2        3        3    IAH   DFW\n## 5   2011     1          5         3       -3        5    IAH   DFW\n## 6   2011     1          6         4       -7       -1    IAH   DFW\n## 7   2011     1          7         5       -1       -1    IAH   DFW\n## 8   2011     1          8         6      -16       -5    IAH   DFW\n## 9   2011     1          9         7       44       43    IAH   DFW\n## 10  2011     1         10         1       43       43    IAH   DFW\n## # ... with 227,486 more rows, and 6 more variables: Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>",
            "title": "The select verb"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#helper-functions-for-variable-selection",
            "text": "select :   starts_with(\"X\") ; every name that starts with  \"X\" ,  ends_with(\"X\") ; every name that ends with  \"X\" ,  contains(\"X\") ; every name that contains  \"X\" ,  matches(\"X\") ; every name that matches  \"X\" , where  \"X\"  can be a regular expression,  num_range(\"x\", 1:5) ; the variables named  x01 ,  x02 ,  x03 ,  x04  and  x05 ,  one_of(x) ; every name that appears in  x , which should be a character vector.    1\n2 # Print out a tbl containing just ArrDelay and DepDelay \nselect ( hflights ,  ArrDelay ,  DepDelay )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ## # A tibble: 227,496 \u00d7 2\n##    ArrDelay DepDelay\n## *     <int>    <int>\n## 1       -10        0\n## 2        -9        1\n## 3        -8       -8\n## 4         3        3\n## 5        -3        5\n## 6        -7       -1\n## 7        -1       -1\n## 8       -16       -5\n## 9        44       43\n## 10       43       43\n## # ... with 227,486 more rows   1\n2 # Print out a tbl as described in the second instruction, using both helper functions and variable names \nselect ( hflights ,  UniqueCarrier ,  ends_with ( 'Num' ),  starts_with ( 'Cancel' ))     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ## # A tibble: 227,496 \u00d7 5\n##    UniqueCarrier FlightNum TailNum Cancelled CancellationCode\n## *          <chr>     <int>   <chr>     <int>            <chr>\n## 1       American       428  N576AA         0                 \n## 2       American       428  N557AA         0                 \n## 3       American       428  N541AA         0                 \n## 4       American       428  N403AA         0                 \n## 5       American       428  N492AA         0                 \n## 6       American       428  N262AA         0                 \n## 7       American       428  N493AA         0                 \n## 8       American       428  N477AA         0                 \n## 9       American       428  N476AA         0                 \n## 10      American       428  N504AA         0                 \n## # ... with 227,486 more rows   1\n2 # Print out a tbl as described in the third instruction, using only helper functions. \nselect ( hflights ,  ends_with ( 'Time' ),  ends_with ( 'Delay' ))     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ## # A tibble: 227,496 \u00d7 6\n##    DepTime ArrTime ActualElapsedTime AirTime ArrDelay DepDelay\n## *    <int>   <int>             <int>   <int>    <int>    <int>\n## 1     1400    1500                60      40      -10        0\n## 2     1401    1501                60      45       -9        1\n## 3     1352    1502                70      48       -8       -8\n## 4     1403    1513                70      39        3        3\n## 5     1405    1507                62      44       -3        5\n## 6     1359    1503                64      45       -7       -1\n## 7     1359    1509                70      43       -1       -1\n## 8     1355    1454                59      40      -16       -5\n## 9     1443    1554                71      41       44       43\n## 10    1443    1553                70      45       43       43\n## # ... with 227,486 more rows",
            "title": "Helper functions for variable selection"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#comparison-to-basic-r",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # add \nex1r  <-  hflights [ c ( 'TaxiIn' , 'TaxiOut' , 'Distance' )] \n\nex1d  <-  select ( hflights ,  starts_with ( 'Taxi' ),  Distance ) \n\nex2r  <-  hflights [ c ( 'Year' , 'Month' , 'DayOfWeek' , 'DepTime' , 'ArrTime' )] \n\nex2d  <-  select ( hflights ,  Year ,  Month ,  DayOfWeek ,  DepTime ,  ArrTime ) \n\nex3r  <-  hflights [ c ( 'TailNum' , 'TaxiIn' , 'TaxiOut' )] \n\nex3d  <-  select ( hflights ,  TailNum ,  starts_with ( 'Taxi' ))",
            "title": "Comparison to basic R"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#mutate-is-creating",
            "text": "1\n2\n3 # Add the new variable ActualGroundTime to a copy of hflights and save the result as g1 \ng1  <-  mutate ( hflights ,  ActualGroundTime  =  ActualElapsedTime  -  AirTime ) \nglimpse ( hflights )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 ## Observations: 227,496\n## Variables: 21\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   1 glimpse ( g1 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 ## Observations: 227,496\n## Variables: 22\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ ActualGroundTime  <int> 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...   1\n2\n3\n4 # Add the new variable GroundTime to a g1; save the result as g2 \ng2  <-  mutate ( g1 ,  GroundTime  =  TaxiIn  +  TaxiOut )  head ( g1 $ ActualGroundTime  ==  g2 $ GroundTime ,   20 )    1\n2 ##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n## [15] TRUE TRUE TRUE TRUE TRUE TRUE   1\n2\n3 # Add the new variable AverageSpeed to g2; save the result as g3 \ng3  <-  mutate ( g2 ,  AverageSpeed  =  Distance  /  AirTime  *   60 ) \ng3    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 ## # A tibble: 227,496 \u00d7 24\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6    1400    1500      American\n## 2   2011     1          2         7    1401    1501      American\n## 3   2011     1          3         1    1352    1502      American\n## 4   2011     1          4         2    1403    1513      American\n## 5   2011     1          5         3    1405    1507      American\n## 6   2011     1          6         4    1359    1503      American\n## 7   2011     1          7         5    1359    1509      American\n## 8   2011     1          8         6    1355    1454      American\n## 9   2011     1          9         7    1443    1554      American\n## 10  2011     1         10         1    1443    1553      American\n## # ... with 227,486 more rows, and 17 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>, ActualGroundTime <int>, GroundTime <int>,\n## #   AverageSpeed <dbl>",
            "title": "mutate is creating"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#add-multiple-variables-using-mutate",
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 # Add a second variable loss_percent to the dataset: m1 \nm1  <-  mutate ( hflights ,  loss  =  ArrDelay  -  DepDelay ,  loss_percent  =   ( ArrDelay  -  DepDelay ) / DepDelay * 100 )  # Copy and adapt the previous command to reduce redendancy: m2 \nm2  <-  mutate ( hflights ,  loss  =  ArrDelay  -  DepDelay ,  loss_percent  =  loss / DepDelay  *   100 )  # Add the three variables as described in the third instruction: m3 \nm3  <-  mutate ( hflights ,  TotalTaxi  =  TaxiIn  +  TaxiOut ,  ActualGroundTime  =  ActualElapsedTime  -  AirTime ,  Diff  =  TotalTaxi  -  ActualGroundTime ) \nglimpse ( m3 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 ## Observations: 227,496\n## Variables: 24\n## $ Year              <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 20...\n## $ Month             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n## $ DayofMonth        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1...\n## $ DayOfWeek         <int> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,...\n## $ DepTime           <int> 1400, 1401, 1352, 1403, 1405, 1359, 1359, 13...\n## $ ArrTime           <int> 1500, 1501, 1502, 1513, 1507, 1503, 1509, 14...\n## $ UniqueCarrier     <chr> \"American\", \"American\", \"American\", \"America...\n## $ FlightNum         <int> 428, 428, 428, 428, 428, 428, 428, 428, 428,...\n## $ TailNum           <chr> \"N576AA\", \"N557AA\", \"N541AA\", \"N403AA\", \"N49...\n## $ ActualElapsedTime <int> 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, ...\n## $ AirTime           <int> 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, ...\n## $ ArrDelay          <int> -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29,...\n## $ DepDelay          <int> 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, ...\n## $ Origin            <chr> \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"I...\n## $ Dest              <chr> \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"D...\n## $ Distance          <int> 224, 224, 224, 224, 224, 224, 224, 224, 224,...\n## $ TaxiIn            <int> 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6...\n## $ TaxiOut           <int> 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11...\n## $ Cancelled         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ CancellationCode  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ...\n## $ Diverted          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n## $ TotalTaxi         <int> 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n## $ ActualGroundTime  <int> 20, 15, 22, 31, 18, 19, 27, 19, 30, 25, 28, ...\n## $ Diff              <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...",
            "title": "Add multiple variables using mutate"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#filter-and-arrange",
            "text": "",
            "title": "filter and arrange"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#logical-operators",
            "text": "filter :   x < y ;  TRUE  if  x  is less than  y .  x <= y ;  TRUE  if  x  is less than or equal to  y .  x == y ;  TRUE  if  x  equals  y .  x != y ;  TRUE  if  x  does not equal  y .  x >= y ;  TRUE  if  x  is greater than or equal to  y .  x > y ;  TRUE  if  x  is greater than  y .  x %in% c(a, b, c) ;  TRUE  if  x  is in the vector  c(a, b, c) .    1\n2 # All flights that traveled 3000 miles or more \nfilter ( hflights ,  Distance  >=   3000 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 527 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1         31         1     924    1413   Continental\n## 2   2011     1         30         7     925    1410   Continental\n## 3   2011     1         29         6    1045    1445   Continental\n## 4   2011     1         28         5    1516    1916   Continental\n## 5   2011     1         27         4     950    1344   Continental\n## 6   2011     1         26         3     944    1350   Continental\n## 7   2011     1         25         2     924    1337   Continental\n## 8   2011     1         24         1    1144    1605   Continental\n## 9   2011     1         23         7     926    1335   Continental\n## 10  2011     1         22         6     942    1340   Continental\n## # ... with 517 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>   1\n2 # All flights flown by one of JetBlue, Southwest, or Delta \nfilter ( hflights ,  UniqueCarrier  %in%   c ( 'JetBlue' , 'Southwest' , 'Delta' ))     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 48,679 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          1         6     654    1124       JetBlue\n## 2   2011     1          1         6    1639    2110       JetBlue\n## 3   2011     1          2         7     703    1113       JetBlue\n## 4   2011     1          2         7    1604    2040       JetBlue\n## 5   2011     1          3         1     659    1100       JetBlue\n## 6   2011     1          3         1    1801    2200       JetBlue\n## 7   2011     1          4         2     654    1103       JetBlue\n## 8   2011     1          4         2    1608    2034       JetBlue\n## 9   2011     1          5         3     700    1103       JetBlue\n## 10  2011     1          5         3    1544    1954       JetBlue\n## # ... with 48,669 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>   1\n2 # All flights where taxiing took longer than flying \nfilter ( hflights ,   ( TaxiIn  +  TaxiOut )   >  AirTime )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 1,389 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1         24         1     731     904      American\n## 2   2011     1         30         7    1959    2132      American\n## 3   2011     1         24         1    1621    1749      American\n## 4   2011     1         10         1     941    1113      American\n## 5   2011     1         31         1    1301    1356   Continental\n## 6   2011     1         31         1    2113    2215   Continental\n## 7   2011     1         31         1    1434    1539   Continental\n## 8   2011     1         31         1     900    1006   Continental\n## 9   2011     1         30         7    1304    1408   Continental\n## 10  2011     1         30         7    2004    2128   Continental\n## # ... with 1,379 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>",
            "title": "Logical operators"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#combining-tests-using-boolean-operators",
            "text": "1\n2 # All flights that departed before 5am or arrived after 10pm \nfilter ( hflights ,  DepTime  <   500   |  ArrTime  >   2200 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 27,799 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          4         2    2100    2207      American\n## 2   2011     1         14         5    2119    2229      American\n## 3   2011     1         10         1    1934    2235      American\n## 4   2011     1         26         3    1905    2211      American\n## 5   2011     1         30         7    1856    2209      American\n## 6   2011     1          9         7    1938    2228        Alaska\n## 7   2011     1         31         1    1919    2231   Continental\n## 8   2011     1         31         1    2116    2344   Continental\n## 9   2011     1         31         1    1850    2211   Continental\n## 10  2011     1         31         1    2102    2216   Continental\n## # ... with 27,789 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>   1\n2 # All flights that departed late but arrived ahead of schedule \nfilter ( hflights ,  DepDelay  >   0   &  ArrDelay  <   0 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 27,712 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1          2         7    1401    1501      American\n## 2   2011     1          5         3    1405    1507      American\n## 3   2011     1         18         2    1408    1508      American\n## 4   2011     1         18         2     721     827      American\n## 5   2011     1         12         3    2015    2113      American\n## 6   2011     1         13         4    2020    2116      American\n## 7   2011     1         26         3    2009    2103      American\n## 8   2011     1          1         6    1631    1736      American\n## 9   2011     1         10         1    1639    1740      American\n## 10  2011     1         12         3    1631    1739      American\n## # ... with 27,702 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>   1\n2 # All cancelled weekend flights \nfilter ( hflights ,  DayOfWeek  %in%   c ( 6 , 7 )   &  Cancelled  ==   1 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 585 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime      UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>              <chr>\n## 1   2011     1          9         7      NA      NA           American\n## 2   2011     1         29         6      NA      NA        Continental\n## 3   2011     1          9         7      NA      NA        Continental\n## 4   2011     1          9         7      NA      NA              Delta\n## 5   2011     1          9         7      NA      NA            SkyWest\n## 6   2011     1          2         7      NA      NA          Southwest\n## 7   2011     1         29         6      NA      NA              Delta\n## 8   2011     1          9         7      NA      NA Atlantic_Southeast\n## 9   2011     1          1         6      NA      NA            AirTran\n## 10  2011     1          9         7      NA      NA            AirTran\n## # ... with 575 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>   1\n2 # All flights that were cancelled after being delayed \nfilter ( hflights ,  DepDelay  >   0   &  Cancelled  ==   1 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 40 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     1         26         3    1926      NA   Continental\n## 2   2011     1         11         2    1100      NA    US_Airways\n## 3   2011     1         19         3    1811      NA    ExpressJet\n## 4   2011     1          7         5    2028      NA    ExpressJet\n## 5   2011     2          4         5    1638      NA      American\n## 6   2011     2          8         2    1057      NA   Continental\n## 7   2011     2          2         3     802      NA    ExpressJet\n## 8   2011     2          9         3     904      NA    ExpressJet\n## 9   2011     2          1         2    1508      NA       SkyWest\n## 10  2011     3         31         4    1016      NA   Continental\n## # ... with 30 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>",
            "title": "Combining tests using boolean operators"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#blend-together",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 # Select the flights that had JFK as their destination: c1 \nc1  <-  filter ( hflights ,  Dest  ==   'JFK' )  # Combine the Year, Month and DayofMonth variables to create a Date column: c2 \nc2  <-  mutate ( c1 ,  Date  =   paste ( Year ,  Month ,  DayofMonth ,  sep  =   '-' ))  # Print out a selection of columns of c2 \nselect ( c2 ,  Date ,  DepTime ,  ArrTime ,  TailNum )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ## # A tibble: 695 \u00d7 4\n##        Date DepTime ArrTime TailNum\n##       <chr>   <int>   <int>   <chr>\n## 1  2011-1-1     654    1124  N324JB\n## 2  2011-1-1    1639    2110  N324JB\n## 3  2011-1-2     703    1113  N324JB\n## 4  2011-1-2    1604    2040  N324JB\n## 5  2011-1-3     659    1100  N229JB\n## 6  2011-1-3    1801    2200  N206JB\n## 7  2011-1-4     654    1103  N267JB\n## 8  2011-1-4    1608    2034  N267JB\n## 9  2011-1-5     700    1103  N708JB\n## 10 2011-1-5    1544    1954  N644JB\n## # ... with 685 more rows",
            "title": "Blend together"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#arranging-your-data",
            "text": "1\n2\n3\n4\n5 # Definition of dtc \ndtc  <-  filter ( hflights ,  Cancelled  ==   1 ,   ! is.na ( DepDelay ))  # Arrange dtc by departure delays \narrange ( dtc ,  DepDelay )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>          <chr>\n## 1   2011     7         23         6     605      NA       Frontier\n## 2   2011     1         17         1     916      NA     ExpressJet\n## 3   2011    12          1         4     541      NA     US_Airways\n## 4   2011    10         12         3    2022      NA American_Eagle\n## 5   2011     7         29         5    1424      NA    Continental\n## 6   2011     9         29         4    1639      NA        SkyWest\n## 7   2011     2          9         3     555      NA American_Eagle\n## 8   2011     5          9         1     715      NA        SkyWest\n## 9   2011     1         20         4    1413      NA         United\n## 10  2011     1         17         1     831      NA      Southwest\n## # ... with 58 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>   1\n2 # Arrange dtc so that cancellation reasons are grouped \narrange ( dtc ,  CancellationCode )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>          <chr>\n## 1   2011     1         20         4    1413      NA         United\n## 2   2011     1          7         5    2028      NA     ExpressJet\n## 3   2011     2          4         5    1638      NA       American\n## 4   2011     2          8         2    1057      NA    Continental\n## 5   2011     2          1         2    1508      NA        SkyWest\n## 6   2011     2         21         1    2257      NA        SkyWest\n## 7   2011     2          9         3     555      NA American_Eagle\n## 8   2011     3         18         5     727      NA         United\n## 9   2011     4          4         1    1632      NA          Delta\n## 10  2011     4          8         5    1608      NA      Southwest\n## # ... with 58 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>   1\n2 # Arrange dtc according to carrier and departure delays \narrange ( dtc ,  UniqueCarrier ,  DepDelay )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 68 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime      UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>              <chr>\n## 1   2011     6         11         6    1649      NA            AirTran\n## 2   2011     8         18         4    1808      NA           American\n## 3   2011     2          4         5    1638      NA           American\n## 4   2011    10         12         3    2022      NA     American_Eagle\n## 5   2011     2          9         3     555      NA     American_Eagle\n## 6   2011     7         17         7    1917      NA     American_Eagle\n## 7   2011     4         30         6     612      NA Atlantic_Southeast\n## 8   2011     4         10         7    1147      NA Atlantic_Southeast\n## 9   2011     5         23         1     657      NA Atlantic_Southeast\n## 10  2011     9         29         4     723      NA Atlantic_Southeast\n## # ... with 58 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>",
            "title": "Arranging your data"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#reverse-the-order-of-arranging",
            "text": "1\n2 # Arrange according to carrier and decreasing departure delays \narrange ( hflights ,  UniqueCarrier ,  desc ( DepDelay ))     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     2         19         6    1902    2143       AirTran\n## 2   2011     3         14         1    2024    2309       AirTran\n## 3   2011     2         16         3    2349     227       AirTran\n## 4   2011    11         13         7    2312     213       AirTran\n## 5   2011     5         26         4    2353     305       AirTran\n## 6   2011     5         26         4    1922    2229       AirTran\n## 7   2011     4         28         4    1045    1328       AirTran\n## 8   2011     6          5         7    2207      52       AirTran\n## 9   2011     5          7         6    1009    1256       AirTran\n## 10  2011     7         25         1    2107      14       AirTran\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>   1\n2 # Arrange flights by total delay (normal order). \narrange ( hflights ,   ( ArrDelay + DepDelay ))     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 227,496 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>         <chr>\n## 1   2011     7          3         7    1914    2039    ExpressJet\n## 2   2011     8         31         3     934    1039       SkyWest\n## 3   2011     8         21         7     935    1039       SkyWest\n## 4   2011     8         28         7    2059    2206       SkyWest\n## 5   2011     8         29         1     935    1041       SkyWest\n## 6   2011    12         25         7     741     926       SkyWest\n## 7   2011     1         30         7     620     812       SkyWest\n## 8   2011     8          3         3    1741    1810    ExpressJet\n## 9   2011     8          4         4     930    1041       SkyWest\n## 10  2011     8         18         4     939    1043       SkyWest\n## # ... with 227,486 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>   1\n2 # Keep flights leaving to DFW before 8am and arrange according to decreasing AirTime  \narrange ( filter ( hflights ,  Dest  ==   'DFW'   &  DepTime < 800 ),  desc ( AirTime ))     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 799 \u00d7 21\n##     Year Month DayofMonth DayOfWeek DepTime ArrTime  UniqueCarrier\n##    <int> <int>      <int>     <int>   <int>   <int>          <chr>\n## 1   2011    11         22         2     635     825       American\n## 2   2011     8         25         4     602     758 American_Eagle\n## 3   2011    10         12         3     559     738 American_Eagle\n## 4   2011     5          2         1     716     854       American\n## 5   2011     4          4         1     741     949       American\n## 6   2011     4          4         1     627     742 American_Eagle\n## 7   2011     6         21         2     726     848     ExpressJet\n## 8   2011     9          1         4     715     844       American\n## 9   2011     3         14         1     729     917    Continental\n## 10  2011    12          5         1     724     847    Continental\n## # ... with 789 more rows, and 14 more variables: FlightNum <int>,\n## #   TailNum <chr>, ActualElapsedTime <int>, AirTime <int>, ArrDelay <int>,\n## #   DepDelay <int>, Origin <chr>, Dest <chr>, Distance <int>,\n## #   TaxiIn <int>, TaxiOut <int>, Cancelled <int>, CancellationCode <chr>,\n## #   Diverted <int>",
            "title": "Reverse the order of arranging"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#summarise-and-the-pipe-operator",
            "text": "",
            "title": "summarise and the Pipe Operator"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#the-syntax-of-summarise",
            "text": "1\n2 # Print out a summary with variables min_dist and max_dist \nsummarise ( hflights ,  min_dist  =   min ( Distance ),  max_dist  =   max ( Distance ))    1\n2\n3\n4 ## # A tibble: 1 \u00d7 2\n##   min_dist max_dist\n##      <int>    <int>\n## 1       79     3904   1\n2 # Print out a summary with variable max_div \nsummarise ( filter ( hflights ,  Diverted  ==   1 ),  max_div  =   max ( Distance ))    1\n2\n3\n4 ## # A tibble: 1 \u00d7 1\n##   max_div\n##     <int>\n## 1    3904",
            "title": "The syntax of summarise"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#aggregate-functions",
            "text": "summarise :   min(x) ; minimum value of vector x.  max(x) ; maximum value of vector x.  mean(x) ; mean value of vector x.  median(x) ; median value of vector x.  quantile(x, p) ; pth quantile of vector x.  sd(x) ; standard deviation of vector x.  var(x) ; variance of vector x.  IQR(x) ; Inter Quartile Range (IQR) of vector x.  diff(range(x)) ; total range of vector x.    1\n2\n3\n4\n5 # Remove rows that have NA ArrDelay: temp1 \ntemp1  <-  filter ( hflights ,   ! is.na ( ArrDelay ))  # Generate summary about ArrDelay column of temp1 \nsummarise ( temp1 ,  earliest  =   min ( ArrDelay ),  average  =   mean ( ArrDelay ),  latest  =   max ( ArrDelay ),  sd  =  sd ( ArrDelay ))    1\n2\n3\n4 ## # A tibble: 1 \u00d7 4\n##   earliest  average latest       sd\n##      <int>    <dbl>  <int>    <dbl>\n## 1      -70 7.094334    978 30.70852   1\n2\n3\n4\n5 # Keep rows that have no NA TaxiIn and no NA TaxiOut: temp2 \ntemp2  <-  filter ( hflights ,   ! is.na ( TaxiIn ),   ! is.na ( TaxiOut ))  # Print the maximum taxiing difference of temp2 with summarise() \nsummarise ( temp2 ,  max_taxi_diff  =   max ( abs ( TaxiIn  -  TaxiOut )))    1\n2\n3\n4 ## # A tibble: 1 \u00d7 1\n##   max_taxi_diff\n##           <int>\n## 1           160",
            "title": "Aggregate functions"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#dplyr-aggregate-functions",
            "text": "first(x) ; the first element of vector  x .  last(x) ; the last element of vector  x .  nth(x, n) ; The  n th element of vector  x .  n() ; The number of rows in the data.frame or group of observations that  summarise()  describes.  n_distinct(x) ; The number of unique values in vector  x .    1\n2 # Generate summarizing statistics for hflights \nsummarise ( hflights ,  n_obs  =  n (),  n_carrier  =  n_distinct ( UniqueCarrier ),  n_dest  =  n_distinct ( Dest ),  dest100  =  nth ( Dest ,   100 ))    1\n2\n3\n4 ## # A tibble: 1 \u00d7 4\n##    n_obs n_carrier n_dest dest100\n##    <int>     <int>  <int>   <chr>\n## 1 227496        15    116     DFW   1\n2\n3\n4\n5 # Filter hflights to keep all American Airline flights: aa \naa  <-  filter ( hflights ,  UniqueCarrier  ==   'American' )  # Generate summarizing statistics for aa  \nsummarise ( aa ,  n_flights  =  n (),  n_canc  =   sum ( Cancelled ),  p_canc  =  n_canc / n_flights * 100 ,  avg_delay  =   mean ( ArrDelay ,  na.rm  =   TRUE ))    1\n2\n3\n4 ## # A tibble: 1 \u00d7 4\n##   n_flights n_canc   p_canc avg_delay\n##       <int>  <int>    <dbl>     <dbl>\n## 1      3244     60 1.849568 0.8917558   Overview of syntax  1\n2\n3\n4\n5 # Write the 'piped' version of the English sentences \nhflights  %>% \n    mutate ( diff  =  TaxiOut  -  TaxiIn )   %>% \n    filter ( ! is.na ( diff ))   %>% \n    summarise (  avg  =   mean ( diff ))    1\n2\n3\n4 ## # A tibble: 1 \u00d7 1\n##        avg\n##      <dbl>\n## 1 8.992064   Drive or fly? Part 1 of 2   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # Part 1, concerning the selection and creation of columns \nd  <-  hflights  %>% \n  select ( Dest ,  UniqueCarrier ,  Distance ,  ActualElapsedTime )   %>%   \n  mutate ( RealTime  =  ActualElapsedTime  +   100 ,  mph  =  Distance  /  RealTime  *   60 )  # Part 2, concerning flights that had an actual average speed of < 70 mph. \nd  %>% \n  filter ( ! is.na ( mph ),  mph  <   70 )   %>% \n  summarise (  n_less  =  n (),  \n             n_dest  =  n_distinct ( Dest ),  \n             min_dist  =   min ( Distance ),  \n             max_dist  =   max ( Distance ))    1\n2\n3\n4 ## # A tibble: 1 \u00d7 4\n##   n_less n_dest min_dist max_dist\n##    <int>  <int>    <int>    <int>\n## 1   6726     13       79      305   Drive or fly? Part 2 of 2  1\n2\n3\n4\n5 # Solve the exercise using a combination of dplyr verbs and %>% \nhflights  %>% \n     #summarise(all_flights = n()) %>% \n    filter ((( Distance  /   ( ActualElapsedTime  +   100 )   *   60 )   <   105 )   |  Cancelled  ==   1   |  Diverted  ==   1 )   %>% \n    summarise ( n_non  =  n (),  p_non  =  n_non  /   22751   * 100 ,  n_dest  =  n_distinct ( Dest ),  min_dist  =   min ( Distance ),  max_dist  =   max ( Distance ))    1\n2\n3\n4 ## # A tibble: 1 \u00d7 5\n##   n_non    p_non n_dest min_dist max_dist\n##   <int>    <dbl>  <int>    <int>    <int>\n## 1 42400 186.3654    113       79     3904   Advanced piping exercise  1\n2\n3\n4 # Count the number of overnight flights \nhflights  %>% \n    filter ( ArrTime  <  DepTime  &   ! is.na ( DepTime )   &   ! is.na ( ArrTime ))   %>% \n    summarise ( n  =  n ())    1\n2\n3\n4 ## # A tibble: 1 \u00d7 1\n##       n\n##   <int>\n## 1  2718",
            "title": "dplyr aggregate functions"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#group_by-and-working-with-data",
            "text": "",
            "title": "group_by and working with data"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#unite-and-conquer-using-group_by",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 # Make an ordered per-carrier summary of hflights \nhflights  %>% \n   group_by ( UniqueCarrier )   %>% \n   summarise ( n_flights  =  n (),  \n             n_canc  =   sum ( Cancelled  ==   1 ),  \n             p_canc  =   mean ( Cancelled  ==   1 )   *   100 ,  \n             avg_delay  =   mean ( ArrDelay ,  na.rm  =   TRUE ))   %>% \n   arrange ( avg_delay ,  p_canc )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 15 \u00d7 5\n##         UniqueCarrier n_flights n_canc    p_canc  avg_delay\n##                 <chr>     <int>  <int>     <dbl>      <dbl>\n## 1          US_Airways      4082     46 1.1268986 -0.6307692\n## 2            American      3244     60 1.8495684  0.8917558\n## 3             AirTran      2139     21 0.9817672  1.8536239\n## 4              Alaska       365      0 0.0000000  3.1923077\n## 5                Mesa        79      1 1.2658228  4.0128205\n## 6               Delta      2641     42 1.5903067  6.0841374\n## 7         Continental     70032    475 0.6782614  6.0986983\n## 8      American_Eagle      4648    135 2.9044750  7.1529751\n## 9  Atlantic_Southeast      2204     76 3.4482759  7.2569543\n## 10          Southwest     45343    703 1.5504047  7.5871430\n## 11           Frontier       838      6 0.7159905  7.6682692\n## 12         ExpressJet     73053   1132 1.5495599  8.1865242\n## 13            SkyWest     16061    224 1.3946828  8.6934922\n## 14            JetBlue       695     18 2.5899281  9.8588410\n## 15             United      2072     34 1.6409266 10.4628628   1\n2\n3\n4\n5 # Make an ordered per-day summary of hflights \nhflights  %>%  \n   group_by ( DayOfWeek )   %>% \n   summarise ( avg_taxi  =   mean ( TaxiIn  +  TaxiOut ,  na.rm = TRUE ))   %>% \n   arrange ( desc ( avg_taxi ))     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ## # A tibble: 7 \u00d7 2\n##   DayOfWeek avg_taxi\n##       <int>    <dbl>\n## 1         1 21.77027\n## 2         2 21.43505\n## 3         4 21.26076\n## 4         3 21.19055\n## 5         5 21.15805\n## 6         7 20.93726\n## 7         6 20.43061",
            "title": "Unite and conquer using group_by"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#combine-group_by-with-mutate",
            "text": "1\n2\n3\n4\n5\n6\n7 # Solution to first instruction \nhflights  %>% \n    filter ( ! is.na ( ArrDelay ))   %>% \n    group_by ( UniqueCarrier )   %>% \n    summarise ( p_delay  =   sum ( ArrDelay  >   0 )   /  n ())   %>% \n    mutate ( rank  =   rank ( p_delay ))   %>% \n    arrange ( rank )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 15 \u00d7 3\n##         UniqueCarrier   p_delay  rank\n##                 <chr>     <dbl> <dbl>\n## 1            American 0.3030208     1\n## 2             AirTran 0.3112269     2\n## 3          US_Airways 0.3267990     3\n## 4  Atlantic_Southeast 0.3677511     4\n## 5      American_Eagle 0.3696714     5\n## 6               Delta 0.3871092     6\n## 7             JetBlue 0.3952452     7\n## 8              Alaska 0.4368132     8\n## 9           Southwest 0.4644557     9\n## 10               Mesa 0.4743590    10\n## 11        Continental 0.4907385    11\n## 12         ExpressJet 0.4943420    12\n## 13             United 0.4963109    13\n## 14            SkyWest 0.5350105    14\n## 15           Frontier 0.5564904    15   1\n2\n3\n4\n5\n6\n7 # Solution to second instruction \nhflights  %>% \n    filter ( ! is.na ( ArrDelay ),  ArrDelay  >   0 )   %>% \n    group_by ( UniqueCarrier )   %>% \n    summarise ( avg  =   mean ( ArrDelay ))   %>% \n    mutate ( rank  =   rank ( avg ))   %>% \n    arrange ( rank )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## # A tibble: 15 \u00d7 3\n##         UniqueCarrier      avg  rank\n##                 <chr>    <dbl> <dbl>\n## 1                Mesa 18.67568     1\n## 2            Frontier 18.68683     2\n## 3          US_Airways 20.70235     3\n## 4         Continental 22.13374     4\n## 5              Alaska 22.91195     5\n## 6             SkyWest 24.14663     6\n## 7          ExpressJet 24.19337     7\n## 8           Southwest 25.27750     8\n## 9             AirTran 27.85693     9\n## 10           American 28.49740    10\n## 11              Delta 32.12463    11\n## 12             United 32.48067    12\n## 13     American_Eagle 38.75135    13\n## 14 Atlantic_Southeast 40.24231    14\n## 15            JetBlue 45.47744    15",
            "title": "Combine group_by with mutate"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#advanced-group_by",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 # Which plane (by tail number) flew out of Houston the most times? How many times? adv1 \nadv1  <-  hflights  %>% \n          group_by ( TailNum )   %>% \n          summarise ( n  =  n ())   %>% \n          filter ( n  ==   max ( n ))  # How many airplanes only flew to one destination from Houston? adv2 \nadv2  <-  hflights  %>% \n          group_by ( TailNum )   %>% \n          summarise ( ndest  =  n_distinct ( Dest ))   %>% \n          filter ( ndest  ==   1 )   %>% \n          summarise ( nplanes  =  n ())  # Find the most visited destination for each carrier: adv3 \nadv3  <-  hflights  %>%  \n          group_by ( UniqueCarrier ,  Dest )   %>% \n          summarise ( n  =  n ())   %>% \n          mutate ( rank  =   rank ( desc ( n )))   %>% \n          filter ( rank  ==   1 )  # Find the carrier that travels to each destination the most: adv4 \nadv4  <-  hflights  %>%  \n          group_by ( Dest ,  UniqueCarrier )   %>% \n          summarise ( n  =  n ())   %>% \n          mutate ( rank  =   rank ( desc ( n )))   %>% \n          filter ( rank  ==   1 )",
            "title": "Advanced group_by"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#dplyr-deals-with-different-types",
            "text": "1\n2\n3 # Use summarise to calculate n_carrier \ns2  <-  hflights  %>% \n    summarise ( n_carrier  =  n_distinct ( UniqueCarrier ))",
            "title": "dplyr deals with different types"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#dplyr-and-mysql-databases",
            "text": "Code only.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # set up a src that connects to the mysql database (src_mysql is provided by dplyr) \nmy_db  <-  src_mysql ( dbname  =   'dplyr' ,  \n                  host  =   'dplyr.csrrinzqubik.us-east-1.rds.amazonaws.com' ,  \n                  port  =   3306 , \n                  user  =   'dplyr' , \n                  password  =   'dplyr' )  # and reference a table within that src: nycflights is now available as an R object that references to the remote nycflights table \nnycflights  <-  tbl ( my_db ,   'dplyr' )  # glimpse at nycflights \nglimpse ( nycflights )  # Calculate the grouped summaries detailed in the instructions \nnycflights  %>% \n   group_by ( carrier )   %>% \n   summarise ( n_flights  =  n (),  avg_delay  =   mean ( arr_delay ))   %>% \n   arrange ( avg_delay )",
            "title": "dplyr and mySQL databases"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#adding-tidyr-functions",
            "text": "complete .  drop_na .  expand .  extract .  extract_numeric .  complete .  fill .  full_seq .  gather .  nest .  replace_na .  separate .  separate_rows .  separate_rows_ .  smiths .  spread .  table1 .  unite .  unnest .  who .",
            "title": "Adding tidyr Functions"
        },
        {
            "location": "/Data_Manipulation_in_R_with_dplyr/#joining-data-in-r-with-dplyr",
            "text": "In development  1\n2\n3\n4\n5\n6\n7\n8\n9 ## 1, Mutating Joins\n\n## 2, Filtering Joins and Set Operations\n\n## 3, Assembling Data\n\n## 4, Advanced Joining\n\n## 5, Case Study",
            "title": "Joining Data in R with dplyr"
        },
        {
            "location": "/Plot_snippets_-_Basics/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nCode snippets and results.\n\n\nSome data might necessitate more specialized packages.\n\n\nFor explaining data, presenting results, reporting and publishing, we can generate prettier graphics with \nggvis\n or \nggplot2\n, and interactive packages such as \nshiny\n.\n\n\n\n\n\n\nPlotting Packages\n\u00b6\n\n\nGraphics:\n\n\n\n\nmaps\n for grids and mapping.\n\n\ndiagram\n for flow charts.\n\n\nplotrix\n for ternary, polar plots.\n\n\ngplots\n.\n\n\npixmap\n, \npng\n, \nrtiff\n, \nReadImages\n, \nEBImage\n, \nRImageJ\n.\n\n\nleaflet\n.\n\n\n\n\nGrid:\n\n\n\n\nvcd\n for mosaic, ternary plots.\n\n\ngrImport\n for vectors.\n\n\nggplot2\n and extensions.\n\n\nlattice\n and \nlatticeExtra\n.\n\n\ngridBase\n.\n\n\n\n\nDevices:\n\n\n\n\nJavaGD\n.\n\n\nCairo\n.\n\n\ntikzDevice\n.\n\n\n\n\nInteractive:\n\n\n\n\nrgl\n.\n\n\nggvis\n.\n\n\niplots\n.\n\n\nrggobi\n.\n\n\n\n\nOthers:\n\n\n\n\nash\n for density plots.\n\n\ncluster\n for dendrograms.\n\n\ncopula\n for multivariate analyses.\n\n\ncorrplot\n for correlations.\n\n\ncompositions\n for geometries, ternary plots.\n\n\nextracat\n for missing values.\n\n\nsoiltexture\n for ternary plots and more.\n\n\nKernSmooth\n for histograms-density plots.\n\n\nopenair\n for polar, circular plots.\n\n\nsm\n for density plots.\n\n\ncar\n for scatter plots.\n\n\nvioplot\n for boxplots.\n\n\nvcd\n for mosaic plots and multivariate analyses.\n\n\nhexbin\n for scatter plots.\n\n\nscatterplot3d\n for 3D scatter plots.\n\n\ncluster\n for dendrograms.\n\n\nshiny\n for interactive plots.\n\n\nggvis\n.\n\n\n\n\nData Type & Dataset\n\u00b6\n\n\nData Types\n\u00b6\n\n\n\n\ncontinuous vs categorical (or discrete).\n\n\ncontinuous: float, x-y-z, 3D, map coordinates, trianguar, lat-long, polar, degree-distance, angle-vector.\n\n\ncategorical: integer, binary, dichotomic, dummy, factor, ordinal (ordered).\n\n\n\n\nContinuous variable characteristics:\n\n\n\n\nasymmetry.\n\n\noutliers.\n\n\nmultimodality.\n\n\ngaps, missing values.\n\n\nheaping, redundance.\n\n\nrounding, integer.\n\n\nimpossibilities, anomalies.\n\n\nerrors.\n\n\n\u2026\n\n\n\n\nCategorical variable characteristics:\n\n\n\n\nunexpected pattern of results.\n\n\nuneven distribution.\n\n\nextra categories.\n\n\nunbalanced experiments.\n\n\nlarge numbers of categories.\n\n\nNA, errors, missings\u2026\n\n\nnominal: no fixed order.\n\n\nordinal: fixed order (scale of 1 to 5).\n\n\ndiscrete: counts, integers.\n\n\ndependencies, correlation, associations.\n\n\ncausal relationships, outliers, groups, clusters, gaps, barriers, conditional relationship.\n\n\n\u2026\n\n\n\n\nUnivariate main plots:\n\n\n\n\nhistogram.\n\n\ndensity.\n\n\nqqmath chart.\n\n\nbox & whickers chart.\n\n\nbar chart.\n\n\ndot.\n\n\n\n\nBivariate main plots:\n\n\n\n\nxy chart.\n\n\nqq chart.\n\n\n\n\nTrivariate main plots:\n\n\n\n\ncloud.\n\n\nwireframe.\n\n\ncountour.\n\n\nlevel.\n\n\n\n\nMultivariate main plots:\n\n\n\n\nsploms.\n\n\nparallel charts (coordinate).\n\n\n\n\nSpecialized plots:\n\n\n\n\nfrequencies, crosstabs: bar charts, mosaic plots, association plots.\n\n\ncorrelations: sploms, pairs, correlograms.\n\n\nt-tests, non-parrametric tests of group differences: box plot, density plot.\n\n\nregression: scatter plot.\n\n\nANOVA: box plots, line plots.\n\n\n\n\nFunctions\n\u00b6\n\n\nCreate a new variable\n\n\n1\n2\niris2 \n<-\n \nwithin\n(\niris\n,\n area \n<-\n Petal.Width\n*\nPetal.Length\n)\n\n\nhead\n(\niris2\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species area\n## 1          5.1         3.5          1.4         0.2  setosa 0.28\n## 2          4.9         3.0          1.4         0.2  setosa 0.28\n## 3          4.7         3.2          1.3         0.2  setosa 0.26\n\n\n\n\n\n\n1\n2\narea \n<-\n \nwith\n(\niris\n,\n area \n<-\n Petal.Width\n*\nPetal.Length\n)\n\n\nhead\n(\narea\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n## [1] 0.28 0.28 0.26\n\n\n\n\n\n\nDataset\n\u00b6\n\n\nFor most examples, we use the \nmtcars\n dataset.\n\n\nPrepare the dataset.\n\n\n1\nattach\n(\nmtcars\n)\n\n\n\n\n\n\n\nGet data attached to a package (an example).\n\n\n1\ndata\n(\ngvhd10\n,\n package \n=\n \n'latticeExtra'\n)\n\n\n\n\n\n\n\nThe Basic Package\n\u00b6\n\n\nBasic Plots, Options & Parameters\n\u00b6\n\n\nStandardize the parameters (an example)\n\n\n1\n2\n# color and tick mark text orientation\n\npar\n(\ncol \n=\n \n'black'\n,\n las \n=\n \n1\n)\n\n\n\n\n\n\n\nGrid and layout\n\n\nOne plot.\n\n\n1\nplot\n(\nhp\n,\n mpg\n,\n xlab \n=\n \n'horsepower'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\nA grid of plots.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n \n1\n))\n\n\nplot\n(\nmpg\n,\n hp\n,\n ylab \n=\n \n'horsepower'\n,\n xlab \n=\n \n'miles per gallon'\n)\n\nboxplot\n(\nmpg \n~\n cyl\n,\n xlab \n=\n \n'mile per gallon'\n,\n ylab \n=\n \n'number of cylinders'\n,\n horizontal \n=\n \nTRUE\n)\n\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n2\n))\n\n\nplot\n(\nmpg\n,\n hp\n,\n ylab \n=\n \n'horsepower'\n,\n xlab \n=\n \n'miles per gallon'\n)\n\nboxplot\n(\nmpg \n~\n cyl\n,\n xlab \n=\n \n'mile per gallon'\n,\n ylab \n=\n \n'number of cylinders'\n,\n horizontal \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n\n\n1\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n1\n))\n\n\n\n\n\n\n\nOther grids.\n\n\n1\n2\n3\n4\n5\nlayout\n(\nmatrix\n(\nc\n(\n1\n,\n1\n,\n2\n,\n3\n),\n \n2\n,\n \n2\n,\n byrow \n=\n \nTRUE\n))\n\n\nplot\n(\nmpg\n,\n xlab \n=\n \n'observations'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\nplot\n(\nhp\n,\n mpg\n,\n xlab \n=\n \n'horsepower'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\nboxplot\n(\nmpg \n~\n cyl\n,\n ylab \n=\n \n'mile per gallon'\n,\n xlab \n=\n \n'number of cylinders'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# view\n\n\nmatrix\n(\nc\n(\n1\n,\n2\n,\n1\n,\n3\n),\n \n2\n,\n \n2\n,\n byrow \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n1\n2\n3\n##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    3\n\n\n\n\n\n\n1\n2\n3\n4\n5\nlayout\n(\nmatrix\n(\nc\n(\n1\n,\n2\n,\n1\n,\n3\n),\n \n2\n,\n \n2\n,\n byrow \n=\n \nTRUE\n))\n\n\nhist\n(\nwt\n)\n\nhist\n(\nmpg\n)\n\nhist\n(\ndisp\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\nlayout\n(\nmatrix\n(\nc\n(\n1\n,\n1\n,\n2\n,\n3\n),\n \n2\n,\n \n2\n,\n byrow \n=\n \nTRUE\n),\n  widths \n=\n \nc\n(\n3\n,\n1\n),\n heights \n=\n \nc\n(\n1\n,\n2\n))\n\n\nhist\n(\nwt\n)\n\nhist\n(\nmpg\n)\n\nhist\n(\ndisp\n)\n\n\n\n\n\n\n\n\n\n1\n2\nnf \n<-\n layout\n(\nmatrix\n(\nc\n(\n1\n,\n1\n,\n2\n,\n3\n),\n \n2\n,\n \n2\n,\n byrow \n=\n \nTRUE\n),\n widths \n=\n lcm\n(\n12\n),\n heights \n=\n lcm\n(\n6\n))\n\nlayout.show\n(\nnf\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\nplot\n(\nmpg\n,\n xlab \n=\n \n'observations'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\nplot\n(\nhp\n,\n mpg\n,\n xlab \n=\n \n'horsepower'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\nboxplot\n(\nmpg \n~\n cyl\n,\n ylab \n=\n \n'mile per gallon'\n,\n xlab \n=\n \n'number of cylinders'\n)\n\n\n\n\n\n\n\n\n\nGridview with additional packages.\n\n\n1\nlibrary\n(\nvcd\n)\n\n\n\n\n\n\n\n1\nmplot\n(\nA\n,\n B\n,\n C\n)\n\n\n\n\n\n\n\n\n\nSee the \nlattice\n and \nlatticeExtra\n packages for built-in facet/gridview. \nggplot2\n as well.\n\n\nPlot and add ablines\n\n\n1\n2\n3\n4\n5\n6\n7\nplot\n(\nhp\n,\n mpg\n,\n xlab \n=\n \n'horsepower'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\n\n# abline(h = yvalues, v = xvalues)\n\nabline\n(\nlm\n(\nmpg \n~\n hp\n))\n\n\n\n# main = 'Title' or...\n\ntitle\n(\n'Title'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\nplot\n(\nhp\n,\n mpg\n,\n xlab \n=\n \n'horsepower'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\nabline\n(\nh \n=\n \nc\n(\n20\n,\n \n25\n))\n\nabline\n(\nv \n=\n \nc\n(\n50\n,\n \n150\n))\n\nabline\n(\nv \n=\n \nseq\n(\n200\n,\n \n300\n,\n \n50\n),\n lty \n=\n \n2\n,\n col \n=\n \n'blue'\n)\n\n\n\n\n\n\n\n\n\nAdd a legend\n\n\n1\n2\n3\n4\nboxplot\n(\nmpg \n~\n cyl\n,\n main \n=\n \n'Title'\n,\n\n   yaxt \n=\n \n'n'\n,\n xlab \n=\n \n'mile per gallon'\n,\n horizontal \n=\n \nTRUE\n,\n col \n=\n terrain.colors\n(\n3\n))\n\n\nlegend\n(\n'topright'\n,\n inset \n=\n \n0.05\n,\n title \n=\n \n'number of cylinders'\n,\n \nc\n(\n'4'\n,\n'6'\n,\n'8'\n),\n fill \n=\n terrain.colors\n(\n3\n),\n horiz \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n\n\nSave\n\n\n1\n2\n3\n4\n5\n6\n7\nmygraph \n<-\n plot\n(\nhp\n,\n mpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepower'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\npdf\n(\n'mygraph.pdf'\n)\n\npng\n(\n'mygraph.png'\n)\n\njpeg\n(\n'mygraph.jpg'\n)\n\nbmp\n(\n'mygraph.bmp'\n)\n\npostscript\n(\n'mygraph.ps'\n)\n\n\n\n\n\n\n\nView in a new window\n\n\nTyping the function will open a new window to render the plot.\n\n\n\n\nwindows()\n for Windows.\n\n\nX11()\n for Linux.\n\n\nquartz()\n for OS X.\n\n\n\n\n\n\n\n1\n2\n3\n4\n# open the new windows\n\nwindows\n()\n\n\nplot\n(\nhp\n,\n mpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepower'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\nEnrich the plot, add text\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nplot\n(\nhp\n,\n mpg\n,\n\n     main \n=\n \n'Title'\n,\n col.main \n=\n \n'blue'\n,\n\n     sub \n=\n \n'figure 1'\n,\n col.sub \n=\n \n'blue'\n,\n\n     xlab \n=\n \n'horsepower'\n,\n \n     ylab \n=\n \n'miles per gallon'\n,\n\n     col.lab \n=\n \n'red'\n,\n cex.lab \n=\n \n0.9\n,\n\n     xlim \n=\n \nc\n(\n50\n,\n \n350\n),\n\n     ylim \n=\n \nc\n(\n0\n,\n \n40\n))\n\n\ntext\n(\n100\n,\n \n10\n,\n \n'text 1'\n)\n \n# x and y coordinate\n\nmtext\n(\n'text 2'\n,\n \n4\n,\n line \n=\n \n0.5\n)\n \n# pos = 1 (bottom), 2 (left), 3 (top), 4 (right); line (margin)\n\n\n\n\n\n\n\n\n\nWith \nlocator()\n, use the mouse; with 1 for 1 click, 2 for\u2026 Find the coordinates to be entered in the code. For example (after two clicks):\n\n\n1\n2\n3\n4\n5\n6\n> locator(2)\n$x\n[1] 212.5308 293.7854\n\n$y\n[1] 33.34040 31.87281\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\nplot\n(\nhp\n,\n mpg\n,\n\n     main \n=\n \n'Title'\n,\n\n     xlab \n=\n \n'horsepower'\n,\n \n     ylab \n=\n \n'miles per gallon'\n)\n\n\ntext\n(\nhp\n,\n mpg\n,\n \nrow.names\n(\nmtcars\n),\n cex \n=\n \n0.7\n,\n pos \n=\n \n4\n,\n col \n=\n \n'red'\n)\n\n\n\n\n\n\n\n\n\nEnrich the plot, add symbols\n\n\n1\n2\n3\n4\n5\n6\n7\nplot\n(\nhp\n,\n mpg\n,\n\n     main \n=\n \n'Title'\n,\n\n     xlab \n=\n \n'horsepower'\n,\n \n     ylab \n=\n \n'miles per gallon'\n)\n\n\nsymbols\n(\n250\n,\n \n20\n,\n squares \n=\n \n1\n,\n add \n=\n \nTRUE\n,\n inches \n=\n \n0.1\n,\n fg \n=\n \n'red'\n)\n\nsymbols\n(\n250\n,\n \n25\n,\n circles \n=\n \n1\n,\n add \n=\n \nTRUE\n,\n inches \n=\n \n0.1\n,\n fg \n=\n \n'red'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n#rectangles\n\n\n#stars\n\n\n#thermometers\n\n\n#boxplots\n\n\n\n\n\n\n\nCombine plots; change \npch =\n & \ncol =\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n2\n))\n\n\n\n# 1\n\nplot\n(\nhp\n,\n mpg\n,\n\n     main \n=\n \n'P1'\n,\n\n     xlab \n=\n \n'horsepower'\n,\n \n     ylab \n=\n \n'miles per gallon'\n,\n\n     pch \n=\n \n1\n,\n\n     col \n=\n \n'black'\n)\n\n\n\n# 2\n\nplot\n(\nhp\n,\n mpg\n,\n\n     main \n=\n \n'P2'\n,\n\n     xlab \n=\n \n'horsepower'\n,\n \n     ylab \n=\n \n'miles per gallon'\n,\n\n     pch \n=\n \n3\n,\n\n     col \n=\n \n'blue'\n,\n\n     cex \n=\n \n0.5\n)\n\n\n\n# 3\n\nplot\n(\nhp\n,\n mpg\n,\n\n     main \n=\n \n'P3'\n,\n\n     xlab \n=\n \n'horsepower'\n,\n \n     ylab \n=\n \n'miles per gallon'\n,\n\n     pch \n=\n \n5\n,\n\n     col \n=\n \n'red'\n,\n\n     cex \n=\n \n2\n)\n\n\n\n# 4\n\nplot\n(\nhp\n,\n mpg\n,\n\n     main \n=\n \n'P4'\n,\n\n     xlab \n=\n \n'horsepower'\n,\n \n     ylab \n=\n \n'miles per gallon'\n,\n\n     pch \n=\n \n7\n,\n\n     col \n=\n \n'green'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# reverse\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n1\n))\n\n\n\n\n\n\n\nChange \ncol =\n\n\n\n\nChange \npch =\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\npar\n(\nfig \n=\n \nc\n(\n0\n,\n0.8\n,\n0\n,\n0.8\n))\n\n\nplot\n(\nmtcars\n$\nwt\n,\n mtcars\n$\nmpg\n,\n xlab \n=\n \n'Car Weight'\n,\n   ylab \n=\n \n'miles Per Gallon'\n)\n\n\npar\n(\nfig \n=\n \nc\n(\n0\n,\n0.8\n,\n0.55\n,\n1\n),\n new \n=\n \nTRUE\n)\n\n\nboxplot\n(\nmtcars\n$\nwt\n,\n horizontal \n=\n \nTRUE\n,\n axes \n=\n \nFALSE\n)\n\n\npar\n(\nfig \n=\n \nc\n(\n0.65\n,\n1\n,\n0\n,\n0.8\n),\n new \n=\n \nTRUE\n)\n\n\nboxplot\n(\nmtcars\n$\nmpg\n,\n axes \n=\n \nFALSE\n)\n\n\nmtext\n(\n'Enhanced Scatterplot'\n,\n side \n=\n \n3\n,\n outer \n=\n \nTRUE\n,\n line \n=\n \n-3\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# reverse\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n1\n))\n\n\n\n\n\n\n\nChange \ntype =\n; without dots\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nx \n<-\n \nc\n(\n1\n:\n5\n);\n y \n<-\n x\n\npar\n(\npch \n=\n \n22\n,\n col \n=\n \n'red'\n)\n \n# plotting symbol and color\n\n\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n4\n))\n \n# all plots on one page\n\nopts \n=\n \nc\n(\n'p'\n,\n'l'\n,\n'o'\n,\n'b'\n,\n'c'\n,\n's'\n,\n'S'\n,\n'h'\n)\n\n\n\nfor\n \n(\ni \nin\n \n1\n:\nlength\n(\nopts\n))\n \n{\n\n  heading \n=\n \npaste\n(\n'type ='\n,\nopts\n[\ni\n])\n\n  plot\n(\nx\n,\n y\n,\n type \n=\n \n'n'\n,\n main \n=\n heading\n)\n\n  lines\n(\nx\n,\n y\n,\n type \n=\n opts\n[\ni\n])\n\n\n}\n\n\n\n\n\n\n\n\n\n1\n2\n# reverse\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n1\n),\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\nChange \ntype =\n; with dots\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nx \n<-\n \nc\n(\n1\n:\n5\n);\n y \n<-\n x\n\npar\n(\npch \n=\n \n22\n,\n col \n=\n \n'blue'\n)\n \n# plotting symbol and color\n\n\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n4\n))\n \n# all plots on one page\n\nopts \n=\n \nc\n(\n'p'\n,\n'l'\n,\n'o'\n,\n'b'\n,\n'c'\n,\n's'\n,\n'S'\n,\n'h'\n)\n\n\n\nfor\n \n(\ni \nin\n \n1\n:\nlength\n(\nopts\n))\n \n{\n\n  heading \n=\n \npaste\n(\n'type ='\n,\nopts\n[\ni\n])\n\n  plot\n(\nx\n,\n y\n,\n main \n=\n heading\n)\n\n  lines\n(\nx\n,\n y\n,\n type \n=\n opts\n[\ni\n])\n\n\n}\n\n\n\n\n\n\n\n\n\n1\n2\n# reverse\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n1\n),\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\nAdd or modify the axes\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nplot\n(\nhp\n,\n mpg\n,\n\n     main \n=\n \n'Title'\n,\n\n     xlab \n=\n \n'horsepower'\n,\n \n     ylab \n=\n \n'miles per gallon'\n,\n\n     xaxt \n=\n \n'n'\n,\n\n     yaxt \n=\n \n'n'\n)\n\n\naxis\n(\n1\n,\n at \n=\n \nc\n(\n100\n,\n \n200\n,\n \n300\n),\n labels \n=\n \nNULL\n,\n pos \n=\n \n15\n,\n lty \n=\n \n'dashed'\n,\n col \n=\n \n'green'\n,\n las \n=\n \n2\n,\n tck \n=\n \n-0.05\n)\n\n\naxis\n(\n4\n,\n at \n=\n \nc\n(\n20\n,\n \n30\n),\n labels \n=\n \nc\n(\n'bt'\n,\n \n'up'\n),\n pos \n=\n \n125\n,\n lty \n=\n \n'dashed'\n,\n col \n=\n \n'blue'\n,\n las \n=\n \n2\n,\n tck \n=\n \n-0.05\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# reverse\n\npar\n(\nlas \n=\n \n1\n)\n\n\n\n\n\n\n\nAdd layers to the first plot\n\n\n1\n2\n3\n4\n5\n6\n7\nplot\n(\nmpg\n,\n\n     main \n=\n \n'Title'\n,\n\n     xlab \n=\n \n'horsepower'\n,\n \n     ylab \n=\n \n'miles per gallon'\n)\n\n\n\n# add lines\n\nlines\n(\nmpg\n[\n1\n:\n10\n],\n type \n=\n \n'l'\n,\n col \n=\n \n'green'\n)\n\n\n\n\n\n\n\n\n\nUnivariate Plots\n\u00b6\n\n\nPlot; continuous\n\n\n1\nplot\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'observations'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\nPlot; categorical\n\n\n1\nplot\n(\ncyl\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'observations'\n,\n ylab \n=\n \n'cylinders'\n)\n\n\n\n\n\n\n\n\n\nQQnorm; continuous\n\n\n1\nqqnorm\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'observations'\n,\n ylab \n=\n \n'cylinders'\n)\n\n\n\n\n\n\n\n\n\nQQnorm; categorical\n\n\n1\nqqnorm\n(\ncyl\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'observations'\n,\n ylab \n=\n \n'cylinders'\n)\n\n\n\n\n\n\n\n\n\nStripchart; continuous\n\n\n1\nstripchart\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\nStripchart; categorical\n\n\n1\nstripchart\n(\ncyl\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinders'\n)\n\n\n\n\n\n\n\n\n\nBarplot (vertical); continuous\n\n\n1\nbarplot\n(\nmpg\n[\n1\n:\n10\n],\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'observations'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\nBarplot (horizontal); categorical\n\n\n1\nbarplot\n(\ncyl\n[\n1\n:\n10\n],\n main \n=\n \n'Title'\n,\n horiz \n=\n \nTRUE\n,\n xlab \n=\n \n'cylinders'\n,\n ylab \n=\n \n'observations'\n)\n\n\n\n\n\n\n\n\n\nBarplots options\n\n\nGroup with \ntable()\n.\n\n\n1\n2\ncounts \n<-\n \ntable\n(\ncyl\n)\n\ncounts\n\n\n\n\n\n\n1\n2\n3\n## cyl\n##  4  6  8 \n## 11  7 14\n\n\n\n\n\n\n1\nbarplot\n(\ncounts\n,\n main \n=\n \n'Title'\n,\n horiz \n=\n \nTRUE\n,\n xlab \n=\n \n'count'\n,\n names.arg \n=\n \nc\n(\n'4 Cyl'\n,\n \n'6 Cyl'\n,\n \n'8 Cyl'\n))\n\n\n\n\n\n\n\n\n\n1\n2\ncounts \n<-\n \ntable\n(\nvs\n,\n gear\n)\n\ncounts\n\n\n\n\n\n\n1\n2\n3\n4\n##    gear\n## vs   3  4  5\n##   0 12  2  4\n##   1  3 10  1\n\n\n\n\n\n\n1\nbarplot\n(\ncounts\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'gearbox'\n,\n col \n=\n \nc\n(\n'darkblue'\n,\n \n'red'\n),\n legend \n=\n \nrownames\n(\ncounts\n))\n \n\n\n\n\n\n\n\n\n1\n2\ncounts \n<-\n \ntable\n(\nvs\n,\n gear\n)\n\ncounts\n\n\n\n\n\n\n1\n2\n3\n4\n##    gear\n## vs   3  4  5\n##   0 12  2  4\n##   1  3 10  1\n\n\n\n\n\n\n1\nbarplot\n(\ncounts\n,\n main \n=\n \n'Title'\n,\n xlab\n=\n'gearbox'\n,\n col \n=\n \nc\n(\n'darkblue'\n,\n \n'red'\n),\n legend \n=\n  \nrownames\n(\ncounts\n),\n beside \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n\n\nGroup with \naggregate()\n.\n\n\n1\naggregate\n(\nmtcars\n,\n by \n=\n \nlist\n(\ncyl\n,\n vs\n),\n FUN \n=\n \nmean\n,\n na.rm \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n##   Group.1 Group.2      mpg cyl   disp       hp     drat       wt     qsec\n## 1       4       0 26.00000   4 120.30  91.0000 4.430000 2.140000 16.70000\n## 2       6       0 20.56667   6 155.00 131.6667 3.806667 2.755000 16.32667\n## 3       8       0 15.10000   8 353.10 209.2143 3.229286 3.999214 16.77214\n## 4       4       1 26.73000   4 103.62  81.8000 4.035000 2.300300 19.38100\n## 5       6       1 19.12500   6 204.55 115.2500 3.420000 3.388750 19.21500\n##   vs        am     gear     carb\n## 1  0 1.0000000 5.000000 2.000000\n## 2  0 1.0000000 4.333333 4.666667\n## 3  0 0.1428571 3.285714 3.500000\n## 4  1 0.7000000 4.000000 1.500000\n## 5  1 0.0000000 3.500000 2.500000\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\npar\n(\nlas \n=\n \n2\n)\n \n# make label text perpendicular to axis\n\n\npar\n(\nmar \n=\n \nc\n(\n5\n,\n \n8\n,\n \n4\n,\n \n2\n))\n \n# increase y-axis margin.\n\n\ncounts \n<-\n \ntable\n(\nmtcars\n$\ngear\n)\n\nbarplot\n(\ncounts\n,\n main \n=\n \n'Car Distribution'\n,\n horiz \n=\n \nTRUE\n,\n names.arg \n=\n \nc\n(\n'3 Gears'\n,\n \n'4 Gears'\n,\n \n'5   Gears'\n),\n cex.names \n=\n \n0.8\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# reverse\n\npar\n(\nlas \n=\n \n1\n)\n\n\n\n\n\n\n\nColors.\n\n\n1\n2\n3\n4\n5\n6\nlibrary\n(\nRColorBrewer\n)\n\n\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n \n1\n))\n\n\nbarplot\n(\niris\n$\nPetal.Length\n)\n\nbarplot\n(\ntable\n(\niris\n$\nSpecies\n,\n iris\n$\nSepal.Length\n),\n col \n=\n brewer.pal\n(\n3\n,\n \n'Set1'\n))\n\n\n\n\n\n\n\n\n\n1\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n1\n))\n\n\n\n\n\n\n\nPie Chart\n\n\nAvoid!\n\n\nDotchart; continuous\n\n\n1\ndotchart\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'observations'\n)\n\n\n\n\n\n\n\n\n\nDotchart; categorical\n\n\n1\ndotchart\n(\ncyl\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinders'\n,\n ylab \n=\n \n'observations'\n)\n\n\n\n\n\n\n\n\n\nDotchart options\n\n\n1\ndotchart\n(\nmpg\n,\nlabels \n=\n \nrow.names\n(\nmtcars\n),\n cex \n=\n \n0.7\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# sort by mpg\n\nx \n<-\n mtcars\n[\norder\n(\nmpg\n),]\n\n\n\n# must be factors\n\nx\n$\ncyl \n<-\n \nfactor\n(\nx\n$\ncyl\n)\n\nx\n$\ncolor\n[\nx\n$\ncyl \n==\n \n4\n]\n \n<-\n \n'red'\n\nx\n$\ncolor\n[\nx\n$\ncyl \n==\n \n6\n]\n \n<-\n \n'blue'\n\nx\n$\ncolor\n[\nx\n$\ncyl \n==\n \n8\n]\n \n<-\n \n'darkgreen'\n\n\ndotchart\n(\nx\n$\nmpg\n,\n labels \n=\n \nrow.names\n(\nx\n),\n cex \n=\n \n0.7\n,\n groups \n=\n x\n$\ncyl\n,\n main \n=\n \n'Title'\n,\n  xlab \n=\n \n'miles per gallon'\n,\n gcolor \n=\n \n'black'\n,\n color \n=\n x\n$\ncolor\n)\n\n\n\n\n\n\n\n\n\nMore with the \nhmisc\n package and \npanel.dotplot()\n and in the \nlattice\n\npackage section.\n\n\nBoxplot; continuous\n\n\n1\nboxplot\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'observations'\n)\n\n\n\n\n\n\n\n\n\nStem; continuous\n\n\n1\nstem\n(\nmpg\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n## \n##   The decimal point is at the |\n## \n##   10 | 44\n##   12 | 3\n##   14 | 3702258\n##   16 | 438\n##   18 | 17227\n##   20 | 00445\n##   22 | 88\n##   24 | 4\n##   26 | 03\n##   28 | \n##   30 | 44\n##   32 | 49\n\n\n\n\n\n\nHistogram; continuous\n\n\n1\nhist\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon - bins'\n,\n ylab \n=\n \n'count'\n)\n\n\n\n\n\n\n\n\n\nHistogram; categorical\n\n\n1\nhist\n(\ncyl\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinders - bins'\n,\n ylab \n=\n \n'count'\n)\n\n\n\n\n\n\n\n\n\nHistogram options\n\n\n1\nhist\n(\nmpg\n,\n breaks \n=\n \n12\n,\n col \n=\n \n'red'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nx \n<-\n mpg\n\nh \n<-\n hist\n(\nx\n,\n breaks \n=\n \n10\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n)\n\n\nxfit \n<-\n \nseq\n(\nmin\n(\nx\n),\n \nmax\n(\nx\n),\nlength \n=\n \n40\n)\n\nyfit \n<-\n dnorm\n(\nxfit\n,\n mean \n=\n \nmean\n(\nx\n),\n sd \n=\n sd\n(\nx\n))\n\nyfit \n<-\n yfit\n*\ndiff\n(\nh\n$\nmids\n[\n1\n:\n2\n])\n*\nlength\n(\nx\n)\n\n\nlines\n(\nxfit\n,\n yfit\n,\n col \n=\n \n'blue'\n,\n lwd \n=\n \n2\n)\n\n\n\n\n\n\n\n\n\nColors.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nlibrary\n(\nRColorBrewer\n)\n\n\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n \n3\n))\n\n\nhist\n(\nVADeaths\n,\n breaks \n=\n \n10\n,\n col \n=\n brewer.pal\n(\n3\n,\n \n'Set3'\n),\n main \n=\n \n'3, Set3'\n)\n\nhist\n(\nVADeaths\n,\n breaks \n=\n \n4\n,\n col \n=\n brewer.pal\n(\n3\n,\n \n'Set2'\n),\n main \n=\n \n'3, Set2'\n)\n\nhist\n(\nVADeaths\n,\n breaks \n=\n \n8\n,\n col \n=\n brewer.pal\n(\n3\n,\n \n'Set1'\n),\n main \n=\n \n'3, Set1'\n)\n\nhist\n(\nVADeaths\n,\n breaks \n=\n \n2\n,\n col \n=\n brewer.pal\n(\n8\n,\n \n'Set3'\n),\n main \n=\n \n'8, Set3'\n)\n\nhist\n(\nVADeaths\n,\n breaks \n=\n \n10\n,\n col \n=\n brewer.pal\n(\n8\n,\n \n'Greys'\n),\n main \n=\n \n'8, Greys'\n)\n\nhist\n(\nVADeaths\n,\n breaks \n=\n \n10\n,\n col \n=\n brewer.pal\n(\n8\n,\n \n'Greens'\n),\n main \n=\n \n'8, Greens'\n)\n\n\n\n\n\n\n\n\n\n1\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n1\n))\n\n\n\n\n\n\n\nDensity Plot; continuous\n\n\n1\nplot\n(\ndensity\n(\nmpg\n),\n main \n=\n \n'Title'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\nplot\n(\ndensity\n(\nmpg\n),\n main \n=\n \n'Title'\n)\n\n\npolygon\n(\ndensity\n(\nmpg\n),\n col \n=\n \n'red'\n,\n border \n=\n \n'blue'\n)\n \n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\nd1 \n<-\n density\n(\nmtcars\n$\nmpg\n)\n\nplot\n(\nd1\n)\n\nrug\n(\nmtcars\n$\nmpg\n)\n\n\nlines\n(\ndensity\n(\nmtcars\n$\nmpg\n,\n d1\n$\nbw\n/\n2\n),\n col \n=\n \n'green'\n)\n\nlines\n(\ndensity\n(\nmtcars\n$\nmpg\n,\n d1\n$\nbw\n/\n5\n),\n col \n=\n \n'blue'\n)\n\n\n\n\n\n\n\n\n\nBivariate (Multivariate) Plots\n\u00b6\n\n\nPlot, continuous/continuous\n\n\n1\nplot\n(\nmpg\n,\n hp\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'horsepowers'\n)\n\n\n\n\n\n\n\n\n\nPlot, continuous/categorical\n\n\n1\nplot\n(\nmpg\n,\n cyl\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'cylinders'\n)\n\n\n\n\n\n\n\n\n\nPlot options\n\n\n1\n2\n3\n4\nplot\n(\nwt\n,\n mpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'weight'\n,\n ylab \n=\n \n'miles per gallon '\n)\n\n\nabline\n(\nlm\n(\nmpg \n~\n wt\n),\n col \n=\n \n'red'\n)\n \n# regression\n\nlines\n(\nlowess\n(\nwt\n,\n mpg\n),\n col \n=\n \n'blue'\n)\n \n# lowess line\n\n\n\n\n\n\n\n\n\nSmoothScatter; continuous/continuous\n\n\n1\nsmoothScatter\n(\nmpg\n,\n hp\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'horsepowers'\n)\n\n\n\n\n\n\n\n\n\nSunflowerplot; categorical/categorical\n\n\nSpecial symbols at each location: one observation = one dot; more observations = cross, star, etc.\n\n\n1\nsunflowerplot\n(\ngear\n,\n cyl\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'gearbox'\n,\n ylab \n=\n \n'cylinders'\n)\n\n\n\n\n\n\n\n\n\nBoxplot\n\n\n1\nboxplot\n(\nmpg \n~\n cyl\n,\n main \n=\n \n'Title'\n,\n   xlab \n=\n \n'cylinders'\n,\n ylab \n=\n \n'miles per gallon'\n)\n \n\n\n\n\n\n\n\n\nColors.\n\n\n1\n2\n3\n4\n5\n6\nlibrary\n(\nRColorBrewer\n)\n\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n2\n))\n\n\nboxplot\n(\niris\n$\nSepal.Length\n,\n col \n=\n \n'red'\n)\n\nboxplot\n(\niris\n$\nSepal.Length \n~\n iris\n$\nSpecies\n,\n col \n=\n topo.colors\n(\n3\n))\n\n\n\n\n\n\n\n\n\n1\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n1\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\nlibrary\n(\ndplyr\n)\n\n\ndata\n(\nPima.tr2\n,\n package \n=\n \n'MASS'\n)\n\n\nPimaV \n<-\n select\n(\nPima.tr2\n,\n glu\n:\nage\n)\n\nboxplot\n(\nscale\n(\nPimaV\n),\n pch \n=\n \n16\n,\n outcol \n=\n \n'red'\n)\n\n\n\n\n\n\n\n\n\nBoxplot options\n\n\n1\n2\n3\n4\n5\n6\n7\nfour \n<-\n \nsubset\n(\nmpg\n,\n cyl \n==\n \n4\n)\n\nsix \n<-\n \nsubset\n(\nmpg\n,\n cyl \n==\n \n6\n)\n\neight \n<-\n \nsubset\n(\nmpg\n,\n cyl \n==\n \n8\n)\n\n\nboxplot\n(\nfour\n,\n six\n,\n eight\n,\n main \n=\n \n'Title'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\naxis\n(\n1\n,\n at \n=\n \nc\n(\n1\n,\n \n2\n,\n \n3\n),\n labels \n=\n \nc\n(\n'4 Cyl'\n,\n \n'6 Cyl'\n,\n \n'8 Cyl'\n))\n\n\n\n\n\n\n\n\n\nDotchart\n\n\n1\n2\ncounts \n<-\n \ntable\n(\ngear\n,\n cyl\n)\n\ncounts\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\n\n\n\n1\ndotchart\n(\ncounts\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'count'\n,\n ylab \n=\n \n'cylinders/gearbox'\n)\n\n\n\n\n\n\n\n\n\n1\n2\ncounts \n<-\n \ntable\n(\ncyl\n,\n gear\n)\n\ncounts\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##    gear\n## cyl  3  4  5\n##   4  1  8  2\n##   6  2  4  1\n##   8 12  0  2\n\n\n\n\n\n\n1\ndotchart\n(\ncounts\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'count'\n,\n ylab \n=\n \n'gearbox/cylinders'\n)\n\n\n\n\n\n\n\n\n\nBarplot with its options\n\n\nVertical or horizontal. The legend as well can be horizontal or vertical.\n\n\n1\n2\ncounts \n<-\n \ntable\n(\ngear\n,\n cyl\n)\n\ncounts\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\n\n\n\n1\n2\n3\n4\nbarplot\n(\ncounts\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinders'\n,\n ylab \n=\n \n'count'\n,\n ylim \n=\n \nc\n(\n0\n,\n \n20\n),\n col \n=\n terrain.colors\n(\n3\n))\n\n\nlegend\n(\n'topleft'\n,\n inset \n=\n \n.04\n,\n title \n=\n \n'gearbox'\n,\n\n   \nc\n(\n'3'\n,\n'4'\n,\n'5'\n),\n fill \n=\n terrain.colors\n(\n3\n),\n horiz \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n\n\n1\n2\ncounts \n<-\n \ntable\n(\ngear\n,\n cyl\n)\n\ncounts\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\n\n\n\n1\nbarplot\n(\ncounts\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinders'\n,\n ylab \n=\n \n'count'\n,\n ylim \n=\n \nc\n(\n0\n,\n \n25\n),\n col \n=\n terrain.colors\n(\n3\n),\n legend \n=\n \nrownames\n(\ncounts\n))\n\n\n\n\n\n\n\n\n\n1\n2\ncounts \n<-\n \ntable\n(\ngear\n,\n cyl\n)\n\ncounts\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\n\n\n\n1\nbarplot\n(\ncounts\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinders'\n,\n ylab \n=\n \n'count'\n,\n ylim \n=\n \nc\n(\n0\n,\n \n20\n),\n col \n=\n terrain.colors\n(\n3\n),\n legend \n=\n \nrownames\n(\ncounts\n),\n beside \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n\n\nSpineplot\n\n\n\u2018Count\u2019 = blocks; categorical (with factors).\n\n\n1\n2\n3\n4\ncyl2 \n<-\n \nas.factor\n(\ncyl\n)\n \n# mandatory for the y\n\ngear2 \n<-\n \nas.factor\n(\ngear\n)\n\n\nspineplot\n(\ngear2\n,\n cyl2\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'gearbox'\n,\n ylab \n=\n \n'cylinders'\n)\n\n\n\n\n\n\n\n\n\nCount = blocks; continuous.\n\n\n1\nspineplot\n(\nmpg\n,\n cyl2\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'cylinders'\n)\n\n\n\n\n\n\n\n\n\nMosaicplot\n\n\nCount = blocks.\n\n\n1\n2\ncounts \n<-\n \ntable\n(\ngear\n,\n cyl\n)\n\ncounts\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2\n\n\n\n\n\n\n1\nmosaicplot\n(\ncounts\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'gearbox'\n,\n ylab \n=\n \n'cylinders'\n)\n\n\n\n\n\n\n\n\n\nMultivariate Plots\n\u00b6\n\n\nPairs\n\n\n1\npairs\n(\n \n~\nmpg \n+\n disp \n+\n hp\n)\n\n\n\n\n\n\n\n\n\nCoplot\n\n\n1\ncoplot\n(\nmpg \n~\n hp \n|\n wt\n)\n\n\n\n\n\n\n\n\n\nCorrelograms\n\n\n1\n2\n3\nlibrary\n(\ncorrgram\n)\n\n\ncorrgram\n(\nmtcars\n,\n order \n=\n \nTRUE\n,\n lower.panel \n=\n panel.shade\n,\n upper.panel\n=\npanel.pie\n,\n text.panel \n=\n panel.txt\n,\n main \n=\n \n'Car Milage Data in PC2/PC1 Order'\n)\n\n\n\n\n\n\n\n\n\nPlot a dataset with colors\n\n\n1\n2\n3\nlibrary\n(\nRColorBrewer\n)\n\n\nplot\n(\niris\n,\n col \n=\n brewer.pal\n(\n3\n,\n \n'Set1'\n))\n\n\n\n\n\n\n\n\n\nStars\n\n\nThe star branches are explanatory; be careful with the interpretation! Well-advised for visual and pattern exploration.\n\n\n1\nmtcars\n[\n1\n:\n4\n,\n \nc\n(\n1\n,\n \n4\n,\n \n6\n)]\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##                 mpg  hp    wt\n## Mazda RX4      21.0 110 2.620\n## Mazda RX4 Wag  21.0 110 2.875\n## Datsun 710     22.8  93 2.320\n## Hornet 4 Drive 21.4 110 3.215\n\n\n\n\n\n\n1\nstars\n(\nmtcars\n[\n1\n:\n4\n,\n \nc\n(\n1\n,\n \n4\n,\n \n6\n)])\n\n\n\n\n\n\n\n\n\nTrivariate plots\n\n\n\n\nimage()\n.\n\n\ncontour()\n.\n\n\nfilled.contour()\n.\n\n\npersp()\n.\n\n\nsymbols()\n.\n\n\n\n\nTimes Series\n\u00b6\n\n\nAdd packages: \nzoo\n and \nxts\n.\n\n\nBasics\n\n\n1\nplot\n(\nAirPassengers\n,\n type \n=\n \n'l'\n)\n\n\n\n\n\n\n\n\n\nChange the \ntype =\n\n\n1\n2\n3\n4\n5\n6\ny1 \n<-\n rnorm\n(\n100\n)\n\n\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n \n1\n))\n\n\nplot\n(\ny1\n,\n type \n=\n \n'p'\n,\n main \n=\n \n'p vs l'\n)\n\nplot\n(\ny1\n,\n type \n=\n \n'l'\n)\n\n\n\n\n\n\n\n\n\n1\n2\nplot\n(\ny1\n,\n type \n=\n \n'l'\n,\n main \n=\n \n'l vs h'\n)\n\nplot\n(\ny1\n,\n type \n=\n \n'h'\n)\n\n\n\n\n\n\n\n\n\n1\n2\nplot\n(\ny1\n,\n type \n=\n \n'l'\n,\n lty \n=\n \n3\n,\n main \n=\n \n'l 3 vs o'\n)\n\nplot\n(\ny1\n,\n type \n=\n \n'o'\n)\n\n\n\n\n\n\n\n\n\n1\n2\nplot\n(\ny1\n,\n type \n=\n \n'b'\n,\n main \n=\n \n'b vs c'\n)\n\nplot\n(\ny1\n,\n type \n=\n \n'c'\n)\n\n\n\n\n\n\n\n\n\n1\n2\nplot\n(\ny1\n,\n type \n=\n \n's'\n,\n main \n=\n \n's vs S'\n)\n\nplot\n(\ny1\n,\n type \n=\n \n'S'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# reverse\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n1\n))\n\n\n\n\n\n\n\nAdd a box\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\ny1 \n<-\n rnorm\n(\n100\n)\n\ny2 \n<-\n rnorm\n(\n100\n)\n\n\npar\n(\nmfrow \n=\n \n(\nc\n(\n2\n,\n \n1\n)))\n\n\nplot\n(\ny1\n,\n type \n=\n \n'l'\n,\n axes \n=\n \nFALSE\n,\n xlab \n=\n \n''\n,\n ylab \n=\n \n''\n,\n main \n=\n \n''\n)\n\n\nbox\n(\ncol \n=\n \n'gray'\n)\n\n\nlines\n(\nx \n=\n \nc\n(\n20\n,\n \n20\n,\n \n40\n,\n \n40\n),\n y \n=\n \nc\n(\n-7\n,\n \nmax\n(\ny1\n),\n \nmax\n(\ny1\n),\n \n-7\n),\n lwd \n=\n \n3\n,\n col \n=\n \n'gray'\n)\n\n\nplot\n(\ny2\n,\n type \n=\n \n'l'\n,\n axes \n=\n \nFALSE\n,\n xlab \n=\n \n''\n,\n ylab \n=\n \n''\n,\n main \n=\n \n''\n)\n\n\nbox\n(\ncol \n=\n \n'gray'\n)\n\n\nlines\n(\nx \n=\n \nc\n(\n20\n,\n \n20\n,\n \n40\n,\n \n40\n),\n y \n=\n \nc\n(\n7\n,\n \nmin\n(\ny2\n),\n \nmin\n(\ny2\n),\n \n7\n),\n lwd \n=\n \n3\n,\n col \n=\n \n'gray'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# reverse\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n1\n))\n\n\n\n\n\n\n\nAdd lines and text within the plot\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\ny1 \n<-\n rnorm\n(\n100\n)\n\n\n\n# x goes from 0 to 100\n\n\n# xaxt = 'n' remove the x ticks\n\nplot\n(\ny1\n,\n type \n=\n \n'l'\n,\n lwd \n=\n \n2\n,\n lty \n=\n \n'longdash'\n,\n main \n=\n \n'Title'\n,\n ylab \n=\n \n'y'\n,\n xlab \n=\n \n'time'\n,\n xaxt \n=\n \n'n'\n)\n\n\nabline\n(\nh \n=\n \n0\n,\n lty \n=\n \n'longdash'\n)\n\n\nabline\n(\nv \n=\n \n20\n,\n lty \n=\n \n'longdash'\n)\n\nabline\n(\nv \n=\n \n50\n,\n lty \n=\n \n'longdash'\n)\n\nabline\n(\nv \n=\n \n95\n,\n lty \n=\n \n'longdash'\n)\n\n\ntext\n(\n17\n,\n \n1.5\n,\n srt \n=\n \n90\n,\n adj \n=\n \n0\n,\n labels \n=\n \n'Tag 1'\n,\n cex \n=\n \n0.8\n)\n\ntext\n(\n47\n,\n \n1.5\n,\n srt \n=\n \n90\n,\n adj \n=\n \n0\n,\n labels \n=\n \n'Tag a'\n,\n cex \n=\n \n0.8\n)\n\ntext\n(\n92\n,\n \n1.5\n,\n srt \n=\n \n90\n,\n adj \n=\n \n0\n,\n labels \n=\n \n'Tag alpha'\n,\n cex \n=\n \n0.8\n)\n\n\n\n\n\n\n\n\n\nA comprehensive example\n\n\n1\n2\n# new data\n\n\nhead\n(\nOrange\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   Tree  age circumference\n## 1    1  118            30\n## 2    1  484            58\n## 3    1  664            87\n## 4    1 1004           115\n## 5    1 1231           120\n## 6    1 1372           142\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n# convert factor to numeric for convenience\n\nOrange\n$\nTree \n<-\n \nas.numeric\n(\nOrange\n$\nTree\n)\n\nntrees \n<-\n \nmax\n(\nOrange\n$\nTree\n)\n\n\n\n# get the range for the x and y axis\n\nxrange \n<-\n \nrange\n(\nOrange\n$\nage\n)\n\nyrange \n<-\n \nrange\n(\nOrange\n$\ncircumference\n)\n\n\n\n# set up the plot\n\nplot\n(\nxrange\n,\n yrange\n,\n type \n=\n \n'n'\n,\n xlab \n=\n \n'Age (days)'\n,\n\n   ylab \n=\n \n'Circumference (mm)'\n \n)\n\ncolors \n<-\n rainbow\n(\nntrees\n)\n\nlinetype \n<-\n \nc\n(\n1\n:\nntrees\n)\n\nplotchar \n<-\n \nseq\n(\n18\n,\n \n18\n \n+\n ntrees\n,\n \n1\n)\n\n\n\n# add lines\n\n\nfor\n \n(\ni \nin\n \n1\n:\nntrees\n)\n \n{\n\n  tree \n<-\n \nsubset\n(\nOrange\n,\n Tree \n==\n i\n)\n\n  lines\n(\ntree\n$\nage\n,\n tree\n$\ncircumference\n,\n type \n=\n \n'b'\n,\n lwd \n=\n \n1.5\n,\n\n    lty \n=\n linetype\n[\ni\n],\n col \n=\n colors\n[\ni\n],\n pch \n=\n plotchar\n[\ni\n])\n\n\n}\n\n\n\n# add a title and subtitle\n\ntitle\n(\n'Tree Growth'\n,\n \n'example of line plot'\n)\n\n\n\n# add a legend\n\nlegend\n(\nxrange\n[\n1\n],\n yrange\n[\n2\n],\n \n1\n:\nntrees\n,\n cex \n=\n \n0.8\n,\n col \n=\n colors\n,\n\n   pch \n=\n plotchar\n,\n lty \n=\n linetype\n,\n title \n=\n \n'Tree'\n)\n\n\n\n\n\n\n\n\n\nChange \nlty =\n\n\n\n\nRegressions and Residual Plots\n\u00b6\n\n\n1\n2\n3\n4\n# first\n\nregr \n<-\n lm\n(\nmpg \n~\n hp\n)\n\n\n\nsummary\n(\nregr\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## \n## Call:\n## lm(formula = mpg ~ hp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.7121 -2.1122 -0.8854  1.5819  8.2360 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 30.09886    1.63392  18.421  < 2e-16 ***\n## hp          -0.06823    0.01012  -6.742 1.79e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.863 on 30 degrees of freedom\n## Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 \n## F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n\n\n\n\n\n1\n2\nplot\n(\nmpg \n~\n hp\n)\n\nabline\n(\nregr\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n \n2\n))\n\n\n\n# then\n\nplot\n(\nregr\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# reverse\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n1\n))\n\n\n\n\n\n\n\nThe \nlattice\n and \nlatticeExtra\n Packages\n\u00b6\n\n\n1\nlibrary\n(\nlattice\n)\n\n\n\n\n\n\n\nColoring\n\u00b6\n\n\n1\n2\n# Show the default settings\n\nshow.settings\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# Save the default theme\n\nmytheme \n<-\n trellis.par.get\n()\n\n\n\n# Turn the B&W\n\ntrellis.par.set\n(\ncanonical.theme\n(\ncolor \n=\n \nFALSE\n))\n\nshow.settings\n()\n\n\n\n\n\n\n\n\n\nDocumentation\n\u00b6\n\n\n\n\nNational Park Service, Advanced\n\n    Graphics (Lattice)\n\n\nTreillis\n\n    Plots\n\n\n\n\nA note on reordering the levels (factors)\n\u00b6\n\n\n1\n2\n3\n4\n# start\n\ncyl \n<-\n mtcars\n$\ncyl\ncyl \n<-\n \nas.factor\n(\ncyl\n)\n\ncyl\n\n\n\n\n\n\n1\n2\n##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n## Levels: 4 6 8\n\n\n\n\n\n\n1\nlevels\n(\ncyl\n)\n\n\n\n\n\n\n\n1\n## [1] \"4\" \"6\" \"8\"\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# option 1\n\ncyl \n<-\n \nfactor\n(\ncyl\n,\n levels \n=\n \nc\n(\n'8'\n,\n \n'6'\n,\n \n'4'\n))\n\n\n# or levels = 3:1\n\n\n# or levels = letters[3:1]\n\n\nlevels\n(\ncyl\n)\n\n\n\n\n\n\n\n1\n## [1] \"8\" \"6\" \"4\"\n\n\n\n\n\n\n1\n2\n3\n4\n5\ncyl \n<-\n mtcars\n$\ncyl\ncyl \n<-\n \nas.factor\n(\ncyl\n)\n\n\n# option 2\n\ncyl \n<-\n reorder\n(\ncyl\n,\n new.order \n=\n \n3\n:\n1\n)\n\n\nlevels\n(\ncyl\n)\n\n\n\n\n\n\n\n1\n## [1] \"8\" \"6\" \"4\"\n\n\n\n\n\n\n1\n2\n3\n4\nlibrary\n(\nlattice\n)\n\n\n\n# normalized x-axis for comparison\n\nbarchart\n(\nClass \n~\n Freq \n|\n Sex \n+\n Age\n,\n data \n=\n \nas.data.frame\n(\nTitanic\n),\n groups \n=\n Survived\n,\n stack \n=\n \nTRUE\n,\n layout \n=\n \nc\n(\n4\n,\n \n1\n),\n auto.key \n=\n \nlist\n(\ntitle \n=\n \n'Survived'\n,\n columns \n=\n \n2\n))\n\n\n\n\n\n\n\n\n\n1\n2\n# free x-axis\n\nbarchart\n(\nClass \n~\n Freq \n|\n Sex \n+\n Age\n,\n data \n=\n \nas.data.frame\n(\nTitanic\n),\n groups \n=\n Survived\n,\n stack \n=\n \nTRUE\n,\n layout \n=\n \nc\n(\n4\n,\n \n1\n),\n auto.key \n=\n \nlist\n(\ntitle \n=\n \n'Survived'\n,\n columns \n=\n \n2\n),\n scales \n=\n \nlist\n(\nx \n=\n \n'free'\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# or\n\nbc.titanic \n<-\n barchart\n(\nClass \n~\n Freq \n|\n Sex \n+\n Age\n,\n data \n=\n \nas.data.frame\n(\nTitanic\n),\n groups \n=\n Survived\n,\n stack \n=\n \nTRUE\n,\n layout \n=\n \nc\n(\n4\n,\n \n1\n),\n auto.key \n=\n \nlist\n(\ntitle \n=\n \n'Survived'\n,\n columns \n=\n \n2\n),\n scales \n=\n \nlist\n(\nx \n=\n \n'free'\n))\n\n\nbc.titanic\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# add bg grid\n\nupdate\n(\nbc.titanic\n,\n panel \n=\n \nfunction\n(\n...\n)\n \n{\n\n  panel.grid\n(\nh \n=\n \n0\n,\n v \n=\n \n-1\n)\n\n  panel.barchart\n(\n...\n)\n\n\n})\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# remove lines\n\nupdate\n(\nbc.titanic\n,\n panel \n=\n \nfunction\n(\n...\n)\n \n{\n\n  panel.barchart\n(\n...\n,\n border \n=\n \n'transparent'\n)\n\n\n})\n\n\n\n\n\n\n\n\n\n1\n2\n# or\n\nupdate\n(\nbc.titanic\n,\n border \n=\n \n'transparent'\n)\n\n\n\n\n\n\n\n\n\n1\n2\nTitanic1 \n<-\n \nas.data.frame\n(\nas.table\n(\nTitanic\n[,\n \n,\n \n'Adult'\n \n,]))\n\nTitanic1\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n##    Class    Sex Survived Freq\n## 1    1st   Male       No  118\n## 2    2nd   Male       No  154\n## 3    3rd   Male       No  387\n## 4   Crew   Male       No  670\n## 5    1st Female       No    4\n## 6    2nd Female       No   13\n## 7    3rd Female       No   89\n## 8   Crew Female       No    3\n## 9    1st   Male      Yes   57\n## 10   2nd   Male      Yes   14\n## 11   3rd   Male      Yes   75\n## 12  Crew   Male      Yes  192\n## 13   1st Female      Yes  140\n## 14   2nd Female      Yes   80\n## 15   3rd Female      Yes   76\n## 16  Crew Female      Yes   20\n\n\n\n\n\n\n1\nbarchart\n(\nClass \n~\n Freq \n|\n Sex\n,\n Titanic1\n,\n groups \n=\n Survived\n,\n stack \n=\n \nTRUE\n,\n auto.key \n=\n \nlist\n(\ntitle \n=\n \n'Survived'\n,\n columns \n=\n \n2\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\nTitanic2 \n<-\n reshape\n(\nTitanic1\n,\n direction \n=\n \n'wide'\n,\n v.names \n=\n \n'Freq'\n,\n idvar \n=\n \nc\n(\n'Class'\n,\n \n'Sex'\n),\n timevar \n=\n \n'Survived'\n)\n\n\n\nnames\n(\nTitanic2\n)\n \n<-\n \nc\n(\n'Class'\n,\n \n'Sex'\n,\n \n'Dead'\n,\n \n'Alive'\n)\n\n\nbarchart\n(\nClass \n~\n Dead \n+\n Alive \n|\n Sex\n,\n Titanic2\n,\n stack \n=\n \nTRUE\n,\n auto.key \n=\n \nlist\n(\ncolumns \n=\n \n2\n))\n\n\n\n\n\n\n\n\n\nUni-, Bi-, Multivariate Plots\n\u00b6\n\n\nBarchart\n\n\nLike \nbarplot()\n.\n\n\n1\n2\n# y ~ x\n\nbarchart\n(\nmpg \n~\n hp\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# y ~ x\n\nbarchart\n(\nmpg \n~\n hp\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n,\n horizontal \n=\n \nFALSE\n)\n\n\n\n\n\n\n\n\n\n1\nbarchart\n(\nVADeaths\n,\n groups \n=\n \nFALSE\n,\n layout \n=\n \nc\n(\n1\n,\n \n4\n),\n aspect \n=\n \n0.7\n,\n reference \n=\nFALSE\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'rate per 100'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\ndata\n(\npostdoc\n,\n package \n=\n \n'latticeExtra'\n)\n\n\nbarchart\n(\nprop.table\n(\npostdoc\n,\n margin \n=\n \n1\n),\n xlab \n=\n \n'Proportion'\n,\n auto.key \n=\n \nlist\n(\nadj \n=\n \n1\n))\n\n\n\n\n\n\n\n\n\nChange \nlayout = c(x, y, page)\n\n\n1\nbarchart\n(\nmpg \n~\n hp \n|\n \nfactor\n(\ncyl\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepowers'\n,\n ylab \n=\n \n'cylinders - miles per gallon'\n,\n layout \n=\n \nc\n(\n1\n,\n3\n))\n\n\n\n\n\n\n\n\n\n1\nbarchart\n(\nmpg \n~\n hp \n|\n \nfactor\n(\ncyl\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinders - horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n,\n layout \n=\n \nc\n(\n3\n,\n1\n))\n\n\n\n\n\n\n\n\n\nChange \naspect = 1\n\n\n1\n for square.\n\n\n1\nbarchart\n(\nmpg \n~\n hp \n|\n \nfactor\n(\ncyl\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n aspect \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\nColors\n\n\n1\nbarchart\n(\nmpg \n~\n hp\n,\n group \n=\n cyl\n,\n auto.key \n=\n \nlist\n(\nspace \n=\n \n'right'\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\n\n\nshingle()\n; control the ranges.\n\n\nequal.count()\n; grid.\n\n\n\n\nDotplot\n\n\nLike \ndotchart()\n.\n\n\n1\ndotplot\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\n1\ndotplot\n(\nfactor\n(\ncyl\n)\n \n~\n mpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'cylinders'\n)\n\n\n\n\n\n\n\n\n\n1\ndotplot\n(\nfactor\n(\ncyl\n)\n \n~\n mpg \n|\n \nfactor\n(\ngear\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'gearbox - miles per gallon'\n,\n ylab \n=\n \n'cylinders'\n,\n layout \n=\n \nc\n(\n3\n,\n1\n))\n\n\n\n\n\n\n\n\n\n1\ndotplot\n(\nfactor\n(\ncyl\n)\n \n~\n mpg \n|\n \nfactor\n(\ngear\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'gearbox - cylinders'\n,\n layout \n=\n \nc\n(\n1\n,\n3\n),\n aspect \n=\n \n0.3\n)\n\n\n\n\n\n\n\n\n\n1\ndotplot\n(\nfactor\n(\ncyl\n)\n \n~\n mpg \n|\n \nfactor\n(\ngear\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'gearbox - cylinders'\n,\n layout \n=\n \nc\n(\n1\n,\n3\n),\n aspect \n=\n \n0.3\n,\n origin \n=\n \n0\n)\n\n\n\n\n\n\n\n\n\n1\ndotplot\n(\nfactor\n(\ncyl\n)\n \n~\n mpg \n|\n \nfactor\n(\ngear\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'gearbox - cylinders'\n,\n layout \n=\n \nc\n(\n1\n,\n3\n),\n aspect \n=\n \n0.3\n,\n origin \n=\n \n0\n,\n type \n=\n \nc\n(\n'p'\n,\n \n'h'\n))\n\n\n\n\n\n\n\n\n\nSet \nauto.key\n.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# maybe we'll want this later\n\nold.pars \n<-\n trellis.par.get\n()\n\n\n\n#trellis.par.set(superpose.symbol = list(pch = c(1,3), col = 12:14))\n\n\ntrellis.par.set\n(\nsuperpose.symbol \n=\n \nlist\n(\npch \n=\n \nc\n(\n1\n,\n3\n),\n col \n=\n \n1\n))\n\n\n\n# Optionally put things back how they were\n\n\n#trellis.par.set(old.pars)\n\n\n\n\n\n\n\nUse \nauto.key\n.\n\n\n1\ndotplot\n(\nfactor\n(\ncyl\n)\n \n~\n mpg \n|\n \nfactor\n(\ngear\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'gearbox - cylinders'\n,\n layout \n=\n \nc\n(\n1\n,\n3\n),\n groups \n=\n vs\n,\n auto.key \n=\n \nlist\n(\nspace \n=\n \n'right'\n))\n\n\n\n\n\n\n\n\n\n1\ntrellis.par.set\n(\nold.pars\n)\n\n\n\n\n\n\n\n1\n2\n3\ntrellis.par.set\n(\nsuperpose.symbol \n=\n \nlist\n(\npch \n=\n \nc\n(\n1\n,\n3\n),\n col \n=\n \n1\n))\n\n\ndotplot\n(\nvariety \n~\n yield \n|\n site\n,\n barley\n,\n layout \n=\n \nc\n(\n1\n,\n \n6\n),\n aspect \n=\n \nc\n(\n0.7\n),\n groups \n=\n year\n,\n auto.key \n=\n \nlist\n(\nspace \n=\n \n'right'\n))\n\n\n\n\n\n\n\n\n\n1\ntrellis.par.set\n(\nold.pars\n)\n\n\n\n\n\n\n\nVertical.\n\n\n1\ndotplot\n(\nmpg \n~\n \nfactor\n(\ncyl\n)\n \n|\n \nfactor\n(\ngear\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinders'\n,\n ylab \n=\n \n'gearbox - miles per gallon'\n,\n layout \n=\n \nc\n(\n1\n,\n3\n),\n aspect \n=\n \n0.3\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\nlibrary\n(\nreadr\n)\n\ndensity \n<-\n read_csv\n(\n'density.csv'\n)\n\ndensity\n$\nDensity \n<-\n \nas.numeric\n(\ndensity\n$\nDensity\n)\n\n\ndotplot\n(\nreorder\n(\nMetropolitanArea\n,\n Density\n)\n \n~\n Density\n,\n density\n,\n type \n=\n \nc\n(\n'p'\n,\n \n'h'\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'Population Density (pop / sq.mi)'\n)\n\n\n\n\n\n\n\n\n\n1\ndotplot\n(\nreorder\n(\nMetropolitanArea\n,\n Density\n)\n \n~\n Density \n|\n Region\n,\n density\n,\n type \n=\n \nc\n(\n'p'\n,\n \n'h'\n),\n strip \n=\n \nFALSE\n,\n strip.left \n=\n \nTRUE\n,\n layout \n=\n \nc\n(\n1\n,\n \n3\n),\n scales \n=\n \nlist\n(\ny \n=\n \nlist\n(\nrelation \n=\n \n'free'\n)),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'Population Density (pop / sq.mi)'\n)\n\n\n\n\n\n\n\n\n\nStripplot\n\n\nLike \nstripchart()\n.\n\n\n1\nstripplot\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\n1\nstripplot\n(\nfactor\n(\ncyl\n)\n \n~\n mpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'cylinders'\n)\n\n\n\n\n\n\n\n\n\n1\nstripplot\n(\nfactor\n(\ncyl\n)\n \n~\n mpg \n|\n \nfactor\n(\ngear\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'gearbox - miles per gallon'\n,\n ylab \n=\n \n'cylinders'\n,\n layout \n=\n \nc\n(\n1\n,\n3\n))\n\n\n\n\n\n\n\n\n\n1\nstripplot\n(\nfactor\n(\ncyl\n)\n \n~\n mpg \n|\n \nfactor\n(\ngear\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'gearbox - miles per gallon'\n,\n ylab \n=\n \n'cylinders'\n,\n layout \n=\n \nc\n(\n1\n,\n3\n),\n groups \n=\n vs\n,\n auto.key \n=\n \nlist\n(\nspace \n=\n \n'right'\n))\n\n\n\n\n\n\n\n\n\n1\nstripplot\n(\nmpg \n~\n \nfactor\n(\ncyl\n)\n \n|\n \nfactor\n(\ngear\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinders'\n,\n ylab \n=\n \n'gearbox - miles per gallon'\n,\n layout \n=\n \nc\n(\n1\n,\n3\n))\n\n\n\n\n\n\n\n\n\nHistogram\n\n\nLike \nhist()\n.\n\n\n1\nhistogram\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\n1\nhistogram\n(\n~\nmpg \n|\n \nfactor\n(\ncyl\n),\n layout \n=\n \nc\n(\n1\n,\n \n3\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'density'\n)\n\n\n\n\n\n\n\n\n\nDensityplot\n\n\nLike \nplot.density()\n.\n\n\n1\ndensityplot\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'density'\n)\n\n\n\n\n\n\n\n\n\n1\ndensityplot\n(\n~\nmpg \n|\n \nfactor\n(\ncyl\n),\n layout \n=\n \nc\n(\n1\n,\n \n3\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'density'\n)\n\n\n\n\n\n\n\n\n\nECDFplot\n\n\n1\n2\n3\nlibrary\n(\nlatticeExtra\n)\n\n\necdfplot\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n''\n)\n\n\n\n\n\n\n\n\n\nBWplot\n\n\nLike \nboxplot\n.\n\n\n1\nbwplot\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'density'\n)\n\n\n\n\n\n\n\n\n\n1\nbwplot\n(\nfactor\n(\ncyl\n)\n \n~\n mpg\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'cylinders'\n)\n\n\n\n\n\n\n\n\n\n1\nbwplot\n(\nfactor\n(\ncyl\n)\n \n~\n mpg \n|\n \nfactor\n(\ngear\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'miles per gallon'\n,\n ylab \n=\n \n'gearbox - cylinders'\n,\n layout \n=\n \nc\n(\n1\n,\n3\n))\n\n\n\n\n\n\n\n\n\n1\nbwplot\n(\nmpg \n~\n \nfactor\n(\ncyl\n)\n \n|\n \nfactor\n(\ngear\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'gearbox - cylinders'\n,\n ylab \n=\n \n'miles per gallon'\n,\n layout \n=\n \nc\n(\n3\n,\n1\n))\n\n\n\n\n\n\n\n\n\nQQmath\n\n\nLike \nqqnorm()\n.\n\n\n1\nqqmath\n(\nmpg\n,\n main \n=\n \n'Title'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\nXYplot\n\n\nLike \nplot()\n.\n\n\n1\nxyplot\n(\nmpg \n~\n disp \n|\n \nfactor\n(\ncyl\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepower'\n,\n ylab \n=\n \n'cylinders - miles per gallon'\n,\n layout \n=\n \nc\n(\n1\n,\n3\n))\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmpg \n~\n disp \n|\n \nfactor\n(\ncyl\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinder - horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n,\n layout \n=\n \nc\n(\n3\n,\n1\n))\n\n\n\n\n\n\n\n\n\nXYplot options\n\n\n1\nxyplot\n(\nmpg \n~\n disp \n|\n \nfactor\n(\ncyl\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinder - horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n aspect \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmpg \n~\n disp \n|\n \nfactor\n(\ncyl\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinder - horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n aspect \n=\n \n1\n,\n scales \n=\n \nlist\n(\ny \n=\n \nlist\n(\nat \n=\n \nseq\n(\n10\n,\n \n30\n,\n \n10\n))))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\nmeanmpg \n<-\n \nmean\n(\nmpg\n)\n\n\nxyplot\n(\nmpg \n~\n disp \n|\n \nfactor\n(\ncyl\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinder - horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n aspect \n=\n \n1\n,\n panel \n=\n \nfunction\n(\n...\n)\n \n{\n\n  panel.xyplot\n(\n...\n)\n\n  panel.abline\n(\nh \n=\n meanmpg\n,\n lty \n=\n \n'dashed'\n)\n\n  panel.text\n(\n450\n,\n meanmpg \n+\n \n1\n,\n \n'avg'\n,\n adj \n=\n \nc\n(\n1\n,\n  \n0\n),\n cex \n=\n \n0.7\n)\n\n\n})\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\nxyplot\n(\nmpg \n~\n disp \n|\n \nfactor\n(\ncyl\n),\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'cylinder - horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n aspect \n=\n \n1\n,\n panel \n=\n \nfunction\n(\nx\n,\n y\n,\n \n...\n)\n \n{\n\n    panel.lmline\n(\nx\n,\n y\n)\n\n    panel.xyplot\n(\nx\n,\n y\n,\n \n...\n)\n\n\n})\n\n\n\n\n\n\n\n\n\n\n\npanel.points()\n.\n\n\npanel.lines()\n.\n\n\npanel.segments()\n.\n\n\npanel.arrows()\n.\n\n\npanel.rect()\n.\n\n\npanel.polygon()\n.\n\n\npanel.text()\n.\n\n\npanel.abline()\n.\n\n\npanel.lmline()\n.\n\n\npanel.xyplot()\n.\n\n\npanel.curve()\n.\n\n\npanel.rug()\n.\n\n\npanel.grid()\n.\n\n\npanel.bwplot()\n.\n\n\npanel.histogram()\n.\n\n\npanel.loess()\n.\n\n\npanel.violin()\n.\n\n\npanel.smoothScatter()\n.\n\n\n\u2026\n\n\npar.settings\n.\n\n\n\u2026\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\nlibrary\n(\nlattice\n)\n\n\ndata\n(\nSeatacWeather\n,\n package \n=\n \n'latticeExtra'\n)\n\n\nxyplot\n(\nmin.temp \n+\n max.temp \n+\n precip \n~\n day \n|\n month\n,\n ylab \n=\n \n'Temperature and Rainfall'\n,\n data \n=\n SeatacWeather\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n type \n=\n \n'l'\n,\n lty \n=\n \n1\n,\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmin.temp \n+\n max.temp \n+\n precip \n~\n day \n|\n month\n,\n ylab \n=\n \n'Temperature and Rainfall'\n,\n data \n=\n SeatacWeather\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n type \n=\n \n'p'\n,\n lty \n=\n \n1\n,\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmin.temp \n+\n max.temp \n+\n precip \n~\n day \n|\n month\n,\n ylab \n=\n \n'Temperature and Rainfall'\n,\n data \n=\n SeatacWeather\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n type \n=\n \n'l'\n,\n lty \n=\n \n1\n,\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmin.temp \n+\n max.temp \n+\n precip \n~\n day \n|\n month\n,\n ylab \n=\n \n'Temperature and Rainfall'\n,\n data \n=\n SeatacWeather\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n type \n=\n \n'o'\n,\n lty \n=\n \n1\n,\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmin.temp \n+\n max.temp \n+\n precip \n~\n day \n|\n month\n,\n ylab \n=\n \n'Temperature and Rainfall'\n,\n data \n=\n SeatacWeather\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n type \n=\n \n'r'\n,\n lty \n=\n \n1\n,\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmin.temp \n+\n max.temp \n+\n precip \n~\n day \n|\n month\n,\n ylab \n=\n \n'Temperature and Rainfall'\n,\n data \n=\n SeatacWeather\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n type \n=\n \n'g'\n,\n lty \n=\n \n1\n,\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmin.temp \n+\n max.temp \n+\n precip \n~\n day \n|\n month\n,\n ylab \n=\n \n'Temperature and Rainfall'\n,\n data \n=\n SeatacWeather\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n type \n=\n \n's'\n,\n lty \n=\n \n1\n,\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmin.temp \n+\n max.temp \n+\n precip \n~\n day \n|\n month\n,\n ylab \n=\n \n'Temperature and Rainfall'\n,\n data \n=\n SeatacWeather\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n type \n=\n \n'S'\n,\n lty \n=\n \n1\n,\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmin.temp \n+\n max.temp \n+\n precip \n~\n day \n|\n month\n,\n ylab \n=\n \n'Temperature and Rainfall'\n,\n data \n=\n SeatacWeather\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n type \n=\n \n'h'\n,\n lty \n=\n \n1\n,\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmin.temp \n+\n max.temp \n+\n precip \n~\n day \n|\n month\n,\n ylab \n=\n \n'Temperature and Rainfall'\n,\n data \n=\n SeatacWeather\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n type \n=\n \n'a'\n,\n lty \n=\n \n1\n,\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmin.temp \n+\n max.temp \n+\n precip \n~\n day \n|\n month\n,\n ylab \n=\n \n'Temperature and Rainfall'\n,\n data \n=\n SeatacWeather\n,\n layout \n=\n \nc\n(\n3\n,\n1\n),\n type \n=\n \n'smooth'\n,\n lty \n=\n \n1\n,\n col \n=\n \n'black'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmpg \n~\n hp\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmpg \n~\n hp\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n,\n type \n=\n \n'o'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmpg \n~\n hp\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n,\n type \n=\n \n'o'\n,\n pch \n=\n \n16\n,\n lty \n=\n \n'dashed'\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nmpg \n~\n hp\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\ndata\n(\nUSAge.df\n,\n package \n=\n \n'latticeExtra'\n)\n\n\nxyplot\n(\nPopulation \n~\n Age \n|\n \nfactor\n(\nYear\n),\n USAge.df\n,\n groups \n=\n Sex\n,\n type \n=\n \nc\n(\n'l'\n,\n \n'g'\n),\n auto.key \n=\n \nlist\n(\npoints \n=\n \nFALSE\n,\n lines \n=\n \nTRUE\n,\n columns \n=\n \n2\n),\n aspect \n=\n \n'xy'\n,\n ylab \n=\n \n'Population (millions)'\n,\n subset \n=\n Year \n%in%\n \nseq\n(\n1905\n,\n \n1975\n,\n by \n=\n \n10\n))\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nPopulation \n~\n Year \n|\n \nfactor\n(\nAge\n),\n USAge.df\n,\n groups \n=\n Sex\n,\n type \n=\n \n'l'\n,\n strip \n=\n \nFALSE\n,\n strip.left \n=\n \nTRUE\n,\n layout \n=\n \nc\n(\n1\n,\n \n3\n),\n ylab \n=\n \n'Population (millions)'\n,\n auto.key \n=\n \nlist\n(\nlines \n=\n \nTRUE\n,\n points \n=\n \nFALSE\n,\n columns \n=\n \n2\n),\n subset \n=\n Age \n%in%\n \nc\n(\n0\n,\n \n10\n,\n \n20\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\ndata\n(\nUSCancerRates\n,\n package \n=\n \n'latticeExtra'\n)\n\n\nxyplot\n(\nrate.male \n~\n rate.female \n|\n state\n,\n USCancerRates\n,\n aspect \n=\n \n'iso'\n,\n pch \n=\n \n'.'\n,\n cex \n=\n \n2\n,\n index.cond \n=\n \nfunction\n(\nx\n,\n y\n)\n \n{\n median\n(\ny \n-\n x\n,\n na.rm \n=\n \nTRUE\n)\n \n},\n scales \n=\n \nlist\n(\nlog \n=\n \n2\n,\n at \n=\n \nc\n(\n75\n,\n \n150\n,\n \n300\n,\n \n600\n)),\n panel \n=\n \nfunction\n(\n...\n)\n \n{\n \n  panel.grid\n(\nh \n=\n \n-1\n,\n v \n=\n \n-1\n)\n\n  panel.abline\n(\n0\n,\n \n1\n)\n\n  panel.xyplot\n(\n...\n)\n\n  \n},\n\n  xlab \n=\n \n'a'\n,\n\n  ylab \n=\n \n'b'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\ndata\n(\nbiocAccess\n,\n package \n=\n \n'latticeExtra'\n)\n\n\nbaxy \n<-\n xyplot\n(\nlog10\n(\ncounts\n)\n \n~\n hour \n|\n month \n+\n weekday\n,\n biocAccess\n,\n type \n=\n \nc\n(\n'p'\n,\n \n'a'\n),\n as.table \n=\n \nTRUE\n,\n pch \n=\n \n'.'\n,\n cex \n=\n \n2\n,\n col.line \n=\n \n'black'\n)\n\n\nbaxy\n\n\n\n\n\n\n\n\n1\n2\nlibrary\n(\nlatticeExtra\n)\n\nuseOuterStrips\n(\nbaxy\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nsunspot.year\n,\n aspect \n=\n \n'xy'\n,\n strip \n=\n \nFALSE\n,\n strip.left \n=\n \nTRUE\n,\n cut \n=\n \nlist\n(\nnumber \n=\n \n4\n,\n overlap \n=\n \n0.05\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\ndata\n(\nbiocAccess\n,\n package \n=\n \n'latticeExtra'\n)\n\n\nssd \n<-\n stl\n(\nts\n(\nbiocAccess\n$\ncounts\n[\n1\n:\n(\n24\n \n*\n \n30\n \n*\n2\n)],\n frequency \n=\n \n24\n),\n \n'periodic'\n)\n\n\nxyplot\n(\nssd\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'Time (Days)'\n)\n\n\n\n\n\n\n\n\n\nSplom\n\n\n1\nsplom\n(\nmtcars\n[\nc\n(\n1\n,\n \n3\n,\n \n6\n)],\n groups \n=\n cyl\n,\n data \n=\n mtcars\n,\n panel \n=\n panel.superpose\n,\n key \n=\n \nlist\n(\ntitle \n=\n \n'Three Cylinder Options'\n,\n columns \n=\n \n3\n,\n points \n=\n \nlist\n(\ntext \n=\n \nlist\n(\nc\n(\n'4 Cylinder'\n,\n \n'6 Cylinder'\n,\n \n'8 Cylinder'\n)))))\n\n\n\n\n\n\n\n\n\n1\n2\n3\ntrellis.par.set\n(\nsuperpose.symbol \n=\n \nlist\n(\npch \n=\n \nc\n(\n1\n,\n3\n,\n \n22\n),\n col \n=\n \n1\n,\n alpha \n=\n \n0.5\n))\n\n\nsplom\n(\n~\ndata.frame\n(\nmpg\n,\n disp\n,\n hp\n,\n drat\n,\n wt\n,\n qsec\n),\n data \n=\n mtcars\n,\n groups \n=\n cyl\n,\n pscales \n=\n \n0\n,\n varnames \n=\n \nc\n(\n'miles\\nper\\ngallon'\n,\n \n'displacement\\n(cu.in('\n,\n \n'horsepower'\n,\n \n'rear\\naxle\\nratio'\n,\n \n'weight'\n,\n \n'1/4\\nmile\\ntime'\n),\n auto.key \n=\n \nlist\n(\ncolumns \n=\n \n3\n,\n title \n=\n \n'Title'\n))\n\n\n\n\n\n\n\n\n\n1\ntrellis.par.set\n(\nold.pars\n)\n\n\n\n\n\n\n\n1\nsplom\n(\nUSArrests\n)\n\n\n\n\n\n\n\n\n\n1\nsplom\n(\n~\nUSArrests\n[\nc\n(\n3\n,\n1\n,\n2\n,\n4\n)]\n \n|\n state.region\n,\n pscales \n=\n \n0\n,\n type \n=\n \nc\n(\n'g'\n,\n \n'p'\n,\n \n'smooth'\n))\n\n\n\n\n\n\n\n\n\nParallel plot\n\n\nFor multivariate continuous data.\n\n\n1\nparallelplot\n(\n~\niris\n[\n1\n:\n4\n])\n\n\n\n\n\n\n\n\n\n1\nparallelplot\n(\n~\niris\n[\n1\n:\n4\n],\n horizontal.axis \n=\n \nFALSE\n)\n\n\n\n\n\n\n\n\n\n1\nparallelplot\n(\n~\niris\n[\n1\n:\n4\n],\n scales \n=\n \nlist\n(\nx \n=\n \nlist\n(\nrot \n=\n \n90\n)))\n\n\n\n\n\n\n\n\n\n1\nparallelplot\n(\n~\niris\n[\n1\n:\n4\n]\n \n|\n Species\n,\n iris\n)\n\n\n\n\n\n\n\n\n\n1\n2\nparallelplot\n(\n~\niris\n[\n1\n:\n4\n],\n iris\n,\n groups \n=\n Species\n,\n\n             horizontal.axis \n=\n \nFALSE\n,\n scales \n=\n \nlist\n(\nx \n=\n \nlist\n(\nrot \n=\n \n90\n)))\n\n\n\n\n\n\n\n\n\nTrivariate plots\n\n\nLike \nimage()\n, \ncontour()\n, \nfilled.contour()\n, \npersp()\n, \nsymbols()\n.\n\n\n\n\nlevelplot()\n.\n\n\ncontourplot()\n.\n\n\ncloud()\n.\n\n\nwireframe()\n.\n\n\n\n\nAdditional Packages\n\u00b6\n\n\nThe \nsm\n Package (density)\n\u00b6\n\n\n1\nlibrary\n(\nsm\n)\n\n\n\n\n\n\n\nDensity plot\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# create value labels\n\ncyl.f \n<-\n \nfactor\n(\ncyl\n,\n levels \n=\n \nc\n(\n4\n,\n \n6\n,\n \n8\n),\n labels \n=\n \nc\n(\n'4 cyl'\n,\n \n'6 cyl'\n,\n \n'8 cyl'\n))\n\n\n\n# plot densities\n\nsm.density.compare\n(\nmpg\n,\n cyl\n,\n xlab \n=\n \n'miles per gallon'\n)\n\n\ntitle\n(\nmain \n=\n \n'Title'\n)\n\n\n\n# add legend via mouse click\n\ncolfill \n<-\n \nc\n(\n2\n:\n(\n2\n \n+\n \nlength\n(\nlevels\n(\ncyl.f\n))))\n\nlegend\n(\n25\n,\n \n0.19\n,\n \nlevels\n(\ncyl.f\n),\n fill \n=\n colfill\n)\n \n\n\n\n\n\n\n\n\nThe \ncar\n Package (scatter)\n\u00b6\n\n\n1\nlibrary\n(\ncar\n)\n\n\n\n\n\n\n\nScatter plot\n\n\n1\nscatterplot\n(\nmpg \n~\n wt \n|\n cyl\n,\n data \n=\n mtcars\n,\n    xlab \n=\n \n'weight'\n,\n ylab \n=\n \n'miles per gallon'\n,\n labels \n=\n \nrow.names\n(\nmtcars\n))\n \n\n\n\n\n\n\n\n\nSplom\n\n\n1\nscatterplotMatrix\n(\n \n~\nmpg \n+\n disp \n+\n drat \n+\n wt \n|\n cyl\n,\n data \n=\n mtcars\n,\n main \n=\n \n'Title'\n)\n\n\n\n\n\n\n\n\n\nscatterplotMatrix == spm\n.\n\n\n1\nspm\n(\n \n~\nmpg \n+\n disp \n+\n drat \n+\n wt \n|\n cyl\n,\n data \n=\n mtcars\n,\n main \n=\n \n'Title'\n)\n\n\n\n\n\n\n\n\n\nThe \nvioplot\n Package (boxplot)\n\u00b6\n\n\n1\nlibrary\n(\nvioplot\n)\n\n\n\n\n\n\n\nViolin boxplot\n\n\n1\n2\n3\n4\n5\n6\n7\nx1 \n<-\n mpg\n[\nmtcars\n$\ncyl \n==\n \n4\n]\n\nx2 \n<-\n mpg\n[\nmtcars\n$\ncyl \n==\n \n6\n]\n\nx3 \n<-\n mpg\n[\nmtcars\n$\ncyl \n==\n \n8\n]\n\n\nvioplot\n(\nx1\n,\n x2\n,\n x3\n,\n names \n=\n \nc\n(\n'4 cyl'\n,\n \n'6 cyl'\n,\n \n'8 cyl'\n),\n col \n=\n \n'green'\n)\n\n\ntitle\n(\n'Title'\n)\n\n\n\n\n\n\n\n\n\nThe \nvcd\n Package (count, correlation, mosaic)\n\u00b6\n\n\n1\nlibrary\n(\nvcd\n)\n\n\n\n\n\n\n\nThe package provides a variety of methods for visualizing multivariate categorical data.\n\n\nCount\n\n\n1\n2\ncounts \n<-\n \ntable\n(\ngear\n,\n cyl\n)\n\ncounts\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##     cyl\n## gear  8  6  4\n##    3 12  2  1\n##    4  0  4  8\n##    5  2  1  2\n\n\n\n\n\n\n1\nmosaic\n(\ncounts\n,\n shade \n=\n \nTRUE\n,\n legend \n=\n \nTRUE\n)\n \n\n\n\n\n\n\n\n\nCorrelation\n\n\n1\n2\ncounts \n<-\n \ntable\n(\ngear\n,\n cyl\n)\n\ncounts\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##     cyl\n## gear  8  6  4\n##    3 12  2  1\n##    4  0  4  8\n##    5  2  1  2\n\n\n\n\n\n\n1\nassoc\n(\ncounts\n,\n shade \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n\n\nMosaic\n\n\n1\n2\n3\n4\n5\n6\nucb \n<-\n \ndata.frame\n(\nUCBAdmissions\n)\n\nucb \n<-\n \nwithin\n(\nucb\n,\n Accept \n<-\n \nfactor\n(\nAdmit\n,\n levels \n=\n \nc\n(\n'Rejected'\n,\n \n'Admitted'\n)))\n\n\n\nlibrary\n(\nvcd\n);\n \nlibrary\n(\ngrid\n)\n\n\ndoubledecker\n(\nxtabs\n(\nFreq\n~\n Dept \n+\n Gender \n+\n Accept\n,\n data \n=\n ucb\n),\n gp \n=\n gpar\n(\nfill \n=\n \nc\n(\n'grey90'\n,\n \n'steelblue'\n)))\n\n\n\n\n\n\n\n\n\n1\n2\n3\ndata\n(\nFertility\n,\n package \n=\n \n'AER'\n)\n\n\ndoubledecker\n(\nmorekids \n~\n age\n,\n data \n=\n Fertility\n,\n gp \n=\n gpar\n(\nfill \n=\n \nc\n(\n'grey90'\n,\n \n'green'\n)),\n spacing \n=\n spacing_equal\n(\n0\n))\n\n\n\n\n\n\n\n\n\n1\ndoubledecker\n(\nmorekids \n~\n gender1 \n+\n gender2\n,\n data \n=\n Fertility\n,\n gp \n=\n gpar\n(\nfill \n=\n \nc\n(\n'grey90'\n,\n \n'green'\n)))\n\n\n\n\n\n\n\n\n\n1\ndoubledecker\n(\nmorekids \n~\n age \n+\n gender1 \n+\n gender2\n,\n data \n=\n Fertility\n,\n gp \n=\n gpar\n(\nfill \n=\n \nc\n(\n'grey90'\n,\n \n'green'\n)),\n spacing \n=\n spacing_dimequal\n(\nc\n(\n0.1\n,\n \n0\n,\n \n0\n,\n \n0\n)))\n\n\n\n\n\n\n\n\n\nThe \nhexbin\n Package (scatter)\n\u00b6\n\n\n1\nlibrary\n(\nhexbin\n)\n\n\n\n\n\n\n\nScatter plot\n\n\n1\n2\n3\n4\n5\n# new data\n\ndata\n(\nNHANES\n)\n\n\n\n# compare\n\nplot\n(\nSerum.Iron \n~\n Transferin\n,\n NHANES\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'Transferin'\n,\n ylab \n=\n \n'Iron'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# with\n\nhexbinplot\n(\nSerum.Iron \n~\n Transferin\n,\n NHANES\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'Transferin'\n,\n ylab \n=\n \n'Iron'\n)\n\n\n\n\n\n\n\n\n\n1\nhexbinplot\n(\nmpg \n~\n hp\n,\n main \n=\n \n'Title'\n,\n xlab \n=\n \n'horsepowers'\n,\n ylab \n=\n \n'miles per gallon'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\nx \n<-\n rnorm\n(\n1000\n)\n\ny \n<-\n rnorm\n(\n1000\n)\n\n\nbin \n<-\n hexbin\n(\nx\n,\n y\n,\n xbins \n=\n \n50\n)\n\nplot\n(\nbin\n,\n main \n=\n \n'Title'\n)\n \n\n\n\n\n\n\n\n\n1\n2\n3\n4\nx \n<-\n rnorm\n(\n1000\n)\n\ny \n<-\n rnorm\n(\n1000\n)\n\n\nplot\n(\nx\n,\n y\n,\n main \n=\n \n'Title'\n,\n col \n=\n  rgb\n(\n0\n,\n \n100\n,\n \n0\n,\n \n50\n,\n maxColorValue \n=\n \n255\n),\n pch \n=\n \n16\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\ndata\n(\nDiamonds\n,\n package \n=\n \n'Stat2Data'\n)\n\n\na \n=\n hexbin\n(\nDiamonds\n$\nPricePerCt\n,\n Diamonds\n$\nCarat\n,\n xbins \n=\n \n40\n)\n\n\n\nlibrary\n(\nRColorBrewer\n)\n\n\nplot\n(\na\n)\n\n\n\n\n\n\n\n\n\nColors.\n\n\n1\n2\n3\nrf \n<-\n colorRampPalette\n(\nrev\n(\nbrewer.pal\n(\n12\n,\n \n'Set3'\n)))\n\n\nhexbinplot\n(\nDiamonds\n$\nPricePerCt \n~\n Diamonds\n$\nCarat\n,\n colramp \n=\n rf\n)\n\n\n\n\n\n\n\n\n\nMix \nlattice\n and \nhexbin\n\n\n1\n2\n3\ndata\n(\ngvhd10\n,\n package \n=\n \n'latticeExtra'\n)\n\n\nxyplot\n(\nasinh\n(\nSSC.H\n)\n \n~\n \nasinh\n(\nFL2.H\n),\n gvhd10\n,\n aspect \n=\n \n1\n,\n panel \n=\n panel.hexbinplot\n,\n \n.\naspect.ratio \n=\n \n1\n,\n trans \n=\n \nsqrt\n)\n\n\n\n\n\n\n\n\n\n1\nxyplot\n(\nasinh\n(\nSSC.H\n)\n \n~\n \nasinh\n(\nFL2.H\n)\n \n|\n Days\n,\n gvhd10\n,\n aspect \n=\n \n1\n,\n panel \n=\n panel.hexbinplot\n,\n \n.\naspect.ratio \n=\n \n1\n,\n trans \n=\nsqrt\n)\n\n\n\n\n\n\n\n\n\nThe \ncar\n Package (scatter)\n\u00b6\n\n\n1\nlibrary\n(\ncar\n)\n\n\n\n\n\n\n\nScatter plot\n\n\n1\n2\nscatterplotMatrix\n(\n~\nmpg \n+\n disp \n+\n drat \n+\n wt \n|\n cyl\n,\n data \n=\n mtcars\n,\n\n   main \n=\n \n'Three Cylinder Options'\n)\n\n\n\n\n\n\n\n\n\nThe \nscatterplot3d\n Package\n\u00b6\n\n\n1\nlibrary\n(\nscatterplot3d\n)\n\n\n\n\n\n\n\nScatter plot\n\n\n1\nscatterplot3d\n(\nwt\n,\n disp\n,\n mpg\n,\n main \n=\n \n'Title'\n)\n\n\n\n\n\n\n\n\n\n1\nscatterplot3d\n(\nwt\n,\n disp\n,\n mpg\n,\n pch \n=\n \n16\n,\n highlight.3d \n=\n \nTRUE\n,\n type \n=\n \n'h'\n,\n main \n=\n \n'Title'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\ns3d \n<-\n scatterplot3d\n(\nwt\n,\n disp\n,\n mpg\n,\n pch \n=\n \n16\n,\n highlight.3d \n=\n \nTRUE\n,\n type \n=\n \n'h'\n,\n main \n=\n \n'   Title'\n)\n\n\nfit \n<-\n lm\n(\nmpg \n~\n wt \n+\n disp\n)\n\n\ns3d\n$\nplane3d\n(\nfit\n)\n\n\n\n\n\n\n\n\n\nThe \nrgl\n Package (interactive)\n\u00b6\n\n\n1\nlibrary\n(\nrgl\n)\n\n\n\n\n\n\n\nInteractive plot\n\n\nThe plot will open a new window.\n\n\n1\nplot3d\n(\nwt\n,\n disp\n,\n mpg\n,\n col \n=\n \n'red'\n,\n size \n=\n \n3\n)\n\n\n\n\n\n\n\nThe \ncluster\n Package (dendrogram)\n\u00b6\n\n\n1\nlibrary\n(\ncluster\n)\n\n\n\n\n\n\n\nDendrogram\n\n\nUse the \niris\n dataset.\n\n\n1\n2\n3\nsubset \n<-\n \nsample\n(\n1\n:\n150\n,\n \n20\n)\n\ncS \n<-\n \nas.character\n(\nSp \n<-\n iris\n$\nSpecies\n[\nsubset\n])\n\ncS\n\n\n\n\n\n\n1\n2\n3\n4\n##  [1] \"setosa\"     \"versicolor\" \"setosa\"     \"virginica\"  \"virginica\" \n##  [6] \"setosa\"     \"setosa\"     \"setosa\"     \"virginica\"  \"setosa\"    \n## [11] \"versicolor\" \"versicolor\" \"virginica\"  \"setosa\"     \"versicolor\"\n## [16] \"versicolor\" \"setosa\"     \"virginica\"  \"versicolor\" \"versicolor\"\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\ncS\n[\nSp \n==\n \n'setosa'\n]\n \n<-\n \n'S'\n\ncS\n[\nSp \n==\n \n'versicolor'\n]\n \n<-\n \n'V'\n\ncS\n[\nSp \n==\n \n'virginica'\n]\n \n<-\n \n'g'\n\n\nai \n<-\n agnes\n(\niris\n[\nsubset\n,\n \n1\n:\n4\n])\n\n\nplot\n(\nai\n,\n label \n=\n cS\n)\n\n\n\n\n\n\n\n\n\nThe \nextracat\n Package (splom)\n\u00b6\n\n\n1\nlibrary\n(\nextracat\n)\n\n\n\n\n\n\n\nSplom\n\n\nFor missing values. Binary matrix with reordering and filtering of rows\n\nand columns. The x-axis shows the frequency of NA. The y-axis shows the\n\nmarginal distribution of NA.\n\n\n1\n2\n3\n4\n# example 1\n\ndata\n(\nCHAIN\n,\n package \n=\n \n'mi'\n)\n\n\nvisna\n(\nCHAIN\n,\n sort \n=\n \n'b'\n)\n\n\n\n\n\n\n\n\n\n1\nsummary\n(\nCHAIN\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n##    log_virus           age            income          healthy     \n##  Min.   : 0.000   Min.   :21.00   Min.   : 1.000   Min.   :16.67  \n##  1st Qu.: 0.000   1st Qu.:37.00   1st Qu.: 2.000   1st Qu.:35.00  \n##  Median : 0.000   Median :43.00   Median : 3.000   Median :45.37  \n##  Mean   : 4.324   Mean   :42.56   Mean   : 3.377   Mean   :44.40  \n##  3rd Qu.: 9.105   3rd Qu.:48.00   3rd Qu.: 5.000   3rd Qu.:54.89  \n##  Max.   :13.442   Max.   :70.00   Max.   :10.000   Max.   :70.11  \n##  NA's   :179      NA's   :24      NA's   :38       NA's   :24     \n##      mental           damage        treatment     \n##  Min.   :0.0000   Min.   :1.000   Min.   :0.0000  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:0.0000  \n##  Median :0.0000   Median :4.000   Median :1.0000  \n##  Mean   :0.2717   Mean   :3.578   Mean   :0.8602  \n##  3rd Qu.:1.0000   3rd Qu.:5.000   3rd Qu.:2.0000  \n##  Max.   :1.0000   Max.   :5.000   Max.   :2.0000  \n##  NA's   :24       NA's   :63      NA's   :24\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# example 2\n\ndata\n(\noly12\n,\n package \n=\n \n'VGAMdata'\n)\n\n\noly12d \n<-\n oly12\n[,\n \nnames\n(\noly12\n)\n \n!=\n \n'DOB'\n]\n\noly12a \n<-\n oly12\n\n\nnames\n(\noly12a\n)\n \n<-\n \nabbreviate\n(\nnames\n(\noly12\n),\n \n3\n)\n\n\nvisna\n(\noly12a\n,\n sort \n=\n \n'b'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# example 3\n\ndata\n(\nfreetrade\n,\n package \n=\n \n'Amelia'\n)\n\n\nfreetrade \n<-\n \nwithin\n(\nfreetrade\n,\n land1 \n<-\n reorder\n(\ncountry\n,\n tariff\n,\n \nfunction\n(\nx\n)\n \nsum\n(\nis.na\n(\nx\n))))\n\n\nfluctile\n(\nxtabs\n(\nis.na\n(\ntariff\n)\n \n~\n land1 \n+\n year\n,\n data \n=\n freetrade\n))\n\n\n\n\n\n\n\n\n\n1\n## viewport[base]\n\n\n\n\n\n\n1\n2\n3\n4\n# example 4\n\ndata\n(\nPima.tr2\n,\n package \n=\n \n'MASS'\n)\n\n\nvisna\n(\nPima.tr2\n,\n sort \n=\n \n'b'\n)\n\n\n\n\n\n\n\n\n\nThe \nash\n Package (density)\n\u00b6\n\n\n1\nlibrary\n(\nash\n)\n\n\n\n\n\n\n\nDensity plot\n\n\n1\nplot\n(\nash1\n(\nbin1\n(\nmtcars\n$\nmpg\n,\n nbin \n=\n \n50\n)),\n type \n=\n \n'l'\n)\n\n\n\n\n\n\n\n1\n## [1] \"ash estimate nonzero outside interval ab\"\n\n\n\n\n\n\n\n\nThe \nKernSmooth\n Package (density)\n\u00b6\n\n\n1\nlibrary\n(\nKernSmooth\n)\n\n\n\n\n\n\n\nDensity plot\n\n\n1\n2\n3\n4\n5\nwith\n(\nmtcars\n,\n \n{\n\n  hist\n(\nmpg\n,\n freq \n=\n \nFALSE\n,\n main \n=\n \n''\n,\n col \n=\n \n'bisque2'\n,\n ylab \n=\n \n''\n)\n\n  lines\n(\ndensity\n(\nmpg\n),\n lwd \n=\n \n2\n)\n\n  ks1 \n<-\n bkde\n(\nmpg\n,\n bandwidth \n=\n dpik\n(\nmpg\n))\n\n  lines\n(\nks1\n,\n col \n=\n \n'red'\n,\n lty \n=\n \n5\n,\n lwd \n=\n \n2\n)})\n\n\n\n\n\n\n\n\n\nThe \ncorrplot\n Package (correlation)\n\u00b6\n\n\n1\nlibrary\n(\ncorrplot\n)\n\n\n\n\n\n\n\nSplom\n\n\n1\n2\n3\n4\n# Create a correlation matrix for the dataset (9-14 are the '2' variables only)\n\ncorrelations \n<-\n cor\n(\nmtcars\n)\n\n\ncorrplot\n(\ncorrelations\n)",
            "title": "Plot Snippets for Exploratory (and some Explanatory) Analyses"
        },
        {
            "location": "/Plot_snippets_-_Basics/#data-type-dataset",
            "text": "",
            "title": "Data Type &amp; Dataset"
        },
        {
            "location": "/Plot_snippets_-_Basics/#data-types",
            "text": "continuous vs categorical (or discrete).  continuous: float, x-y-z, 3D, map coordinates, trianguar, lat-long, polar, degree-distance, angle-vector.  categorical: integer, binary, dichotomic, dummy, factor, ordinal (ordered).   Continuous variable characteristics:   asymmetry.  outliers.  multimodality.  gaps, missing values.  heaping, redundance.  rounding, integer.  impossibilities, anomalies.  errors.  \u2026   Categorical variable characteristics:   unexpected pattern of results.  uneven distribution.  extra categories.  unbalanced experiments.  large numbers of categories.  NA, errors, missings\u2026  nominal: no fixed order.  ordinal: fixed order (scale of 1 to 5).  discrete: counts, integers.  dependencies, correlation, associations.  causal relationships, outliers, groups, clusters, gaps, barriers, conditional relationship.  \u2026   Univariate main plots:   histogram.  density.  qqmath chart.  box & whickers chart.  bar chart.  dot.   Bivariate main plots:   xy chart.  qq chart.   Trivariate main plots:   cloud.  wireframe.  countour.  level.   Multivariate main plots:   sploms.  parallel charts (coordinate).   Specialized plots:   frequencies, crosstabs: bar charts, mosaic plots, association plots.  correlations: sploms, pairs, correlograms.  t-tests, non-parrametric tests of group differences: box plot, density plot.  regression: scatter plot.  ANOVA: box plots, line plots.",
            "title": "Data Types"
        },
        {
            "location": "/Plot_snippets_-_Basics/#functions",
            "text": "Create a new variable  1\n2 iris2  <-   within ( iris ,  area  <-  Petal.Width * Petal.Length )  head ( iris2 ,   3 )    1\n2\n3\n4 ##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species area\n## 1          5.1         3.5          1.4         0.2  setosa 0.28\n## 2          4.9         3.0          1.4         0.2  setosa 0.28\n## 3          4.7         3.2          1.3         0.2  setosa 0.26   1\n2 area  <-   with ( iris ,  area  <-  Petal.Width * Petal.Length )  head ( area ,   3 )    1 ## [1] 0.28 0.28 0.26",
            "title": "Functions"
        },
        {
            "location": "/Plot_snippets_-_Basics/#dataset",
            "text": "For most examples, we use the  mtcars  dataset.  Prepare the dataset.  1 attach ( mtcars )    Get data attached to a package (an example).  1 data ( gvhd10 ,  package  =   'latticeExtra' )",
            "title": "Dataset"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-basic-package",
            "text": "",
            "title": "The Basic Package"
        },
        {
            "location": "/Plot_snippets_-_Basics/#basic-plots-options-parameters",
            "text": "Standardize the parameters (an example)  1\n2 # color and tick mark text orientation \npar ( col  =   'black' ,  las  =   1 )    Grid and layout  One plot.  1 plot ( hp ,  mpg ,  xlab  =   'horsepower' ,  ylab  =   'miles per gallon' )     A grid of plots.  1\n2\n3\n4\n5\n6\n7\n8\n9 par ( mfrow  =   c ( 2 ,   1 )) \n\nplot ( mpg ,  hp ,  ylab  =   'horsepower' ,  xlab  =   'miles per gallon' ) \nboxplot ( mpg  ~  cyl ,  xlab  =   'mile per gallon' ,  ylab  =   'number of cylinders' ,  horizontal  =   TRUE ) \n\npar ( mfrow  =   c ( 1 ,   2 )) \n\nplot ( mpg ,  hp ,  ylab  =   'horsepower' ,  xlab  =   'miles per gallon' ) \nboxplot ( mpg  ~  cyl ,  xlab  =   'mile per gallon' ,  ylab  =   'number of cylinders' ,  horizontal  =   TRUE )     1 par ( mfrow  =   c ( 1 ,   1 ))    Other grids.  1\n2\n3\n4\n5 layout ( matrix ( c ( 1 , 1 , 2 , 3 ),   2 ,   2 ,  byrow  =   TRUE )) \n\nplot ( mpg ,  xlab  =   'observations' ,  ylab  =   'miles per gallon' ) \nplot ( hp ,  mpg ,  xlab  =   'horsepower' ,  ylab  =   'miles per gallon' ) \nboxplot ( mpg  ~  cyl ,  ylab  =   'mile per gallon' ,  xlab  =   'number of cylinders' )     1\n2 # view  matrix ( c ( 1 , 2 , 1 , 3 ),   2 ,   2 ,  byrow  =   TRUE )    1\n2\n3 ##      [,1] [,2]\n## [1,]    1    2\n## [2,]    1    3   1\n2\n3\n4\n5 layout ( matrix ( c ( 1 , 2 , 1 , 3 ),   2 ,   2 ,  byrow  =   TRUE )) \n\nhist ( wt ) \nhist ( mpg ) \nhist ( disp )     1\n2\n3\n4\n5 layout ( matrix ( c ( 1 , 1 , 2 , 3 ),   2 ,   2 ,  byrow  =   TRUE ),   widths  =   c ( 3 , 1 ),  heights  =   c ( 1 , 2 )) \n\nhist ( wt ) \nhist ( mpg ) \nhist ( disp )     1\n2 nf  <-  layout ( matrix ( c ( 1 , 1 , 2 , 3 ),   2 ,   2 ,  byrow  =   TRUE ),  widths  =  lcm ( 12 ),  heights  =  lcm ( 6 )) \nlayout.show ( nf )     1\n2\n3 plot ( mpg ,  xlab  =   'observations' ,  ylab  =   'miles per gallon' ) \nplot ( hp ,  mpg ,  xlab  =   'horsepower' ,  ylab  =   'miles per gallon' ) \nboxplot ( mpg  ~  cyl ,  ylab  =   'mile per gallon' ,  xlab  =   'number of cylinders' )     Gridview with additional packages.  1 library ( vcd )    1 mplot ( A ,  B ,  C )     See the  lattice  and  latticeExtra  packages for built-in facet/gridview.  ggplot2  as well.  Plot and add ablines  1\n2\n3\n4\n5\n6\n7 plot ( hp ,  mpg ,  xlab  =   'horsepower' ,  ylab  =   'miles per gallon' )  # abline(h = yvalues, v = xvalues) \nabline ( lm ( mpg  ~  hp ))  # main = 'Title' or... \ntitle ( 'Title' )     1\n2\n3\n4\n5 plot ( hp ,  mpg ,  xlab  =   'horsepower' ,  ylab  =   'miles per gallon' ) \n\nabline ( h  =   c ( 20 ,   25 )) \nabline ( v  =   c ( 50 ,   150 )) \nabline ( v  =   seq ( 200 ,   300 ,   50 ),  lty  =   2 ,  col  =   'blue' )     Add a legend  1\n2\n3\n4 boxplot ( mpg  ~  cyl ,  main  =   'Title' , \n   yaxt  =   'n' ,  xlab  =   'mile per gallon' ,  horizontal  =   TRUE ,  col  =  terrain.colors ( 3 )) \n\nlegend ( 'topright' ,  inset  =   0.05 ,  title  =   'number of cylinders' ,   c ( '4' , '6' , '8' ),  fill  =  terrain.colors ( 3 ),  horiz  =   TRUE )     Save  1\n2\n3\n4\n5\n6\n7 mygraph  <-  plot ( hp ,  mpg ,  main  =   'Title' ,  xlab  =   'horsepower' ,  ylab  =   'miles per gallon' ) \n\npdf ( 'mygraph.pdf' ) \npng ( 'mygraph.png' ) \njpeg ( 'mygraph.jpg' ) \nbmp ( 'mygraph.bmp' ) \npostscript ( 'mygraph.ps' )    View in a new window  Typing the function will open a new window to render the plot.   windows()  for Windows.  X11()  for Linux.  quartz()  for OS X.    1\n2\n3\n4 # open the new windows \nwindows () \n\nplot ( hp ,  mpg ,  main  =   'Title' ,  xlab  =   'horsepower' ,  ylab  =   'miles per gallon' )    Enrich the plot, add text   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 plot ( hp ,  mpg , \n     main  =   'Title' ,  col.main  =   'blue' , \n     sub  =   'figure 1' ,  col.sub  =   'blue' , \n     xlab  =   'horsepower' ,  \n     ylab  =   'miles per gallon' , \n     col.lab  =   'red' ,  cex.lab  =   0.9 , \n     xlim  =   c ( 50 ,   350 ), \n     ylim  =   c ( 0 ,   40 )) \n\ntext ( 100 ,   10 ,   'text 1' )   # x and y coordinate \nmtext ( 'text 2' ,   4 ,  line  =   0.5 )   # pos = 1 (bottom), 2 (left), 3 (top), 4 (right); line (margin)     With  locator() , use the mouse; with 1 for 1 click, 2 for\u2026 Find the coordinates to be entered in the code. For example (after two clicks):  1\n2\n3\n4\n5\n6 > locator(2)\n$x\n[1] 212.5308 293.7854\n\n$y\n[1] 33.34040 31.87281   1\n2\n3\n4\n5\n6 plot ( hp ,  mpg , \n     main  =   'Title' , \n     xlab  =   'horsepower' ,  \n     ylab  =   'miles per gallon' ) \n\ntext ( hp ,  mpg ,   row.names ( mtcars ),  cex  =   0.7 ,  pos  =   4 ,  col  =   'red' )     Enrich the plot, add symbols  1\n2\n3\n4\n5\n6\n7 plot ( hp ,  mpg , \n     main  =   'Title' , \n     xlab  =   'horsepower' ,  \n     ylab  =   'miles per gallon' ) \n\nsymbols ( 250 ,   20 ,  squares  =   1 ,  add  =   TRUE ,  inches  =   0.1 ,  fg  =   'red' ) \nsymbols ( 250 ,   25 ,  circles  =   1 ,  add  =   TRUE ,  inches  =   0.1 ,  fg  =   'red' )     1\n2\n3\n4 #rectangles  #stars  #thermometers  #boxplots    Combine plots; change  pch =  &  col =   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 par ( mfrow  =   c ( 2 , 2 ))  # 1 \nplot ( hp ,  mpg , \n     main  =   'P1' , \n     xlab  =   'horsepower' ,  \n     ylab  =   'miles per gallon' , \n     pch  =   1 , \n     col  =   'black' )  # 2 \nplot ( hp ,  mpg , \n     main  =   'P2' , \n     xlab  =   'horsepower' ,  \n     ylab  =   'miles per gallon' , \n     pch  =   3 , \n     col  =   'blue' , \n     cex  =   0.5 )  # 3 \nplot ( hp ,  mpg , \n     main  =   'P3' , \n     xlab  =   'horsepower' ,  \n     ylab  =   'miles per gallon' , \n     pch  =   5 , \n     col  =   'red' , \n     cex  =   2 )  # 4 \nplot ( hp ,  mpg , \n     main  =   'P4' , \n     xlab  =   'horsepower' ,  \n     ylab  =   'miles per gallon' , \n     pch  =   7 , \n     col  =   'green' )     1\n2 # reverse \npar ( mfrow  =   c ( 1 , 1 ))    Change  col =   Change  pch =    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 par ( fig  =   c ( 0 , 0.8 , 0 , 0.8 )) \n\nplot ( mtcars $ wt ,  mtcars $ mpg ,  xlab  =   'Car Weight' ,    ylab  =   'miles Per Gallon' ) \n\npar ( fig  =   c ( 0 , 0.8 , 0.55 , 1 ),  new  =   TRUE ) \n\nboxplot ( mtcars $ wt ,  horizontal  =   TRUE ,  axes  =   FALSE ) \n\npar ( fig  =   c ( 0.65 , 1 , 0 , 0.8 ),  new  =   TRUE ) \n\nboxplot ( mtcars $ mpg ,  axes  =   FALSE ) \n\nmtext ( 'Enhanced Scatterplot' ,  side  =   3 ,  outer  =   TRUE ,  line  =   -3 )     1\n2 # reverse \npar ( mfrow  =   c ( 1 , 1 ))    Change  type = ; without dots   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 x  <-   c ( 1 : 5 );  y  <-  x\n\npar ( pch  =   22 ,  col  =   'red' )   # plotting symbol and color \n\npar ( mfrow  =   c ( 2 , 4 ))   # all plots on one page \nopts  =   c ( 'p' , 'l' , 'o' , 'b' , 'c' , 's' , 'S' , 'h' )  for   ( i  in   1 : length ( opts ))   { \n  heading  =   paste ( 'type =' , opts [ i ]) \n  plot ( x ,  y ,  type  =   'n' ,  main  =  heading ) \n  lines ( x ,  y ,  type  =  opts [ i ])  }     1\n2 # reverse \npar ( mfrow  =   c ( 1 , 1 ),  col  =   'black' )    Change  type = ; with dots   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 x  <-   c ( 1 : 5 );  y  <-  x\n\npar ( pch  =   22 ,  col  =   'blue' )   # plotting symbol and color \n\npar ( mfrow  =   c ( 2 , 4 ))   # all plots on one page \nopts  =   c ( 'p' , 'l' , 'o' , 'b' , 'c' , 's' , 'S' , 'h' )  for   ( i  in   1 : length ( opts ))   { \n  heading  =   paste ( 'type =' , opts [ i ]) \n  plot ( x ,  y ,  main  =  heading ) \n  lines ( x ,  y ,  type  =  opts [ i ])  }     1\n2 # reverse \npar ( mfrow  =   c ( 1 , 1 ),  col  =   'black' )    Add or modify the axes   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 plot ( hp ,  mpg , \n     main  =   'Title' , \n     xlab  =   'horsepower' ,  \n     ylab  =   'miles per gallon' , \n     xaxt  =   'n' , \n     yaxt  =   'n' ) \n\naxis ( 1 ,  at  =   c ( 100 ,   200 ,   300 ),  labels  =   NULL ,  pos  =   15 ,  lty  =   'dashed' ,  col  =   'green' ,  las  =   2 ,  tck  =   -0.05 ) \n\naxis ( 4 ,  at  =   c ( 20 ,   30 ),  labels  =   c ( 'bt' ,   'up' ),  pos  =   125 ,  lty  =   'dashed' ,  col  =   'blue' ,  las  =   2 ,  tck  =   -0.05 )     1\n2 # reverse \npar ( las  =   1 )    Add layers to the first plot  1\n2\n3\n4\n5\n6\n7 plot ( mpg , \n     main  =   'Title' , \n     xlab  =   'horsepower' ,  \n     ylab  =   'miles per gallon' )  # add lines \nlines ( mpg [ 1 : 10 ],  type  =   'l' ,  col  =   'green' )",
            "title": "Basic Plots, Options &amp; Parameters"
        },
        {
            "location": "/Plot_snippets_-_Basics/#univariate-plots",
            "text": "Plot; continuous  1 plot ( mpg ,  main  =   'Title' ,  xlab  =   'observations' ,  ylab  =   'miles per gallon' )     Plot; categorical  1 plot ( cyl ,  main  =   'Title' ,  xlab  =   'observations' ,  ylab  =   'cylinders' )     QQnorm; continuous  1 qqnorm ( mpg ,  main  =   'Title' ,  xlab  =   'observations' ,  ylab  =   'cylinders' )     QQnorm; categorical  1 qqnorm ( cyl ,  main  =   'Title' ,  xlab  =   'observations' ,  ylab  =   'cylinders' )     Stripchart; continuous  1 stripchart ( mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon' )     Stripchart; categorical  1 stripchart ( cyl ,  main  =   'Title' ,  xlab  =   'cylinders' )     Barplot (vertical); continuous  1 barplot ( mpg [ 1 : 10 ],  main  =   'Title' ,  xlab  =   'observations' ,  ylab  =   'miles per gallon' )     Barplot (horizontal); categorical  1 barplot ( cyl [ 1 : 10 ],  main  =   'Title' ,  horiz  =   TRUE ,  xlab  =   'cylinders' ,  ylab  =   'observations' )     Barplots options  Group with  table() .  1\n2 counts  <-   table ( cyl ) \ncounts   1\n2\n3 ## cyl\n##  4  6  8 \n## 11  7 14   1 barplot ( counts ,  main  =   'Title' ,  horiz  =   TRUE ,  xlab  =   'count' ,  names.arg  =   c ( '4 Cyl' ,   '6 Cyl' ,   '8 Cyl' ))     1\n2 counts  <-   table ( vs ,  gear ) \ncounts   1\n2\n3\n4 ##    gear\n## vs   3  4  5\n##   0 12  2  4\n##   1  3 10  1   1 barplot ( counts ,  main  =   'Title' ,  xlab  =   'gearbox' ,  col  =   c ( 'darkblue' ,   'red' ),  legend  =   rownames ( counts ))      1\n2 counts  <-   table ( vs ,  gear ) \ncounts   1\n2\n3\n4 ##    gear\n## vs   3  4  5\n##   0 12  2  4\n##   1  3 10  1   1 barplot ( counts ,  main  =   'Title' ,  xlab = 'gearbox' ,  col  =   c ( 'darkblue' ,   'red' ),  legend  =    rownames ( counts ),  beside  =   TRUE )     Group with  aggregate() .  1 aggregate ( mtcars ,  by  =   list ( cyl ,  vs ),  FUN  =   mean ,  na.rm  =   TRUE )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ##   Group.1 Group.2      mpg cyl   disp       hp     drat       wt     qsec\n## 1       4       0 26.00000   4 120.30  91.0000 4.430000 2.140000 16.70000\n## 2       6       0 20.56667   6 155.00 131.6667 3.806667 2.755000 16.32667\n## 3       8       0 15.10000   8 353.10 209.2143 3.229286 3.999214 16.77214\n## 4       4       1 26.73000   4 103.62  81.8000 4.035000 2.300300 19.38100\n## 5       6       1 19.12500   6 204.55 115.2500 3.420000 3.388750 19.21500\n##   vs        am     gear     carb\n## 1  0 1.0000000 5.000000 2.000000\n## 2  0 1.0000000 4.333333 4.666667\n## 3  0 0.1428571 3.285714 3.500000\n## 4  1 0.7000000 4.000000 1.500000\n## 5  1 0.0000000 3.500000 2.500000   1\n2\n3\n4\n5\n6 par ( las  =   2 )   # make label text perpendicular to axis \n\npar ( mar  =   c ( 5 ,   8 ,   4 ,   2 ))   # increase y-axis margin. \n\ncounts  <-   table ( mtcars $ gear ) \nbarplot ( counts ,  main  =   'Car Distribution' ,  horiz  =   TRUE ,  names.arg  =   c ( '3 Gears' ,   '4 Gears' ,   '5   Gears' ),  cex.names  =   0.8 )     1\n2 # reverse \npar ( las  =   1 )    Colors.  1\n2\n3\n4\n5\n6 library ( RColorBrewer ) \n\npar ( mfrow  =   c ( 2 ,   1 )) \n\nbarplot ( iris $ Petal.Length ) \nbarplot ( table ( iris $ Species ,  iris $ Sepal.Length ),  col  =  brewer.pal ( 3 ,   'Set1' ))     1 par ( mfrow  =   c ( 1 ,   1 ))    Pie Chart  Avoid!  Dotchart; continuous  1 dotchart ( mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'observations' )     Dotchart; categorical  1 dotchart ( cyl ,  main  =   'Title' ,  xlab  =   'cylinders' ,  ylab  =   'observations' )     Dotchart options  1 dotchart ( mpg , labels  =   row.names ( mtcars ),  cex  =   0.7 ,  main  =   'Title' ,  xlab  =   'miles per gallon' )      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # sort by mpg \nx  <-  mtcars [ order ( mpg ),]  # must be factors \nx $ cyl  <-   factor ( x $ cyl ) \nx $ color [ x $ cyl  ==   4 ]   <-   'red' \nx $ color [ x $ cyl  ==   6 ]   <-   'blue' \nx $ color [ x $ cyl  ==   8 ]   <-   'darkgreen' \n\ndotchart ( x $ mpg ,  labels  =   row.names ( x ),  cex  =   0.7 ,  groups  =  x $ cyl ,  main  =   'Title' ,   xlab  =   'miles per gallon' ,  gcolor  =   'black' ,  color  =  x $ color )     More with the  hmisc  package and  panel.dotplot()  and in the  lattice \npackage section.  Boxplot; continuous  1 boxplot ( mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'observations' )     Stem; continuous  1 stem ( mpg )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 ## \n##   The decimal point is at the |\n## \n##   10 | 44\n##   12 | 3\n##   14 | 3702258\n##   16 | 438\n##   18 | 17227\n##   20 | 00445\n##   22 | 88\n##   24 | 4\n##   26 | 03\n##   28 | \n##   30 | 44\n##   32 | 49   Histogram; continuous  1 hist ( mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon - bins' ,  ylab  =   'count' )     Histogram; categorical  1 hist ( cyl ,  main  =   'Title' ,  xlab  =   'cylinders - bins' ,  ylab  =   'count' )     Histogram options  1 hist ( mpg ,  breaks  =   12 ,  col  =   'red' )     1\n2\n3\n4\n5\n6\n7\n8\n9 x  <-  mpg\n\nh  <-  hist ( x ,  breaks  =   10 ,  main  =   'Title' ,  xlab  =   'miles per gallon' ) \n\nxfit  <-   seq ( min ( x ),   max ( x ), length  =   40 ) \nyfit  <-  dnorm ( xfit ,  mean  =   mean ( x ),  sd  =  sd ( x )) \nyfit  <-  yfit * diff ( h $ mids [ 1 : 2 ]) * length ( x ) \n\nlines ( xfit ,  yfit ,  col  =   'blue' ,  lwd  =   2 )     Colors.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 library ( RColorBrewer ) \n\npar ( mfrow  =   c ( 2 ,   3 )) \n\nhist ( VADeaths ,  breaks  =   10 ,  col  =  brewer.pal ( 3 ,   'Set3' ),  main  =   '3, Set3' ) \nhist ( VADeaths ,  breaks  =   4 ,  col  =  brewer.pal ( 3 ,   'Set2' ),  main  =   '3, Set2' ) \nhist ( VADeaths ,  breaks  =   8 ,  col  =  brewer.pal ( 3 ,   'Set1' ),  main  =   '3, Set1' ) \nhist ( VADeaths ,  breaks  =   2 ,  col  =  brewer.pal ( 8 ,   'Set3' ),  main  =   '8, Set3' ) \nhist ( VADeaths ,  breaks  =   10 ,  col  =  brewer.pal ( 8 ,   'Greys' ),  main  =   '8, Greys' ) \nhist ( VADeaths ,  breaks  =   10 ,  col  =  brewer.pal ( 8 ,   'Greens' ),  main  =   '8, Greens' )     1 par ( mfrow  =   c ( 1 ,   1 ))    Density Plot; continuous  1 plot ( density ( mpg ),  main  =   'Title' )     1\n2\n3 plot ( density ( mpg ),  main  =   'Title' ) \n\npolygon ( density ( mpg ),  col  =   'red' ,  border  =   'blue' )      1\n2\n3\n4\n5\n6 d1  <-  density ( mtcars $ mpg ) \nplot ( d1 ) \nrug ( mtcars $ mpg ) \n\nlines ( density ( mtcars $ mpg ,  d1 $ bw / 2 ),  col  =   'green' ) \nlines ( density ( mtcars $ mpg ,  d1 $ bw / 5 ),  col  =   'blue' )",
            "title": "Univariate Plots"
        },
        {
            "location": "/Plot_snippets_-_Basics/#bivariate-multivariate-plots",
            "text": "Plot, continuous/continuous  1 plot ( mpg ,  hp ,  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'horsepowers' )     Plot, continuous/categorical  1 plot ( mpg ,  cyl ,  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'cylinders' )     Plot options  1\n2\n3\n4 plot ( wt ,  mpg ,  main  =   'Title' ,  xlab  =   'weight' ,  ylab  =   'miles per gallon ' ) \n\nabline ( lm ( mpg  ~  wt ),  col  =   'red' )   # regression \nlines ( lowess ( wt ,  mpg ),  col  =   'blue' )   # lowess line     SmoothScatter; continuous/continuous  1 smoothScatter ( mpg ,  hp ,  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'horsepowers' )     Sunflowerplot; categorical/categorical  Special symbols at each location: one observation = one dot; more observations = cross, star, etc.  1 sunflowerplot ( gear ,  cyl ,  main  =   'Title' ,  xlab  =   'gearbox' ,  ylab  =   'cylinders' )     Boxplot  1 boxplot ( mpg  ~  cyl ,  main  =   'Title' ,    xlab  =   'cylinders' ,  ylab  =   'miles per gallon' )      Colors.  1\n2\n3\n4\n5\n6 library ( RColorBrewer ) \n\npar ( mfrow  =   c ( 1 ,   2 )) \n\nboxplot ( iris $ Sepal.Length ,  col  =   'red' ) \nboxplot ( iris $ Sepal.Length  ~  iris $ Species ,  col  =  topo.colors ( 3 ))     1 par ( mfrow  =   c ( 1 ,   1 ))    1\n2\n3\n4\n5\n6 library ( dplyr ) \n\ndata ( Pima.tr2 ,  package  =   'MASS' ) \n\nPimaV  <-  select ( Pima.tr2 ,  glu : age ) \nboxplot ( scale ( PimaV ),  pch  =   16 ,  outcol  =   'red' )     Boxplot options  1\n2\n3\n4\n5\n6\n7 four  <-   subset ( mpg ,  cyl  ==   4 ) \nsix  <-   subset ( mpg ,  cyl  ==   6 ) \neight  <-   subset ( mpg ,  cyl  ==   8 ) \n\nboxplot ( four ,  six ,  eight ,  main  =   'Title' ,  ylab  =   'miles per gallon' ) \n\naxis ( 1 ,  at  =   c ( 1 ,   2 ,   3 ),  labels  =   c ( '4 Cyl' ,   '6 Cyl' ,   '8 Cyl' ))     Dotchart  1\n2 counts  <-   table ( gear ,  cyl ) \ncounts   1\n2\n3\n4\n5 ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2   1 dotchart ( counts ,  main  =   'Title' ,  xlab  =   'count' ,  ylab  =   'cylinders/gearbox' )     1\n2 counts  <-   table ( cyl ,  gear ) \ncounts   1\n2\n3\n4\n5 ##    gear\n## cyl  3  4  5\n##   4  1  8  2\n##   6  2  4  1\n##   8 12  0  2   1 dotchart ( counts ,  main  =   'Title' ,  xlab  =   'count' ,  ylab  =   'gearbox/cylinders' )     Barplot with its options  Vertical or horizontal. The legend as well can be horizontal or vertical.  1\n2 counts  <-   table ( gear ,  cyl ) \ncounts   1\n2\n3\n4\n5 ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2   1\n2\n3\n4 barplot ( counts ,  main  =   'Title' ,  xlab  =   'cylinders' ,  ylab  =   'count' ,  ylim  =   c ( 0 ,   20 ),  col  =  terrain.colors ( 3 )) \n\nlegend ( 'topleft' ,  inset  =   .04 ,  title  =   'gearbox' , \n    c ( '3' , '4' , '5' ),  fill  =  terrain.colors ( 3 ),  horiz  =   TRUE )     1\n2 counts  <-   table ( gear ,  cyl ) \ncounts   1\n2\n3\n4\n5 ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2   1 barplot ( counts ,  main  =   'Title' ,  xlab  =   'cylinders' ,  ylab  =   'count' ,  ylim  =   c ( 0 ,   25 ),  col  =  terrain.colors ( 3 ),  legend  =   rownames ( counts ))     1\n2 counts  <-   table ( gear ,  cyl ) \ncounts   1\n2\n3\n4\n5 ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2   1 barplot ( counts ,  main  =   'Title' ,  xlab  =   'cylinders' ,  ylab  =   'count' ,  ylim  =   c ( 0 ,   20 ),  col  =  terrain.colors ( 3 ),  legend  =   rownames ( counts ),  beside  =   TRUE )     Spineplot  \u2018Count\u2019 = blocks; categorical (with factors).  1\n2\n3\n4 cyl2  <-   as.factor ( cyl )   # mandatory for the y \ngear2  <-   as.factor ( gear ) \n\nspineplot ( gear2 ,  cyl2 ,  main  =   'Title' ,  xlab  =   'gearbox' ,  ylab  =   'cylinders' )     Count = blocks; continuous.  1 spineplot ( mpg ,  cyl2 ,  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'cylinders' )     Mosaicplot  Count = blocks.  1\n2 counts  <-   table ( gear ,  cyl ) \ncounts   1\n2\n3\n4\n5 ##     cyl\n## gear  4  6  8\n##    3  1  2 12\n##    4  8  4  0\n##    5  2  1  2   1 mosaicplot ( counts ,  main  =   'Title' ,  xlab  =   'gearbox' ,  ylab  =   'cylinders' )",
            "title": "Bivariate (Multivariate) Plots"
        },
        {
            "location": "/Plot_snippets_-_Basics/#multivariate-plots",
            "text": "Pairs  1 pairs (   ~ mpg  +  disp  +  hp )     Coplot  1 coplot ( mpg  ~  hp  |  wt )     Correlograms  1\n2\n3 library ( corrgram ) \n\ncorrgram ( mtcars ,  order  =   TRUE ,  lower.panel  =  panel.shade ,  upper.panel = panel.pie ,  text.panel  =  panel.txt ,  main  =   'Car Milage Data in PC2/PC1 Order' )     Plot a dataset with colors  1\n2\n3 library ( RColorBrewer ) \n\nplot ( iris ,  col  =  brewer.pal ( 3 ,   'Set1' ))     Stars  The star branches are explanatory; be careful with the interpretation! Well-advised for visual and pattern exploration.  1 mtcars [ 1 : 4 ,   c ( 1 ,   4 ,   6 )]    1\n2\n3\n4\n5 ##                 mpg  hp    wt\n## Mazda RX4      21.0 110 2.620\n## Mazda RX4 Wag  21.0 110 2.875\n## Datsun 710     22.8  93 2.320\n## Hornet 4 Drive 21.4 110 3.215   1 stars ( mtcars [ 1 : 4 ,   c ( 1 ,   4 ,   6 )])     Trivariate plots   image() .  contour() .  filled.contour() .  persp() .  symbols() .",
            "title": "Multivariate Plots"
        },
        {
            "location": "/Plot_snippets_-_Basics/#times-series",
            "text": "Add packages:  zoo  and  xts .  Basics  1 plot ( AirPassengers ,  type  =   'l' )     Change the  type =  1\n2\n3\n4\n5\n6 y1  <-  rnorm ( 100 ) \n\npar ( mfrow  =   c ( 2 ,   1 )) \n\nplot ( y1 ,  type  =   'p' ,  main  =   'p vs l' ) \nplot ( y1 ,  type  =   'l' )     1\n2 plot ( y1 ,  type  =   'l' ,  main  =   'l vs h' ) \nplot ( y1 ,  type  =   'h' )     1\n2 plot ( y1 ,  type  =   'l' ,  lty  =   3 ,  main  =   'l 3 vs o' ) \nplot ( y1 ,  type  =   'o' )     1\n2 plot ( y1 ,  type  =   'b' ,  main  =   'b vs c' ) \nplot ( y1 ,  type  =   'c' )     1\n2 plot ( y1 ,  type  =   's' ,  main  =   's vs S' ) \nplot ( y1 ,  type  =   'S' )     1\n2 # reverse \npar ( mfrow  =   c ( 1 ,   1 ))    Add a box   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 y1  <-  rnorm ( 100 ) \ny2  <-  rnorm ( 100 ) \n\npar ( mfrow  =   ( c ( 2 ,   1 ))) \n\nplot ( y1 ,  type  =   'l' ,  axes  =   FALSE ,  xlab  =   '' ,  ylab  =   '' ,  main  =   '' ) \n\nbox ( col  =   'gray' ) \n\nlines ( x  =   c ( 20 ,   20 ,   40 ,   40 ),  y  =   c ( -7 ,   max ( y1 ),   max ( y1 ),   -7 ),  lwd  =   3 ,  col  =   'gray' ) \n\nplot ( y2 ,  type  =   'l' ,  axes  =   FALSE ,  xlab  =   '' ,  ylab  =   '' ,  main  =   '' ) \n\nbox ( col  =   'gray' ) \n\nlines ( x  =   c ( 20 ,   20 ,   40 ,   40 ),  y  =   c ( 7 ,   min ( y2 ),   min ( y2 ),   7 ),  lwd  =   3 ,  col  =   'gray' )     1\n2 # reverse \npar ( mfrow  =   c ( 1 , 1 ))    Add lines and text within the plot   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 y1  <-  rnorm ( 100 )  # x goes from 0 to 100  # xaxt = 'n' remove the x ticks \nplot ( y1 ,  type  =   'l' ,  lwd  =   2 ,  lty  =   'longdash' ,  main  =   'Title' ,  ylab  =   'y' ,  xlab  =   'time' ,  xaxt  =   'n' ) \n\nabline ( h  =   0 ,  lty  =   'longdash' ) \n\nabline ( v  =   20 ,  lty  =   'longdash' ) \nabline ( v  =   50 ,  lty  =   'longdash' ) \nabline ( v  =   95 ,  lty  =   'longdash' ) \n\ntext ( 17 ,   1.5 ,  srt  =   90 ,  adj  =   0 ,  labels  =   'Tag 1' ,  cex  =   0.8 ) \ntext ( 47 ,   1.5 ,  srt  =   90 ,  adj  =   0 ,  labels  =   'Tag a' ,  cex  =   0.8 ) \ntext ( 92 ,   1.5 ,  srt  =   90 ,  adj  =   0 ,  labels  =   'Tag alpha' ,  cex  =   0.8 )     A comprehensive example  1\n2 # new data  head ( Orange )    1\n2\n3\n4\n5\n6\n7 ##   Tree  age circumference\n## 1    1  118            30\n## 2    1  484            58\n## 3    1  664            87\n## 4    1 1004           115\n## 5    1 1231           120\n## 6    1 1372           142    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 # convert factor to numeric for convenience \nOrange $ Tree  <-   as.numeric ( Orange $ Tree ) \nntrees  <-   max ( Orange $ Tree )  # get the range for the x and y axis \nxrange  <-   range ( Orange $ age ) \nyrange  <-   range ( Orange $ circumference )  # set up the plot \nplot ( xrange ,  yrange ,  type  =   'n' ,  xlab  =   'Age (days)' , \n   ylab  =   'Circumference (mm)'   ) \ncolors  <-  rainbow ( ntrees ) \nlinetype  <-   c ( 1 : ntrees ) \nplotchar  <-   seq ( 18 ,   18   +  ntrees ,   1 )  # add lines  for   ( i  in   1 : ntrees )   { \n  tree  <-   subset ( Orange ,  Tree  ==  i ) \n  lines ( tree $ age ,  tree $ circumference ,  type  =   'b' ,  lwd  =   1.5 , \n    lty  =  linetype [ i ],  col  =  colors [ i ],  pch  =  plotchar [ i ])  }  # add a title and subtitle \ntitle ( 'Tree Growth' ,   'example of line plot' )  # add a legend \nlegend ( xrange [ 1 ],  yrange [ 2 ],   1 : ntrees ,  cex  =   0.8 ,  col  =  colors , \n   pch  =  plotchar ,  lty  =  linetype ,  title  =   'Tree' )     Change  lty =",
            "title": "Times Series"
        },
        {
            "location": "/Plot_snippets_-_Basics/#regressions-and-residual-plots",
            "text": "1\n2\n3\n4 # first \nregr  <-  lm ( mpg  ~  hp )  summary ( regr )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## \n## Call:\n## lm(formula = mpg ~ hp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.7121 -2.1122 -0.8854  1.5819  8.2360 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 30.09886    1.63392  18.421  < 2e-16 ***\n## hp          -0.06823    0.01012  -6.742 1.79e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.863 on 30 degrees of freedom\n## Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 \n## F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07   1\n2 plot ( mpg  ~  hp ) \nabline ( regr )     1\n2\n3\n4 par ( mfrow  =   c ( 2 ,   2 ))  # then \nplot ( regr )     1\n2 # reverse \npar ( mfrow  =   c ( 1 ,   1 ))",
            "title": "Regressions and Residual Plots"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-lattice-and-latticeextra-packages",
            "text": "1 library ( lattice )",
            "title": "The lattice and latticeExtra Packages"
        },
        {
            "location": "/Plot_snippets_-_Basics/#coloring",
            "text": "1\n2 # Show the default settings \nshow.settings ()     1\n2\n3\n4\n5\n6 # Save the default theme \nmytheme  <-  trellis.par.get ()  # Turn the B&W \ntrellis.par.set ( canonical.theme ( color  =   FALSE )) \nshow.settings ()",
            "title": "Coloring"
        },
        {
            "location": "/Plot_snippets_-_Basics/#documentation",
            "text": "National Park Service, Advanced \n    Graphics (Lattice)  Treillis \n    Plots",
            "title": "Documentation"
        },
        {
            "location": "/Plot_snippets_-_Basics/#a-note-on-reordering-the-levels-factors",
            "text": "1\n2\n3\n4 # start \ncyl  <-  mtcars $ cyl\ncyl  <-   as.factor ( cyl ) \ncyl   1\n2 ##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n## Levels: 4 6 8   1 levels ( cyl )    1 ## [1] \"4\" \"6\" \"8\"   1\n2\n3\n4\n5 # option 1 \ncyl  <-   factor ( cyl ,  levels  =   c ( '8' ,   '6' ,   '4' ))  # or levels = 3:1  # or levels = letters[3:1]  levels ( cyl )    1 ## [1] \"8\" \"6\" \"4\"   1\n2\n3\n4\n5 cyl  <-  mtcars $ cyl\ncyl  <-   as.factor ( cyl )  # option 2 \ncyl  <-  reorder ( cyl ,  new.order  =   3 : 1 )  levels ( cyl )    1 ## [1] \"8\" \"6\" \"4\"   1\n2\n3\n4 library ( lattice )  # normalized x-axis for comparison \nbarchart ( Class  ~  Freq  |  Sex  +  Age ,  data  =   as.data.frame ( Titanic ),  groups  =  Survived ,  stack  =   TRUE ,  layout  =   c ( 4 ,   1 ),  auto.key  =   list ( title  =   'Survived' ,  columns  =   2 ))     1\n2 # free x-axis \nbarchart ( Class  ~  Freq  |  Sex  +  Age ,  data  =   as.data.frame ( Titanic ),  groups  =  Survived ,  stack  =   TRUE ,  layout  =   c ( 4 ,   1 ),  auto.key  =   list ( title  =   'Survived' ,  columns  =   2 ),  scales  =   list ( x  =   'free' ))     1\n2\n3\n4 # or \nbc.titanic  <-  barchart ( Class  ~  Freq  |  Sex  +  Age ,  data  =   as.data.frame ( Titanic ),  groups  =  Survived ,  stack  =   TRUE ,  layout  =   c ( 4 ,   1 ),  auto.key  =   list ( title  =   'Survived' ,  columns  =   2 ),  scales  =   list ( x  =   'free' )) \n\nbc.titanic    1\n2\n3\n4\n5 # add bg grid \nupdate ( bc.titanic ,  panel  =   function ( ... )   { \n  panel.grid ( h  =   0 ,  v  =   -1 ) \n  panel.barchart ( ... )  })     1\n2\n3\n4 # remove lines \nupdate ( bc.titanic ,  panel  =   function ( ... )   { \n  panel.barchart ( ... ,  border  =   'transparent' )  })     1\n2 # or \nupdate ( bc.titanic ,  border  =   'transparent' )     1\n2 Titanic1  <-   as.data.frame ( as.table ( Titanic [,   ,   'Adult'   ,])) \nTitanic1    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ##    Class    Sex Survived Freq\n## 1    1st   Male       No  118\n## 2    2nd   Male       No  154\n## 3    3rd   Male       No  387\n## 4   Crew   Male       No  670\n## 5    1st Female       No    4\n## 6    2nd Female       No   13\n## 7    3rd Female       No   89\n## 8   Crew Female       No    3\n## 9    1st   Male      Yes   57\n## 10   2nd   Male      Yes   14\n## 11   3rd   Male      Yes   75\n## 12  Crew   Male      Yes  192\n## 13   1st Female      Yes  140\n## 14   2nd Female      Yes   80\n## 15   3rd Female      Yes   76\n## 16  Crew Female      Yes   20   1 barchart ( Class  ~  Freq  |  Sex ,  Titanic1 ,  groups  =  Survived ,  stack  =   TRUE ,  auto.key  =   list ( title  =   'Survived' ,  columns  =   2 ))     1\n2\n3\n4\n5 Titanic2  <-  reshape ( Titanic1 ,  direction  =   'wide' ,  v.names  =   'Freq' ,  idvar  =   c ( 'Class' ,   'Sex' ),  timevar  =   'Survived' )  names ( Titanic2 )   <-   c ( 'Class' ,   'Sex' ,   'Dead' ,   'Alive' ) \n\nbarchart ( Class  ~  Dead  +  Alive  |  Sex ,  Titanic2 ,  stack  =   TRUE ,  auto.key  =   list ( columns  =   2 ))",
            "title": "A note on reordering the levels (factors)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#uni-bi-multivariate-plots",
            "text": "Barchart  Like  barplot() .  1\n2 # y ~ x \nbarchart ( mpg  ~  hp ,  main  =   'Title' ,  xlab  =   'horsepowers' ,  ylab  =   'miles per gallon' )     1\n2 # y ~ x \nbarchart ( mpg  ~  hp ,  main  =   'Title' ,  xlab  =   'horsepowers' ,  ylab  =   'miles per gallon' ,  horizontal  =   FALSE )     1 barchart ( VADeaths ,  groups  =   FALSE ,  layout  =   c ( 1 ,   4 ),  aspect  =   0.7 ,  reference  = FALSE ,  main  =   'Title' ,  xlab  =   'rate per 100' )     1\n2\n3 data ( postdoc ,  package  =   'latticeExtra' ) \n\nbarchart ( prop.table ( postdoc ,  margin  =   1 ),  xlab  =   'Proportion' ,  auto.key  =   list ( adj  =   1 ))     Change  layout = c(x, y, page)  1 barchart ( mpg  ~  hp  |   factor ( cyl ),  main  =   'Title' ,  xlab  =   'horsepowers' ,  ylab  =   'cylinders - miles per gallon' ,  layout  =   c ( 1 , 3 ))     1 barchart ( mpg  ~  hp  |   factor ( cyl ),  main  =   'Title' ,  xlab  =   'cylinders - horsepowers' ,  ylab  =   'miles per gallon' ,  layout  =   c ( 3 , 1 ))     Change  aspect = 1  1  for square.  1 barchart ( mpg  ~  hp  |   factor ( cyl ),  main  =   'Title' ,  xlab  =   'horsepowers' ,  ylab  =   'miles per gallon' ,  layout  =   c ( 3 , 1 ),  aspect  =   1 )     Colors  1 barchart ( mpg  ~  hp ,  group  =  cyl ,  auto.key  =   list ( space  =   'right' ),  main  =   'Title' ,  xlab  =   'horsepowers' ,  ylab  =   'miles per gallon' )      shingle() ; control the ranges.  equal.count() ; grid.   Dotplot  Like  dotchart() .  1 dotplot ( mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon' )     1 dotplot ( factor ( cyl )   ~  mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'cylinders' )     1 dotplot ( factor ( cyl )   ~  mpg  |   factor ( gear ),  main  =   'Title' ,  xlab  =   'gearbox - miles per gallon' ,  ylab  =   'cylinders' ,  layout  =   c ( 3 , 1 ))     1 dotplot ( factor ( cyl )   ~  mpg  |   factor ( gear ),  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'gearbox - cylinders' ,  layout  =   c ( 1 , 3 ),  aspect  =   0.3 )     1 dotplot ( factor ( cyl )   ~  mpg  |   factor ( gear ),  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'gearbox - cylinders' ,  layout  =   c ( 1 , 3 ),  aspect  =   0.3 ,  origin  =   0 )     1 dotplot ( factor ( cyl )   ~  mpg  |   factor ( gear ),  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'gearbox - cylinders' ,  layout  =   c ( 1 , 3 ),  aspect  =   0.3 ,  origin  =   0 ,  type  =   c ( 'p' ,   'h' ))     Set  auto.key .  1\n2\n3\n4\n5\n6\n7\n8\n9 # maybe we'll want this later \nold.pars  <-  trellis.par.get ()  #trellis.par.set(superpose.symbol = list(pch = c(1,3), col = 12:14)) \n\ntrellis.par.set ( superpose.symbol  =   list ( pch  =   c ( 1 , 3 ),  col  =   1 ))  # Optionally put things back how they were  #trellis.par.set(old.pars)    Use  auto.key .  1 dotplot ( factor ( cyl )   ~  mpg  |   factor ( gear ),  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'gearbox - cylinders' ,  layout  =   c ( 1 , 3 ),  groups  =  vs ,  auto.key  =   list ( space  =   'right' ))     1 trellis.par.set ( old.pars )    1\n2\n3 trellis.par.set ( superpose.symbol  =   list ( pch  =   c ( 1 , 3 ),  col  =   1 )) \n\ndotplot ( variety  ~  yield  |  site ,  barley ,  layout  =   c ( 1 ,   6 ),  aspect  =   c ( 0.7 ),  groups  =  year ,  auto.key  =   list ( space  =   'right' ))     1 trellis.par.set ( old.pars )    Vertical.  1 dotplot ( mpg  ~   factor ( cyl )   |   factor ( gear ),  main  =   'Title' ,  xlab  =   'cylinders' ,  ylab  =   'gearbox - miles per gallon' ,  layout  =   c ( 1 , 3 ),  aspect  =   0.3 )     1\n2\n3\n4\n5 library ( readr ) \ndensity  <-  read_csv ( 'density.csv' ) \ndensity $ Density  <-   as.numeric ( density $ Density ) \n\ndotplot ( reorder ( MetropolitanArea ,  Density )   ~  Density ,  density ,  type  =   c ( 'p' ,   'h' ),  main  =   'Title' ,  xlab  =   'Population Density (pop / sq.mi)' )     1 dotplot ( reorder ( MetropolitanArea ,  Density )   ~  Density  |  Region ,  density ,  type  =   c ( 'p' ,   'h' ),  strip  =   FALSE ,  strip.left  =   TRUE ,  layout  =   c ( 1 ,   3 ),  scales  =   list ( y  =   list ( relation  =   'free' )),  main  =   'Title' ,  xlab  =   'Population Density (pop / sq.mi)' )     Stripplot  Like  stripchart() .  1 stripplot ( mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon' )     1 stripplot ( factor ( cyl )   ~  mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'cylinders' )     1 stripplot ( factor ( cyl )   ~  mpg  |   factor ( gear ),  main  =   'Title' ,  xlab  =   'gearbox - miles per gallon' ,  ylab  =   'cylinders' ,  layout  =   c ( 1 , 3 ))     1 stripplot ( factor ( cyl )   ~  mpg  |   factor ( gear ),  main  =   'Title' ,  xlab  =   'gearbox - miles per gallon' ,  ylab  =   'cylinders' ,  layout  =   c ( 1 , 3 ),  groups  =  vs ,  auto.key  =   list ( space  =   'right' ))     1 stripplot ( mpg  ~   factor ( cyl )   |   factor ( gear ),  main  =   'Title' ,  xlab  =   'cylinders' ,  ylab  =   'gearbox - miles per gallon' ,  layout  =   c ( 1 , 3 ))     Histogram  Like  hist() .  1 histogram ( mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon' )     1 histogram ( ~ mpg  |   factor ( cyl ),  layout  =   c ( 1 ,   3 ),  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'density' )     Densityplot  Like  plot.density() .  1 densityplot ( mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'density' )     1 densityplot ( ~ mpg  |   factor ( cyl ),  layout  =   c ( 1 ,   3 ),  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'density' )     ECDFplot  1\n2\n3 library ( latticeExtra ) \n\necdfplot ( mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   '' )     BWplot  Like  boxplot .  1 bwplot ( mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'density' )     1 bwplot ( factor ( cyl )   ~  mpg ,  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'cylinders' )     1 bwplot ( factor ( cyl )   ~  mpg  |   factor ( gear ),  main  =   'Title' ,  xlab  =   'miles per gallon' ,  ylab  =   'gearbox - cylinders' ,  layout  =   c ( 1 , 3 ))     1 bwplot ( mpg  ~   factor ( cyl )   |   factor ( gear ),  main  =   'Title' ,  xlab  =   'gearbox - cylinders' ,  ylab  =   'miles per gallon' ,  layout  =   c ( 3 , 1 ))     QQmath  Like  qqnorm() .  1 qqmath ( mpg ,  main  =   'Title' ,  ylab  =   'miles per gallon' )     XYplot  Like  plot() .  1 xyplot ( mpg  ~  disp  |   factor ( cyl ),  main  =   'Title' ,  xlab  =   'horsepower' ,  ylab  =   'cylinders - miles per gallon' ,  layout  =   c ( 1 , 3 ))     1 xyplot ( mpg  ~  disp  |   factor ( cyl ),  main  =   'Title' ,  xlab  =   'cylinder - horsepowers' ,  ylab  =   'miles per gallon' ,  layout  =   c ( 3 , 1 ))     XYplot options  1 xyplot ( mpg  ~  disp  |   factor ( cyl ),  main  =   'Title' ,  xlab  =   'cylinder - horsepowers' ,  ylab  =   'miles per gallon' ,  layout  =   c ( 3 , 1 ),  aspect  =   1 )     1 xyplot ( mpg  ~  disp  |   factor ( cyl ),  main  =   'Title' ,  xlab  =   'cylinder - horsepowers' ,  ylab  =   'miles per gallon' ,  layout  =   c ( 3 , 1 ),  aspect  =   1 ,  scales  =   list ( y  =   list ( at  =   seq ( 10 ,   30 ,   10 ))))     1\n2\n3\n4\n5\n6\n7 meanmpg  <-   mean ( mpg ) \n\nxyplot ( mpg  ~  disp  |   factor ( cyl ),  main  =   'Title' ,  xlab  =   'cylinder - horsepowers' ,  ylab  =   'miles per gallon' ,  layout  =   c ( 3 , 1 ),  aspect  =   1 ,  panel  =   function ( ... )   { \n  panel.xyplot ( ... ) \n  panel.abline ( h  =  meanmpg ,  lty  =   'dashed' ) \n  panel.text ( 450 ,  meanmpg  +   1 ,   'avg' ,  adj  =   c ( 1 ,    0 ),  cex  =   0.7 )  })     1\n2\n3\n4 xyplot ( mpg  ~  disp  |   factor ( cyl ),  main  =   'Title' ,  xlab  =   'cylinder - horsepowers' ,  ylab  =   'miles per gallon' ,  layout  =   c ( 3 , 1 ),  aspect  =   1 ,  panel  =   function ( x ,  y ,   ... )   { \n    panel.lmline ( x ,  y ) \n    panel.xyplot ( x ,  y ,   ... )  })      panel.points() .  panel.lines() .  panel.segments() .  panel.arrows() .  panel.rect() .  panel.polygon() .  panel.text() .  panel.abline() .  panel.lmline() .  panel.xyplot() .  panel.curve() .  panel.rug() .  panel.grid() .  panel.bwplot() .  panel.histogram() .  panel.loess() .  panel.violin() .  panel.smoothScatter() .  \u2026  par.settings .  \u2026    1\n2\n3\n4\n5 library ( lattice ) \n\ndata ( SeatacWeather ,  package  =   'latticeExtra' ) \n\nxyplot ( min.temp  +  max.temp  +  precip  ~  day  |  month ,  ylab  =   'Temperature and Rainfall' ,  data  =  SeatacWeather ,  layout  =   c ( 3 , 1 ),  type  =   'l' ,  lty  =   1 ,  col  =   'black' )     1 xyplot ( min.temp  +  max.temp  +  precip  ~  day  |  month ,  ylab  =   'Temperature and Rainfall' ,  data  =  SeatacWeather ,  layout  =   c ( 3 , 1 ),  type  =   'p' ,  lty  =   1 ,  col  =   'black' )     1 xyplot ( min.temp  +  max.temp  +  precip  ~  day  |  month ,  ylab  =   'Temperature and Rainfall' ,  data  =  SeatacWeather ,  layout  =   c ( 3 , 1 ),  type  =   'l' ,  lty  =   1 ,  col  =   'black' )     1 xyplot ( min.temp  +  max.temp  +  precip  ~  day  |  month ,  ylab  =   'Temperature and Rainfall' ,  data  =  SeatacWeather ,  layout  =   c ( 3 , 1 ),  type  =   'o' ,  lty  =   1 ,  col  =   'black' )     1 xyplot ( min.temp  +  max.temp  +  precip  ~  day  |  month ,  ylab  =   'Temperature and Rainfall' ,  data  =  SeatacWeather ,  layout  =   c ( 3 , 1 ),  type  =   'r' ,  lty  =   1 ,  col  =   'black' )     1 xyplot ( min.temp  +  max.temp  +  precip  ~  day  |  month ,  ylab  =   'Temperature and Rainfall' ,  data  =  SeatacWeather ,  layout  =   c ( 3 , 1 ),  type  =   'g' ,  lty  =   1 ,  col  =   'black' )     1 xyplot ( min.temp  +  max.temp  +  precip  ~  day  |  month ,  ylab  =   'Temperature and Rainfall' ,  data  =  SeatacWeather ,  layout  =   c ( 3 , 1 ),  type  =   's' ,  lty  =   1 ,  col  =   'black' )     1 xyplot ( min.temp  +  max.temp  +  precip  ~  day  |  month ,  ylab  =   'Temperature and Rainfall' ,  data  =  SeatacWeather ,  layout  =   c ( 3 , 1 ),  type  =   'S' ,  lty  =   1 ,  col  =   'black' )     1 xyplot ( min.temp  +  max.temp  +  precip  ~  day  |  month ,  ylab  =   'Temperature and Rainfall' ,  data  =  SeatacWeather ,  layout  =   c ( 3 , 1 ),  type  =   'h' ,  lty  =   1 ,  col  =   'black' )     1 xyplot ( min.temp  +  max.temp  +  precip  ~  day  |  month ,  ylab  =   'Temperature and Rainfall' ,  data  =  SeatacWeather ,  layout  =   c ( 3 , 1 ),  type  =   'a' ,  lty  =   1 ,  col  =   'black' )     1 xyplot ( min.temp  +  max.temp  +  precip  ~  day  |  month ,  ylab  =   'Temperature and Rainfall' ,  data  =  SeatacWeather ,  layout  =   c ( 3 , 1 ),  type  =   'smooth' ,  lty  =   1 ,  col  =   'black' )     1 xyplot ( mpg  ~  hp ,  main  =   'Title' ,  xlab  =   'horsepowers' ,  ylab  =   'miles per gallon' )     1 xyplot ( mpg  ~  hp ,  main  =   'Title' ,  xlab  =   'horsepowers' ,  ylab  =   'miles per gallon' ,  type  =   'o' )     1 xyplot ( mpg  ~  hp ,  main  =   'Title' ,  xlab  =   'horsepowers' ,  ylab  =   'miles per gallon' ,  type  =   'o' ,  pch  =   16 ,  lty  =   'dashed' )     1 xyplot ( mpg  ~  hp ,  main  =   'Title' ,  xlab  =   'horsepowers' ,  ylab  =   'miles per gallon' )     1\n2\n3 data ( USAge.df ,  package  =   'latticeExtra' ) \n\nxyplot ( Population  ~  Age  |   factor ( Year ),  USAge.df ,  groups  =  Sex ,  type  =   c ( 'l' ,   'g' ),  auto.key  =   list ( points  =   FALSE ,  lines  =   TRUE ,  columns  =   2 ),  aspect  =   'xy' ,  ylab  =   'Population (millions)' ,  subset  =  Year  %in%   seq ( 1905 ,   1975 ,  by  =   10 ))     1 xyplot ( Population  ~  Year  |   factor ( Age ),  USAge.df ,  groups  =  Sex ,  type  =   'l' ,  strip  =   FALSE ,  strip.left  =   TRUE ,  layout  =   c ( 1 ,   3 ),  ylab  =   'Population (millions)' ,  auto.key  =   list ( lines  =   TRUE ,  points  =   FALSE ,  columns  =   2 ),  subset  =  Age  %in%   c ( 0 ,   10 ,   20 ))     1\n2\n3\n4\n5\n6\n7\n8\n9 data ( USCancerRates ,  package  =   'latticeExtra' ) \n\nxyplot ( rate.male  ~  rate.female  |  state ,  USCancerRates ,  aspect  =   'iso' ,  pch  =   '.' ,  cex  =   2 ,  index.cond  =   function ( x ,  y )   {  median ( y  -  x ,  na.rm  =   TRUE )   },  scales  =   list ( log  =   2 ,  at  =   c ( 75 ,   150 ,   300 ,   600 )),  panel  =   function ( ... )   {  \n  panel.grid ( h  =   -1 ,  v  =   -1 ) \n  panel.abline ( 0 ,   1 ) \n  panel.xyplot ( ... ) \n   }, \n  xlab  =   'a' , \n  ylab  =   'b' )     1\n2\n3\n4\n5 data ( biocAccess ,  package  =   'latticeExtra' ) \n\nbaxy  <-  xyplot ( log10 ( counts )   ~  hour  |  month  +  weekday ,  biocAccess ,  type  =   c ( 'p' ,   'a' ),  as.table  =   TRUE ,  pch  =   '.' ,  cex  =   2 ,  col.line  =   'black' ) \n\nbaxy    1\n2 library ( latticeExtra ) \nuseOuterStrips ( baxy )     1 xyplot ( sunspot.year ,  aspect  =   'xy' ,  strip  =   FALSE ,  strip.left  =   TRUE ,  cut  =   list ( number  =   4 ,  overlap  =   0.05 ))     1\n2\n3\n4\n5 data ( biocAccess ,  package  =   'latticeExtra' ) \n\nssd  <-  stl ( ts ( biocAccess $ counts [ 1 : ( 24   *   30   * 2 )],  frequency  =   24 ),   'periodic' ) \n\nxyplot ( ssd ,  main  =   'Title' ,  xlab  =   'Time (Days)' )     Splom  1 splom ( mtcars [ c ( 1 ,   3 ,   6 )],  groups  =  cyl ,  data  =  mtcars ,  panel  =  panel.superpose ,  key  =   list ( title  =   'Three Cylinder Options' ,  columns  =   3 ,  points  =   list ( text  =   list ( c ( '4 Cylinder' ,   '6 Cylinder' ,   '8 Cylinder' )))))     1\n2\n3 trellis.par.set ( superpose.symbol  =   list ( pch  =   c ( 1 , 3 ,   22 ),  col  =   1 ,  alpha  =   0.5 )) \n\nsplom ( ~ data.frame ( mpg ,  disp ,  hp ,  drat ,  wt ,  qsec ),  data  =  mtcars ,  groups  =  cyl ,  pscales  =   0 ,  varnames  =   c ( 'miles\\nper\\ngallon' ,   'displacement\\n(cu.in(' ,   'horsepower' ,   'rear\\naxle\\nratio' ,   'weight' ,   '1/4\\nmile\\ntime' ),  auto.key  =   list ( columns  =   3 ,  title  =   'Title' ))     1 trellis.par.set ( old.pars )    1 splom ( USArrests )     1 splom ( ~ USArrests [ c ( 3 , 1 , 2 , 4 )]   |  state.region ,  pscales  =   0 ,  type  =   c ( 'g' ,   'p' ,   'smooth' ))     Parallel plot  For multivariate continuous data.  1 parallelplot ( ~ iris [ 1 : 4 ])     1 parallelplot ( ~ iris [ 1 : 4 ],  horizontal.axis  =   FALSE )     1 parallelplot ( ~ iris [ 1 : 4 ],  scales  =   list ( x  =   list ( rot  =   90 )))     1 parallelplot ( ~ iris [ 1 : 4 ]   |  Species ,  iris )     1\n2 parallelplot ( ~ iris [ 1 : 4 ],  iris ,  groups  =  Species , \n             horizontal.axis  =   FALSE ,  scales  =   list ( x  =   list ( rot  =   90 )))     Trivariate plots  Like  image() ,  contour() ,  filled.contour() ,  persp() ,  symbols() .   levelplot() .  contourplot() .  cloud() .  wireframe() .",
            "title": "Uni-, Bi-, Multivariate Plots"
        },
        {
            "location": "/Plot_snippets_-_Basics/#additional-packages",
            "text": "",
            "title": "Additional Packages"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-sm-package-density",
            "text": "1 library ( sm )    Density plot   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # create value labels \ncyl.f  <-   factor ( cyl ,  levels  =   c ( 4 ,   6 ,   8 ),  labels  =   c ( '4 cyl' ,   '6 cyl' ,   '8 cyl' ))  # plot densities \nsm.density.compare ( mpg ,  cyl ,  xlab  =   'miles per gallon' ) \n\ntitle ( main  =   'Title' )  # add legend via mouse click \ncolfill  <-   c ( 2 : ( 2   +   length ( levels ( cyl.f )))) \nlegend ( 25 ,   0.19 ,   levels ( cyl.f ),  fill  =  colfill )",
            "title": "The sm Package (density)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-car-package-scatter",
            "text": "1 library ( car )    Scatter plot  1 scatterplot ( mpg  ~  wt  |  cyl ,  data  =  mtcars ,     xlab  =   'weight' ,  ylab  =   'miles per gallon' ,  labels  =   row.names ( mtcars ))      Splom  1 scatterplotMatrix (   ~ mpg  +  disp  +  drat  +  wt  |  cyl ,  data  =  mtcars ,  main  =   'Title' )     scatterplotMatrix == spm .  1 spm (   ~ mpg  +  disp  +  drat  +  wt  |  cyl ,  data  =  mtcars ,  main  =   'Title' )",
            "title": "The car Package (scatter)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-vioplot-package-boxplot",
            "text": "1 library ( vioplot )    Violin boxplot  1\n2\n3\n4\n5\n6\n7 x1  <-  mpg [ mtcars $ cyl  ==   4 ] \nx2  <-  mpg [ mtcars $ cyl  ==   6 ] \nx3  <-  mpg [ mtcars $ cyl  ==   8 ] \n\nvioplot ( x1 ,  x2 ,  x3 ,  names  =   c ( '4 cyl' ,   '6 cyl' ,   '8 cyl' ),  col  =   'green' ) \n\ntitle ( 'Title' )",
            "title": "The vioplot Package (boxplot)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-vcd-package-count-correlation-mosaic",
            "text": "1 library ( vcd )    The package provides a variety of methods for visualizing multivariate categorical data.  Count  1\n2 counts  <-   table ( gear ,  cyl ) \ncounts   1\n2\n3\n4\n5 ##     cyl\n## gear  8  6  4\n##    3 12  2  1\n##    4  0  4  8\n##    5  2  1  2   1 mosaic ( counts ,  shade  =   TRUE ,  legend  =   TRUE )      Correlation  1\n2 counts  <-   table ( gear ,  cyl ) \ncounts   1\n2\n3\n4\n5 ##     cyl\n## gear  8  6  4\n##    3 12  2  1\n##    4  0  4  8\n##    5  2  1  2   1 assoc ( counts ,  shade  =   TRUE )     Mosaic  1\n2\n3\n4\n5\n6 ucb  <-   data.frame ( UCBAdmissions ) \nucb  <-   within ( ucb ,  Accept  <-   factor ( Admit ,  levels  =   c ( 'Rejected' ,   'Admitted' )))  library ( vcd );   library ( grid ) \n\ndoubledecker ( xtabs ( Freq ~  Dept  +  Gender  +  Accept ,  data  =  ucb ),  gp  =  gpar ( fill  =   c ( 'grey90' ,   'steelblue' )))     1\n2\n3 data ( Fertility ,  package  =   'AER' ) \n\ndoubledecker ( morekids  ~  age ,  data  =  Fertility ,  gp  =  gpar ( fill  =   c ( 'grey90' ,   'green' )),  spacing  =  spacing_equal ( 0 ))     1 doubledecker ( morekids  ~  gender1  +  gender2 ,  data  =  Fertility ,  gp  =  gpar ( fill  =   c ( 'grey90' ,   'green' )))     1 doubledecker ( morekids  ~  age  +  gender1  +  gender2 ,  data  =  Fertility ,  gp  =  gpar ( fill  =   c ( 'grey90' ,   'green' )),  spacing  =  spacing_dimequal ( c ( 0.1 ,   0 ,   0 ,   0 )))",
            "title": "The vcd Package (count, correlation, mosaic)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-hexbin-package-scatter",
            "text": "1 library ( hexbin )    Scatter plot  1\n2\n3\n4\n5 # new data \ndata ( NHANES )  # compare \nplot ( Serum.Iron  ~  Transferin ,  NHANES ,  main  =   'Title' ,  xlab  =   'Transferin' ,  ylab  =   'Iron' )     1\n2 # with \nhexbinplot ( Serum.Iron  ~  Transferin ,  NHANES ,  main  =   'Title' ,  xlab  =   'Transferin' ,  ylab  =   'Iron' )     1 hexbinplot ( mpg  ~  hp ,  main  =   'Title' ,  xlab  =   'horsepowers' ,  ylab  =   'miles per gallon' )     1\n2\n3\n4\n5 x  <-  rnorm ( 1000 ) \ny  <-  rnorm ( 1000 ) \n\nbin  <-  hexbin ( x ,  y ,  xbins  =   50 ) \nplot ( bin ,  main  =   'Title' )      1\n2\n3\n4 x  <-  rnorm ( 1000 ) \ny  <-  rnorm ( 1000 ) \n\nplot ( x ,  y ,  main  =   'Title' ,  col  =   rgb ( 0 ,   100 ,   0 ,   50 ,  maxColorValue  =   255 ),  pch  =   16 )     1\n2\n3\n4\n5\n6\n7 data ( Diamonds ,  package  =   'Stat2Data' ) \n\na  =  hexbin ( Diamonds $ PricePerCt ,  Diamonds $ Carat ,  xbins  =   40 )  library ( RColorBrewer ) \n\nplot ( a )     Colors.  1\n2\n3 rf  <-  colorRampPalette ( rev ( brewer.pal ( 12 ,   'Set3' ))) \n\nhexbinplot ( Diamonds $ PricePerCt  ~  Diamonds $ Carat ,  colramp  =  rf )     Mix  lattice  and  hexbin  1\n2\n3 data ( gvhd10 ,  package  =   'latticeExtra' ) \n\nxyplot ( asinh ( SSC.H )   ~   asinh ( FL2.H ),  gvhd10 ,  aspect  =   1 ,  panel  =  panel.hexbinplot ,   . aspect.ratio  =   1 ,  trans  =   sqrt )     1 xyplot ( asinh ( SSC.H )   ~   asinh ( FL2.H )   |  Days ,  gvhd10 ,  aspect  =   1 ,  panel  =  panel.hexbinplot ,   . aspect.ratio  =   1 ,  trans  = sqrt )",
            "title": "The hexbin Package (scatter)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-car-package-scatter_1",
            "text": "1 library ( car )    Scatter plot  1\n2 scatterplotMatrix ( ~ mpg  +  disp  +  drat  +  wt  |  cyl ,  data  =  mtcars , \n   main  =   'Three Cylinder Options' )",
            "title": "The car Package (scatter)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-scatterplot3d-package",
            "text": "1 library ( scatterplot3d )    Scatter plot  1 scatterplot3d ( wt ,  disp ,  mpg ,  main  =   'Title' )     1 scatterplot3d ( wt ,  disp ,  mpg ,  pch  =   16 ,  highlight.3d  =   TRUE ,  type  =   'h' ,  main  =   'Title' )     1\n2\n3\n4\n5 s3d  <-  scatterplot3d ( wt ,  disp ,  mpg ,  pch  =   16 ,  highlight.3d  =   TRUE ,  type  =   'h' ,  main  =   '   Title' ) \n\nfit  <-  lm ( mpg  ~  wt  +  disp ) \n\ns3d $ plane3d ( fit )",
            "title": "The scatterplot3d Package"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-rgl-package-interactive",
            "text": "1 library ( rgl )    Interactive plot  The plot will open a new window.  1 plot3d ( wt ,  disp ,  mpg ,  col  =   'red' ,  size  =   3 )",
            "title": "The rgl Package (interactive)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-cluster-package-dendrogram",
            "text": "1 library ( cluster )    Dendrogram  Use the  iris  dataset.  1\n2\n3 subset  <-   sample ( 1 : 150 ,   20 ) \ncS  <-   as.character ( Sp  <-  iris $ Species [ subset ]) \ncS   1\n2\n3\n4 ##  [1] \"setosa\"     \"versicolor\" \"setosa\"     \"virginica\"  \"virginica\" \n##  [6] \"setosa\"     \"setosa\"     \"setosa\"     \"virginica\"  \"setosa\"    \n## [11] \"versicolor\" \"versicolor\" \"virginica\"  \"setosa\"     \"versicolor\"\n## [16] \"versicolor\" \"setosa\"     \"virginica\"  \"versicolor\" \"versicolor\"   1\n2\n3\n4\n5\n6\n7 cS [ Sp  ==   'setosa' ]   <-   'S' \ncS [ Sp  ==   'versicolor' ]   <-   'V' \ncS [ Sp  ==   'virginica' ]   <-   'g' \n\nai  <-  agnes ( iris [ subset ,   1 : 4 ]) \n\nplot ( ai ,  label  =  cS )",
            "title": "The cluster Package (dendrogram)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-extracat-package-splom",
            "text": "1 library ( extracat )    Splom  For missing values. Binary matrix with reordering and filtering of rows \nand columns. The x-axis shows the frequency of NA. The y-axis shows the \nmarginal distribution of NA.  1\n2\n3\n4 # example 1 \ndata ( CHAIN ,  package  =   'mi' ) \n\nvisna ( CHAIN ,  sort  =   'b' )     1 summary ( CHAIN )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 ##    log_virus           age            income          healthy     \n##  Min.   : 0.000   Min.   :21.00   Min.   : 1.000   Min.   :16.67  \n##  1st Qu.: 0.000   1st Qu.:37.00   1st Qu.: 2.000   1st Qu.:35.00  \n##  Median : 0.000   Median :43.00   Median : 3.000   Median :45.37  \n##  Mean   : 4.324   Mean   :42.56   Mean   : 3.377   Mean   :44.40  \n##  3rd Qu.: 9.105   3rd Qu.:48.00   3rd Qu.: 5.000   3rd Qu.:54.89  \n##  Max.   :13.442   Max.   :70.00   Max.   :10.000   Max.   :70.11  \n##  NA's   :179      NA's   :24      NA's   :38       NA's   :24     \n##      mental           damage        treatment     \n##  Min.   :0.0000   Min.   :1.000   Min.   :0.0000  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:0.0000  \n##  Median :0.0000   Median :4.000   Median :1.0000  \n##  Mean   :0.2717   Mean   :3.578   Mean   :0.8602  \n##  3rd Qu.:1.0000   3rd Qu.:5.000   3rd Qu.:2.0000  \n##  Max.   :1.0000   Max.   :5.000   Max.   :2.0000  \n##  NA's   :24       NA's   :63      NA's   :24   1\n2\n3\n4\n5\n6\n7\n8\n9 # example 2 \ndata ( oly12 ,  package  =   'VGAMdata' ) \n\noly12d  <-  oly12 [,   names ( oly12 )   !=   'DOB' ] \noly12a  <-  oly12 names ( oly12a )   <-   abbreviate ( names ( oly12 ),   3 ) \n\nvisna ( oly12a ,  sort  =   'b' )     1\n2\n3\n4\n5\n6 # example 3 \ndata ( freetrade ,  package  =   'Amelia' ) \n\nfreetrade  <-   within ( freetrade ,  land1  <-  reorder ( country ,  tariff ,   function ( x )   sum ( is.na ( x )))) \n\nfluctile ( xtabs ( is.na ( tariff )   ~  land1  +  year ,  data  =  freetrade ))     1 ## viewport[base]   1\n2\n3\n4 # example 4 \ndata ( Pima.tr2 ,  package  =   'MASS' ) \n\nvisna ( Pima.tr2 ,  sort  =   'b' )",
            "title": "The extracat Package (splom)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-ash-package-density",
            "text": "1 library ( ash )    Density plot  1 plot ( ash1 ( bin1 ( mtcars $ mpg ,  nbin  =   50 )),  type  =   'l' )    1 ## [1] \"ash estimate nonzero outside interval ab\"",
            "title": "The ash Package (density)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-kernsmooth-package-density",
            "text": "1 library ( KernSmooth )    Density plot  1\n2\n3\n4\n5 with ( mtcars ,   { \n  hist ( mpg ,  freq  =   FALSE ,  main  =   '' ,  col  =   'bisque2' ,  ylab  =   '' ) \n  lines ( density ( mpg ),  lwd  =   2 ) \n  ks1  <-  bkde ( mpg ,  bandwidth  =  dpik ( mpg )) \n  lines ( ks1 ,  col  =   'red' ,  lty  =   5 ,  lwd  =   2 )})",
            "title": "The KernSmooth Package (density)"
        },
        {
            "location": "/Plot_snippets_-_Basics/#the-corrplot-package-correlation",
            "text": "1 library ( corrplot )    Splom  1\n2\n3\n4 # Create a correlation matrix for the dataset (9-14 are the '2' variables only) \ncorrelations  <-  cor ( mtcars ) \n\ncorrplot ( correlations )",
            "title": "The corrplot Package (correlation)"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nDocumentation\n\u00b6\n\n\n\n\nggplot2\n.\n\n\nggthemes\n.\n\n\nr4stats\n.\n\n\n\n\nDataset\n\u00b6\n\n\nFor most examples, we use the \nmtcars\n, \ndiamonds\n, \niris\n, \nChickWeight\n, \nrecess\n, \nfish\n, \nVocab\n, \nTitanic\n, \nmamsleep\n, \nbarley\n, \nadult\n datasets.\n\n\nThe \nggplot2\n Package\n\u00b6\n\n\n1\nlibrary\n(\nggplot2\n)\n\n\n\n\n\n\n\nImport additional packages.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nlibrary\n(\ndigest\n)\n\n\nlibrary\n(\ngrid\n)\n\n\nlibrary\n(\ngtable\n)\n\n\nlibrary\n(\nMASS\n)\n\n\nlibrary\n(\nplyr\n)\n\n\nlibrary\n(\nreshape2\n)\n\n\nlibrary\n(\nscales\n)\n\n\nlibrary\n(\nstats\n)\n\n\nlibrary\n(\ntidyr\n)\n\n\n\n\n\n\n\nFor this project, import additional packages.\n\n\n1\n2\n3\n4\n5\n6\n7\nlibrary\n(\nggthemes\n)\n\n\nlibrary\n(\nRColorBrewer\n)\n\n\nlibrary\n(\ngridExtra\n)\n\n\nlibrary\n(\nGGally\n)\n\n\nlibrary\n(\ncar\n)\n\n\nlibrary\n(\nHmisc\n)\n\n\nlibrary\n(\ndplyr\n)\n\n\n\n\n\n\n\nSuggested additional packages\u2026\n\n\nSECTION 1\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nExploring \nggplot2\n, part 1\n\n\n1\n2\n3\n# basic plot\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n y \n=\n mpg\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\nExploring \nggplot2\n, part 2\n\n\n1\n2\n3\n# cyl is a factor\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n \nfactor\n(\ncyl\n),\n y \n=\n mpg\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\nExploring \nggplot2\n, part 3\n\n\n1\n2\n3\n# scatter plot\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# add color\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n disp\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change size\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n size \n=\n disp\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\nExploring \nggplot2\n, part 4\n\n\n1\n2\n3\n# Add geom_point() with +\n\nggplot\n(\ndiamonds\n,\n aes\n(\nx \n=\n Carat\n,\n y \n=\n PricePerCt\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# Add geom_point() and geom_smooth() with +\n\nggplot\n(\ndiamonds\n,\n aes\n(\nx \n=\n Carat\n,\n y \n=\n PricePerCt\n))\n \n+\n\n  geom_point\n()\n \n+\n geom_smooth\n()\n\n\n\n\n\n\n\n\n\nExploring \nggplot2\n, part 5\n\n\n1\n2\n3\n# only the smooth line\n\nggplot\n(\ndiamonds\n,\n aes\n(\nx \n=\n Carat\n,\n y \n=\n PricePerCt\n))\n \n+\n\n  geom_smooth\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change col\n\nggplot\n(\ndiamonds\n,\n aes\n(\nx \n=\n Carat\n,\n y \n=\n PricePerCt\n,\n col \n=\n Clarity\n))\n \n+\n \n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change the alpha\n\nggplot\n(\ndiamonds\n,\n aes\n(\nx \n=\n Carat\n,\n y \n=\n PricePerCt\n,\n col \n=\n Clarity\n))\n \n+\n\n  geom_point\n(\nalpha \n=\n \n0.4\n)\n\n\n\n\n\n\n\n\n\nExploring \nggplot2\n, part 6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n# 2 facets for comparison\n\n\nlibrary\n(\ngridExtra\n)\n\n\ndata\n(\nfather.son\n,\n package \n=\n \n'UsingR'\n)\n\n\na \n<-\n ggplot\n(\nfather.son\n,\n aes\n(\nfheight\n,\n sheight\n))\n \n+\n\n  geom_point\n()\n \n+\n\n  geom_smooth\n(\nmethod \n=\n \n'lm'\n,\n colour \n=\n \n'red'\n)\n \n+\n\n  geom_abline\n(\nslope \n=\n \n1\n,\n intercept \n=\n \n0\n)\n\n\nb \n<-\n ggplot\n(\nfather.son\n,\n aes\n(\nfheight\n,\n sheight\n))\n \n+\n\n  geom_point\n()\n \n+\n\n  geom_smooth\n(\nmethod \n=\n \n'lm'\n,\n colour \n=\n \n'red'\n,\n se \n=\n \nFALSE\n)\n \n+\n\n  stat_smooth\n()\n\n\ngrid.arrange\n(\na\n,\n b\n,\n nrow \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# load more data\n\ndata\n(\noly12\n,\n package \n=\n \n'VGAMdata'\n)\n\n\n\n# 2 facets for comparison\n\nggplot\n(\noly12\n,\n aes\n(\nHeight\n,\n Weight\n))\n \n+\n\n  geom_point\n(\nsize \n=\n \n1\n)\n \n+\n\n  facet_wrap\n(\n~\nSex\n,\n ncol \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# create a new variable inside de data frame\n\noly12S \n<-\n \nwithin\n(\noly12\n,\n oly12\n$\nSport \n<-\n \nabbreviate\n(\noly12\n$\nSport\n,\n \n12\n))\n\n\n\n# multiple facets or splom\n\nggplot\n(\noly12S\n,\n aes\n(\nHeight\n,\n Weight\n))\n \n+\n\n  geom_point\n(\nsize \n=\n \n1\n)\n \n+\n\n  facet_wrap\n(\n~\nSport\n)\n \n+\n\n  ggtitle\n(\n'Weight and Height by Sport'\n)\n \n\n\n\n\n\n\n\n\nUnderstanding the grammar, part 1\n\n\n1\n2\n3\n4\n5\n6\n# create the object containing the data and aes layers\n\ndia_plot \n<-\n ggplot\n(\ndiamonds\n,\n aes\n(\nx \n=\n Carat\n,\n y \n=\n PricePerCt\n))\n\n\n\n# add a geom layer\n\ndia_plot \n+\n \n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# add the same geom layer, but with aes() inside\n\ndia_plot \n+\n\n  geom_point\n(\naes\n(\ncol \n=\n Clarity\n))\n\n\n\n\n\n\n\n\n\nUnderstanding the grammar, part 2\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nset.seed\n(\n1\n)\n\n\n\n# create the object containing the data and aes layers\n\ndia_plot \n<-\n ggplot\n(\ndiamonds\n,\n aes\n(\nx \n=\n Carat\n,\n y \n=\n PricePerCt\n))\n\n\n\n# add geom_point() with alpha set to 0.2\n\ndia_plot \n<-\n dia_plot \n+\n\n  geom_point\n(\nalpha \n=\n \n0.2\n)\n\n\ndia_plot\n\n\n\n\n\n\n\n\n1\n2\n3\n# plot dia_plot with additional geom_smooth() with se set to FALSE\n\ndia_plot \n+\n\n  geom_smooth\n(\nse \n=\n \nFALSE\n)\n\n\n\n\n\n\n\n\n\nData\n\u00b6\n\n\nBase package and \nggplot2\n, part 1 - plot\n\n\n1\n2\n# basic plot\n\nplot\n(\nmtcars\n$\nwt\n,\n mtcars\n$\nmpg\n,\n col \n=\n mtcars\n$\ncyl\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# change cyl inside mtcars to a factor\n\nmtcars\n$\ncyl \n<-\n \nas.factor\n(\nmtcars\n$\ncyl\n)\n\n\n\n# make the same plot as in the first instruction\n\nplot\n(\nmtcars\n$\nwt\n,\n mtcars\n$\nmpg\n,\n col \n=\n mtcars\n$\ncyl\n)\n\n\n\n\n\n\n\n\n\nBase package and \nggplot2\n, part 2 - lm\n\n\ntransfer to other\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Basic plot\n\nmtcars\n$\ncyl \n<-\n \nas.factor\n(\nmtcars\n$\ncyl\n)\n\nplot\n(\nmtcars\n$\nwt\n,\n mtcars\n$\nmpg\n,\n col \n=\n mtcars\n$\ncyl\n)\n\n\n\n# use lm() to calculate a linear model and save it as carModel\n\ncarModel \n<-\n lm\n(\nmpg \n~\n wt\n,\n data \n=\n mtcars\n)\n\n\n\n# Call abline() with carModel as first argument and lty as second\n\nabline\n(\ncarModel\n,\n lty \n=\n \n2\n)\n\n\n\n# plot each subset efficiently with lapply\n\n\nlapply\n(\nmtcars\n$\ncyl\n,\n \nfunction\n(\nx\n)\n \n{\n\n  abline\n(\nlm\n(\nmpg \n~\n wt\n,\n mtcars\n,\n subset \n=\n \n(\ncyl \n==\n x\n)),\n col \n=\n x\n)\n\n  \n})\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL\n## \n## [[8]]\n## NULL\n## \n## [[9]]\n## NULL\n## \n## [[10]]\n## NULL\n## \n## [[11]]\n## NULL\n## \n## [[12]]\n## NULL\n## \n## [[13]]\n## NULL\n## \n## [[14]]\n## NULL\n## \n## [[15]]\n## NULL\n## \n## [[16]]\n## NULL\n## \n## [[17]]\n## NULL\n## \n## [[18]]\n## NULL\n## \n## [[19]]\n## NULL\n## \n## [[20]]\n## NULL\n## \n## [[21]]\n## NULL\n## \n## [[22]]\n## NULL\n## \n## [[23]]\n## NULL\n## \n## [[24]]\n## NULL\n## \n## [[25]]\n## NULL\n## \n## [[26]]\n## NULL\n## \n## [[27]]\n## NULL\n## \n## [[28]]\n## NULL\n## \n## [[29]]\n## NULL\n## \n## [[30]]\n## NULL\n## \n## [[31]]\n## NULL\n## \n## [[32]]\n## NULL\n\n\n\n\n\n\n1\n2\n# draw the legend of the plot\n\nlegend\n(\nx \n=\n \n5\n,\n y \n=\n \n33\n,\n legend \n=\n \nlevels\n(\nmtcars\n$\ncyl\n),\n col \n=\n \n1\n:\n3\n,\n pch \n=\n \n1\n,\n bty \n=\n \n'n'\n)\n\n\n\n\n\n\n\n\n\nBase package and \nggplot2\n, part 3\n\n\n1\n2\n3\n# scatter plot\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl\n))\n \n+\n \n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# include the lines of the linear models, per cyl\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl\n))\n \n+\n\n  geom_point\n()\n \n+\n\n  geom_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# include a lm for the entire dataset in its whole\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl\n))\n \n+\n\n  geom_point\n()\n \n+\n\n  geom_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n \n+\n\n  geom_smooth\n(\naes\n(\ngroup \n=\n \n1\n),\n method \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n,\n linetype \n=\n \n2\n)\n\n\n\n\n\n\n\n\n\nVariables to visuals, part 1\n\n\n1\n2\n3\n4\n5\n6\n7\niris.tidy \n<-\n iris \n%>%\n\n  gather\n(\nkey\n,\n Value\n,\n \n-\nSpecies\n)\n \n%>%\n\n  separate\n(\nkey\n,\n \nc\n(\n'Part'\n,\n \n'Measure'\n),\n \n'\\\\.'\n)\n\n\n\n# create 2 facets\n\nggplot\n(\niris.tidy\n,\n aes\n(\nx \n=\n Species\n,\n y \n=\n Value\n,\n col \n=\n Part\n))\n \n+\n\n  geom_jitter\n()\n \n+\n facet_grid\n(\n.\n \n~\n Measure\n)\n\n\n\n\n\n\n\n\n\nVariables to visuals, part 2\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# Add a new column, Flower, to iris that contains unique ids\n\niris\n$\nFlower \n<-\n \n1\n:\nnrow\n(\niris\n)\n\n\niris.wide \n<-\n iris \n%>%\n\n  gather\n(\nkey\n,\n value\n,\n \n-\nSpecies\n,\n \n-\nFlower\n)\n \n%>%\n\n  separate\n(\nkey\n,\n \nc\n(\n'Part'\n,\n \n'Measure'\n),\n \n'\\\\.'\n)\n \n%>%\n\n  spread\n(\nMeasure\n,\n value\n)\n\n\n\n# create 3 facets\n\nggplot\n(\niris.wide\n,\n aes\n(\nx \n=\n Length\n,\n y \n=\n Width\n,\n col \n=\n Part\n))\n \n+\n \n  geom_jitter\n()\n \n+\n\n  facet_grid\n(\n.\n \n~\n Species\n)\n\n\n\n\n\n\n\n\n\nAesthetics\n\u00b6\n\n\nAll about aesthetics, part 1\n\n\n1\n2\n3\n# map cyl to y\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n mpg\n,\n y \n=\n cyl\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# map cyl to x\n\nggplot\n(\nmtcars\n,\n aes\n(\ny \n=\n mpg\n,\n x \n=\n cyl\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# map cyl to col\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change shape and size of the points\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl\n))\n \n+\n\n  geom_point\n(\nshape \n=\n \n1\n,\n size \n=\n \n4\n)\n\n\n\n\n\n\n\n\n\nAll about aesthetics, part 2\n\n\n1\n2\n3\n# map cyl to fill\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n fill \n=\n cyl\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# Change shape, size and alpha of the points in the above plot\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n fill \n=\n cyl\n))\n \n+\n\n  geom_point\n(\nshape \n=\n \n16\n,\n size \n=\n \n6\n,\n alpha \n=\n \n0.6\n)\n\n\n\n\n\n\n\n\n\nAll about aesthetics, part 3\n\n\n1\n2\n3\n# map cyl to size\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n size \n=\n cyl\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# map cyl to alpha\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n alpha \n=\n cyl\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# map cyl to shape \n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n shape \n=\n cyl\n,\n label \n=\n cyl\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# map cyl to labels\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n label \n=\n cyl\n))\n \n+\n\n  geom_text\n()\n\n\n\n\n\n\n\n\n\nAll about attributes, part 1\n\n\n1\n2\n3\n4\n5\n6\n# define a hexadecimal color\n\nmy_color \n<-\n \n'#123456'\n\n\n\n# set the color aesthetic \n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# set the color aesthetic and attribute \n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl\n))\n \n+\n\n  geom_point\n(\ncol \n=\n my_color\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# set the fill aesthetic and color, size and shape attributes\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n fill \n=\n cyl\n))\n \n+\n\n  geom_point\n(\nsize \n=\n \n10\n,\n shape \n=\n \n23\n,\n col \n=\n my_color\n)\n\n\n\n\n\n\n\n\n\nAll about attributes, part 2\n\n\n1\n2\n3\n# draw points with alpha 0.5\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n fill \n=\n cyl\n))\n \n+\n\n  geom_point\n(\nalpha \n=\n \n0.5\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# raw points with shape 24 and color yellow\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n fill \n=\n cyl\n))\n \n+\n\n  geom_point\n(\nshape \n=\n \n24\n,\n col \n=\n \n'yellow'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# draw text with label x, color red and size 10\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n fill \n=\n cyl\n))\n \n+\n\n  geom_text\n(\nlabel \n=\n \n'x'\n,\n col \n=\n \n'red'\n,\n size \n=\n \n10\n)\n\n\n\n\n\n\n\n\n\nGoing all out\n\n\n1\n2\n3\n# Map mpg onto x, qsec onto y and factor(cyl) onto col\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n mpg\n,\n y \n=\n qsec\n,\n col \n=\n \nfactor\n(\ncyl\n)))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# Add mapping: factor(am) onto shape\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n mpg\n,\n y \n=\n qsec\n,\n col \n=\n \nfactor\n(\ncyl\n),\n shape \n=\n \nfactor\n(\nam\n)))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# Add mapping: (hp/wt) onto size\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n mpg\n,\n y \n=\n qsec\n,\n col \n=\n \nfactor\n(\ncyl\n),\n shape \n=\n \nfactor\n(\nam\n),\n size \n=\n hp\n/\nwt\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# Add mapping: rownames(mtcars) onto label\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n mpg\n,\n y \n=\n qsec\n,\n col \n=\n \nfactor\n(\ncyl\n),\n shape \n=\n \nfactor\n(\nam\n),\n size \n=\n hp\n/\nwt\n))\n \n+\n\n  geom_text\n(\naes\n(\nlabel \n=\n \nrownames\n(\nmtcars\n)))\n\n\n\n\n\n\n\n\n\nPosition\n\n\n1\n2\n3\n4\n5\n6\n# base layers\n\ncyl.am \n<-\n ggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n \nfactor\n(\ncyl\n),\n fill \n=\n \nfactor\n(\nam\n)))\n\n\n\n# add geom (position = 'stack'' by default)\n\ncyl.am \n+\n \n  geom_bar\n(\nposition \n=\n \n'stack'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# show proportion\n\ncyl.am \n+\n \n  geom_bar\n(\nposition \n=\n \n'fill'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# dodging\n\ncyl.am \n+\n \n  geom_bar\n(\nposition \n=\n \n'dodge'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# clean up the axes with scale_ functions\n\nval \n=\n \nc\n(\n'#E41A1C'\n,\n \n'#377EB8'\n)\n\nlab \n=\n \nc\n(\n'Manual'\n,\n \n'Automatic'\n)\n\n\ncyl.am \n+\n geom_bar\n(\nposition \n=\n \n'dodge'\n,\n \n)\n \n+\n\n  scale_x_discrete\n(\n'Cylinders'\n)\n \n+\n\n  scale_y_continuous\n(\n'Number'\n)\n \n+\n\n  scale_fill_manual\n(\n'Transmission'\n,\n values \n=\n val\n,\n labels \n=\n lab\n)\n\n\n\n\n\n\n\n\n\nSetting a dummy aesthetic\n\n\n1\n2\n3\n4\n5\n# add a new column called group\n\nmtcars\n$\ngroup \n<-\n \n0\n\n\n\n# create jittered plot of mtcars: mpg onto x, group onto y\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n mpg\n,\n y \n=\n group\n))\n \n+\n   geom_jitter\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# change the y aesthetic limits\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n mpg\n,\n y \n=\n group\n))\n \n+\n\n  geom_jitter\n()\n \n+\n\n  scale_y_continuous\n(\nlimits \n=\n \nc\n(\n-2\n,\n \n2\n))\n\n\n\n\n\n\n\n\n\nOverplotting 1 - Point shape and transparency\n\n\n1\n2\n3\n# basic scatter plot: wt on x-axis and mpg on y-axis; map cyl to col\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl\n))\n \n+\n\n  geom_point\n(\nsize \n=\n \n4\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# hollow circles - an improvement\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl\n))\n \n+\n\n  geom_point\n(\nsize \n=\n \n4\n,\n shape \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# add transparency - very nice\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl\n))\n \n+\n\n  geom_point\n(\nsize \n=\n \n4\n,\n shape \n=\n \n1\n,\n alpha \n=\n \n0.6\n)\n\n\n\n\n\n\n\n\n\nOverplotting 2 - alpha with large datasets\n\n\n1\n2\n3\n# scatter plot: carat (x), price (y), clarity (col)\n\nggplot\n(\ndiamonds\n,\n aes\n(\nx \n=\n Carat\n,\n y \n=\n PricePerCt\n,\n col \n=\n Clarity\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# adjust for overplotting\n\nggplot\n(\ndiamonds\n,\n aes\n(\nx \n=\n Carat\n,\n y \n=\n PricePerCt\n,\n col \n=\n Clarity\n))\n \n+\n\n  geom_point\n(\nalpha \n=\n \n0.5\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# scatter plot: clarity (x), carat (y), price (col)\n\nggplot\n(\ndiamonds\n,\n aes\n(\nx \n=\n Clarity\n,\n y \n=\n Carat\n,\n col \n=\n PricePerCt\n))\n \n+\n\n  geom_point\n(\nalpha \n=\n \n0.5\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# dot plot with jittering\n\nggplot\n(\ndiamonds\n,\n aes\n(\nx \n=\n Clarity\n,\n y \n=\n Carat\n,\n col \n=\n PricePerCt\n))\n \n+\n\n  geom_point\n(\nalpha \n=\n \n0.5\n,\n position \n=\n \n'jitter'\n)\n\n\n\n\n\n\n\n\n\nGeometries\n\u00b6\n\n\nScatter plots and jittering (1)\n\n\n1\n2\n3\n# plot the cyl on the x-axis and wt on the y-axis\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n y \n=\n wt\n))\n \n+\n\n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# Use geom_jitter() instead of geom_point()\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n y \n=\n wt\n))\n \n+\n\n  geom_jitter\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# Define the position object using position_jitter(): posn.j\n\nposn.j \n<-\n  position_jitter\n(\n0.1\n)\n\n\n\n# Use posn.j in geom_point()\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n y \n=\n wt\n))\n \n+\n\n  geom_point\n(\nposition \n=\n posn.j\n)\n\n\n\n\n\n\n\n\n\nScatter plots and jittering (2)\n\n\n1\n2\n3\n# scatter plot of vocabulary (y) against education (x). Use geom_point()\n\nggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n y \n=\n vocabulary\n))\n \n+\n \n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# use geom_jitter() instead of geom_point()\n\nggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n y \n=\n vocabulary\n))\n \n+\n\n  geom_jitter\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# set alpha to a very low 0.2\n\nggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n y \n=\n vocabulary\n))\n \n+\n\n  geom_jitter\n(\nalpha \n=\n \n0.2\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# set the shape to 1\n\nggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n y \n=\n vocabulary\n))\n \n+\n\n  geom_jitter\n(\nalpha \n=\n \n0.2\n,\n shape \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\nHistograms\n\n\n1\n2\n3\n# univariate histogram\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n mpg\n))\n \n+\n\n  geom_histogram\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change the bin width to 1\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n mpg\n))\n \n+\n\n  geom_histogram\n(\nbinwidth \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change the y aesthetic to density\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n mpg\n))\n \n+\n\n  geom_histogram\n(\naes\n(\ny \n=\n \n..\ndensity..\n),\n binwidth \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# custom color code\n\nmyBlue \n<-\n \n'#377EB8'\n\n\n\n# Change the fill color to myBlue\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n mpg\n))\n \n+\n\n  geom_histogram\n(\naes\n(\ny \n=\n \n..\ndensity..\n),\n binwidth \n=\n \n1\n,\n fill \n=\n myBlue\n)\n\n\n\n\n\n\n\n\n\nPosition\n\n\n1\n2\n3\n4\n5\nmtcars\n$\nam \n<-\n \nas.factor\n(\nmtcars\n$\nam\n)\n\n\n\n# bar plot of cyl, filled according to am\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n fill \n=\n am\n))\n \n+\n\n  geom_bar\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change the position argument to stack\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n fill \n=\n am\n))\n \n+\n\n  geom_bar\n(\nposition \n=\n \n'stack'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change the position argument to fill\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n fill \n=\n am\n))\n \n+\n\n  geom_bar\n(\nposition \n=\n \n'fill'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change the position argument to dodge\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n fill \n=\n am\n))\n \n+\n\n  geom_bar\n(\nposition \n=\n \n'dodge'\n)\n\n\n\n\n\n\n\n\n\nOverlapping bar plots\n\n\n1\n2\n3\n# bar plot of cyl, filled according to am\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n fill \n=\n am\n))\n \n+\n\n  geom_bar\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change the position argument to 'dodge'\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n fill \n=\n am\n))\n \n+\n\n  geom_bar\n(\nposition \n=\n \n'dodge'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# define posn_d with position_dodge()\n\nposn_d \n<-\n position_dodge\n(\n0.2\n)\n\n\n\n# change the position argument to posn_d\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n fill \n=\n am\n))\n \n+\n\n  geom_bar\n(\nposition \n=\n posn_d\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# use posn_d as position and adjust alpha to 0.6\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n fill \n=\n am\n))\n \n+\n\n  geom_bar\n(\nposition \n=\n posn_d\n,\n alpha \n=\n \n0.6\n)\n\n\n\n\n\n\n\n\n\nOverlapping histograms\n\n\n1\n2\n3\n# histogram, add coloring defined by cyl \n\nggplot\n(\nmtcars\n,\n aes\n(\nmpg\n,\n fill \n=\n cyl\n))\n \n+\n\n  geom_histogram\n(\nbinwidth \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change position to identity \n\nggplot\n(\nmtcars\n,\n aes\n(\nmpg\n,\n fill \n=\n cyl\n))\n \n+\n\n  geom_histogram\n(\nbinwidth \n=\n \n1\n,\n position \n=\n \n'identity'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change geom to freqpoly (position is identity by default) \n\nggplot\n(\nmtcars\n,\n aes\n(\nmpg\n,\n col \n=\n cyl\n))\n \n+\n\n  geom_freqpoly\n(\nbinwidth \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\nFacets or splom histograms\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# load the package\n\n\nlibrary\n(\nreshape2\n)\n\n\n\n# load new data\n\ndata\n(\nuniranks\n,\n package \n=\n \n'GDAdata'\n)\n\n\n\n# name the variables\n\n\nnames\n(\nuniranks\n)[\nc\n(\n5\n,\n \n6\n,\n \n8\n,\n \n8\n,\n \n10\n,\n \n11\n,\n \n13\n)]\n \n<-\n \nc\n(\n'AvTeach'\n,\n \n'NSSTeach'\n,\n \n'SpendperSt'\n,\n \n'StudentStaffR'\n,\n \n'Careers'\n,\n \n'VAddScore'\n,\n \n'NSSFeedb'\n)\n\n\n\n# reshape the data frame\n\nur2 \n<-\n melt\n(\nuniranks\n[,\n \nc\n(\n3\n,\n \n5\n:\n13\n)],\n id.vars \n=\n \n'UniGroup'\n,\n variable.name \n=\n \n'uniV'\n,\n value.name \n=\n \n'uniX'\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# Splom\n\nggplot\n(\nur2\n,\n aes\n(\nuniX\n))\n \n+\n\n  geom_histogram\n()\n \n+\n\n  xlab\n(\n''\n)\n \n+\n\n  ylab\n(\n''\n)\n \n+\n\n  facet_grid\n(\nUniGroup \n~\n uniV\n,\n scales \n=\n \n'free_x'\n)\n\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nlibrary\n(\nggplot2\n)\n\n\nlibrary\n(\ngridExtra\n)\n\ndata\n(\nPima.tr2\n,\n package \n=\n \n'MASS'\n)\n\n\nh1 \n<-\n ggplot\n(\nPima.tr2\n,\n aes\n(\nglu\n))\n \n+\n geom_histogram\n()\n\nh2 \n<-\n ggplot\n(\nPima.tr2\n,\n aes\n(\nbp\n))\n \n+\n geom_histogram\n()\n\nh3 \n<-\n ggplot\n(\nPima.tr2\n,\n aes\n(\nskin\n))\n \n+\n geom_histogram\n()\n\nh4 \n<-\n ggplot\n(\nPima.tr2\n,\n aes\n(\nbmi\n))\n \n+\n geom_histogram\n()\n\nh5 \n<-\n ggplot\n(\nPima.tr2\n,\n aes\n(\nped\n))\n \n+\n geom_histogram\n()\n\nh6 \n<-\n ggplot\n(\nPima.tr2\n,\n aes\n(\nage\n))\n \n+\n geom_histogram\n()\n\n\ngrid.arrange\n(\nh1\n,\n h2\n,\n h3\n,\n h4\n,\n h5\n,\n h6\n,\n nrow \n=\n \n2\n)\n\n\n\n\n\n\n\n\n\nBar plots with color ramp, part 1\n\n\n1\n2\n3\n4\n# Example of how to use a brewed color palette\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n fill \n=\n am\n))\n \n+\n\n  geom_bar\n()\n \n+\n \n  scale_fill_brewer\n(\npalette \n=\n \n'Set1'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\nVocab\n$\neducation \n<-\n \nas.factor\n(\nVocab\n$\neducation\n)\n\nVocab\n$\nvocabulary \n<-\n \nas.factor\n(\nVocab\n$\nvocabulary\n)\n\n\n\n# Plot education on x and vocabulary on fill\n\n\n# Use the default brewed color palette\n\nggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n fill \n=\n vocabulary\n))\n \n+\n geom_bar\n(\nposition \n=\n \n'fill'\n)\n \n+\n scale_fill_brewer\n(\npalette \n=\n \n'Set3'\n)\n\n\n\n\n\n\n\n\n\nBar plots with color ramp, part 2\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Definition of a set of blue colors\n\nblues \n<-\n brewer.pal\n(\n9\n,\n \n'Blues'\n)\n\n\n\n# Make a color range using colorRampPalette() and the set of blues\n\nblue_range \n<-\n colorRampPalette\n(\nblues\n)\n\n\n\n# Use blue_range to adjust the color of the bars, use scale_fill_manual()\n\nggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n fill \n=\n vocabulary\n))\n \n+\n \n  geom_bar\n(\nposition \n=\n \n'fill'\n)\n \n+\n\n  scale_fill_manual\n(\nvalues \n=\n blue_range\n(\n11\n))\n\n\n\n\n\n\n\n\n\nOverlapping histograms (2)\n\n\n1\n2\n# histogram\n\nggplot\n(\nmtcars\n,\n aes\n(\nmpg\n))\n \n+\n geom_histogram\n(\nbinwidth \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# expand the histogram to fill using am\n\nggplot\n(\nmtcars\n,\n aes\n(\nmpg\n,\n fill \n=\n am\n))\n \n+\n\n  geom_histogram\n(\nbinwidth \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change the position argument to 'dodge'\n\nggplot\n(\nmtcars\n,\n aes\n(\nmpg\n,\n fill \n=\n am\n))\n \n+\n\n  geom_histogram\n(\nposition \n=\n \n'dodge'\n,\n binwidth \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change the position argument to 'fill'\n\nggplot\n(\nmtcars\n,\n aes\n(\nmpg\n,\n fill \n=\n am\n))\n \n+\n\n  geom_histogram\n(\nposition \n=\n \n'fill'\n,\n binwidth \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change the position argument to 'identity' and set alpha to 0.4\n\nggplot\n(\nmtcars\n,\n aes\n(\nmpg\n,\n fill \n=\n am\n))\n \n+\n\n  geom_histogram\n(\nposition \n=\n \n'identity'\n,\n binwidth \n=\n \n1\n,\n alpha \n=\n \n0.4\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change fill to cyl\n\nggplot\n(\nmtcars\n,\n aes\n(\nmpg\n,\n fill \n=\n cyl\n))\n \n+\n\n  geom_histogram\n(\nposition \n=\n \n'identity'\n,\n binwidth \n=\n \n1\n,\n alpha \n=\n \n0.4\n)\n\n\n\n\n\n\n\n\n\nLine plots\n\n\n1\n2\n3\n# plot unemploy as a function of date using a line plot\n\nggplot\n(\neconomics\n,\n aes\n(\nx \n=\n \ndate\n,\n y \n=\n unemploy\n))\n \n+\n\n  geom_line\n()\n  \n\n\n\n\n\n\n\n\n1\n2\n3\n# adjust plot to represent the fraction of total population that is unemployed\n\nggplot\n(\neconomics\n,\n aes\n(\nx \n=\n \ndate\n,\n y \n=\n unemploy\n/\npop\n))\n \n+\n\n  geom_line\n()\n\n\n\n\n\n\n\n\n\nPeriods of recession\n\n\n1\n2\n3\n4\n# draw the recess periods\n\nggplot\n(\neconomics\n,\n aes\n(\nx \n=\n \ndate\n,\n y \n=\n unemploy\n/\npop\n))\n \n+\n\n  geom_line\n()\n \n+\n\n  geom_rect\n(\ndata \n=\n recess\n,\n inherit.aes \n=\n \nFALSE\n,\n aes\n(\nxmin \n=\n begin\n,\n xmax \n=\n end\n,\n ymin \n=\n \n-\nInf\n,\n ymax \n=\n \n+\nInf\n),\n fill \n=\n \n'red'\n,\n alpha \n=\n \n0.2\n)\n\n\n\n\n\n\n\n\n\nMultiple time series, part 1\n\n\n1\n2\n# use gather to go from fish to fish.tidy.\n\nfish.tidy \n<-\n gather\n(\nfish\n,\n Species\n,\n Capture\n,\n \n-\nYear\n)\n\n\n\n\n\n\n\nMultiple time series, part 2\n\n\n1\n2\n3\n# plot\n\nggplot\n(\nfish.tidy\n,\n aes\n(\nx \n=\n Year\n,\n y \n=\n Capture\n,\n col \n=\n Species\n))\n \n+\n\n  geom_line\n()\n\n\n\n\n\n\n\n\n\nqplot\n and wrap-up\n\u00b6\n\n\nUsing \nqplot\n\n\n1\n2\n# the old way\n\nplot\n(\nmpg \n~\n wt\n,\n data \n=\n mtcars\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# using ggplot\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n))\n \n+\n\n  geom_point\n(\nshape \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# Using qplot\n\nqplot\n(\nwt\n,\n mpg\n,\n data \n=\n mtcars\n)\n\n\n\n\n\n\n\n\n\nUsing aesthetics\n\n\n1\n2\n# Categorical: cyl\n\nqplot\n(\nwt\n,\n mpg\n,\n data \n=\n mtcars\n,\n size \n=\n cyl\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# gear\n\nqplot\n(\nwt\n,\n mpg\n,\n data \n=\n mtcars\n,\n size \n=\n gear\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# Continuous: hp\n\nqplot\n(\nwt\n,\n mpg\n,\n data \n=\n mtcars\n,\n col \n=\n hp\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# qsec\n\nqplot\n(\nwt\n,\n mpg\n,\n data \n=\n mtcars\n,\n size \n=\n qsec\n)\n\n\n\n\n\n\n\n\n\nChoosing geoms, part 1\n\n\n1\n2\n# qplot() with x only\n\nqplot\n(\nfactor\n(\ncyl\n),\n data \n=\n mtcars\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# qplot() with x and y\n\nqplot\n(\nfactor\n(\ncyl\n),\n \nfactor\n(\nvs\n),\n data \n=\n mtcars\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# qplot() with geom set to jitter manually\n\nqplot\n(\nfactor\n(\ncyl\n),\n \nfactor\n(\nvs\n),\n data \n=\n mtcars\n,\n geom \n=\n \n'jitter'\n)\n\n\n\n\n\n\n\n\n\nChoosing geoms, part 2 - dotplot\n\n\n1\n2\n3\n# make a dot plot with ggplot\n\nggplot\n(\nmtcars\n,\n aes\n(\ncyl\n,\n wt\n,\n fill \n=\n am\n))\n \n+\n \n  geom_dotplot\n(\nstackdir \n=\n \n'center'\n,\n binaxis \n=\n \n'y'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# qplot with geom 'dotplot', binaxis = 'y' and stackdir = 'center'\n\nqplot\n(\nas.numeric\n(\ncyl\n),\n wt\n,\n data \n=\n mtcars\n,\n fill \n=\n am\n,\n geom \n=\n \n'dotplot'\n,\n stackdir \n=\n \n'center'\n,\n binaxis \n=\n \n'y'\n)\n\n\n\n\n\n\n\n\n\nChicken weight\n\n\n1\n2\n3\n# base\n\nggplot\n(\nChickWeight\n,\n aes\n(\nx \n=\n Time\n,\n y \n=\n weight\n))\n \n+\n\n  geom_line\n(\naes\n(\ngroup \n=\n Chick\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# color\n\nggplot\n(\nChickWeight\n,\n aes\n(\nx \n=\n Time\n,\n y \n=\n weight\n,\n col \n=\n Diet\n))\n \n+\n\n  geom_line\n(\naes\n(\ngroup \n=\n Chick\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# lines\n\nggplot\n(\nChickWeight\n,\n aes\n(\nx \n=\n Time\n,\n y \n=\n weight\n,\n col \n=\n Diet\n))\n \n+\n\n  geom_line\n(\naes\n(\ngroup \n=\n Chick\n),\n alpha \n=\n \n0.3\n)\n \n+\n\n  geom_smooth\n(\nlwd \n=\n \n2\n,\n se \n=\n \nFALSE\n)\n\n\n\n\n\n\n\n\n\nTitanic\n\n\n1\n2\n3\n# Use ggplot() for the first instruction\n\nggplot\n(\ntitanic\n,\n aes\n(\nx \n=\n \nfactor\n(\nPclass\n),\n fill \n=\n \nfactor\n(\nSex\n)))\n \n+\n\n  geom_bar\n(\nposition \n=\n \n'dodge'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Use ggplot() for the second instruction\n\nggplot\n(\ntitanic\n,\n aes\n(\nx \n=\n \nfactor\n(\nPclass\n),\n fill \n=\n \nfactor\n(\nSex\n)))\n \n+\n\n  geom_bar\n(\nposition \n=\n \n'dodge'\n)\n \n+\n\n  facet_grid\n(\n'. ~ Survived'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# position jitter\n\nposn.j \n<-\n position_jitter\n(\n0.5\n,\n \n0\n)\n\n\n\n# Use ggplot() for the last instruction\n\nggplot\n(\ntitanic\n,\n aes\n(\nx \n=\n \nfactor\n(\nPclass\n),\n y \n=\n Age\n,\n col \n=\n \nfactor\n(\nSex\n)))\n \n+\n\n  geom_jitter\n(\nsize \n=\n \n3\n,\n alpha \n=\n \n0.5\n,\n position \n=\n posn.j\n)\n \n+\n\n  facet_grid\n(\n'. ~ Survived'\n)\n\n\n\n\n\n\n\n\n\nSECTION 2\n\u00b6\n\n\nStatistics\n\u00b6\n\n\nSmoothing\n\n\n1\n2\n3\n4\n# scatter plot with LOESS smooth with a CI ribbon\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n))\n \n+\n\n  geom_point\n()\n \n+\n\n  geom_smooth\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# scatter plot with LOESS smooth without CI\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n))\n \n+\n\n  geom_point\n()\n \n+\n\n  geom_smooth\n(\nse \n=\n \nFALSE\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# scatter plot with an OLS linear model\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n))\n \n+\n\n  geom_point\n()\n \n+\n\n  geom_smooth\n(\nmethod \n=\n \n'lm'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# scatter plot with an OLS linear model without points\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n))\n \n+\n\n  geom_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n\n\n\n\n\n\n\n\n\nGrouping variables\n\n\n1\n2\n3\n4\n# cyl as a factor variable\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n \nfactor\n(\ncyl\n)))\n \n+\n\n  geom_point\n()\n \n+\n\n  stat_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# set the group aesthetic\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n \nfactor\n(\ncyl\n),\n group \n=\n \n1\n))\n \n+\n\n  geom_point\n()\n \n+\n\n  stat_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nF\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# add a second smooth layer in which the group aesthetic is set\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n \nfactor\n(\ncyl\n)))\n \n+\n\n  geom_point\n()\n \n+\n\n  stat_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n \n+\n\n  stat_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n,\n aes\n(\ngroup \n=\n \n1\n))\n\n\n\n\n\n\n\n\n\nModifying \nstat_smooth\n\n\n1\n2\n3\n4\n# change the LOESS span\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n))\n \n+\n\n  geom_point\n()\n \n+\n\n  geom_smooth\n(\nse \n=\n \nFALSE\n,\n span \n=\n \n0.7\n,\n method \n=\n \n'auto'\n)\n\n\n\n\n\n\n\n\n\n1\n# method = 'auto' is by default\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# set the model to the default LOESS and use a span of 0.7\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n \nfactor\n(\ncyl\n)))\n \n+\n\n  geom_point\n()\n \n+\n\n  stat_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n \n+\n\n  stat_smooth\n(\nmethod \n=\n \n'auto'\n,\n se \n=\n \nFALSE\n,\n aes\n(\ngroup \n=\n \n1\n),\n col \n=\n \n'black'\n,\n span \n=\n \n0.7\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# set col to 'All', inside the aes layer\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n \nfactor\n(\ncyl\n)))\n \n+\n\n  geom_point\n()\n \n+\n\n  stat_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n \n+\n\n  stat_smooth\n(\nmethod \n=\n \n'auto'\n,\n se \n=\n \nFALSE\n,\n aes\n(\ngroup \n=\n \n1\n,\n col \n=\n \n'All cyl'\n),\n span \n=\n \n0.7\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# add `scale_color_manual` to change the colors\n\nmyColors \n<-\n \nc\n(\nbrewer.pal\n(\n3\n,\n \n'Dark2'\n),\n \n'black'\n)\n\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n \nfactor\n(\ncyl\n)))\n \n+\n\n  geom_point\n()\n \n+\n\n  stat_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n \n+\n\n  stat_smooth\n(\nmethod \n=\n \n'auto'\n,\n se \n=\n \nFALSE\n,\n aes\n(\ngroup \n=\n \n1\n,\n col \n=\n \n'All cyl'\n),\n span \n=\n \n0.7\n)\n \n+\n\n  scale_color_manual\n(\n'Cylinders'\n,\n values \n=\n myColors\n)\n\n\n\n\n\n\n\n\n\nModifying \nstat_smooth\n (2)\n\n\n1\n2\n3\n4\n# jittered scatter plot, add a linear model (lm) smooth\n\nggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n y \n=\n vocabulary\n))\n \n+\n\n  geom_jitter\n(\nalpha \n=\n \n0.2\n)\n \n+\n\n  stat_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# only lm, colored by year\n\nggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n y \n=\n vocabulary\n,\n col \n=\n \nfactor\n(\nyear\n)))\n \n+\n\n  stat_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# set a color brewer palette\n\nggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n y \n=\n vocabulary\n,\n col \n=\n \nfactor\n(\nyear\n)))\n \n+\n \n  stat_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n \n+\n\n  scale_color_brewer\n(\n'Accent'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# change col and group, specify alpha, size and geom, and add scale_color_gradient\n\nggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n y \n=\n vocabulary\n,\n col \n=\n year\n,\n group \n=\n \nfactor\n(\nyear\n)))\n \n+\n \n  stat_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n,\n alpha \n=\n \n0.6\n,\n size \n=\n \n2\n,\n geom \n=\n \n'path'\n)\n \n+\n\n  scale_color_brewer\n(\n'Blues'\n)\n \n+\n\n  scale_color_gradientn\n(\ncolors \n=\n brewer.pal\n(\n9\n,\n \n'YlOrRd'\n))\n\n\n\n\n\n\n\n\n\nQuantiles\n\n\n1\n2\n3\n# use stat_quantile instead of stat_smooth\n\nggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n y \n=\n vocabulary\n,\n col \n=\n year\n,\n group \n=\n \nfactor\n(\nyear\n)))\n \n+\n  stat_quantile\n(\nalpha \n=\n \n0.6\n,\n size \n=\n \n2\n)\n \n+\n \n  scale_color_gradientn\n(\ncolors \n=\n brewer.pal\n(\n9\n,\n'YlOrRd'\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# set quantile to 0.5\n\nggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n y \n=\n vocabulary\n,\n col \n=\n year\n,\n group \n=\n \nfactor\n(\nyear\n)))\n \n+\n \n  stat_quantile\n(\nalpha \n=\n \n0.6\n,\n size \n=\n \n2\n,\n quantiles \n=\n \nc\n(\n0.5\n))\n \n+\n \n  scale_color_gradientn\n(\ncolors \n=\n brewer.pal\n(\n9\n,\n'YlOrRd'\n))\n\n\n\n\n\n\n\n\n\nSum\n\n\n1\n2\n3\n4\n5\n6\n7\n# plot with linear and loess model\n\np \n<-\n ggplot\n(\nVocab\n,\n aes\n(\nx \n=\n education\n,\n y \n=\n vocabulary\n))\n \n+\n \n  stat_smooth\n(\nmethod \n=\n \n'loess'\n,\n aes\n(\ncol \n=\n \n'red'\n),\n se \n=\n \nF\n)\n \n+\n \n  stat_smooth\n(\nmethod \n=\n \n'lm'\n,\n aes\n(\ncol \n=\n \n'blue'\n),\n se \n=\n \nF\n)\n \n+\n \n  scale_color_discrete\n(\n'Model'\n,\n labels \n=\n \nc\n(\n'red'\n \n=\n \n'LOESS'\n,\n \n'blue'\n \n=\n \n'lm'\n))\n\n\np\n\n\n\n\n\n\n\n\n1\n2\n3\n# add stat_sum (by overall proportion)\n\np \n+\n \n  stat_sum\n()\n\n\n\n\n\n\n\n\n\n1\n#aes(group = 1)\n\n\n\n\n\n\n\n1\n2\n3\n4\n# set size range\n\np \n+\n \n  stat_sum\n()\n \n+\n \n  scale_size\n(\nrange \n=\n \nc\n(\n1\n,\n10\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# proportional within years of education; set group aesthetic\n\np \n+\n \n  stat_sum\n(\naes\n(\ngroup \n=\n education\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# set the n\n\np \n+\n \n  stat_sum\n(\naes\n(\ngroup \n=\n education\n,\n size \n=\n \n..\nn..\n))\n\n\n\n\n\n\n\n\n\nPreparations\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# convert cyl and am to factors\n\nmtcars\n$\ncyl \n<-\n \nas.factor\n(\nmtcars\n$\ncyl\n)\n\nmtcars\n$\nam \n<-\n \nas.factor\n(\nmtcars\n$\nam\n)\n\n\n\n# define positions\n\nposn.d \n<-\n position_dodge\n(\nwidth \n=\n \n0.1\n)\n\nposn.jd \n<-\n position_jitterdodge\n(\njitter.width \n=\n \n0.1\n,\n dodge.width \n=\n \n0.2\n)\n\nposn.j \n<-\n position_jitter\n(\nwidth \n=\n \n0.2\n)\n\n\n\n# base layers\n\nwt.cyl.am \n<-\n ggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n y \n=\n wt\n,\n col \n=\n am\n,\n group \n=\n am\n,\n fill \n=\n am\n))\n\n\n\n\n\n\n\nPlotting variations\n\n\n1\n2\n# base layer\n\nwt.cyl.am \n<-\n ggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n  y \n=\n wt\n,\n col \n=\n am\n,\n fill \n=\n am\n,\n group \n=\n am\n))\n\n\n\n\n\n\n\n1\n2\n3\n# jittered, dodged scatter plot with transparent points\n\nwt.cyl.am \n+\n \n  geom_point\n(\nposition \n=\n posn.jd\n,\n alpha \n=\n \n0.6\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# mean and sd\n\nwt.cyl.am \n+\n\n  geom_point\n(\nposition \n=\n posn.jd\n,\n alpha \n=\n \n0.6\n)\n \n+\n stat_summary\n(\nfun.data \n=\n mean_sdl\n,\n fun.args \n=\n \nlist\n(\nmult \n=\n \n1\n),\n position \n=\n posn.d\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# mean and 95% CI\n\nwt.cyl.am \n+\n \n  geom_point\n(\nposition \n=\n posn.jd\n,\n alpha \n=\n \n0.6\n)\n \n+\n \n  stat_summary\n(\nfun.data \n=\n mean_cl_normal\n,\n position \n=\n posn.d\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# mean and SD with T-tipped error bars\n\nwt.cyl.am \n+\n \n  geom_point\n(\nposition \n=\n posn.jd\n,\n alpha \n=\n \n0.6\n)\n \n+\n \n  stat_summary\n(\ngeom \n=\n \n'point'\n,\n fun.y \n=\n \nmean\n,\n position \n=\n posn.d\n)\n \n+\n \n  stat_summary\n(\ngeom \n=\n \n'errorbar'\n,\n fun.data \n=\n mean_sdl\n,\n fun.args \n=\n \nlist\n(\nmult \n=\n \n1\n),\n width \n=\n \n0.1\n,\n position \n=\n posn.d\n)\n\n\n\n\n\n\n\n\n\nCoordinates and Facets\n\u00b6\n\n\nZooming In\n\n\n1\n2\n3\n4\n# basic\n\np \n<-\n ggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n hp\n,\n col \n=\n am\n))\n \n+\n \n  geom_point\n()\n \n+\n \n  geom_smooth\n()\n\n\n\n\n\n\n\n1\n2\n3\n# add scale_x_continuous\n\np \n+\n \n  scale_x_continuous\n(\nlimits \n=\n \nc\n(\n3\n,\n \n6\n),\n expand \n=\n \nc\n(\n0\n,\n0\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# zoom in\n\np \n+\n \n  coord_cartesian\n(\nxlim \n=\n \nc\n(\n3\n,\n \n6\n))\n\n\n\n\n\n\n\n\n\nAspect Ratio\n\n\n1\n2\n3\n4\n# scatter plot\n\nbase.plot \n<-\n ggplot\n(\niris\n,\n aes\n(\ny \n=\n Sepal.Width\n,\n x \n=\n Sepal.Length\n,\n col \n=\n Species\n))\n \n+\n \n  geom_jitter\n()\n \n+\n \n  geom_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n# default aspect ratio\n\n\n# fix aspect ratio (1:1)\n\nbase.plot \n+\n \n  coord_equal\n()\n\n\n\n\n\n\n\n\n\n1\n2\nbase.plot \n+\n \n  coord_fixed\n()\n\n\n\n\n\n\n\n\n\nPie Charts\n\n\n1\n2\n3\n4\n5\n# stacked bar plot\n\nthin.bar \n<-\n ggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n \n1\n,\n fill \n=\n cyl\n))\n \n+\n \n  geom_bar\n()\n\n\nthin.bar\n\n\n\n\n\n\n\n\n1\n2\n3\n# convert thin.bar to pie chart\n\nthin.bar \n+\n \n  coord_polar\n(\ntheta \n=\n \n'y'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# create stacked bar plot\n\nwide.bar \n<-\n ggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n \n1\n,\n fill \n=\n cyl\n))\n \n+\n \n  geom_bar\n(\nwidth \n=\n \n1\n)\n\n\nwide.bar\n\n\n\n\n\n\n\n\n1\n2\n# Convert wide.bar to pie chart\n\nwide.bar \n+\n coord_polar\n(\ntheta \n=\n \n'y'\n)\n\n\n\n\n\n\n\n\n\nFacets: the basics\n\n\n1\n2\n# scatter plot\n\np \n<-\n ggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n))\n \n+\n geom_point\n()\n\n\n\n\n\n\n\n1\n2\n3\n4\n# separate rows according am\n\n\n# facet_grid(rows ~ cols)\n\np \n+\n \n  facet_grid\n(\nam \n~\n \n.\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# separate columns according to cyl\n\n\n# facet_grid(rows ~ cols)\n\np \n+\n facet_grid\n(\n.\n \n~\n cyl\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# separate by both columns and rows \n\n\n# facet_grid(rows ~ cols)\n\np \n+\n \n  facet_grid\n(\nam \n~\n cyl\n)\n\n\n\n\n\n\n\n\n\nMany variables\n\n\n1\n2\n3\n4\n5\n# create the `cyl_am` col and `myCol` vector\n\nmtcars\n$\ncyl_am \n<-\n \npaste\n(\nmtcars\n$\ncyl\n,\n mtcars\n$\nam\n,\n sep \n=\n \n'_'\n)\n\n\nmyCol \n<-\n \nrbind\n(\nbrewer.pal\n(\n9\n,\n \n'Blues'\n)[\nc\n(\n3\n,\n6\n,\n8\n)],\n\n               brewer.pal\n(\n9\n,\n \n'Reds'\n)[\nc\n(\n3\n,\n6\n,\n8\n)])\n\n\n\n\n\n\n\n1\n2\n3\n4\n# scatter plot, add color scale\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl_am\n))\n \n+\n \n  geom_point\n()\n \n+\n \n  scale_color_manual\n(\nvalues \n=\n myCol\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# facet according on rows and columns\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl_am\n))\n \n+\n\n  geom_point\n()\n \n+\n \n  scale_color_manual\n(\nvalues \n=\n myCol\n)\n \n+\n \n  facet_grid\n(\ngear \n~\n vs\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# add more variables\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n cyl_am\n,\n size \n=\n disp\n))\n \n+\n \n  geom_point\n()\n \n+\n \n  scale_color_manual\n(\nvalues \n=\n myCol\n)\n \n+\n \n  facet_grid\n(\ngear \n~\n vs\n)\n\n\n\n\n\n\n\n\n\nDropping levels\n\n\n1\n2\n3\n# scatter plot\n\nggplot\n(\nmamsleep\n,\n aes\n(\nx \n=\n time\n,\n y \n=\n name\n,\n col \n=\n sleep\n))\n \n+\n \n  geom_point\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# facet rows according to `vore`\n\nggplot\n(\nmamsleep\n,\n aes\n(\nx \n=\n time\n,\n y \n=\n name\n,\n col \n=\n sleep\n))\n \n+\n \n  geom_point\n()\n \n+\n \n  facet_grid\n(\nvore \n~\n \n.\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# specify scale and space arguments to free up rows\n\nggplot\n(\nmamsleep\n,\n aes\n(\nx \n=\n time\n,\n y \n=\n name\n,\n col \n=\n sleep\n))\n \n+\n \n  geom_point\n()\n \n+\n \n  facet_grid\n(\nvore \n~\n \n.\n,\n scale \n=\n \n'free_y'\n,\n space \n=\n \n'free_y'\n)\n\n\n\n\n\n\n\n\n\nThemes\n\u00b6\n\n\nRectangles\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# separate columns according to cyl\n\n\n# facet_grid(rows ~ cols)\n\nmtcars\n$\ncyl \n<-\n \nc\n(\n6\n,\n \n6\n,\n \n4\n,\n \n6\n,\n \n8\n,\n \n6\n,\n \n8\n,\n \n4\n,\n \n4\n,\n \n6\n,\n \n6\n,\n \n8\n,\n \n8\n,\n \n8\n,\n \n8\n,\n \n8\n,\n \n8\n,\n \n4\n,\n \n4\n,\n \n4\n,\n \n4\n,\n \n8\n,\n \n8\n,\n \n8\n,\n \n8\n,\n \n4\n,\n \n4\n,\n \n4\n,\n \n8\n,\n \n6\n,\n \n8\n,\n \n4\n)\n\n\nmtcars\n$\nCylinders \n<-\n \nfactor\n(\nmtcars\n$\ncyl\n)\n\n\nz \n<-\n ggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n wt\n,\n y \n=\n mpg\n,\n col \n=\n Cylinders\n))\n \n+\n \n  geom_point\n(\nsize \n=\n \n2\n,\n alpha \n=\n \n0.7\n)\n \n+\n \n  facet_grid\n(\n.\n \n~\n cyl\n)\n \n+\n \n  labs\n(\nx \n=\n \n'Weight (lb/1000)'\n,\n y \n=\n \n'Miles/(US) gallon'\n)\n \n+\n \n  geom_smooth\n(\nmethod \n=\n \n'lm'\n,\n se \n=\n \nFALSE\n)\n \n+\n\n  theme_base\n()\n \n+\n\n  scale_colour_economist\n()\n\nz\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# change the plot background color to myPink (#FEE0D2)\n\nmyPink \n<-\n \n'#FEE0D2'\n\n\nz \n+\n \n  theme\n(\nplot.background \n=\n element_rect\n(\nfill \n=\n myPink\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# adjust the border to be a black line of size 3\n\nz \n+\n \n  theme\n(\nplot.background \n=\n element_rect\n(\nfill \n=\n myPink\n,\n color \n=\n \n'black'\n,\n size \n=\n \n3\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# adjust the border to be a black line of size 3\n\nz \n+\n \n  theme\n(\nplot.background \n=\n element_rect\n(\ncolor \n=\n \n'black'\n,\n size \n=\n \n3\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# set panel.background, legend.key, legend.background and strip.background to element_blank()\n\nz \n+\n \n  theme\n(\nplot.background \n=\n element_rect\n(\nfill \n=\n myPink\n,\n color \n=\n \n'black'\n,\n size \n=\n \n3\n),\n panel.background \n=\n element_blank\n(),\n legend.key \n=\n element_blank\n(),\n legend.background \n=\n element_blank\n(),\n strip.background \n=\n element_blank\n())\n\n\n\n\n\n\n\n\n\nLines\n\n\n1\n2\n3\n# Extend z with theme() and three arguments\n\nz \n+\n\n    theme\n(\npanel.grid \n=\n element_blank\n(),\n axis.line \n=\n element_line\n(\ncolor \n=\n \n'black'\n),\n axis.ticks \n=\n element_line\n(\ncolor \n=\n \n'black'\n))\n\n\n\n\n\n\n\n\n\nText\n\n\n1\n2\n3\n4\n5\n# extend z with theme() function and four arguments\n\nmyRed \n<-\n \n'#99000D'\n\n\nz \n+\n\n    theme\n(\nstrip.text \n=\n element_text\n(\nsize \n=\n \n16\n,\n color \n=\n myRed\n),\n axis.title.x \n=\n element_text\n(\ncolor \n=\n myRed\n,\n hjust \n=\n \n0\n,\n face \n=\n \n'italic'\n),\n axis.title.y \n=\n element_text\n(\ncolor \n=\n myRed\n,\n hjust \n=\n \n0\n,\n face \n=\n \n'italic'\n),\n axis.text \n=\n element_text\n(\ncolor \n=\n \n'black'\n))\n\n\n\n\n\n\n\n\n\nLegends\n\n\n1\n2\n3\n# move legend by position\n\nz \n+\n \n  theme\n(\nlegend.position \n=\n \nc\n(\n0.85\n,\n \n0.85\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change direction\n\nz \n+\n \n  theme\n(\nlegend.direction \n=\n \n'horizontal'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# change location by name\n\nz \n+\n \n  theme\n(\nlegend.position \n=\n \n'bottom'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# remove legend entirely\n\nz \n+\n \n  theme\n(\nlegend.position \n=\n \n'none'\n)\n\n\n\n\n\n\n\n\n\nPositions\n\n\n1\n2\n3\n# increase spacing between facets\n\nz \n+\n \n  theme\n(\npanel.margin.x \n=\n unit\n(\n2\n,\n \n'cm'\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# add code to remove any excess plot margin space\n\nz \n+\n \n  theme\n(\npanel.margin.x \n=\n unit\n(\n2\n,\n \n'cm'\n),\n plot.margin \n=\n unit\n(\nc\n(\n0\n,\n0\n,\n0\n,\n0\n),\n \n'cm'\n))\n\n\n\n\n\n\n\n\n\nUpdate Themestheme update\n\n\n1\n2\n# theme layer saved as an object, theme_pink\n\ntheme_pink \n<-\n theme\n(\npanel.background \n=\n element_blank\n(),\n legend.key \n=\n element_blank\n(),\n legend.background \n=\n element_blank\n(),\n strip.background \n=\n element_blank\n(),\n plot.background \n=\n element_rect\n(\nfill \n=\n myPink\n,\n color \n=\n \n'black'\n,\n size \n=\n \n3\n),\n panel.grid \n=\n element_blank\n(),\n axis.line \n=\n element_line\n(\ncolor \n=\n \n'black'\n),\n axis.ticks \n=\n element_line\n(\ncolor \n=\n \n'black'\n),\n strip.text \n=\n element_text\n(\nsize \n=\n \n16\n,\n color \n=\n myRed\n),\n axis.title.y \n=\n element_text\n(\ncolor \n=\n myRed\n,\n hjust \n=\n \n0\n,\n face \n=\n \n'italic'\n),\n axis.title.x \n=\n element_text\n(\ncolor \n=\n myRed\n,\n hjust \n=\n \n0\n,\n face \n=\n \n'italic'\n),\n axis.text \n=\n element_text\n(\ncolor \n=\n \n'black'\n),\n legend.position \n=\n \n'none'\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\nz2 \n<-\n z\n\n\n# apply theme_pink to z2\n\nz2 \n+\n \n  theme_pink\n\n\n\n\n\n\n\n\n1\n2\n# change code so that old theme is saved as old\n\nold \n<-\n theme_update\n(\npanel.background \n=\n element_blank\n(),\n legend.key \n=\n element_blank\n(),\n legend.background \n=\n element_blank\n(),\n strip.background \n=\n element_blank\n(),\n plot.background \n=\n element_rect\n(\nfill \n=\n myPink\n,\n color \n=\n \n'black'\n,\n size \n=\n \n3\n),\n panel.grid \n=\n element_blank\n(),\naxis.line \n=\n element_line\n(\ncolor \n=\n \n'black'\n),\n axis.ticks \n=\n element_line\n(\ncolor \n=\n \n'black'\n),\n strip.text \n=\n element_text\n(\nsize \n=\n \n16\n,\n color \n=\n myRed\n),\n axis.title.y \n=\n element_text\n(\ncolor \n=\n myRed\n,\n hjust \n=\n \n0\n,\n face \n=\n \n'italic'\n),\n axis.title.x \n=\n element_text\n(\ncolor \n=\n myRed\n,\n hjust \n=\n \n0\n,\n face \n=\n \n'italic'\n),\n axis.text \n=\n element_text\n(\ncolor \n=\n \n'black'\n),\n legend.position \n=\n \n'none'\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# display the plot z2\n\ntheme_set\n(\ntheme_pink\n)\n\n\nz2 \n+\n \n  theme_pink\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# restore the old plot\n\ntheme_set\n(\nold\n)\n\n\nz2\n\n\n\n\n\n\n\n\nExploring ggthemes\n\n\n1\n2\n3\n4\n5\n6\n7\n# apply theme_tufte\n\n\n# set the theme with theme_set\n\ntheme_set\n(\ntheme_tufte\n())\n\n\n\n# or apply it in the ggplot command\n\nz2 \n+\n \n  theme_tufte\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# apply theme_tufte, modified\n\n\n# set the theme with theme_set\n\ntheme_set\n(\ntheme_tufte\n()\n \n+\n \n  theme\n(\nlegend.position \n=\n \nc\n(\n0.9\n,\n \n0.9\n),\n axis.title \n=\n element_text\n(\nface \n=\n \n'italic'\n,\n size \n=\n \n12\n),\n  legend.title \n=\n element_text\n(\nface \n=\n \n'italic'\n,\n size \n=\n \n12\n)))\n\n\n\n# or apply it in the ggplot command\n\nz2 \n+\n \n  theme_tufte\n()\n \n+\n\n  theme\n(\nlegend.position \n=\n \nc\n(\n0.9\n,\n \n0.9\n),\n        axis.title \n=\n element_text\n(\nface \n=\n \n'italic'\n,\n size \n=\n \n12\n),\n legend.title \n=\n element_text\n(\nface \n=\n \n'italic'\n,\n size \n=\n \n12\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# apply theme_igray\n\n\n# set the theme with `theme_set`\n\ntheme_set\n(\ntheme_igray\n())\n\n\n\n# or apply it in the ggplot command\n\nz2 \n+\n \n  theme_igray\n()\n\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# apply `theme_igray`, modified\n\n\n# set the theme with `theme_set`\n\ntheme_set\n(\ntheme_igray\n()\n \n+\n \n  theme\n(\nlegend.position \n=\n \nc\n(\n0.9\n,\n \n0.9\n),\n legend.key \n=\n element_blank\n(),\n legend.background \n=\n element_rect\n(\nfill \n=\n \n'grey90'\n)))\n\n\nz2 \n+\n \n  \n# Or apply it in the ggplot command\n\n  theme_igray\n()\n \n+\n\n  theme\n(\nlegend.position \n=\n \nc\n(\n0.9\n,\n \n0.9\n),\n\n        legend.key \n=\n element_blank\n(),\n\n        legend.background \n=\n element_rect\n(\nfill \n=\n \n'grey90'\n))\n\n\n\n\n\n\n\n\n\nBest Practices\n\u00b6\n\n\nBar Plots (1)\n\n\n1\n2\n# base layers\n\nm \n<-\n ggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n y \n=\n wt\n))\n\n\n\n\n\n\n\n1\n2\n3\n# dynamite plot\n\nm \n+\n \n  stat_summary\n(\nfun.y \n=\n \nmean\n,\n geom \n=\n \n'bar'\n,\n fill \n=\n \n'skyblue'\n)\n \n+\n stat_summary\n(\nfun.data \n=\n mean_sdl\n,\n fun.args \n=\n \nlist\n(\nmult \n=\n \n1\n),\n geom \n=\n \n'errorbar'\n,\n width \n=\n \n0.1\n)\n\n\n\n\n\n\n\n\n\nBar Plots (2)\n\n\n1\n2\n# base layers\n\nm \n<-\n ggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\ny \n=\n wt\n,\n col \n=\n am\n,\n fill \n=\n am\n))\n\n\n\n\n\n\n\n1\n2\n3\n# dynamite plot\n\nm \n+\n \n  stat_summary\n(\nfun.y \n=\n \nmean\n,\n geom \n=\n \n'bar'\n)\n \n+\n stat_summary\n(\nfun.data \n=\n mean_sdl\n,\n fun.args \n=\n \nlist\n(\nmult \n=\n \n1\n),\n geom \n=\n \n'errorbar'\n,\n width \n=\n \n0.1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# set position dodge in each `stat` function\n\nm \n+\n \n  stat_summary\n(\nfun.y \n=\n \nmean\n,\n geom \n=\n \n'bar'\n,\n position \n=\n \n'dodge'\n)\n \n+\n stat_summary\n(\nfun.data \n=\n mean_sdl\n,\n fun.args \n=\n \nlist\n(\nmult \n=\n \n1\n),\n geom \n=\n \n'errorbar'\n,\n width \n=\n \n0.1\n,\n position \n=\n \n'dodge'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# set your dodge `posn` manually\n\nposn.d \n<-\n position_dodge\n(\n0.9\n)\n\n\n\n\n\n\n\n1\n2\n3\n# redraw dynamite plot\n\nm \n+\n \n  stat_summary\n(\nfun.y \n=\n \nmean\n,\n geom \n=\n \n'bar'\n,\n position \n=\n posn.d\n)\n \n+\n  stat_summary\n(\nfun.data \n=\n mean_sdl\n,\n fun.args \n=\n \nlist\n(\nmult \n=\n \n1\n),\n geom \n=\n \n'errorbar'\n,\n width \n=\n \n0.1\n,\n position \n=\n posn.d\n)\n\n\n\n\n\n\n\n\n\nBar Plots (3)\n\n\n1\n2\n3\n# base layers\n\nmtcars.cyl \n<-\n mtcars \n%>%\n group_by\n(\ncyl\n)\n \n%>%\n summarise\n(\nwt.avg \n=\n \nmean\n(\nwt\n))\n\nmtcars.cyl\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n## # A tibble: 3 \u00d7 2\n##     cyl   wt.avg\n##   <dbl>    <dbl>\n## 1     4 2.285727\n## 2     6 3.117143\n## 3     8 3.999214\n\n\n\n\n\n\n1\n2\nm \n<-\n ggplot\n(\nmtcars.cyl\n,\n aes\n(\nx \n=\n cyl\n,\n y \n=\n wt.avg\n))\n\nm\n\n\n\n\n\n\n\n\n1\n2\n3\n# draw bar plot\n\nm \n+\n \n  geom_bar\n(\nstat \n=\n \n'identity'\n,\n fill \n=\n \n'skyblue'\n)\n\n\n\n\n\n\n\n\n\nPie Charts (1)\n\n\n1\n2\n# bar chart to pie chart\n\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n fill \n=\n am\n))\n \n+\n geom_bar\n(\nposition \n=\n \n'fill'\n)\n\n\n\n\n\n\n\n\n\n1\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n cyl\n,\n fill \n=\n am\n))\n \n+\n geom_bar\n(\nposition \n=\n \n'fill'\n)\n \n+\n facet_grid\n(\n.\n \n~\n cyl\n)\n\n\n\n\n\n\n\n\n\n1\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n \nfactor\n(\n1\n),\n fill \n=\n am\n))\n \n+\n geom_bar\n(\nposition \n=\n \n'fill'\n)\n \n+\n facet_grid\n(\n.\n \n~\n cyl\n)\n\n\n\n\n\n\n\n\n\n1\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n \nfactor\n(\n1\n),\n fill \n=\n am\n))\n \n+\n geom_bar\n(\nposition \n=\n \n'fill'\n)\n \n+\n facet_grid\n(\n.\n \n~\n cyl\n)\n \n+\n coord_polar\n(\ntheta \n=\n \n'y'\n)\n\n\n\n\n\n\n\n\n\n1\nggplot\n(\nmtcars\n,\n aes\n(\nx \n=\n \nfactor\n(\n1\n),\n fill \n=\n am\n))\n \n+\n geom_bar\n(\nposition \n=\n \n'fill'\n,\n width \n=\n \n1\n)\n \n+\n facet_grid\n(\n.\n \n~\n cyl\n)\n \n+\n coord_polar\n(\ntheta \n=\n \n'y'\n)\n\n\n\n\n\n\n\n\n\nParallel coordinate plot\n\n\n1\n2\n3\n4\n# parallel coordinates plot using `GGally`\n\n\n# all columns except `am` (`am` column is the 9th)\n\ngroup_by_am \n<-\n \n9\n\nmy_names_am \n<-\n \n(\n1\n:\n11\n)[\n-\ngroup_by_am\n]\n\n\n\n\n\n\n\n1\n2\n# parallel plot; each variable plotted as a z-score transformation\n\nggparcoord\n(\nmtcars\n,\n columns \n=\n my_names_am\n,\n groupColumn \n=\n group_by_am\n,\n alpha \n=\n \n0.8\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# scaled between 0-1 and most discriminating variable first\n\nggparcoord\n(\nmtcars\n,\n columns \n=\n my_names_am\n,\n groupColumn \n=\n group_by_am\n,\n alpha \n=\n \n0.8\n,\n scale \n=\n \n'uniminmax'\n,\n order \n=\n \n'anyClass'\n)\n\n\n\n\n\n\n\n\n\n1\nggparcoord\n(\niris\n,\n columns \n=\n \n1\n:\n4\n,\n groupColumn \n=\n \n'Species'\n)\n \n# xlab, ylab, scale_x_discrete, them\n\n\n\n\n\n\n\n\n\n1\nggparcoord\n(\niris\n,\n columns \n=\n \n1\n:\n4\n,\n groupColumn \n=\n \n'Species'\n,\n scale \n=\n \n'uniminmax'\n)\n\n\n\n\n\n\n\n\n\n1\nggparcoord\n(\niris\n,\n columns \n=\n \n1\n:\n4\n,\n groupColumn \n=\n \n'Species'\n,\n scale \n=\n \n'globalminmax'\n)\n\n\n\n\n\n\n\n\n\n1\nggparcoord\n(\niris\n,\n columns \n=\n \n1\n:\n4\n,\n groupColumn \n=\n \n'Species'\n,\n mapping \n=\n aes\n(\nsize \n=\n \n1\n))\n\n\n\n\n\n\n\n\n\n1\nggparcoord\n(\niris\n,\n columns \n=\n \n1\n:\n4\n,\n groupColumn \n=\n \n'Species'\n,\n alphaLines \n=\n \n0.3\n)\n\n\n\n\n\n\n\n\n\n1\nggparcoord\n(\niris\n,\n columns \n=\n \n1\n:\n4\n,\n groupColumn \n=\n \n'Species'\n,\n scale \n=\n \n'center'\n)\n\n\n\n\n\n\n\n\n\n1\nggparcoord\n(\niris\n,\n columns \n=\n \n1\n:\n4\n,\n groupColumn \n=\n \n'Species'\n,\n scaleSummary \n=\n \n'median'\n,\n missing \n=\n \n'exclude'\n)\n\n\n\n\n\n\n\n\n\n1\nggparcoord\n(\niris\n,\n columns \n=\n \n1\n:\n4\n,\n groupColumn \n=\n \n'Species'\n,\n order \n=\n \n'allClass'\n)\n \n# or custom filter\n\n\n\n\n\n\n\n\n\n1\nggparcoord\n(\niris\n,\n columns \n=\n \n1\n:\n4\n,\n groupColumn \n=\n \n'Species'\n,\n scale \n=\n \n'std'\n)\n\n\n\n\n\n\n\n\n\nSplom\n\n\n1\n2\n3\n4\n5\n6\nlibrary\n(\ndplyr\n)\n\n\ndata\n(\nPima.tr2\n,\n package \n=\n \n'MASS'\n)\n\nPimaV \n<-\n select\n(\nPima.tr2\n,\n glu\n:\nage\n)\n\n\nggpairs\n(\nPimaV\n,\n diag \n=\n \nlist\n(\ncontinuous \n=\n \n'density'\n),\n axisLabels \n=\n \n'show'\n)\n\n\n\n\n\n\n\n\n\nHeat Maps\n\n\n1\n2\n# create color palette\n\nmyColors \n<-\n brewer.pal\n(\n9\n,\n \n'Reds'\n)\n\n\n\n\n\n\n\n1\n2\n3\n# heat map\n\nggplot\n(\nbarley\n,\n aes\n(\nx \n=\n year\n,\n y \n=\n variety\n,\n fill \n=\n yield\n))\n \n+\n \n  geom_tile\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# add facet_wrap(~ variable); not like facet_grid(. ~ variable)\n\nggplot\n(\nbarley\n,\n aes\n(\nx \n=\n year\n,\n y \n=\n variety\n,\n fill \n=\n yield\n))\n \n+\n \n  geom_tile\n()\n \n+\n \n  facet_wrap\n(\n \n~\n site\n,\n ncol \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# \n\nggplot\n(\nbarley\n,\n aes\n(\nx \n=\n year\n,\n y \n=\n variety\n,\n fill \n=\n yield\n))\n \n+\n geom_tile\n()\n \n+\n facet_wrap\n(\n \n~\n site\n,\n ncol \n=\n \n1\n)\n \n+\n \n  scale_fill_gradientn\n(\ncolors \n=\n myColors\n)\n\n\n\n\n\n\n\n\n\nHeat Maps Alternatives (1)\n\n\n1\n2\n3\n# line plots\n\nggplot\n(\nbarley\n,\n aes\n(\nx \n=\n year\n,\n y \n=\n yield\n,\n col \n=\n variety\n,\n group \n=\n variety\n))\n \n+\n geom_line\n()\n \n+\n \n  facet_wrap\n(\nfacets \n=\n \n~\n site\n,\n nrow \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\nHeat Maps Alternatives (2)\n\n\n1\n2\n3\n4\n# overlapping ribbon plot\n\nggplot\n(\nbarley\n,\n aes\n(\nx \n=\n year\n,\n y \n=\n yield\n,\n col \n=\n site\n,\n group \n=\n site\n,\n fill \n=\n site\n))\n \n+\n geom_line\n()\n \n+\n \n  stat_summary\n(\nfun.y \n=\n \nmean\n,\n geom \n=\n \n'line'\n)\n \n+\n \n  stat_summary\n(\nfun.data \n=\n mean_sdl\n,\n fun.args \n=\n \nlist\n(\nmult \n=\n \n1\n),\n geom \n=\n \n'ribbon'\n,\n col \n=\n \nNA\n,\n alpha \n=\n \n0.1\n)\n\n\n\n\n\n\n\n\n\nCase Study\n\u00b6\n\n\nSort and order\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# reorder\n\ndata\n(\nCars93\n,\n package \n=\n \n'MASS'\n)\n\n\nCars93 \n<-\n \nwithin\n(\nCars93\n,\n TypeWt \n<-\n reorder\n(\nType\n,\n Weight\n,\n \nmean\n))\n\n\nCars93 \n<-\n \nwithin\n(\nCars93\n,\n Type1 \n<-\n \nfactor\n(\nType\n,\n levels \n=\n \nc\n(\n'Small'\n,\n \n'Sporty'\n,\n \n'Compact'\n,\n \n'Midsize'\n,\n \n'Large'\n,\n \n'Van'\n)))\n\n\n\nwith\n(\nCars93\n,\n \ntable\n(\nTypeWt\n,\n Type1\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n##          Type1\n## TypeWt    Small Sporty Compact Midsize Large Van\n##   Small      21      0       0       0     0   0\n##   Sporty      0     14       0       0     0   0\n##   Compact     0      0      16       0     0   0\n##   Midsize     0      0       0      22     0   0\n##   Large       0      0       0       0    11   0\n##   Van         0      0       0       0     0   9\n\n\n\n\n\n\n1\n2\n3\n4\nggplot\n(\nCars93\n,\n aes\n(\nTypeWt\n,\n \n100\n/\nMPG.city\n))\n \n+\n \n  geom_boxplot\n()\n \n+\n \n  ylab\n(\n'Gallons per 100 miles'\n)\n \n+\n \n  xlab\n(\n'Car type'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nCars93 \n<-\n \nwithin\n(\nCars93\n,\n \n{\n\n  \nlevels\n(\nType1\n)\n \n<-\n \nc\n(\n'Small'\n,\n \n'Large'\n,\n \n'Midsize'\n,\n \n'Small'\n,\n \n'Sporty'\n,\n \n'Large'\n)\n\n\n})\n\n\nggplot\n(\nCars93\n,\n aes\n(\nTypeWt\n,\n \n100\n/\nMPG.city\n))\n \n+\n \n  geom_boxplot\n()\n \n+\n \n  ylab\n(\n'Gallons per 100 miles'\n)\n \n+\n \n  xlab\n(\n'Car type'\n)\n\n\n\n\n\n\n\n\n\nEnsemble plots\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nlibrary\n(\ngridExtra\n)\n\n\ndata\n(\nFertility\n,\n package \n=\n \n'AER'\n)\n\n\np0 \n<-\n ggplot\n(\nFertility\n)\n \n+\n geom_histogram\n(\nbinwidth \n=\n \n1\n)\n \n+\n ylab\n(\n''\n)\n\np1 \n<-\n p0 \n+\n aes\n(\nx \n=\n age\n)\n\np2 \n<-\n p0 \n+\n aes\n(\nx \n=\n work\n)\n \n+\n xlab\n(\n'Weeks worked in 1979'\n)\n\n\nk \n<-\n ggplot\n(\nFertility\n)\n \n+\n \n  geom_bar\n()\n \n+\n ylab\n(\n''\n)\n \n+\n \n  ylim\n(\n0\n,\n \n250000\n)\n\n\np3 \n<-\n k \n+\n aes\n(\nx \n=\n morekids\n)\n \n+\n \n  xlab\n(\n'has more children'\n)\n\np4 \n<-\n k \n+\n aes\n(\nx \n=\n gender1\n)\n \n+\n \n  xlab\n(\n'first child'\n)\n\np5 \n<-\n k \n+\n aes\n(\nx \n=\n gender2\n)\n \n+\n \n  xlab\n(\n'second child'\n)\n\np6 \n<-\n k \n+\n aes\n(\nx \n=\n afam\n)\n \n+\n \n  xlab\n(\n'African-American'\n)\n\np7 \n<-\n k \n+\n aes\n(\nx \n=\n hispanic\n)\n \n+\n \n  xlab\n(\n'Hispanic'\n)\n\np8 \n<-\n k \n+\n aes\n(\nx \n=\n other\n)\n \n+\n \n  xlab\n(\n'other race'\n)\n\n\ngrid.arrange\n(\narrangeGrob\n(\np1\n,\n p2\n,\n ncol \n=\n \n2\n,\n widths \n=\n \nc\n(\n3\n,\n \n3\n)),\n arrangeGrob\n(\np3\n,\n p4\n,\n p5\n,\n p6\n,\n p7\n,\n p8\n,\n ncol \n=\n \n6\n),\n nrow \n=\n \n2\n,\n heights \n=\n \nc\n(\n1.25\n,\n \n1\n))\n\n\n\n\n\n\n\n\n\nExploring Data\n\n\n1\n2\n3\n# histogram\n\nggplot\n(\nadult\n,\n aes\n(\nx \n=\n SRAGE_P\n))\n \n+\n \n  geom_histogram\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# histogram\n\nggplot\n(\nadult\n,\n aes\n(\nx \n=\n BMI_P\n))\n \n+\n \n  geom_histogram\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# color, default binwidth\n\nggplot\n(\nadult\n,\naes\n(\nx \n=\n SRAGE_P\n,\n fill \n=\n \nfactor\n(\nRBMI\n)))\n \n+\n \n  geom_histogram\n(\nbinwidth \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\nData Cleaning\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# remove individual aboves 84\n\nadult \n<-\n adult\n[\nadult\n$\nSRAGE_P \n<=\n \n84\n,\n \n]\n \n\n\n# remove individuals with a BMI below 16 and above or equal to 52\n\nadult \n<-\n adult\n[\nadult\n$\nBMI_P \n>=\n \n16\n \n&\n adult\n$\nBMI_P \n<\n \n52\n,\n \n]\n\n\n\n# relabel race\n\nadult\n$\nRACEHPR2 \n<-\n \nfactor\n(\nadult\n$\nRACEHPR2\n,\n labels \n=\n \nc\n(\n'Latino'\n,\n \n'Asian'\n,\n \n'African American'\n,\n \n'White'\n))\n\n\n\n# relabel the BMI categories variable\n\nadult\n$\nRBMI \n<-\n \nfactor\n(\nadult\n$\nRBMI\n,\n labels \n=\n \nc\n(\n'Under-weight'\n,\n \n'Normal-weight'\n,\n \n'Over-weight'\n,\n \n'Obese'\n))\n\n\n\n\n\n\n\nMultiple Histograms\n\n\n1\n2\n# color palette BMI_fill\n\nBMI_fill \n<-\n scale_fill_brewer\n(\n'BMI Category'\n,\n palette \n=\n \n'Reds'\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# histogram, add BMI_fill and customizations\n\nggplot\n(\nadult\n,\n aes\n(\nx \n=\n SRAGE_P\n,\n fill\n=\n \nfactor\n(\nRBMI\n)))\n \n+\n \n  geom_histogram\n(\nbinwidth \n=\n \n1\n)\n \n+\n \n  BMI_fill \n+\n facet_grid\n(\nRBMI \n~\n \n.\n)\n \n+\n \n  theme_classic\n()\n\n\n\n\n\n\n\n\n\nAlternatives\n\n\n1\n2\n3\n4\n# count histogram\n\nggplot\n(\nadult\n,\n aes\n(\nx \n=\n SRAGE_P\n,\n fill \n=\n \nfactor\n(\nRBMI\n)))\n \n+\n \n  geom_histogram\n(\nbinwidth \n=\n \n1\n)\n \n+\n\n  BMI_fill\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# density histogram\n\nggplot\n(\nadult\n,\n aes\n(\nx \n=\n SRAGE_P\n,\n fill\n=\n \nfactor\n(\nRBMI\n)))\n \n+\n \n  geom_histogram\n(\naes\n(\ny \n=\n \n..\ndensity..\n),\n binwidth \n=\n \n1\n)\n \n+\n\n  BMI_fill\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# faceted count histogram\n\nggplot\n(\nadult\n,\n aes\n(\nx \n=\n SRAGE_P\n,\n fill\n=\n \nfactor\n(\nRBMI\n)))\n \n+\n \n  geom_histogram\n(\nbinwidth \n=\n \n1\n)\n \n+\n\n  BMI_fill \n+\n facet_grid\n(\nRBMI \n~\n \n.\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# faceted density histogram\n\nggplot\n(\nadult\n,\n aes\n(\nx \n=\n SRAGE_P\n,\n fill\n=\n \nfactor\n(\nRBMI\n)))\n \n+\n \n  geom_histogram\n(\naes\n(\ny \n=\n \n..\ndensity..\n),\n binwidth \n=\n \n1\n)\n \n+\n\n  BMI_fill \n+\n facet_grid\n(\nRBMI \n~\n \n.\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# density histogram with `position = 'fill'`\n\nggplot\n(\nadult\n,\n aes \n(\nx \n=\n SRAGE_P\n,\n fill \n=\n \nfactor\n(\nRBMI\n)))\n \n+\n \n  geom_histogram\n(\naes\n(\ny \n=\n \n..\ndensity..\n),\n binwidth \n=\n \n1\n,\n position \n=\n \n'fill'\n)\n \n+\n\n  BMI_fill\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# accurate histogram\n\nggplot\n(\nadult\n,\n aes\n(\nx \n=\n SRAGE_P\n,\n fill \n=\n \nfactor\n(\nRBMI\n)))\n \n+\n \n  geom_histogram\n(\naes\n(\ny \n=\n \n..\ncount..\n/\nsum\n(\n..\ncount..\n)),\n binwidth \n=\n \n1\n,\n position \n=\n \n'fill'\n)\n \n+\n\n  BMI_fill\n\n\n\n\n\n\n\n\nDo Things Manually\n\n\n1\n2\n3\n4\n5\n# an attempt to facet the accurate frequency histogram from before (failed)\n\nggplot\n(\nadult\n,\n aes\n(\nx \n=\n SRAGE_P\n,\n fill \n=\n \nfactor\n(\nRBMI\n)))\n \n+\n \n  geom_histogram\n(\naes\n(\ny \n=\n \n..\ncount..\n/\nsum\n(\n..\ncount..\n)),\n binwidth \n=\n \n1\n,\n position \n=\n \n'fill'\n)\n \n+\n\n  BMI_fill \n+\n\n  facet_grid\n(\nRBMI \n~\n \n.\n)\n\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# create DF with `table()`\n\nDF \n<-\n \ntable\n(\nadult\n$\nRBMI\n,\n adult\n$\nSRAGE_P\n)\n\n\n\n# use apply on DF to get frequency of each group\n\nDF_freq \n<-\n \napply\n(\nDF\n,\n \n2\n,\n \nfunction\n(\nx\n)\n x\n/\nsum\n(\nx\n))\n\n\n\n# melt on DF to create DF_melted\n\nDF_melted \n<-\n melt\n(\nDF_freq\n)\n\n\n\n# change names of DF_melted\n\n\nnames\n(\nDF_melted\n)\n \n<-\n \nc\n(\n'FILL'\n,\n \n'X'\n,\n \n'value'\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# add code to make this a faceted plot\n\nggplot\n(\nDF_melted\n,\n aes\n(\nx \n=\n X\n,\n y \n=\n value\n,\n fill \n=\n FILL\n))\n \n+\n\n  geom_bar\n(\nstat \n=\n \n'identity'\n,\n position \n=\n \n'stack'\n)\n \n+\n\n  BMI_fill \n+\n \n  facet_grid\n(\nFILL \n~\n \n.\n)\n\n\n\n\n\n\n\n\n\nMerimeko/Mosaic Plot\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n# The initial contingency table\n\nDF \n<-\n \nas.data.frame.matrix\n(\ntable\n(\nadult\n$\nSRAGE_P\n,\n adult\n$\nRBMI\n))\n\n\n\n# Add the columns groupsSum, xmax and xmin. Remove groupSum again.\n\nDF\n$\ngroupSum \n<-\n \nrowSums\n(\nDF\n)\n\nDF\n$\nxmax \n<-\n \ncumsum\n(\nDF\n$\ngroupSum\n)\n\nDF\n$\nxmin \n<-\n DF\n$\nxmax \n-\n DF\n$\ngroupSum\n\n# The groupSum column needs to be removed, don't remove this line\n\nDF\n$\ngroupSum \n<-\n \nNULL\n\n\n\n# Copy row names to variable X\n\nDF\n$\nX \n<-\n \nrow.names\n(\nDF\n)\n\n\n\n# Melt the dataset\n\nDF_melted \n<-\n melt\n(\nDF\n,\n id.vars \n=\n \nc\n(\n'X'\n,\n \n'xmin'\n,\n \n'xmax'\n),\n variable.name \n=\n \n'FILL'\n)\n\n\n\n# dplyr call to calculate ymin and ymax - don't change\n\nDF_melted \n<-\n DF_melted \n%>%\n \n  group_by\n(\nX\n)\n \n%>%\n \n  mutate\n(\nymax \n=\n \ncumsum\n(\nvalue\n/\nsum\n(\nvalue\n)),\n\n         ymin \n=\n ymax \n-\n value\n/\nsum\n(\nvalue\n))\n\n\n\n# Plot rectangles - don't change.\n\nggplot\n(\nDF_melted\n,\n aes\n(\nymin \n=\n ymin\n,\n \n                 ymax \n=\n ymax\n,\n\n                 xmin \n=\n xmin\n,\n \n                 xmax \n=\n xmax\n,\n \n                 fill \n=\n FILL\n))\n \n+\n \n  geom_rect\n(\ncolour \n=\n \n'white'\n)\n \n+\n\n  scale_x_continuous\n(\nexpand \n=\n \nc\n(\n0\n,\n0\n))\n \n+\n\n  scale_y_continuous\n(\nexpand \n=\n \nc\n(\n0\n,\n0\n))\n \n+\n\n  BMI_fill \n+\n\n  theme_tufte\n()\n\n\n\n\n\n\n\n\n\nAdding statistics\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# perform chi.sq test (`RBMI` and `SRAGE_P`)\n\nresults \n<-\n chisq.test\n(\ntable\n(\nadult\n$\nRBMI\n,\n adult\n$\nSRAGE_P\n))\n\n\n\n# melt results$residuals and store as resid\n\nresid \n<-\n melt\n(\nresults\n$\nresiduals\n)\n\n\n\n# change names of resid\n\n\nnames\n(\nresid\n)\n \n<-\n \nc\n(\n'FILL'\n,\n \n'X'\n,\n \n'residual'\n)\n\n\n\n# merge the two datasets\n\nDF_all \n<-\n \nmerge\n(\nDF_melted\n,\n resid\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# update plot command\n\nggplot\n(\nDF_all\n,\n aes\n(\nymin \n=\n ymin\n,\n ymax \n=\n ymax\n,\n xmin \n=\n xmin\n,\n xmax \n=\n xmax\n,\n fill \n=\n residual\n))\n \n+\n\n  geom_rect\n()\n \n+\n\n  scale_fill_gradient2\n()\n \n+\n\n  scale_x_continuous\n(\nexpand \n=\n \nc\n(\n0\n,\n0\n))\n \n+\n\n  scale_y_continuous\n(\nexpand \n=\n \nc\n(\n0\n,\n0\n))\n \n+\n\n  theme_tufte\n()\n\n\n\n\n\n\n\n\n\nAdding text\n\n\n1\n2\n3\n4\n5\n6\n# position for labels on x axis\n\nDF_all\n$\nxtext \n<-\n DF_all\n$\nxmin \n+\n \n(\nDF_all\n$\nxmax \n-\n DF_all\n$\nxmin\n)\n \n/\n \n2\n\n\n\n# position for labels on y axis\n\nindex \n<-\n DF_all\n$\nxmax \n==\n \nmax\n(\nDF_all\n$\nxmax\n)\n\nDF_all\n$\nytext \n<-\n DF_all\n$\nymin\n[\nindex\n]\n \n+\n \n(\nDF_all\n$\nymax\n[\nindex\n]\n \n-\n DF_all\n$\nymin\n[\nindex\n])\n/\n2\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# plot\n\nggplot\n(\nDF_all\n,\n aes\n(\nymin \n=\n ymin\n,\n ymax \n=\n ymax\n,\n xmin \n=\n xmin\n,\n xmax \n=\n xmax\n,\n fill \n=\n residual\n))\n \n+\n \n  geom_rect\n(\ncol \n=\n \n'white'\n)\n \n+\n\n  \n# geom_text for ages (i.e. the x axis)\n\n  geom_text\n(\naes\n(\nx \n=\n xtext\n,\n label \n=\n X\n),\n y \n=\n \n1\n,\n size \n=\n \n3\n,\n angle \n=\n \n90\n,\n hjust \n=\n \n1\n,\n show.legend \n=\n \nFALSE\n)\n \n+\n\n  \n# geom_text for BMI (i.e. the fill axis)\n\n  geom_text\n(\naes\n(\nx \n=\n \nmax\n(\nxmax\n),\n y \n=\n ytext\n,\n label \n=\n FILL\n),\n size \n=\n \n3\n,\n hjust \n=\n \n1\n,\n show.legend  \n=\n \nFALSE\n)\n \n+\n\n  scale_fill_gradient2\n()\n \n+\n\n  theme_tufte\n()\n \n+\n\n  theme\n(\nlegend.position \n=\n \n'bottom'\n)\n\n\n\n\n\n\n\n\n\nGeneralizations\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n# script generalized into a function\n\nmosaicGG \n<-\n \nfunction\n(\ndata\n,\n X\n,\n FILL\n)\n \n{\n\n  \n# Proportions in raw data\n\n  DF \n<-\n \nas.data.frame.matrix\n(\ntable\n(\ndata\n[[\nX\n]],\n data\n[[\nFILL\n]]))\n\n  DF\n$\ngroupSum \n<-\n \nrowSums\n(\nDF\n)\n\n  DF\n$\nxmax \n<-\n \ncumsum\n(\nDF\n$\ngroupSum\n)\n\n  DF\n$\nxmin \n<-\n DF\n$\nxmax \n-\n DF\n$\ngroupSum\n  DF\n$\nX \n<-\n \nrow.names\n(\nDF\n)\n\n  DF\n$\ngroupSum \n<-\n \nNULL\n\n  DF_melted \n<-\n melt\n(\nDF\n,\n id \n=\n \nc\n(\n'X'\n,\n \n'xmin'\n,\n \n'xmax'\n),\n variable.name \n=\n \n'FILL'\n)\n\n\n  DF_melted \n<-\n DF_melted \n%>%\n \n    group_by\n(\nX\n)\n \n%>%\n \n    mutate\n(\nymax \n=\n \ncumsum\n(\nvalue\n/\nsum\n(\nvalue\n)),\n\n           ymin \n=\n ymax \n-\n value\n/\nsum\n(\nvalue\n))\n\n\n  \n# Chi-sq test\n\n  results \n<-\n chisq.test\n(\ntable\n(\ndata\n[[\nFILL\n]],\n data\n[[\nX\n]]))\n \n# fill and then x\n\n  resid \n<-\n melt\n(\nresults\n$\nresiduals\n)\n\n  \nnames\n(\nresid\n)\n \n<-\n \nc\n(\n'FILL'\n,\n \n'X'\n,\n \n'residual'\n)\n\n  \n# Merge data\n\n  DF_all \n<-\n \nmerge\n(\nDF_melted\n,\n resid\n)\n\n   \n# Positions for labels\n\n  DF_all\n$\nxtext \n<-\n DF_all\n$\nxmin \n+\n \n(\nDF_all\n$\nxmax \n-\n DF_all\n$\nxmin\n)\n/\n2\n\n  index \n<-\n DF_all\n$\nxmax \n==\n \nmax\n(\nDF_all\n$\nxmax\n)\n\n  DF_all\n$\nytext \n<-\n DF_all\n$\nymin\n[\nindex\n]\n \n+\n \n(\nDF_all\n$\nymax\n[\nindex\n]\n \n-\n DF_all\n$\nymin\n[\nindex\n])\n/\n2\n\n\n  \n# plot\n\n  g \n<-\n ggplot\n(\nDF_all\n,\n aes\n(\nymin \n=\n ymin\n,\n  ymax \n=\n ymax\n,\n xmin \n=\n xmin\n,\n \n                          xmax \n=\n xmax\n,\n fill \n=\n residual\n))\n \n+\n \n    geom_rect\n(\ncol \n=\n \n'white'\n)\n \n+\n\n    geom_text\n(\naes\n(\nx \n=\n xtext\n,\n label \n=\n X\n),\n y \n=\n \n1\n,\n size \n=\n \n3\n,\n angle \n=\n \n90\n,\n hjust \n=\n \n1\n,\n show.legend \n=\n \nFALSE\n)\n \n+\n geom_text\n(\naes\n(\nx \n=\n \nmax\n(\nxmax\n),\n  y \n=\n ytext\n,\n label \n=\n FILL\n),\n size \n=\n \n3\n,\n hjust \n=\n \n1\n,\n show.legend \n=\n \nFALSE\n)\n \n+\n\n    scale_fill_gradient2\n(\n'Residuals'\n)\n \n+\n\n    scale_x_continuous\n(\n'Individuals'\n,\n expand \n=\n \nc\n(\n0\n,\n0\n))\n \n+\n\n    scale_y_continuous\n(\n'Proportion'\n,\n expand \n=\n \nc\n(\n0\n,\n0\n))\n \n+\n\n    theme_tufte\n()\n \n+\n\n    theme\n(\nlegend.position \n=\n \n'bottom'\n)\n\n  \nprint\n(\ng\n)\n\n\n}\n\n\n\n\n\n\n\n1\n2\n# BMI described by age (in x)\n\nmosaicGG\n(\nadult\n,\n \n'SRAGE_P'\n,\n'RBMI'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# poverty described by age (in x)\n\nmosaicGG\n(\nadult\n,\n \n'SRAGE_P'\n,\n \n'POVLL'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# `am` described by `cyl` (in x)\n\nmosaicGG\n(\nmtcars\n,\n \n'cyl'\n,\n \n'am'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# `Vocab` described by education\n\nmosaicGG\n(\nVocab\n,\n \n'education'\n,\n \n'vocabulary'\n)\n\n\n\n\n\n\n\n\n\nSECTION 3\n\u00b6\n\n\nSECTION 4 - Cheat List\n\u00b6\n\n\nggplot(data, aes(x = , y = ), col = , fill = , size = , labels = , alpha\n\n= , shape = , line = , position = \u2018jitter\u2019)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n+ geom_point()\n\n+ geom_point(aes(), col = , position = posn.j)\n\n+ geom_jitter()\n\n+ facet_grid(. ~ x) # y ~ x\n\n+ scale_x_continous('Sepal Length', limits = c(2, 8), breaks = seq(2, 8, 3))\n\n+ scale_color_discrete('Species', labels = c('a', 'b', 'c'))\n\n+ labs(x = , y = , col = )\n\n\n\n\n\n\nposn.j <- position_jitter(width = 0.1)\n\n\nData\n\n\n\n\ndiamonds\n, prices of 50,000 round cut diamonds.\n\n\neconomics\n, economics_long, US economic time series.\n\n\nfaithfuld\n, 2d density estimate of Old Faithful data.\n\n\nluv_colours\n, colors().\n\n\nmidwest\n, midwest demographics.\n\n\nmpg\n, fuel economy data from 1999 and 2008 for 38 popular models of car.\n\n\nmsleep\n, an updated and expanded version of the mammals  sleep dataset.\n\n\npresidential\n, terms of 11 presidents from Eisenhower to Obama.\n\n\nseals\n, vector field of seal movements.\n\n\ntxhousing\n, Housing sales in TX.\n\n\n\n\nAesthetics\n\n\n\n\nx-axis.\n\n\ny-asix.\n\n\ncolor.\n\n\nfill.\n\n\nsize (points, lines).\n\n\nlabels.\n\n\nalpha.\n\n\nshape (points).\n\n\nlinetype (lines).\n\n\naes\n, Define aesth.etic mappings.\n\n\naes_\n (aes_q, aes_string), Define aesthetic mappings from strings, or quoted calls and formulas.\n\n\naes_all\n, Given a character vector, create a set of    identity mappings.\n\n\naes_auto\n, Automatic aesthetic mapping.\n\n\naes_colour_fill_alpha\n (color, colour, fill), Colour related aesthetics: colour, fill and alpha.\n\n\naes_group_order\n (group), Aesthetics: group. aes_linetype_size_shape (linetype, shape, size), Differentiation related aesthetics: linetype, size, shape.\n\n\naes_position\n (x, xend, xmax, xmin, y, yend, ymax, ymin), Position related aesthetics: x, y, xmin, xmax, ymin, ymax, xend, yend.\n\n\n\n\nPosition\n\n\n\n\nposition_dodge\n, Adjust position by dodging overlaps to the side.\n\n\nposition_fill\n (position_stack), Stack overlapping objects on top of one another.\n\n\nposition_identity\n, Don\u2019t adjust position\n\n\nposition_nudge\n, Nudge points.\n\n\nposition_jitter\n, Jitter points to avoid overplotting.\n\n\nposition_jitterdodge\n, Adjust position by simultaneously dodging and jittering.\n\n\n\n\nScales\n\n\n\n\nexpand_limits\n, Expand the plot limits with data.\n\n\nguides\n, Set guides for each scale.\n\n\nguide_legend\n, Legend guide.\n\n\nguide_colourbar\n (guide_colorbar), Continuous colour bar guide.\n\n\nlims\n (xlim, ylim), Convenience functions to set the axis limits.\n\n\nscale_alpha\n (scale_alpha_continuous, scale_alpha_discrete), Alpha scales.\n\n\nscale_colour_brewer\n (scale_color_brewer, scale_color_distiller, scale_colour_distiller, scale_fill_brewer, scale_fill_distiller), Sequential, diverging and qualitative colour scales from colorbrewer.org\n\n\nscale_colour_gradient\n (scale_color_continuous, scale_color_gradient, scale_color_gradient2, scale_color_gradientn, scale_colour_continuous, scale_colour_date, scale_colour_datetime, scale_colour_gradient2, scale_colour_gradientn, scale_fill_continuous, scale_fill_date, scale_fill_datetime, scale_fill_gradient, scale_fill_gradient2, scale_fill_gradientn).\n\n\nscale_colour_grey\n (scale_color_grey, scale_fill_grey), Sequential grey colour scale.\n\n\nscale_colour_hue\n (scale_color_discrete, scale_color_hue, scale_colour_discrete, scale_fill_discrete, scale_fill_hue), Qualitative colour scale with evenly spaced hues.\n\n\nscale_identity\n (scale_alpha_identity, scale_color_identity, scale_colour_identity, scale_fill_identity, scale_linetype_identity, scale_shape_identity, scale_size_identity), Use values without scaling.\n\n\nscale_manual\n (scale_alpha_manual, scale_color_manual, scale_colour_manual, scale_fill_manual, scale_linetype_manual, scale_shape_manual, scale_size_manual), Create your own discrete scale.\n\n\nscale_linetype\n (scale_linetype_continuous, scale_linetype_discrete), Scale for line patterns.\n\n\nscale_shape\n (scale_shape_continuous, scale_shape_discrete), Scale for shapes, aka glyphs.\n\n\nscale_size\n (scale_radius, scale_size_area,\n\n    scale_size_continuous, scale_size_date, scale_size_datetime, scale_size_discrete), Scale size (area or radius).\n\n\nscale_x_discrete\n (scale_y_discrete), Discrete position.\n\n\nlabs\n (ggtitle, xlab, ylab), Change axis labels and legend titles.\n\n\nupdate_labels\n, Update axis/legend labels.\n\n\n\n\nGeometries\n\n\n\n\npoint.\n\n\nline.\n\n\nhistogram.\n\n\nbar.\n\n\nboxplot.\n\n\ngeom_abline\n (geom_hline, geom_vline), Lines: horizontal,\n\n    vertical, and specified by slope and intercept.\n\n\ngeom_bar\n (stat_count), Bars, rectangles with bases on x-axis\n\n\ngeom_bin2d\n (stat_bin2d, stat_bin_2d), Add heatmap of 2d bin counts.\n\n\ngeom_blank\n, Blank, draws nothing.\n\n\ngeom_boxplot\n (stat_boxplot), Box and whiskers plot.\n\n\ngeom_contour\n (stat_contour), Display contours of a 3d surface in 2d.\n\n\ngeom_count\n(stat_sum), Count the number of observations at each location.\n\n\ngeom_crossbar\n (geom_errorbar, geom_linerange, geom_pointrange), Vertical intervals: lines, crossbars & errorbars.\n\n\ngeom_density\n (stat_density), Display a smooth density estimate.\n\n\ngeom_density_2d\n (geom_density2d, stat_density2d,    stat_density_2d), Contours from a 2d density estimate.\n\n\ngeom_dotplot\n, Dot plot\n\n\ngeom_errorbarh\n, Horizontal error bars.\n\n\ngeom_freqpoly\n (geom_histogram, stat_bin), Histograms and frequency polygons.\n\n\ngeom_hex\n (stat_bin_hex, stat_binhex), Hexagon binning.\n\n\ngeom_jitter\n, Points, jittered to reduce overplotting.\n\n\ngeom_label\n (geom_text), Textual annotations.\n\n\ngeom_map\n, Polygons from a reference map.\n\n\ngeom_path\n (geom_line, geom_step), Connect observations.\n\n\ngeom_point\n, Points, as for a scatterplot.\n\n\ngeom_polygon\n, Polygon, a filled path.\n\n\ngeom_quantile\n (stat_quantile), Add quantile lines from a quantile regression.\n\n\ngeom_raster\n (geom_rect, geom_tile), Draw rectangles.\n\n\ngeom_ribbon\n (geom_area), Ribbons and area plots.\n\n\ngeom_rug\n, Marginal rug plots.\n\n\ngeom_segment\n (geom_curve), Line segments and curves.\n\n\ngeom_smooth\n (stat_smooth), Add a smoothed conditional mean.\n\n\ngeom_violin\n (stat_ydensity), Violin plot.\n\n\n\n\nFacets\n\n\n\n\ncolumns.\n\n\nrows.\n\n\nfacet_grid\n, Lay out panels in a grid.\n\n\nfacet_null\n, Facet specification: a single panel.\n\n\nfacet_wrap\n, Wrap a 1d ribbon of panels into 2d.\n\n\nlabeller\n, Generic labeller function for facets.\n\n\nlabel_bquote\n, Backquoted labeller.\n\n\n\n\nAnnotation\n\n\n\n\nannotate\n, Create an annotation layer.\n\n\nannotation_custom\n, Annotation: Custom grob.\n\n\nannotation_logticks\n, Annotation: log tick marks.\n\n\nannotation_map\n, Annotation: maps.\n\n\nannotation_raster\n, Annotation: High-performance\n\n    rectangular tiling.\n\n\nborders\n, Create a layer of map borders.\n\n\n\n\nFortify\n\n\n\n\nfortify\n, Fortify a model with data.\n\n\nfortify-multcomp\n (fortify.cld, fortify.confint.glht, fortify.glht, fortify.summary.glht), Fortify methods for objects produced by.\n\n\nfortify.lm\n, Supplement the data fitted to a linear model with model fit statistics.\n\n\nfortify.map\n, Fortify method for map objects.\n\n\nfortify.sp\n (fortify.Line, fortify.Lines, fortify.Polygon, fortify.Polygons, fortify.SpatialLinesDataFrame, fortify.SpatialPolygons, fortify.SpatialPolygonsDataFrame), Fortify method for classes from the sp package.\n\n\nmap_data\n, Create a data frame of map data.\n\n\n\n\nStatistics\n\n\n\n\nbinning.\n\n\nsmoothing.\n\n\ndescriptive.\n\n\ninferential.\n\n\nstat_ecdf\n, Empirical Cumulative Density Function.\n\n\nstat_ellipse\n, Plot data ellipses.\n\n\nstat_function\n, Superimpose a function.\n\n\nstat_identity\n, Identity statistic.\n\n\nstat_qq\n (geom_qq), Calculation for quantile-quantile plot.\n\n\nstat_summary_2d\n (stat_summary2d, stat_summary_hex), Bin and summarise in 2d (rectangle & hexagons)\n\n\nstat_unique\n, Remove duplicates.\n\n\nCoordinates.\n\n\ncartesian.\n\n\nfixes.\n\n\npolar.\n\n\nlimites.\n\n\ncoord_cartesian\n, Cartesian coordinates.\n\n\ncoord_fixed\n (coord_equal), Cartesian coordinates with fixed\n\n    relationship between x and y scales.\n\n\ncoord_flip\n, Flipped cartesian coordinates.\n\n\ncoord_map\n (coord_quickmap), Map projections.\n\n\ncoord_polar\n, Polar coordinates.\n\n\ncoord_trans\n, Transformed cartesian coordinate system.\n\n\n\n\nThemes\n\n\n\n\ntheme_bw\n\n\ntheme_grey\n\n\ntheme_classic\n\n\ntheme_minimal\n\n\nggthemes",
            "title": "Plot Snippets - ggplot2"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#dataset",
            "text": "For most examples, we use the  mtcars ,  diamonds ,  iris ,  ChickWeight ,  recess ,  fish ,  Vocab ,  Titanic ,  mamsleep ,  barley ,  adult  datasets.",
            "title": "Dataset"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#the-ggplot2-package",
            "text": "1 library ( ggplot2 )    Import additional packages.  1\n2\n3\n4\n5\n6\n7\n8\n9 library ( digest )  library ( grid )  library ( gtable )  library ( MASS )  library ( plyr )  library ( reshape2 )  library ( scales )  library ( stats )  library ( tidyr )    For this project, import additional packages.  1\n2\n3\n4\n5\n6\n7 library ( ggthemes )  library ( RColorBrewer )  library ( gridExtra )  library ( GGally )  library ( car )  library ( Hmisc )  library ( dplyr )    Suggested additional packages\u2026",
            "title": "The ggplot2 Package"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#section-1",
            "text": "",
            "title": "SECTION 1"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#introduction",
            "text": "Exploring  ggplot2 , part 1  1\n2\n3 # basic plot \nggplot ( mtcars ,  aes ( x  =  cyl ,  y  =  mpg ))   + \n  geom_point ()     Exploring  ggplot2 , part 2  1\n2\n3 # cyl is a factor \nggplot ( mtcars ,  aes ( x  =   factor ( cyl ),  y  =  mpg ))   + \n  geom_point ()     Exploring  ggplot2 , part 3  1\n2\n3 # scatter plot \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ))   + \n  geom_point ()     1\n2\n3 # add color \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  disp ))   + \n  geom_point ()     1\n2\n3 # change size \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  size  =  disp ))   + \n  geom_point ()     Exploring  ggplot2 , part 4  1\n2\n3 # Add geom_point() with + \nggplot ( diamonds ,  aes ( x  =  Carat ,  y  =  PricePerCt ))   + \n  geom_point ()     1\n2\n3 # Add geom_point() and geom_smooth() with + \nggplot ( diamonds ,  aes ( x  =  Carat ,  y  =  PricePerCt ))   + \n  geom_point ()   +  geom_smooth ()     Exploring  ggplot2 , part 5  1\n2\n3 # only the smooth line \nggplot ( diamonds ,  aes ( x  =  Carat ,  y  =  PricePerCt ))   + \n  geom_smooth ()     1\n2\n3 # change col \nggplot ( diamonds ,  aes ( x  =  Carat ,  y  =  PricePerCt ,  col  =  Clarity ))   +  \n  geom_point ()     1\n2\n3 # change the alpha \nggplot ( diamonds ,  aes ( x  =  Carat ,  y  =  PricePerCt ,  col  =  Clarity ))   + \n  geom_point ( alpha  =   0.4 )     Exploring  ggplot2 , part 6   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 # 2 facets for comparison  library ( gridExtra ) \n\ndata ( father.son ,  package  =   'UsingR' ) \n\na  <-  ggplot ( father.son ,  aes ( fheight ,  sheight ))   + \n  geom_point ()   + \n  geom_smooth ( method  =   'lm' ,  colour  =   'red' )   + \n  geom_abline ( slope  =   1 ,  intercept  =   0 ) \n\nb  <-  ggplot ( father.son ,  aes ( fheight ,  sheight ))   + \n  geom_point ()   + \n  geom_smooth ( method  =   'lm' ,  colour  =   'red' ,  se  =   FALSE )   + \n  stat_smooth () \n\ngrid.arrange ( a ,  b ,  nrow  =   1 )     1\n2\n3\n4\n5\n6\n7 # load more data \ndata ( oly12 ,  package  =   'VGAMdata' )  # 2 facets for comparison \nggplot ( oly12 ,  aes ( Height ,  Weight ))   + \n  geom_point ( size  =   1 )   + \n  facet_wrap ( ~ Sex ,  ncol  =   1 )     1\n2\n3\n4\n5\n6\n7\n8 # create a new variable inside de data frame \noly12S  <-   within ( oly12 ,  oly12 $ Sport  <-   abbreviate ( oly12 $ Sport ,   12 ))  # multiple facets or splom \nggplot ( oly12S ,  aes ( Height ,  Weight ))   + \n  geom_point ( size  =   1 )   + \n  facet_wrap ( ~ Sport )   + \n  ggtitle ( 'Weight and Height by Sport' )      Understanding the grammar, part 1  1\n2\n3\n4\n5\n6 # create the object containing the data and aes layers \ndia_plot  <-  ggplot ( diamonds ,  aes ( x  =  Carat ,  y  =  PricePerCt ))  # add a geom layer \ndia_plot  +  \n  geom_point ()     1\n2\n3 # add the same geom layer, but with aes() inside \ndia_plot  + \n  geom_point ( aes ( col  =  Clarity ))     Understanding the grammar, part 2   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 set.seed ( 1 )  # create the object containing the data and aes layers \ndia_plot  <-  ggplot ( diamonds ,  aes ( x  =  Carat ,  y  =  PricePerCt ))  # add geom_point() with alpha set to 0.2 \ndia_plot  <-  dia_plot  + \n  geom_point ( alpha  =   0.2 ) \n\ndia_plot    1\n2\n3 # plot dia_plot with additional geom_smooth() with se set to FALSE \ndia_plot  + \n  geom_smooth ( se  =   FALSE )",
            "title": "Introduction"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#data",
            "text": "Base package and  ggplot2 , part 1 - plot  1\n2 # basic plot \nplot ( mtcars $ wt ,  mtcars $ mpg ,  col  =  mtcars $ cyl )     1\n2\n3\n4\n5 # change cyl inside mtcars to a factor \nmtcars $ cyl  <-   as.factor ( mtcars $ cyl )  # make the same plot as in the first instruction \nplot ( mtcars $ wt ,  mtcars $ mpg ,  col  =  mtcars $ cyl )     Base package and  ggplot2 , part 2 - lm  transfer to other   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Basic plot \nmtcars $ cyl  <-   as.factor ( mtcars $ cyl ) \nplot ( mtcars $ wt ,  mtcars $ mpg ,  col  =  mtcars $ cyl )  # use lm() to calculate a linear model and save it as carModel \ncarModel  <-  lm ( mpg  ~  wt ,  data  =  mtcars )  # Call abline() with carModel as first argument and lty as second \nabline ( carModel ,  lty  =   2 )  # plot each subset efficiently with lapply  lapply ( mtcars $ cyl ,   function ( x )   { \n  abline ( lm ( mpg  ~  wt ,  mtcars ,  subset  =   ( cyl  ==  x )),  col  =  x ) \n   })     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95 ## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\n## \n## [[4]]\n## NULL\n## \n## [[5]]\n## NULL\n## \n## [[6]]\n## NULL\n## \n## [[7]]\n## NULL\n## \n## [[8]]\n## NULL\n## \n## [[9]]\n## NULL\n## \n## [[10]]\n## NULL\n## \n## [[11]]\n## NULL\n## \n## [[12]]\n## NULL\n## \n## [[13]]\n## NULL\n## \n## [[14]]\n## NULL\n## \n## [[15]]\n## NULL\n## \n## [[16]]\n## NULL\n## \n## [[17]]\n## NULL\n## \n## [[18]]\n## NULL\n## \n## [[19]]\n## NULL\n## \n## [[20]]\n## NULL\n## \n## [[21]]\n## NULL\n## \n## [[22]]\n## NULL\n## \n## [[23]]\n## NULL\n## \n## [[24]]\n## NULL\n## \n## [[25]]\n## NULL\n## \n## [[26]]\n## NULL\n## \n## [[27]]\n## NULL\n## \n## [[28]]\n## NULL\n## \n## [[29]]\n## NULL\n## \n## [[30]]\n## NULL\n## \n## [[31]]\n## NULL\n## \n## [[32]]\n## NULL   1\n2 # draw the legend of the plot \nlegend ( x  =   5 ,  y  =   33 ,  legend  =   levels ( mtcars $ cyl ),  col  =   1 : 3 ,  pch  =   1 ,  bty  =   'n' )     Base package and  ggplot2 , part 3  1\n2\n3 # scatter plot \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl ))   +  \n  geom_point ()     1\n2\n3\n4 # include the lines of the linear models, per cyl \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl ))   + \n  geom_point ()   + \n  geom_smooth ( method  =   'lm' ,  se  =   FALSE )     1\n2\n3\n4\n5 # include a lm for the entire dataset in its whole \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl ))   + \n  geom_point ()   + \n  geom_smooth ( method  =   'lm' ,  se  =   FALSE )   + \n  geom_smooth ( aes ( group  =   1 ),  method  =   'lm' ,  se  =   FALSE ,  linetype  =   2 )     Variables to visuals, part 1  1\n2\n3\n4\n5\n6\n7 iris.tidy  <-  iris  %>% \n  gather ( key ,  Value ,   - Species )   %>% \n  separate ( key ,   c ( 'Part' ,   'Measure' ),   '\\\\.' )  # create 2 facets \nggplot ( iris.tidy ,  aes ( x  =  Species ,  y  =  Value ,  col  =  Part ))   + \n  geom_jitter ()   +  facet_grid ( .   ~  Measure )     Variables to visuals, part 2   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # Add a new column, Flower, to iris that contains unique ids \niris $ Flower  <-   1 : nrow ( iris ) \n\niris.wide  <-  iris  %>% \n  gather ( key ,  value ,   - Species ,   - Flower )   %>% \n  separate ( key ,   c ( 'Part' ,   'Measure' ),   '\\\\.' )   %>% \n  spread ( Measure ,  value )  # create 3 facets \nggplot ( iris.wide ,  aes ( x  =  Length ,  y  =  Width ,  col  =  Part ))   +  \n  geom_jitter ()   + \n  facet_grid ( .   ~  Species )",
            "title": "Data"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#aesthetics",
            "text": "All about aesthetics, part 1  1\n2\n3 # map cyl to y \nggplot ( mtcars ,  aes ( x  =  mpg ,  y  =  cyl ))   + \n  geom_point ()     1\n2\n3 # map cyl to x \nggplot ( mtcars ,  aes ( y  =  mpg ,  x  =  cyl ))   + \n  geom_point ()     1\n2\n3 # map cyl to col \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl ))   + \n  geom_point ()     1\n2\n3 # change shape and size of the points \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl ))   + \n  geom_point ( shape  =   1 ,  size  =   4 )     All about aesthetics, part 2  1\n2\n3 # map cyl to fill \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  fill  =  cyl ))   + \n  geom_point ()     1\n2\n3 # Change shape, size and alpha of the points in the above plot \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  fill  =  cyl ))   + \n  geom_point ( shape  =   16 ,  size  =   6 ,  alpha  =   0.6 )     All about aesthetics, part 3  1\n2\n3 # map cyl to size \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  size  =  cyl ))   + \n  geom_point ()     1\n2\n3 # map cyl to alpha \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  alpha  =  cyl ))   + \n  geom_point ()     1\n2\n3 # map cyl to shape  \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  shape  =  cyl ,  label  =  cyl ))   + \n  geom_point ()     1\n2\n3 # map cyl to labels \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  label  =  cyl ))   + \n  geom_text ()     All about attributes, part 1  1\n2\n3\n4\n5\n6 # define a hexadecimal color \nmy_color  <-   '#123456'  # set the color aesthetic  \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl ))   + \n  geom_point ()     1\n2\n3 # set the color aesthetic and attribute  \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl ))   + \n  geom_point ( col  =  my_color )     1\n2\n3 # set the fill aesthetic and color, size and shape attributes \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  fill  =  cyl ))   + \n  geom_point ( size  =   10 ,  shape  =   23 ,  col  =  my_color )     All about attributes, part 2  1\n2\n3 # draw points with alpha 0.5 \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  fill  =  cyl ))   + \n  geom_point ( alpha  =   0.5 )     1\n2\n3 # raw points with shape 24 and color yellow \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  fill  =  cyl ))   + \n  geom_point ( shape  =   24 ,  col  =   'yellow' )     1\n2\n3 # draw text with label x, color red and size 10 \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  fill  =  cyl ))   + \n  geom_text ( label  =   'x' ,  col  =   'red' ,  size  =   10 )     Going all out  1\n2\n3 # Map mpg onto x, qsec onto y and factor(cyl) onto col \nggplot ( mtcars ,  aes ( x  =  mpg ,  y  =  qsec ,  col  =   factor ( cyl )))   + \n  geom_point ()     1\n2\n3 # Add mapping: factor(am) onto shape \nggplot ( mtcars ,  aes ( x  =  mpg ,  y  =  qsec ,  col  =   factor ( cyl ),  shape  =   factor ( am )))   + \n  geom_point ()     1\n2\n3 # Add mapping: (hp/wt) onto size \nggplot ( mtcars ,  aes ( x  =  mpg ,  y  =  qsec ,  col  =   factor ( cyl ),  shape  =   factor ( am ),  size  =  hp / wt ))   + \n  geom_point ()     1\n2\n3 # Add mapping: rownames(mtcars) onto label \nggplot ( mtcars ,  aes ( x  =  mpg ,  y  =  qsec ,  col  =   factor ( cyl ),  shape  =   factor ( am ),  size  =  hp / wt ))   + \n  geom_text ( aes ( label  =   rownames ( mtcars )))     Position  1\n2\n3\n4\n5\n6 # base layers \ncyl.am  <-  ggplot ( mtcars ,  aes ( x  =   factor ( cyl ),  fill  =   factor ( am )))  # add geom (position = 'stack'' by default) \ncyl.am  +  \n  geom_bar ( position  =   'stack' )     1\n2\n3 # show proportion \ncyl.am  +  \n  geom_bar ( position  =   'fill' )     1\n2\n3 # dodging \ncyl.am  +  \n  geom_bar ( position  =   'dodge' )     1\n2\n3\n4\n5\n6\n7\n8 # clean up the axes with scale_ functions \nval  =   c ( '#E41A1C' ,   '#377EB8' ) \nlab  =   c ( 'Manual' ,   'Automatic' ) \n\ncyl.am  +  geom_bar ( position  =   'dodge' ,   )   + \n  scale_x_discrete ( 'Cylinders' )   + \n  scale_y_continuous ( 'Number' )   + \n  scale_fill_manual ( 'Transmission' ,  values  =  val ,  labels  =  lab )     Setting a dummy aesthetic  1\n2\n3\n4\n5 # add a new column called group \nmtcars $ group  <-   0  # create jittered plot of mtcars: mpg onto x, group onto y \nggplot ( mtcars ,  aes ( x  =  mpg ,  y  =  group ))   +    geom_jitter ()     1\n2\n3\n4 # change the y aesthetic limits \nggplot ( mtcars ,  aes ( x  =  mpg ,  y  =  group ))   + \n  geom_jitter ()   + \n  scale_y_continuous ( limits  =   c ( -2 ,   2 ))     Overplotting 1 - Point shape and transparency  1\n2\n3 # basic scatter plot: wt on x-axis and mpg on y-axis; map cyl to col \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl ))   + \n  geom_point ( size  =   4 )     1\n2\n3 # hollow circles - an improvement \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl ))   + \n  geom_point ( size  =   4 ,  shape  =   1 )     1\n2\n3 # add transparency - very nice \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl ))   + \n  geom_point ( size  =   4 ,  shape  =   1 ,  alpha  =   0.6 )     Overplotting 2 - alpha with large datasets  1\n2\n3 # scatter plot: carat (x), price (y), clarity (col) \nggplot ( diamonds ,  aes ( x  =  Carat ,  y  =  PricePerCt ,  col  =  Clarity ))   + \n  geom_point ()     1\n2\n3 # adjust for overplotting \nggplot ( diamonds ,  aes ( x  =  Carat ,  y  =  PricePerCt ,  col  =  Clarity ))   + \n  geom_point ( alpha  =   0.5 )     1\n2\n3 # scatter plot: clarity (x), carat (y), price (col) \nggplot ( diamonds ,  aes ( x  =  Clarity ,  y  =  Carat ,  col  =  PricePerCt ))   + \n  geom_point ( alpha  =   0.5 )     1\n2\n3 # dot plot with jittering \nggplot ( diamonds ,  aes ( x  =  Clarity ,  y  =  Carat ,  col  =  PricePerCt ))   + \n  geom_point ( alpha  =   0.5 ,  position  =   'jitter' )",
            "title": "Aesthetics"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#geometries",
            "text": "Scatter plots and jittering (1)  1\n2\n3 # plot the cyl on the x-axis and wt on the y-axis \nggplot ( mtcars ,  aes ( x  =  cyl ,  y  =  wt ))   + \n  geom_point ()     1\n2\n3 # Use geom_jitter() instead of geom_point() \nggplot ( mtcars ,  aes ( x  =  cyl ,  y  =  wt ))   + \n  geom_jitter ()     1\n2\n3\n4\n5\n6 # Define the position object using position_jitter(): posn.j \nposn.j  <-   position_jitter ( 0.1 )  # Use posn.j in geom_point() \nggplot ( mtcars ,  aes ( x  =  cyl ,  y  =  wt ))   + \n  geom_point ( position  =  posn.j )     Scatter plots and jittering (2)  1\n2\n3 # scatter plot of vocabulary (y) against education (x). Use geom_point() \nggplot ( Vocab ,  aes ( x  =  education ,  y  =  vocabulary ))   +  \n  geom_point ()     1\n2\n3 # use geom_jitter() instead of geom_point() \nggplot ( Vocab ,  aes ( x  =  education ,  y  =  vocabulary ))   + \n  geom_jitter ()     1\n2\n3 # set alpha to a very low 0.2 \nggplot ( Vocab ,  aes ( x  =  education ,  y  =  vocabulary ))   + \n  geom_jitter ( alpha  =   0.2 )     1\n2\n3 # set the shape to 1 \nggplot ( Vocab ,  aes ( x  =  education ,  y  =  vocabulary ))   + \n  geom_jitter ( alpha  =   0.2 ,  shape  =   1 )     Histograms  1\n2\n3 # univariate histogram \nggplot ( mtcars ,  aes ( x  =  mpg ))   + \n  geom_histogram ()     1\n2\n3 # change the bin width to 1 \nggplot ( mtcars ,  aes ( x  =  mpg ))   + \n  geom_histogram ( binwidth  =   1 )     1\n2\n3 # change the y aesthetic to density \nggplot ( mtcars ,  aes ( x  =  mpg ))   + \n  geom_histogram ( aes ( y  =   .. density.. ),  binwidth  =   1 )     1\n2\n3\n4\n5\n6 # custom color code \nmyBlue  <-   '#377EB8'  # Change the fill color to myBlue \nggplot ( mtcars ,  aes ( x  =  mpg ))   + \n  geom_histogram ( aes ( y  =   .. density.. ),  binwidth  =   1 ,  fill  =  myBlue )     Position  1\n2\n3\n4\n5 mtcars $ am  <-   as.factor ( mtcars $ am )  # bar plot of cyl, filled according to am \nggplot ( mtcars ,  aes ( x  =  cyl ,  fill  =  am ))   + \n  geom_bar ()     1\n2\n3 # change the position argument to stack \nggplot ( mtcars ,  aes ( x  =  cyl ,  fill  =  am ))   + \n  geom_bar ( position  =   'stack' )     1\n2\n3 # change the position argument to fill \nggplot ( mtcars ,  aes ( x  =  cyl ,  fill  =  am ))   + \n  geom_bar ( position  =   'fill' )     1\n2\n3 # change the position argument to dodge \nggplot ( mtcars ,  aes ( x  =  cyl ,  fill  =  am ))   + \n  geom_bar ( position  =   'dodge' )     Overlapping bar plots  1\n2\n3 # bar plot of cyl, filled according to am \nggplot ( mtcars ,  aes ( x  =  cyl ,  fill  =  am ))   + \n  geom_bar ()     1\n2\n3 # change the position argument to 'dodge' \nggplot ( mtcars ,  aes ( x  =  cyl ,  fill  =  am ))   + \n  geom_bar ( position  =   'dodge' )     1\n2\n3\n4\n5\n6 # define posn_d with position_dodge() \nposn_d  <-  position_dodge ( 0.2 )  # change the position argument to posn_d \nggplot ( mtcars ,  aes ( x  =  cyl ,  fill  =  am ))   + \n  geom_bar ( position  =  posn_d )     1\n2\n3 # use posn_d as position and adjust alpha to 0.6 \nggplot ( mtcars ,  aes ( x  =  cyl ,  fill  =  am ))   + \n  geom_bar ( position  =  posn_d ,  alpha  =   0.6 )     Overlapping histograms  1\n2\n3 # histogram, add coloring defined by cyl  \nggplot ( mtcars ,  aes ( mpg ,  fill  =  cyl ))   + \n  geom_histogram ( binwidth  =   1 )     1\n2\n3 # change position to identity  \nggplot ( mtcars ,  aes ( mpg ,  fill  =  cyl ))   + \n  geom_histogram ( binwidth  =   1 ,  position  =   'identity' )     1\n2\n3 # change geom to freqpoly (position is identity by default)  \nggplot ( mtcars ,  aes ( mpg ,  col  =  cyl ))   + \n  geom_freqpoly ( binwidth  =   1 )     Facets or splom histograms   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # load the package  library ( reshape2 )  # load new data \ndata ( uniranks ,  package  =   'GDAdata' )  # name the variables  names ( uniranks )[ c ( 5 ,   6 ,   8 ,   8 ,   10 ,   11 ,   13 )]   <-   c ( 'AvTeach' ,   'NSSTeach' ,   'SpendperSt' ,   'StudentStaffR' ,   'Careers' ,   'VAddScore' ,   'NSSFeedb' )  # reshape the data frame \nur2  <-  melt ( uniranks [,   c ( 3 ,   5 : 13 )],  id.vars  =   'UniGroup' ,  variable.name  =   'uniV' ,  value.name  =   'uniX' )    1\n2\n3\n4\n5\n6 # Splom \nggplot ( ur2 ,  aes ( uniX ))   + \n  geom_histogram ()   + \n  xlab ( '' )   + \n  ylab ( '' )   + \n  facet_grid ( UniGroup  ~  uniV ,  scales  =   'free_x' )      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 library ( ggplot2 )  library ( gridExtra ) \ndata ( Pima.tr2 ,  package  =   'MASS' ) \n\nh1  <-  ggplot ( Pima.tr2 ,  aes ( glu ))   +  geom_histogram () \nh2  <-  ggplot ( Pima.tr2 ,  aes ( bp ))   +  geom_histogram () \nh3  <-  ggplot ( Pima.tr2 ,  aes ( skin ))   +  geom_histogram () \nh4  <-  ggplot ( Pima.tr2 ,  aes ( bmi ))   +  geom_histogram () \nh5  <-  ggplot ( Pima.tr2 ,  aes ( ped ))   +  geom_histogram () \nh6  <-  ggplot ( Pima.tr2 ,  aes ( age ))   +  geom_histogram () \n\ngrid.arrange ( h1 ,  h2 ,  h3 ,  h4 ,  h5 ,  h6 ,  nrow  =   2 )     Bar plots with color ramp, part 1  1\n2\n3\n4 # Example of how to use a brewed color palette \nggplot ( mtcars ,  aes ( x  =  cyl ,  fill  =  am ))   + \n  geom_bar ()   +  \n  scale_fill_brewer ( palette  =   'Set1' )     1\n2\n3\n4\n5\n6 Vocab $ education  <-   as.factor ( Vocab $ education ) \nVocab $ vocabulary  <-   as.factor ( Vocab $ vocabulary )  # Plot education on x and vocabulary on fill  # Use the default brewed color palette \nggplot ( Vocab ,  aes ( x  =  education ,  fill  =  vocabulary ))   +  geom_bar ( position  =   'fill' )   +  scale_fill_brewer ( palette  =   'Set3' )     Bar plots with color ramp, part 2   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Definition of a set of blue colors \nblues  <-  brewer.pal ( 9 ,   'Blues' )  # Make a color range using colorRampPalette() and the set of blues \nblue_range  <-  colorRampPalette ( blues )  # Use blue_range to adjust the color of the bars, use scale_fill_manual() \nggplot ( Vocab ,  aes ( x  =  education ,  fill  =  vocabulary ))   +  \n  geom_bar ( position  =   'fill' )   + \n  scale_fill_manual ( values  =  blue_range ( 11 ))     Overlapping histograms (2)  1\n2 # histogram \nggplot ( mtcars ,  aes ( mpg ))   +  geom_histogram ( binwidth  =   1 )     1\n2\n3 # expand the histogram to fill using am \nggplot ( mtcars ,  aes ( mpg ,  fill  =  am ))   + \n  geom_histogram ( binwidth  =   1 )     1\n2\n3 # change the position argument to 'dodge' \nggplot ( mtcars ,  aes ( mpg ,  fill  =  am ))   + \n  geom_histogram ( position  =   'dodge' ,  binwidth  =   1 )     1\n2\n3 # change the position argument to 'fill' \nggplot ( mtcars ,  aes ( mpg ,  fill  =  am ))   + \n  geom_histogram ( position  =   'fill' ,  binwidth  =   1 )     1\n2\n3 # change the position argument to 'identity' and set alpha to 0.4 \nggplot ( mtcars ,  aes ( mpg ,  fill  =  am ))   + \n  geom_histogram ( position  =   'identity' ,  binwidth  =   1 ,  alpha  =   0.4 )     1\n2\n3 # change fill to cyl \nggplot ( mtcars ,  aes ( mpg ,  fill  =  cyl ))   + \n  geom_histogram ( position  =   'identity' ,  binwidth  =   1 ,  alpha  =   0.4 )     Line plots  1\n2\n3 # plot unemploy as a function of date using a line plot \nggplot ( economics ,  aes ( x  =   date ,  y  =  unemploy ))   + \n  geom_line ()       1\n2\n3 # adjust plot to represent the fraction of total population that is unemployed \nggplot ( economics ,  aes ( x  =   date ,  y  =  unemploy / pop ))   + \n  geom_line ()     Periods of recession  1\n2\n3\n4 # draw the recess periods \nggplot ( economics ,  aes ( x  =   date ,  y  =  unemploy / pop ))   + \n  geom_line ()   + \n  geom_rect ( data  =  recess ,  inherit.aes  =   FALSE ,  aes ( xmin  =  begin ,  xmax  =  end ,  ymin  =   - Inf ,  ymax  =   + Inf ),  fill  =   'red' ,  alpha  =   0.2 )     Multiple time series, part 1  1\n2 # use gather to go from fish to fish.tidy. \nfish.tidy  <-  gather ( fish ,  Species ,  Capture ,   - Year )    Multiple time series, part 2  1\n2\n3 # plot \nggplot ( fish.tidy ,  aes ( x  =  Year ,  y  =  Capture ,  col  =  Species ))   + \n  geom_line ()",
            "title": "Geometries"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#qplot-and-wrap-up",
            "text": "Using  qplot  1\n2 # the old way \nplot ( mpg  ~  wt ,  data  =  mtcars )     1\n2\n3 # using ggplot \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ))   + \n  geom_point ( shape  =   1 )     1\n2 # Using qplot \nqplot ( wt ,  mpg ,  data  =  mtcars )     Using aesthetics  1\n2 # Categorical: cyl \nqplot ( wt ,  mpg ,  data  =  mtcars ,  size  =  cyl )     1\n2 # gear \nqplot ( wt ,  mpg ,  data  =  mtcars ,  size  =  gear )     1\n2 # Continuous: hp \nqplot ( wt ,  mpg ,  data  =  mtcars ,  col  =  hp )     1\n2 # qsec \nqplot ( wt ,  mpg ,  data  =  mtcars ,  size  =  qsec )     Choosing geoms, part 1  1\n2 # qplot() with x only \nqplot ( factor ( cyl ),  data  =  mtcars )     1\n2 # qplot() with x and y \nqplot ( factor ( cyl ),   factor ( vs ),  data  =  mtcars )     1\n2 # qplot() with geom set to jitter manually \nqplot ( factor ( cyl ),   factor ( vs ),  data  =  mtcars ,  geom  =   'jitter' )     Choosing geoms, part 2 - dotplot  1\n2\n3 # make a dot plot with ggplot \nggplot ( mtcars ,  aes ( cyl ,  wt ,  fill  =  am ))   +  \n  geom_dotplot ( stackdir  =   'center' ,  binaxis  =   'y' )     1\n2 # qplot with geom 'dotplot', binaxis = 'y' and stackdir = 'center' \nqplot ( as.numeric ( cyl ),  wt ,  data  =  mtcars ,  fill  =  am ,  geom  =   'dotplot' ,  stackdir  =   'center' ,  binaxis  =   'y' )     Chicken weight  1\n2\n3 # base \nggplot ( ChickWeight ,  aes ( x  =  Time ,  y  =  weight ))   + \n  geom_line ( aes ( group  =  Chick ))     1\n2\n3 # color \nggplot ( ChickWeight ,  aes ( x  =  Time ,  y  =  weight ,  col  =  Diet ))   + \n  geom_line ( aes ( group  =  Chick ))     1\n2\n3\n4 # lines \nggplot ( ChickWeight ,  aes ( x  =  Time ,  y  =  weight ,  col  =  Diet ))   + \n  geom_line ( aes ( group  =  Chick ),  alpha  =   0.3 )   + \n  geom_smooth ( lwd  =   2 ,  se  =   FALSE )     Titanic  1\n2\n3 # Use ggplot() for the first instruction \nggplot ( titanic ,  aes ( x  =   factor ( Pclass ),  fill  =   factor ( Sex )))   + \n  geom_bar ( position  =   'dodge' )     1\n2\n3\n4 # Use ggplot() for the second instruction \nggplot ( titanic ,  aes ( x  =   factor ( Pclass ),  fill  =   factor ( Sex )))   + \n  geom_bar ( position  =   'dodge' )   + \n  facet_grid ( '. ~ Survived' )     1\n2\n3\n4\n5\n6\n7 # position jitter \nposn.j  <-  position_jitter ( 0.5 ,   0 )  # Use ggplot() for the last instruction \nggplot ( titanic ,  aes ( x  =   factor ( Pclass ),  y  =  Age ,  col  =   factor ( Sex )))   + \n  geom_jitter ( size  =   3 ,  alpha  =   0.5 ,  position  =  posn.j )   + \n  facet_grid ( '. ~ Survived' )",
            "title": "qplot and wrap-up"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#section-2",
            "text": "",
            "title": "SECTION 2"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#statistics",
            "text": "Smoothing  1\n2\n3\n4 # scatter plot with LOESS smooth with a CI ribbon \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ))   + \n  geom_point ()   + \n  geom_smooth ()     1\n2\n3\n4 # scatter plot with LOESS smooth without CI \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ))   + \n  geom_point ()   + \n  geom_smooth ( se  =   FALSE )     1\n2\n3\n4 # scatter plot with an OLS linear model \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ))   + \n  geom_point ()   + \n  geom_smooth ( method  =   'lm' )     1\n2\n3 # scatter plot with an OLS linear model without points \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ))   + \n  geom_smooth ( method  =   'lm' ,  se  =   FALSE )     Grouping variables  1\n2\n3\n4 # cyl as a factor variable \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =   factor ( cyl )))   + \n  geom_point ()   + \n  stat_smooth ( method  =   'lm' ,  se  =   FALSE )     1\n2\n3\n4 # set the group aesthetic \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =   factor ( cyl ),  group  =   1 ))   + \n  geom_point ()   + \n  stat_smooth ( method  =   'lm' ,  se  =   F )     1\n2\n3\n4\n5 # add a second smooth layer in which the group aesthetic is set \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =   factor ( cyl )))   + \n  geom_point ()   + \n  stat_smooth ( method  =   'lm' ,  se  =   FALSE )   + \n  stat_smooth ( method  =   'lm' ,  se  =   FALSE ,  aes ( group  =   1 ))     Modifying  stat_smooth  1\n2\n3\n4 # change the LOESS span \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ))   + \n  geom_point ()   + \n  geom_smooth ( se  =   FALSE ,  span  =   0.7 ,  method  =   'auto' )     1 # method = 'auto' is by default    1\n2\n3\n4\n5 # set the model to the default LOESS and use a span of 0.7 \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =   factor ( cyl )))   + \n  geom_point ()   + \n  stat_smooth ( method  =   'lm' ,  se  =   FALSE )   + \n  stat_smooth ( method  =   'auto' ,  se  =   FALSE ,  aes ( group  =   1 ),  col  =   'black' ,  span  =   0.7 )     1\n2\n3\n4\n5 # set col to 'All', inside the aes layer \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =   factor ( cyl )))   + \n  geom_point ()   + \n  stat_smooth ( method  =   'lm' ,  se  =   FALSE )   + \n  stat_smooth ( method  =   'auto' ,  se  =   FALSE ,  aes ( group  =   1 ,  col  =   'All cyl' ),  span  =   0.7 )     1\n2\n3\n4\n5\n6\n7\n8 # add `scale_color_manual` to change the colors \nmyColors  <-   c ( brewer.pal ( 3 ,   'Dark2' ),   'black' ) \n\nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =   factor ( cyl )))   + \n  geom_point ()   + \n  stat_smooth ( method  =   'lm' ,  se  =   FALSE )   + \n  stat_smooth ( method  =   'auto' ,  se  =   FALSE ,  aes ( group  =   1 ,  col  =   'All cyl' ),  span  =   0.7 )   + \n  scale_color_manual ( 'Cylinders' ,  values  =  myColors )     Modifying  stat_smooth  (2)  1\n2\n3\n4 # jittered scatter plot, add a linear model (lm) smooth \nggplot ( Vocab ,  aes ( x  =  education ,  y  =  vocabulary ))   + \n  geom_jitter ( alpha  =   0.2 )   + \n  stat_smooth ( method  =   'lm' ,  se  =   FALSE )     1\n2\n3 # only lm, colored by year \nggplot ( Vocab ,  aes ( x  =  education ,  y  =  vocabulary ,  col  =   factor ( year )))   + \n  stat_smooth ( method  =   'lm' ,  se  =   FALSE )     1\n2\n3\n4 # set a color brewer palette \nggplot ( Vocab ,  aes ( x  =  education ,  y  =  vocabulary ,  col  =   factor ( year )))   +  \n  stat_smooth ( method  =   'lm' ,  se  =   FALSE )   + \n  scale_color_brewer ( 'Accent' )     1\n2\n3\n4\n5 # change col and group, specify alpha, size and geom, and add scale_color_gradient \nggplot ( Vocab ,  aes ( x  =  education ,  y  =  vocabulary ,  col  =  year ,  group  =   factor ( year )))   +  \n  stat_smooth ( method  =   'lm' ,  se  =   FALSE ,  alpha  =   0.6 ,  size  =   2 ,  geom  =   'path' )   + \n  scale_color_brewer ( 'Blues' )   + \n  scale_color_gradientn ( colors  =  brewer.pal ( 9 ,   'YlOrRd' ))     Quantiles  1\n2\n3 # use stat_quantile instead of stat_smooth \nggplot ( Vocab ,  aes ( x  =  education ,  y  =  vocabulary ,  col  =  year ,  group  =   factor ( year )))   +   stat_quantile ( alpha  =   0.6 ,  size  =   2 )   +  \n  scale_color_gradientn ( colors  =  brewer.pal ( 9 , 'YlOrRd' ))     1\n2\n3\n4 # set quantile to 0.5 \nggplot ( Vocab ,  aes ( x  =  education ,  y  =  vocabulary ,  col  =  year ,  group  =   factor ( year )))   +  \n  stat_quantile ( alpha  =   0.6 ,  size  =   2 ,  quantiles  =   c ( 0.5 ))   +  \n  scale_color_gradientn ( colors  =  brewer.pal ( 9 , 'YlOrRd' ))     Sum  1\n2\n3\n4\n5\n6\n7 # plot with linear and loess model \np  <-  ggplot ( Vocab ,  aes ( x  =  education ,  y  =  vocabulary ))   +  \n  stat_smooth ( method  =   'loess' ,  aes ( col  =   'red' ),  se  =   F )   +  \n  stat_smooth ( method  =   'lm' ,  aes ( col  =   'blue' ),  se  =   F )   +  \n  scale_color_discrete ( 'Model' ,  labels  =   c ( 'red'   =   'LOESS' ,   'blue'   =   'lm' )) \n\np    1\n2\n3 # add stat_sum (by overall proportion) \np  +  \n  stat_sum ()     1 #aes(group = 1)    1\n2\n3\n4 # set size range \np  +  \n  stat_sum ()   +  \n  scale_size ( range  =   c ( 1 , 10 ))     1\n2\n3 # proportional within years of education; set group aesthetic \np  +  \n  stat_sum ( aes ( group  =  education ))     1\n2\n3 # set the n \np  +  \n  stat_sum ( aes ( group  =  education ,  size  =   .. n.. ))     Preparations   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # convert cyl and am to factors \nmtcars $ cyl  <-   as.factor ( mtcars $ cyl ) \nmtcars $ am  <-   as.factor ( mtcars $ am )  # define positions \nposn.d  <-  position_dodge ( width  =   0.1 ) \nposn.jd  <-  position_jitterdodge ( jitter.width  =   0.1 ,  dodge.width  =   0.2 ) \nposn.j  <-  position_jitter ( width  =   0.2 )  # base layers \nwt.cyl.am  <-  ggplot ( mtcars ,  aes ( x  =  cyl ,  y  =  wt ,  col  =  am ,  group  =  am ,  fill  =  am ))    Plotting variations  1\n2 # base layer \nwt.cyl.am  <-  ggplot ( mtcars ,  aes ( x  =  cyl ,   y  =  wt ,  col  =  am ,  fill  =  am ,  group  =  am ))    1\n2\n3 # jittered, dodged scatter plot with transparent points \nwt.cyl.am  +  \n  geom_point ( position  =  posn.jd ,  alpha  =   0.6 )     1\n2\n3 # mean and sd \nwt.cyl.am  + \n  geom_point ( position  =  posn.jd ,  alpha  =   0.6 )   +  stat_summary ( fun.data  =  mean_sdl ,  fun.args  =   list ( mult  =   1 ),  position  =  posn.d )     1\n2\n3\n4 # mean and 95% CI \nwt.cyl.am  +  \n  geom_point ( position  =  posn.jd ,  alpha  =   0.6 )   +  \n  stat_summary ( fun.data  =  mean_cl_normal ,  position  =  posn.d )     1\n2\n3\n4\n5 # mean and SD with T-tipped error bars \nwt.cyl.am  +  \n  geom_point ( position  =  posn.jd ,  alpha  =   0.6 )   +  \n  stat_summary ( geom  =   'point' ,  fun.y  =   mean ,  position  =  posn.d )   +  \n  stat_summary ( geom  =   'errorbar' ,  fun.data  =  mean_sdl ,  fun.args  =   list ( mult  =   1 ),  width  =   0.1 ,  position  =  posn.d )",
            "title": "Statistics"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#coordinates-and-facets",
            "text": "Zooming In  1\n2\n3\n4 # basic \np  <-  ggplot ( mtcars ,  aes ( x  =  wt ,  y  =  hp ,  col  =  am ))   +  \n  geom_point ()   +  \n  geom_smooth ()    1\n2\n3 # add scale_x_continuous \np  +  \n  scale_x_continuous ( limits  =   c ( 3 ,   6 ),  expand  =   c ( 0 , 0 ))     1\n2\n3 # zoom in \np  +  \n  coord_cartesian ( xlim  =   c ( 3 ,   6 ))     Aspect Ratio  1\n2\n3\n4 # scatter plot \nbase.plot  <-  ggplot ( iris ,  aes ( y  =  Sepal.Width ,  x  =  Sepal.Length ,  col  =  Species ))   +  \n  geom_jitter ()   +  \n  geom_smooth ( method  =   'lm' ,  se  =   FALSE )    1\n2\n3\n4 # default aspect ratio  # fix aspect ratio (1:1) \nbase.plot  +  \n  coord_equal ()     1\n2 base.plot  +  \n  coord_fixed ()     Pie Charts  1\n2\n3\n4\n5 # stacked bar plot \nthin.bar  <-  ggplot ( mtcars ,  aes ( x  =   1 ,  fill  =  cyl ))   +  \n  geom_bar () \n\nthin.bar    1\n2\n3 # convert thin.bar to pie chart \nthin.bar  +  \n  coord_polar ( theta  =   'y' )     1\n2\n3\n4\n5 # create stacked bar plot \nwide.bar  <-  ggplot ( mtcars ,  aes ( x  =   1 ,  fill  =  cyl ))   +  \n  geom_bar ( width  =   1 ) \n\nwide.bar    1\n2 # Convert wide.bar to pie chart \nwide.bar  +  coord_polar ( theta  =   'y' )     Facets: the basics  1\n2 # scatter plot \np  <-  ggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ))   +  geom_point ()    1\n2\n3\n4 # separate rows according am  # facet_grid(rows ~ cols) \np  +  \n  facet_grid ( am  ~   . )     1\n2\n3 # separate columns according to cyl  # facet_grid(rows ~ cols) \np  +  facet_grid ( .   ~  cyl )     1\n2\n3\n4 # separate by both columns and rows   # facet_grid(rows ~ cols) \np  +  \n  facet_grid ( am  ~  cyl )     Many variables  1\n2\n3\n4\n5 # create the `cyl_am` col and `myCol` vector \nmtcars $ cyl_am  <-   paste ( mtcars $ cyl ,  mtcars $ am ,  sep  =   '_' ) \n\nmyCol  <-   rbind ( brewer.pal ( 9 ,   'Blues' )[ c ( 3 , 6 , 8 )], \n               brewer.pal ( 9 ,   'Reds' )[ c ( 3 , 6 , 8 )])    1\n2\n3\n4 # scatter plot, add color scale \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl_am ))   +  \n  geom_point ()   +  \n  scale_color_manual ( values  =  myCol )     1\n2\n3\n4\n5 # facet according on rows and columns \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl_am ))   + \n  geom_point ()   +  \n  scale_color_manual ( values  =  myCol )   +  \n  facet_grid ( gear  ~  vs )     1\n2\n3\n4\n5 # add more variables \nggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  cyl_am ,  size  =  disp ))   +  \n  geom_point ()   +  \n  scale_color_manual ( values  =  myCol )   +  \n  facet_grid ( gear  ~  vs )     Dropping levels  1\n2\n3 # scatter plot \nggplot ( mamsleep ,  aes ( x  =  time ,  y  =  name ,  col  =  sleep ))   +  \n  geom_point ()     1\n2\n3\n4 # facet rows according to `vore` \nggplot ( mamsleep ,  aes ( x  =  time ,  y  =  name ,  col  =  sleep ))   +  \n  geom_point ()   +  \n  facet_grid ( vore  ~   . )     1\n2\n3\n4 # specify scale and space arguments to free up rows \nggplot ( mamsleep ,  aes ( x  =  time ,  y  =  name ,  col  =  sleep ))   +  \n  geom_point ()   +  \n  facet_grid ( vore  ~   . ,  scale  =   'free_y' ,  space  =   'free_y' )",
            "title": "Coordinates and Facets"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#themes",
            "text": "Rectangles   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # separate columns according to cyl  # facet_grid(rows ~ cols) \nmtcars $ cyl  <-   c ( 6 ,   6 ,   4 ,   6 ,   8 ,   6 ,   8 ,   4 ,   4 ,   6 ,   6 ,   8 ,   8 ,   8 ,   8 ,   8 ,   8 ,   4 ,   4 ,   4 ,   4 ,   8 ,   8 ,   8 ,   8 ,   4 ,   4 ,   4 ,   8 ,   6 ,   8 ,   4 ) \n\nmtcars $ Cylinders  <-   factor ( mtcars $ cyl ) \n\nz  <-  ggplot ( mtcars ,  aes ( x  =  wt ,  y  =  mpg ,  col  =  Cylinders ))   +  \n  geom_point ( size  =   2 ,  alpha  =   0.7 )   +  \n  facet_grid ( .   ~  cyl )   +  \n  labs ( x  =   'Weight (lb/1000)' ,  y  =   'Miles/(US) gallon' )   +  \n  geom_smooth ( method  =   'lm' ,  se  =   FALSE )   + \n  theme_base ()   + \n  scale_colour_economist () \nz    1\n2\n3\n4\n5 # change the plot background color to myPink (#FEE0D2) \nmyPink  <-   '#FEE0D2' \n\nz  +  \n  theme ( plot.background  =  element_rect ( fill  =  myPink ))     1\n2\n3 # adjust the border to be a black line of size 3 \nz  +  \n  theme ( plot.background  =  element_rect ( fill  =  myPink ,  color  =   'black' ,  size  =   3 ))     1\n2\n3 # adjust the border to be a black line of size 3 \nz  +  \n  theme ( plot.background  =  element_rect ( color  =   'black' ,  size  =   3 ))     1\n2\n3 # set panel.background, legend.key, legend.background and strip.background to element_blank() \nz  +  \n  theme ( plot.background  =  element_rect ( fill  =  myPink ,  color  =   'black' ,  size  =   3 ),  panel.background  =  element_blank (),  legend.key  =  element_blank (),  legend.background  =  element_blank (),  strip.background  =  element_blank ())     Lines  1\n2\n3 # Extend z with theme() and three arguments \nz  + \n    theme ( panel.grid  =  element_blank (),  axis.line  =  element_line ( color  =   'black' ),  axis.ticks  =  element_line ( color  =   'black' ))     Text  1\n2\n3\n4\n5 # extend z with theme() function and four arguments \nmyRed  <-   '#99000D' \n\nz  + \n    theme ( strip.text  =  element_text ( size  =   16 ,  color  =  myRed ),  axis.title.x  =  element_text ( color  =  myRed ,  hjust  =   0 ,  face  =   'italic' ),  axis.title.y  =  element_text ( color  =  myRed ,  hjust  =   0 ,  face  =   'italic' ),  axis.text  =  element_text ( color  =   'black' ))     Legends  1\n2\n3 # move legend by position \nz  +  \n  theme ( legend.position  =   c ( 0.85 ,   0.85 ))     1\n2\n3 # change direction \nz  +  \n  theme ( legend.direction  =   'horizontal' )     1\n2\n3 # change location by name \nz  +  \n  theme ( legend.position  =   'bottom' )     1\n2\n3 # remove legend entirely \nz  +  \n  theme ( legend.position  =   'none' )     Positions  1\n2\n3 # increase spacing between facets \nz  +  \n  theme ( panel.margin.x  =  unit ( 2 ,   'cm' ))     1\n2\n3 # add code to remove any excess plot margin space \nz  +  \n  theme ( panel.margin.x  =  unit ( 2 ,   'cm' ),  plot.margin  =  unit ( c ( 0 , 0 , 0 , 0 ),   'cm' ))     Update Themestheme update  1\n2 # theme layer saved as an object, theme_pink \ntheme_pink  <-  theme ( panel.background  =  element_blank (),  legend.key  =  element_blank (),  legend.background  =  element_blank (),  strip.background  =  element_blank (),  plot.background  =  element_rect ( fill  =  myPink ,  color  =   'black' ,  size  =   3 ),  panel.grid  =  element_blank (),  axis.line  =  element_line ( color  =   'black' ),  axis.ticks  =  element_line ( color  =   'black' ),  strip.text  =  element_text ( size  =   16 ,  color  =  myRed ),  axis.title.y  =  element_text ( color  =  myRed ,  hjust  =   0 ,  face  =   'italic' ),  axis.title.x  =  element_text ( color  =  myRed ,  hjust  =   0 ,  face  =   'italic' ),  axis.text  =  element_text ( color  =   'black' ),  legend.position  =   'none' )    1\n2\n3\n4\n5 z2  <-  z # apply theme_pink to z2 \nz2  +  \n  theme_pink    1\n2 # change code so that old theme is saved as old \nold  <-  theme_update ( panel.background  =  element_blank (),  legend.key  =  element_blank (),  legend.background  =  element_blank (),  strip.background  =  element_blank (),  plot.background  =  element_rect ( fill  =  myPink ,  color  =   'black' ,  size  =   3 ),  panel.grid  =  element_blank (), axis.line  =  element_line ( color  =   'black' ),  axis.ticks  =  element_line ( color  =   'black' ),  strip.text  =  element_text ( size  =   16 ,  color  =  myRed ),  axis.title.y  =  element_text ( color  =  myRed ,  hjust  =   0 ,  face  =   'italic' ),  axis.title.x  =  element_text ( color  =  myRed ,  hjust  =   0 ,  face  =   'italic' ),  axis.text  =  element_text ( color  =   'black' ),  legend.position  =   'none' )    1\n2\n3\n4\n5 # display the plot z2 \ntheme_set ( theme_pink ) \n\nz2  +  \n  theme_pink    1\n2\n3\n4 # restore the old plot \ntheme_set ( old ) \n\nz2    Exploring ggthemes  1\n2\n3\n4\n5\n6\n7 # apply theme_tufte  # set the theme with theme_set \ntheme_set ( theme_tufte ())  # or apply it in the ggplot command \nz2  +  \n  theme_tufte ()     1\n2\n3\n4\n5\n6\n7\n8\n9 # apply theme_tufte, modified  # set the theme with theme_set \ntheme_set ( theme_tufte ()   +  \n  theme ( legend.position  =   c ( 0.9 ,   0.9 ),  axis.title  =  element_text ( face  =   'italic' ,  size  =   12 ),   legend.title  =  element_text ( face  =   'italic' ,  size  =   12 )))  # or apply it in the ggplot command \nz2  +  \n  theme_tufte ()   + \n  theme ( legend.position  =   c ( 0.9 ,   0.9 ),         axis.title  =  element_text ( face  =   'italic' ,  size  =   12 ),  legend.title  =  element_text ( face  =   'italic' ,  size  =   12 ))     1\n2\n3\n4\n5\n6\n7 # apply theme_igray  # set the theme with `theme_set` \ntheme_set ( theme_igray ())  # or apply it in the ggplot command \nz2  +  \n  theme_igray ()      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # apply `theme_igray`, modified  # set the theme with `theme_set` \ntheme_set ( theme_igray ()   +  \n  theme ( legend.position  =   c ( 0.9 ,   0.9 ),  legend.key  =  element_blank (),  legend.background  =  element_rect ( fill  =   'grey90' ))) \n\nz2  +  \n   # Or apply it in the ggplot command \n  theme_igray ()   + \n  theme ( legend.position  =   c ( 0.9 ,   0.9 ), \n        legend.key  =  element_blank (), \n        legend.background  =  element_rect ( fill  =   'grey90' ))",
            "title": "Themes"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#best-practices",
            "text": "Bar Plots (1)  1\n2 # base layers \nm  <-  ggplot ( mtcars ,  aes ( x  =  cyl ,  y  =  wt ))    1\n2\n3 # dynamite plot \nm  +  \n  stat_summary ( fun.y  =   mean ,  geom  =   'bar' ,  fill  =   'skyblue' )   +  stat_summary ( fun.data  =  mean_sdl ,  fun.args  =   list ( mult  =   1 ),  geom  =   'errorbar' ,  width  =   0.1 )     Bar Plots (2)  1\n2 # base layers \nm  <-  ggplot ( mtcars ,  aes ( x  =  cyl , y  =  wt ,  col  =  am ,  fill  =  am ))    1\n2\n3 # dynamite plot \nm  +  \n  stat_summary ( fun.y  =   mean ,  geom  =   'bar' )   +  stat_summary ( fun.data  =  mean_sdl ,  fun.args  =   list ( mult  =   1 ),  geom  =   'errorbar' ,  width  =   0.1 )     1\n2\n3 # set position dodge in each `stat` function \nm  +  \n  stat_summary ( fun.y  =   mean ,  geom  =   'bar' ,  position  =   'dodge' )   +  stat_summary ( fun.data  =  mean_sdl ,  fun.args  =   list ( mult  =   1 ),  geom  =   'errorbar' ,  width  =   0.1 ,  position  =   'dodge' )     1\n2 # set your dodge `posn` manually \nposn.d  <-  position_dodge ( 0.9 )    1\n2\n3 # redraw dynamite plot \nm  +  \n  stat_summary ( fun.y  =   mean ,  geom  =   'bar' ,  position  =  posn.d )   +   stat_summary ( fun.data  =  mean_sdl ,  fun.args  =   list ( mult  =   1 ),  geom  =   'errorbar' ,  width  =   0.1 ,  position  =  posn.d )     Bar Plots (3)  1\n2\n3 # base layers \nmtcars.cyl  <-  mtcars  %>%  group_by ( cyl )   %>%  summarise ( wt.avg  =   mean ( wt )) \nmtcars.cyl   1\n2\n3\n4\n5\n6 ## # A tibble: 3 \u00d7 2\n##     cyl   wt.avg\n##   <dbl>    <dbl>\n## 1     4 2.285727\n## 2     6 3.117143\n## 3     8 3.999214   1\n2 m  <-  ggplot ( mtcars.cyl ,  aes ( x  =  cyl ,  y  =  wt.avg )) \nm    1\n2\n3 # draw bar plot \nm  +  \n  geom_bar ( stat  =   'identity' ,  fill  =   'skyblue' )     Pie Charts (1)  1\n2 # bar chart to pie chart \nggplot ( mtcars ,  aes ( x  =  cyl ,  fill  =  am ))   +  geom_bar ( position  =   'fill' )     1 ggplot ( mtcars ,  aes ( x  =  cyl ,  fill  =  am ))   +  geom_bar ( position  =   'fill' )   +  facet_grid ( .   ~  cyl )     1 ggplot ( mtcars ,  aes ( x  =   factor ( 1 ),  fill  =  am ))   +  geom_bar ( position  =   'fill' )   +  facet_grid ( .   ~  cyl )     1 ggplot ( mtcars ,  aes ( x  =   factor ( 1 ),  fill  =  am ))   +  geom_bar ( position  =   'fill' )   +  facet_grid ( .   ~  cyl )   +  coord_polar ( theta  =   'y' )     1 ggplot ( mtcars ,  aes ( x  =   factor ( 1 ),  fill  =  am ))   +  geom_bar ( position  =   'fill' ,  width  =   1 )   +  facet_grid ( .   ~  cyl )   +  coord_polar ( theta  =   'y' )     Parallel coordinate plot  1\n2\n3\n4 # parallel coordinates plot using `GGally`  # all columns except `am` (`am` column is the 9th) \ngroup_by_am  <-   9 \nmy_names_am  <-   ( 1 : 11 )[ - group_by_am ]    1\n2 # parallel plot; each variable plotted as a z-score transformation \nggparcoord ( mtcars ,  columns  =  my_names_am ,  groupColumn  =  group_by_am ,  alpha  =   0.8 )     1\n2 # scaled between 0-1 and most discriminating variable first \nggparcoord ( mtcars ,  columns  =  my_names_am ,  groupColumn  =  group_by_am ,  alpha  =   0.8 ,  scale  =   'uniminmax' ,  order  =   'anyClass' )     1 ggparcoord ( iris ,  columns  =   1 : 4 ,  groupColumn  =   'Species' )   # xlab, ylab, scale_x_discrete, them     1 ggparcoord ( iris ,  columns  =   1 : 4 ,  groupColumn  =   'Species' ,  scale  =   'uniminmax' )     1 ggparcoord ( iris ,  columns  =   1 : 4 ,  groupColumn  =   'Species' ,  scale  =   'globalminmax' )     1 ggparcoord ( iris ,  columns  =   1 : 4 ,  groupColumn  =   'Species' ,  mapping  =  aes ( size  =   1 ))     1 ggparcoord ( iris ,  columns  =   1 : 4 ,  groupColumn  =   'Species' ,  alphaLines  =   0.3 )     1 ggparcoord ( iris ,  columns  =   1 : 4 ,  groupColumn  =   'Species' ,  scale  =   'center' )     1 ggparcoord ( iris ,  columns  =   1 : 4 ,  groupColumn  =   'Species' ,  scaleSummary  =   'median' ,  missing  =   'exclude' )     1 ggparcoord ( iris ,  columns  =   1 : 4 ,  groupColumn  =   'Species' ,  order  =   'allClass' )   # or custom filter     1 ggparcoord ( iris ,  columns  =   1 : 4 ,  groupColumn  =   'Species' ,  scale  =   'std' )     Splom  1\n2\n3\n4\n5\n6 library ( dplyr ) \n\ndata ( Pima.tr2 ,  package  =   'MASS' ) \nPimaV  <-  select ( Pima.tr2 ,  glu : age ) \n\nggpairs ( PimaV ,  diag  =   list ( continuous  =   'density' ),  axisLabels  =   'show' )     Heat Maps  1\n2 # create color palette \nmyColors  <-  brewer.pal ( 9 ,   'Reds' )    1\n2\n3 # heat map \nggplot ( barley ,  aes ( x  =  year ,  y  =  variety ,  fill  =  yield ))   +  \n  geom_tile ()     1\n2\n3\n4 # add facet_wrap(~ variable); not like facet_grid(. ~ variable) \nggplot ( barley ,  aes ( x  =  year ,  y  =  variety ,  fill  =  yield ))   +  \n  geom_tile ()   +  \n  facet_wrap (   ~  site ,  ncol  =   1 )     1\n2\n3 #  \nggplot ( barley ,  aes ( x  =  year ,  y  =  variety ,  fill  =  yield ))   +  geom_tile ()   +  facet_wrap (   ~  site ,  ncol  =   1 )   +  \n  scale_fill_gradientn ( colors  =  myColors )     Heat Maps Alternatives (1)  1\n2\n3 # line plots \nggplot ( barley ,  aes ( x  =  year ,  y  =  yield ,  col  =  variety ,  group  =  variety ))   +  geom_line ()   +  \n  facet_wrap ( facets  =   ~  site ,  nrow  =   1 )     Heat Maps Alternatives (2)  1\n2\n3\n4 # overlapping ribbon plot \nggplot ( barley ,  aes ( x  =  year ,  y  =  yield ,  col  =  site ,  group  =  site ,  fill  =  site ))   +  geom_line ()   +  \n  stat_summary ( fun.y  =   mean ,  geom  =   'line' )   +  \n  stat_summary ( fun.data  =  mean_sdl ,  fun.args  =   list ( mult  =   1 ),  geom  =   'ribbon' ,  col  =   NA ,  alpha  =   0.1 )",
            "title": "Best Practices"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#case-study",
            "text": "Sort and order  1\n2\n3\n4\n5\n6\n7\n8 # reorder \ndata ( Cars93 ,  package  =   'MASS' ) \n\nCars93  <-   within ( Cars93 ,  TypeWt  <-  reorder ( Type ,  Weight ,   mean )) \n\nCars93  <-   within ( Cars93 ,  Type1  <-   factor ( Type ,  levels  =   c ( 'Small' ,   'Sporty' ,   'Compact' ,   'Midsize' ,   'Large' ,   'Van' )))  with ( Cars93 ,   table ( TypeWt ,  Type1 ))    1\n2\n3\n4\n5\n6\n7\n8 ##          Type1\n## TypeWt    Small Sporty Compact Midsize Large Van\n##   Small      21      0       0       0     0   0\n##   Sporty      0     14       0       0     0   0\n##   Compact     0      0      16       0     0   0\n##   Midsize     0      0       0      22     0   0\n##   Large       0      0       0       0    11   0\n##   Van         0      0       0       0     0   9   1\n2\n3\n4 ggplot ( Cars93 ,  aes ( TypeWt ,   100 / MPG.city ))   +  \n  geom_boxplot ()   +  \n  ylab ( 'Gallons per 100 miles' )   +  \n  xlab ( 'Car type' )     1\n2\n3\n4\n5\n6\n7\n8 Cars93  <-   within ( Cars93 ,   { \n   levels ( Type1 )   <-   c ( 'Small' ,   'Large' ,   'Midsize' ,   'Small' ,   'Sporty' ,   'Large' )  }) \n\nggplot ( Cars93 ,  aes ( TypeWt ,   100 / MPG.city ))   +  \n  geom_boxplot ()   +  \n  ylab ( 'Gallons per 100 miles' )   +  \n  xlab ( 'Car type' )     Ensemble plots   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 library ( gridExtra ) \n\ndata ( Fertility ,  package  =   'AER' ) \n\np0  <-  ggplot ( Fertility )   +  geom_histogram ( binwidth  =   1 )   +  ylab ( '' ) \np1  <-  p0  +  aes ( x  =  age ) \np2  <-  p0  +  aes ( x  =  work )   +  xlab ( 'Weeks worked in 1979' ) \n\nk  <-  ggplot ( Fertility )   +  \n  geom_bar ()   +  ylab ( '' )   +  \n  ylim ( 0 ,   250000 ) \n\np3  <-  k  +  aes ( x  =  morekids )   +  \n  xlab ( 'has more children' ) \np4  <-  k  +  aes ( x  =  gender1 )   +  \n  xlab ( 'first child' ) \np5  <-  k  +  aes ( x  =  gender2 )   +  \n  xlab ( 'second child' ) \np6  <-  k  +  aes ( x  =  afam )   +  \n  xlab ( 'African-American' ) \np7  <-  k  +  aes ( x  =  hispanic )   +  \n  xlab ( 'Hispanic' ) \np8  <-  k  +  aes ( x  =  other )   +  \n  xlab ( 'other race' ) \n\ngrid.arrange ( arrangeGrob ( p1 ,  p2 ,  ncol  =   2 ,  widths  =   c ( 3 ,   3 )),  arrangeGrob ( p3 ,  p4 ,  p5 ,  p6 ,  p7 ,  p8 ,  ncol  =   6 ),  nrow  =   2 ,  heights  =   c ( 1.25 ,   1 ))     Exploring Data  1\n2\n3 # histogram \nggplot ( adult ,  aes ( x  =  SRAGE_P ))   +  \n  geom_histogram ()     1\n2\n3 # histogram \nggplot ( adult ,  aes ( x  =  BMI_P ))   +  \n  geom_histogram ()     1\n2\n3 # color, default binwidth \nggplot ( adult , aes ( x  =  SRAGE_P ,  fill  =   factor ( RBMI )))   +  \n  geom_histogram ( binwidth  =   1 )     Data Cleaning   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # remove individual aboves 84 \nadult  <-  adult [ adult $ SRAGE_P  <=   84 ,   ]   # remove individuals with a BMI below 16 and above or equal to 52 \nadult  <-  adult [ adult $ BMI_P  >=   16   &  adult $ BMI_P  <   52 ,   ]  # relabel race \nadult $ RACEHPR2  <-   factor ( adult $ RACEHPR2 ,  labels  =   c ( 'Latino' ,   'Asian' ,   'African American' ,   'White' ))  # relabel the BMI categories variable \nadult $ RBMI  <-   factor ( adult $ RBMI ,  labels  =   c ( 'Under-weight' ,   'Normal-weight' ,   'Over-weight' ,   'Obese' ))    Multiple Histograms  1\n2 # color palette BMI_fill \nBMI_fill  <-  scale_fill_brewer ( 'BMI Category' ,  palette  =   'Reds' )    1\n2\n3\n4\n5 # histogram, add BMI_fill and customizations \nggplot ( adult ,  aes ( x  =  SRAGE_P ,  fill =   factor ( RBMI )))   +  \n  geom_histogram ( binwidth  =   1 )   +  \n  BMI_fill  +  facet_grid ( RBMI  ~   . )   +  \n  theme_classic ()     Alternatives  1\n2\n3\n4 # count histogram \nggplot ( adult ,  aes ( x  =  SRAGE_P ,  fill  =   factor ( RBMI )))   +  \n  geom_histogram ( binwidth  =   1 )   + \n  BMI_fill    1\n2\n3\n4 # density histogram \nggplot ( adult ,  aes ( x  =  SRAGE_P ,  fill =   factor ( RBMI )))   +  \n  geom_histogram ( aes ( y  =   .. density.. ),  binwidth  =   1 )   + \n  BMI_fill    1\n2\n3\n4 # faceted count histogram \nggplot ( adult ,  aes ( x  =  SRAGE_P ,  fill =   factor ( RBMI )))   +  \n  geom_histogram ( binwidth  =   1 )   + \n  BMI_fill  +  facet_grid ( RBMI  ~   . )     1\n2\n3\n4 # faceted density histogram \nggplot ( adult ,  aes ( x  =  SRAGE_P ,  fill =   factor ( RBMI )))   +  \n  geom_histogram ( aes ( y  =   .. density.. ),  binwidth  =   1 )   + \n  BMI_fill  +  facet_grid ( RBMI  ~   . )     1\n2\n3\n4 # density histogram with `position = 'fill'` \nggplot ( adult ,  aes  ( x  =  SRAGE_P ,  fill  =   factor ( RBMI )))   +  \n  geom_histogram ( aes ( y  =   .. density.. ),  binwidth  =   1 ,  position  =   'fill' )   + \n  BMI_fill    1\n2\n3\n4 # accurate histogram \nggplot ( adult ,  aes ( x  =  SRAGE_P ,  fill  =   factor ( RBMI )))   +  \n  geom_histogram ( aes ( y  =   .. count.. / sum ( .. count.. )),  binwidth  =   1 ,  position  =   'fill' )   + \n  BMI_fill    Do Things Manually  1\n2\n3\n4\n5 # an attempt to facet the accurate frequency histogram from before (failed) \nggplot ( adult ,  aes ( x  =  SRAGE_P ,  fill  =   factor ( RBMI )))   +  \n  geom_histogram ( aes ( y  =   .. count.. / sum ( .. count.. )),  binwidth  =   1 ,  position  =   'fill' )   + \n  BMI_fill  + \n  facet_grid ( RBMI  ~   . )      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # create DF with `table()` \nDF  <-   table ( adult $ RBMI ,  adult $ SRAGE_P )  # use apply on DF to get frequency of each group \nDF_freq  <-   apply ( DF ,   2 ,   function ( x )  x / sum ( x ))  # melt on DF to create DF_melted \nDF_melted  <-  melt ( DF_freq )  # change names of DF_melted  names ( DF_melted )   <-   c ( 'FILL' ,   'X' ,   'value' )    1\n2\n3\n4\n5 # add code to make this a faceted plot \nggplot ( DF_melted ,  aes ( x  =  X ,  y  =  value ,  fill  =  FILL ))   + \n  geom_bar ( stat  =   'identity' ,  position  =   'stack' )   + \n  BMI_fill  +  \n  facet_grid ( FILL  ~   . )     Merimeko/Mosaic Plot   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33 # The initial contingency table \nDF  <-   as.data.frame.matrix ( table ( adult $ SRAGE_P ,  adult $ RBMI ))  # Add the columns groupsSum, xmax and xmin. Remove groupSum again. \nDF $ groupSum  <-   rowSums ( DF ) \nDF $ xmax  <-   cumsum ( DF $ groupSum ) \nDF $ xmin  <-  DF $ xmax  -  DF $ groupSum # The groupSum column needs to be removed, don't remove this line \nDF $ groupSum  <-   NULL  # Copy row names to variable X \nDF $ X  <-   row.names ( DF )  # Melt the dataset \nDF_melted  <-  melt ( DF ,  id.vars  =   c ( 'X' ,   'xmin' ,   'xmax' ),  variable.name  =   'FILL' )  # dplyr call to calculate ymin and ymax - don't change \nDF_melted  <-  DF_melted  %>%  \n  group_by ( X )   %>%  \n  mutate ( ymax  =   cumsum ( value / sum ( value )), \n         ymin  =  ymax  -  value / sum ( value ))  # Plot rectangles - don't change. \nggplot ( DF_melted ,  aes ( ymin  =  ymin ,  \n                 ymax  =  ymax , \n                 xmin  =  xmin ,  \n                 xmax  =  xmax ,  \n                 fill  =  FILL ))   +  \n  geom_rect ( colour  =   'white' )   + \n  scale_x_continuous ( expand  =   c ( 0 , 0 ))   + \n  scale_y_continuous ( expand  =   c ( 0 , 0 ))   + \n  BMI_fill  + \n  theme_tufte ()     Adding statistics   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # perform chi.sq test (`RBMI` and `SRAGE_P`) \nresults  <-  chisq.test ( table ( adult $ RBMI ,  adult $ SRAGE_P ))  # melt results$residuals and store as resid \nresid  <-  melt ( results $ residuals )  # change names of resid  names ( resid )   <-   c ( 'FILL' ,   'X' ,   'residual' )  # merge the two datasets \nDF_all  <-   merge ( DF_melted ,  resid )    1\n2\n3\n4\n5\n6\n7 # update plot command \nggplot ( DF_all ,  aes ( ymin  =  ymin ,  ymax  =  ymax ,  xmin  =  xmin ,  xmax  =  xmax ,  fill  =  residual ))   + \n  geom_rect ()   + \n  scale_fill_gradient2 ()   + \n  scale_x_continuous ( expand  =   c ( 0 , 0 ))   + \n  scale_y_continuous ( expand  =   c ( 0 , 0 ))   + \n  theme_tufte ()     Adding text  1\n2\n3\n4\n5\n6 # position for labels on x axis \nDF_all $ xtext  <-  DF_all $ xmin  +   ( DF_all $ xmax  -  DF_all $ xmin )   /   2  # position for labels on y axis \nindex  <-  DF_all $ xmax  ==   max ( DF_all $ xmax ) \nDF_all $ ytext  <-  DF_all $ ymin [ index ]   +   ( DF_all $ ymax [ index ]   -  DF_all $ ymin [ index ]) / 2     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # plot \nggplot ( DF_all ,  aes ( ymin  =  ymin ,  ymax  =  ymax ,  xmin  =  xmin ,  xmax  =  xmax ,  fill  =  residual ))   +  \n  geom_rect ( col  =   'white' )   + \n   # geom_text for ages (i.e. the x axis) \n  geom_text ( aes ( x  =  xtext ,  label  =  X ),  y  =   1 ,  size  =   3 ,  angle  =   90 ,  hjust  =   1 ,  show.legend  =   FALSE )   + \n   # geom_text for BMI (i.e. the fill axis) \n  geom_text ( aes ( x  =   max ( xmax ),  y  =  ytext ,  label  =  FILL ),  size  =   3 ,  hjust  =   1 ,  show.legend   =   FALSE )   + \n  scale_fill_gradient2 ()   + \n  theme_tufte ()   + \n  theme ( legend.position  =   'bottom' )     Generalizations   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39 # script generalized into a function \nmosaicGG  <-   function ( data ,  X ,  FILL )   { \n   # Proportions in raw data \n  DF  <-   as.data.frame.matrix ( table ( data [[ X ]],  data [[ FILL ]])) \n  DF $ groupSum  <-   rowSums ( DF ) \n  DF $ xmax  <-   cumsum ( DF $ groupSum ) \n  DF $ xmin  <-  DF $ xmax  -  DF $ groupSum\n  DF $ X  <-   row.names ( DF ) \n  DF $ groupSum  <-   NULL \n  DF_melted  <-  melt ( DF ,  id  =   c ( 'X' ,   'xmin' ,   'xmax' ),  variable.name  =   'FILL' ) \n\n  DF_melted  <-  DF_melted  %>%  \n    group_by ( X )   %>%  \n    mutate ( ymax  =   cumsum ( value / sum ( value )), \n           ymin  =  ymax  -  value / sum ( value )) \n\n   # Chi-sq test \n  results  <-  chisq.test ( table ( data [[ FILL ]],  data [[ X ]]))   # fill and then x \n  resid  <-  melt ( results $ residuals ) \n   names ( resid )   <-   c ( 'FILL' ,   'X' ,   'residual' ) \n   # Merge data \n  DF_all  <-   merge ( DF_melted ,  resid ) \n    # Positions for labels \n  DF_all $ xtext  <-  DF_all $ xmin  +   ( DF_all $ xmax  -  DF_all $ xmin ) / 2 \n  index  <-  DF_all $ xmax  ==   max ( DF_all $ xmax ) \n  DF_all $ ytext  <-  DF_all $ ymin [ index ]   +   ( DF_all $ ymax [ index ]   -  DF_all $ ymin [ index ]) / 2 \n\n   # plot \n  g  <-  ggplot ( DF_all ,  aes ( ymin  =  ymin ,   ymax  =  ymax ,  xmin  =  xmin ,  \n                          xmax  =  xmax ,  fill  =  residual ))   +  \n    geom_rect ( col  =   'white' )   + \n    geom_text ( aes ( x  =  xtext ,  label  =  X ),  y  =   1 ,  size  =   3 ,  angle  =   90 ,  hjust  =   1 ,  show.legend  =   FALSE )   +  geom_text ( aes ( x  =   max ( xmax ),   y  =  ytext ,  label  =  FILL ),  size  =   3 ,  hjust  =   1 ,  show.legend  =   FALSE )   + \n    scale_fill_gradient2 ( 'Residuals' )   + \n    scale_x_continuous ( 'Individuals' ,  expand  =   c ( 0 , 0 ))   + \n    scale_y_continuous ( 'Proportion' ,  expand  =   c ( 0 , 0 ))   + \n    theme_tufte ()   + \n    theme ( legend.position  =   'bottom' ) \n   print ( g )  }    1\n2 # BMI described by age (in x) \nmosaicGG ( adult ,   'SRAGE_P' , 'RBMI' )     1\n2 # poverty described by age (in x) \nmosaicGG ( adult ,   'SRAGE_P' ,   'POVLL' )     1\n2 # `am` described by `cyl` (in x) \nmosaicGG ( mtcars ,   'cyl' ,   'am' )     1\n2 # `Vocab` described by education \nmosaicGG ( Vocab ,   'education' ,   'vocabulary' )",
            "title": "Case Study"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#section-3",
            "text": "",
            "title": "SECTION 3"
        },
        {
            "location": "/Plot_snippets_-_ggplot2/#section-4-cheat-list",
            "text": "ggplot(data, aes(x = , y = ), col = , fill = , size = , labels = , alpha \n= , shape = , line = , position = \u2018jitter\u2019)   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 + geom_point()\n\n+ geom_point(aes(), col = , position = posn.j)\n\n+ geom_jitter()\n\n+ facet_grid(. ~ x) # y ~ x\n\n+ scale_x_continous('Sepal Length', limits = c(2, 8), breaks = seq(2, 8, 3))\n\n+ scale_color_discrete('Species', labels = c('a', 'b', 'c'))\n\n+ labs(x = , y = , col = )   posn.j <- position_jitter(width = 0.1)  Data   diamonds , prices of 50,000 round cut diamonds.  economics , economics_long, US economic time series.  faithfuld , 2d density estimate of Old Faithful data.  luv_colours , colors().  midwest , midwest demographics.  mpg , fuel economy data from 1999 and 2008 for 38 popular models of car.  msleep , an updated and expanded version of the mammals  sleep dataset.  presidential , terms of 11 presidents from Eisenhower to Obama.  seals , vector field of seal movements.  txhousing , Housing sales in TX.   Aesthetics   x-axis.  y-asix.  color.  fill.  size (points, lines).  labels.  alpha.  shape (points).  linetype (lines).  aes , Define aesth.etic mappings.  aes_  (aes_q, aes_string), Define aesthetic mappings from strings, or quoted calls and formulas.  aes_all , Given a character vector, create a set of    identity mappings.  aes_auto , Automatic aesthetic mapping.  aes_colour_fill_alpha  (color, colour, fill), Colour related aesthetics: colour, fill and alpha.  aes_group_order  (group), Aesthetics: group. aes_linetype_size_shape (linetype, shape, size), Differentiation related aesthetics: linetype, size, shape.  aes_position  (x, xend, xmax, xmin, y, yend, ymax, ymin), Position related aesthetics: x, y, xmin, xmax, ymin, ymax, xend, yend.   Position   position_dodge , Adjust position by dodging overlaps to the side.  position_fill  (position_stack), Stack overlapping objects on top of one another.  position_identity , Don\u2019t adjust position  position_nudge , Nudge points.  position_jitter , Jitter points to avoid overplotting.  position_jitterdodge , Adjust position by simultaneously dodging and jittering.   Scales   expand_limits , Expand the plot limits with data.  guides , Set guides for each scale.  guide_legend , Legend guide.  guide_colourbar  (guide_colorbar), Continuous colour bar guide.  lims  (xlim, ylim), Convenience functions to set the axis limits.  scale_alpha  (scale_alpha_continuous, scale_alpha_discrete), Alpha scales.  scale_colour_brewer  (scale_color_brewer, scale_color_distiller, scale_colour_distiller, scale_fill_brewer, scale_fill_distiller), Sequential, diverging and qualitative colour scales from colorbrewer.org  scale_colour_gradient  (scale_color_continuous, scale_color_gradient, scale_color_gradient2, scale_color_gradientn, scale_colour_continuous, scale_colour_date, scale_colour_datetime, scale_colour_gradient2, scale_colour_gradientn, scale_fill_continuous, scale_fill_date, scale_fill_datetime, scale_fill_gradient, scale_fill_gradient2, scale_fill_gradientn).  scale_colour_grey  (scale_color_grey, scale_fill_grey), Sequential grey colour scale.  scale_colour_hue  (scale_color_discrete, scale_color_hue, scale_colour_discrete, scale_fill_discrete, scale_fill_hue), Qualitative colour scale with evenly spaced hues.  scale_identity  (scale_alpha_identity, scale_color_identity, scale_colour_identity, scale_fill_identity, scale_linetype_identity, scale_shape_identity, scale_size_identity), Use values without scaling.  scale_manual  (scale_alpha_manual, scale_color_manual, scale_colour_manual, scale_fill_manual, scale_linetype_manual, scale_shape_manual, scale_size_manual), Create your own discrete scale.  scale_linetype  (scale_linetype_continuous, scale_linetype_discrete), Scale for line patterns.  scale_shape  (scale_shape_continuous, scale_shape_discrete), Scale for shapes, aka glyphs.  scale_size  (scale_radius, scale_size_area, \n    scale_size_continuous, scale_size_date, scale_size_datetime, scale_size_discrete), Scale size (area or radius).  scale_x_discrete  (scale_y_discrete), Discrete position.  labs  (ggtitle, xlab, ylab), Change axis labels and legend titles.  update_labels , Update axis/legend labels.   Geometries   point.  line.  histogram.  bar.  boxplot.  geom_abline  (geom_hline, geom_vline), Lines: horizontal, \n    vertical, and specified by slope and intercept.  geom_bar  (stat_count), Bars, rectangles with bases on x-axis  geom_bin2d  (stat_bin2d, stat_bin_2d), Add heatmap of 2d bin counts.  geom_blank , Blank, draws nothing.  geom_boxplot  (stat_boxplot), Box and whiskers plot.  geom_contour  (stat_contour), Display contours of a 3d surface in 2d.  geom_count (stat_sum), Count the number of observations at each location.  geom_crossbar  (geom_errorbar, geom_linerange, geom_pointrange), Vertical intervals: lines, crossbars & errorbars.  geom_density  (stat_density), Display a smooth density estimate.  geom_density_2d  (geom_density2d, stat_density2d,    stat_density_2d), Contours from a 2d density estimate.  geom_dotplot , Dot plot  geom_errorbarh , Horizontal error bars.  geom_freqpoly  (geom_histogram, stat_bin), Histograms and frequency polygons.  geom_hex  (stat_bin_hex, stat_binhex), Hexagon binning.  geom_jitter , Points, jittered to reduce overplotting.  geom_label  (geom_text), Textual annotations.  geom_map , Polygons from a reference map.  geom_path  (geom_line, geom_step), Connect observations.  geom_point , Points, as for a scatterplot.  geom_polygon , Polygon, a filled path.  geom_quantile  (stat_quantile), Add quantile lines from a quantile regression.  geom_raster  (geom_rect, geom_tile), Draw rectangles.  geom_ribbon  (geom_area), Ribbons and area plots.  geom_rug , Marginal rug plots.  geom_segment  (geom_curve), Line segments and curves.  geom_smooth  (stat_smooth), Add a smoothed conditional mean.  geom_violin  (stat_ydensity), Violin plot.   Facets   columns.  rows.  facet_grid , Lay out panels in a grid.  facet_null , Facet specification: a single panel.  facet_wrap , Wrap a 1d ribbon of panels into 2d.  labeller , Generic labeller function for facets.  label_bquote , Backquoted labeller.   Annotation   annotate , Create an annotation layer.  annotation_custom , Annotation: Custom grob.  annotation_logticks , Annotation: log tick marks.  annotation_map , Annotation: maps.  annotation_raster , Annotation: High-performance \n    rectangular tiling.  borders , Create a layer of map borders.   Fortify   fortify , Fortify a model with data.  fortify-multcomp  (fortify.cld, fortify.confint.glht, fortify.glht, fortify.summary.glht), Fortify methods for objects produced by.  fortify.lm , Supplement the data fitted to a linear model with model fit statistics.  fortify.map , Fortify method for map objects.  fortify.sp  (fortify.Line, fortify.Lines, fortify.Polygon, fortify.Polygons, fortify.SpatialLinesDataFrame, fortify.SpatialPolygons, fortify.SpatialPolygonsDataFrame), Fortify method for classes from the sp package.  map_data , Create a data frame of map data.   Statistics   binning.  smoothing.  descriptive.  inferential.  stat_ecdf , Empirical Cumulative Density Function.  stat_ellipse , Plot data ellipses.  stat_function , Superimpose a function.  stat_identity , Identity statistic.  stat_qq  (geom_qq), Calculation for quantile-quantile plot.  stat_summary_2d  (stat_summary2d, stat_summary_hex), Bin and summarise in 2d (rectangle & hexagons)  stat_unique , Remove duplicates.  Coordinates.  cartesian.  fixes.  polar.  limites.  coord_cartesian , Cartesian coordinates.  coord_fixed  (coord_equal), Cartesian coordinates with fixed \n    relationship between x and y scales.  coord_flip , Flipped cartesian coordinates.  coord_map  (coord_quickmap), Map projections.  coord_polar , Polar coordinates.  coord_trans , Transformed cartesian coordinate system.   Themes   theme_bw  theme_grey  theme_classic  theme_minimal  ggthemes",
            "title": "SECTION 4 - Cheat List"
        },
        {
            "location": "/Plot_snippets_-_ggvis/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\nggvis\n generates html outputs. Graphics presented here are images: .png files; .gif files when specified.\n\n\n\n\n\n\nDocumentation\n\u00b6\n\n\n\n\nggvis\n Overview\n.\n\n\nggvis\n Cookbook\n.\n\n\n\n\nDataset\n\u00b6\n\n\nFor most examples, we use the \nmtcars\n, \npressure\n, \nfaithful\n datasets.\n\n\nThe \nggvis\n Package\n\u00b6\n\n\n1\nlibrary\n(\nggvis\n)\n\n\n\n\n\n\n\nThe Grammar of Graphics\n\u00b6\n\n\nStart to explore\n\n\n1\n2\n3\n4\n# change the code below to plot the disp variable of mtcars on the x axis\n\nmtcars \n%>%\n\n  ggvis\n(\n~\ndisp\n,\n \n~\nmpg\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\nggvis\n and its capabilities\n\n\n1\n2\n3\n4\n# Change the code below to make a graph with red points\n\nmtcars \n%>%\n\n  ggvis\n(\n~\nwt\n,\n \n~\nmpg\n,\n fill \n:=\n \n\"red\"\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Change the code below draw smooths instead of points\n\nmtcars \n%>%\n\n  ggvis\n(\n~\nwt\n,\n \n~\nmpg\n)\n \n%>%\n\n  layer_smooths\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Change the code below to make a graph containing both points and a smoothed summary line\n\nmtcars \n%>%\n\n  ggvis\n(\n~\nwt\n,\n \n~\nmpg\n)\n \n%>%\n\n  layer_points\n()\n \n%>%\n\n  layer_smooths\n()\n\n\n\n\n\n\n\n\n\nggvis\n grammar\n\n\n1\n2\n3\n4\n# Make a scatterplot of the pressure dataset\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n)\n \n%>%\n\n  layer_points\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Adapt the code you wrote for the first challenge: show bars instead of points\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n)\n \n%>%\n\n  layer_bars\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Adapt the code you wrote for the first challenge: show lines instead of points\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n)\n \n%>%\n\n  layer_lines\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Adapt the code you wrote for the first challenge: map the fill property to the temperature variable\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n fill \n=\n \n~\ntemperature\n)\n \n%>%\n\n  layer_points\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Extend the code you wrote for the previous challenge: map the size property to the pressure variable\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n fill \n=\n \n~\ntemperature\n,\n size \n=\n \n~\npressure\n)\n \n%>%\n\n  layer_points\n\n\n\n\n\n\n\n\n4 essential components of a graph\n\n\n1\n2\n3\n4\n5\n6\nfaithful \n%>%\n\n    ggvis\n(\n~\nwaiting\n,\n \n~\neruptions\n,\n fill \n:=\n \n\"red\"\n)\n \n%>%\n\n    layer_points\n()\n \n%>%\n\n    add_axis\n(\n\"y\"\n,\n title \n=\n \n\"Duration of eruption (m)\"\n,\n\n             values \n=\n \nc\n(\n2\n,\n \n3\n,\n \n4\n,\n \n5\n),\n subdivide \n=\n \n9\n)\n \n%>%\n\n    add_axis\n(\n\"x\"\n,\n title \n=\n \n\"Time since previous eruption (m)\"\n)\n\n\n\n\n\n\n\n\n\nLines and Syntax\n\u00b6\n\n\nThree operators: \n%>%\n, \n=\n and \n:=\n\n\n1\nlayer_points\n(\nggvis\n(\nfaithful\n,\n \n~\nwaiting\n,\n \n~\neruptions\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Rewrite the code with the pipe operator     \n\nfaithful \n%>%\n\n  ggvis\n(\n~\nwaiting\n,\n \n~\neruptions\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Modify this graph to map the size property to the pressure variable\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n size \n=\n \n~\npressure\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Modify this graph by setting the size property\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n size \n:=\n \n100\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\npressure \n%>%\n \n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n fill \n=\n \n\"red\"\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Fix this code to set the fill property to red\n\npressure \n%>%\n \n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n fill \n:=\n \n\"red\"\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\nReferring to different objects\n\n\n1\n2\n3\n4\n5\n6\nred \n<-\n \n\"green\"\n\npressure\n$\nred \n<-\n pressure\n$\ntemperature\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n fill \n:=\n red\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\nProperties for points\n\n\n1\n2\n3\n4\n# Change the code to set the fills using pressure$black\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n fill \n:=\n \n~\n'black'\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Plot the faithful data as described in the second instruction\n\nfaithful \n%>%\n \n  ggvis\n(\n~\nwaiting\n,\n \n~\neruptions\n,\n size \n=\n \n~\neruptions\n,\n opacity \n:=\n \n0.5\n,\n fill \n:=\n \n\"blue\"\n,\n stroke \n:=\n \n\"black\"\n)\n \n%>%\n \n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Plot the faithful data as described in the third instruction\n\nfaithful \n%>%\n \n  ggvis\n(\n~\nwaiting\n,\n \n~\neruptions\n,\n size \n:=\n \n100\n,\n fill \n:=\n \n\"red\"\n,\n fillOpacity \n=\n \n~\neruptions\n,\n stroke \n:=\n \n\"red\"\n,\n shape \n:=\n \n\"cross\"\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\nProperties for lines\n\n\n1\n2\n3\n4\n# Change the code below to use the lines mark\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n)\n \n%>%\n\n  layer_lines\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Set the properties described in the second instruction in the graph below\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n stroke \n:=\n \n\"red\"\n,\n strokeWidth \n:=\n \n2\n,\n strokeDash \n:=\n \n6\n)\n \n%>%\n\n  layer_lines\n()\n\n\n\n\n\n\n\n\n\nDisplay model fits\n\n\nlayer_lines()\n will always connect the points in your plot from the leftmost point to the rightmost point. This can be undesirable if you are trying to plot a specific shape.\n\n\nlayer_paths()\n: this mark connects the points in the order that they appear in the data set. So the paths mark will connect the point that corresponds to the first row of the data to the point that corresponds to the second row of data, and so on - no matter where those points appear in the graph.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# change the third line of code to plot a map of Texas\n\n\nlibrary\n(\nmaps\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\ntexas \n<-\n ggplot2\n::\nmap_data\n(\n\"state\"\n,\n region \n=\n \n\"texas\"\n)\n\n\ntexas \n%>%\n\n  ggvis\n(\n~\nlong\n,\n \n~\nlat\n)\n \n%>%\n\n  layer_paths\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Same plot, but set the fill property of the texas map to dark orange\n\ntexas \n%>%\n\n  ggvis\n(\n~\nlong\n,\n \n~\nlat\n,\n fill \n:=\n \n\"darkorange\"\n)\n \n%>%\n\n  layer_paths\n()\n\n\n\n\n\n\n\n\n\ncompute_smooth()\n to simplify model fits\n\n\ncompute_model_prediction()\n is a useful function to use with line graphs. It takes a data frame as input and returns a new data frame as output. The new data frame will contain the x and y values of a line fitted to the data in the original data frame.\n\n\nGenerate the x and y coordinates for a LOESS smooth line.\n\n\n1\n2\n3\nfaithful \n%>%\n\n  compute_model_prediction\n(\neruptions \n~\n waiting\n,\n model \n=\n \n\"lm\"\n)\n \n%>%\n\n  \nhead\n(\n10\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n##       pred_    resp_\n## 1  43.00000 1.377986\n## 2  43.67089 1.428724\n## 3  44.34177 1.479461\n## 4  45.01266 1.530199\n## 5  45.68354 1.580937\n## 6  46.35443 1.631674\n## 7  47.02532 1.682412\n## 8  47.69620 1.733150\n## 9  48.36709 1.783888\n## 10 49.03797 1.834625\n\n\n\n\n\n\n1\n2\n3\n4\n# Compute the x and y coordinates for a loess smooth line that predicts mpg with the wt\n\nmtcars \n%>%\n\n  compute_smooth\n(\nmpg \n~\n wt\n)\n \n%>%\n\n  \nhead\n(\n10\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n##       pred_    resp_\n## 1  1.513000 32.08897\n## 2  1.562506 31.68786\n## 3  1.612013 31.28163\n## 4  1.661519 30.87037\n## 5  1.711025 30.45419\n## 6  1.760532 30.03318\n## 7  1.810038 29.60745\n## 8  1.859544 29.17711\n## 9  1.909051 28.74224\n## 10 1.958557 28.30017\n\n\n\n\n\n\n1\n#model = \"loess\" is set by default\n\n\n\n\n\n\n\nTransformations\n\u00b6\n\n\nHistograms (1)\n\n\n1\n2\n3\n4\n# Build a histogram of the waiting variable of the faithful data set.\n\nfaithful \n%>%\n\n  ggvis\n(\n~\nwaiting\n)\n \n%>%\n\n  layer_histograms\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Build the same histogram, but with a binwidth (width argument) of 5 units\n\nfaithful \n%>%\n\n  ggvis\n(\n~\nwaiting\n)\n \n%>%\n\n  layer_histograms\n(\nwidth \n=\n \n5\n)\n\n\n\n\n\n\n\n\n\nHistograms (2)\n\n\n1\n2\n3\nfaithful \n%>%\n\n  ggvis\n(\n~\nwaiting\n)\n \n%>%\n\n  layer_histograms\n(\nwidth \n=\n \n5\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# Transform the code: just compute the bins instead of plotting a histogram\n\nfaithful \n%>%\n\n  compute_bin\n(\n~\nwaiting\n,\n width \n=\n \n5\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n##    count_ x_ xmin_ xmax_ width_\n## 1      13 45  42.5  47.5      5\n## 2      24 50  47.5  52.5      5\n## 3      29 55  52.5  57.5      5\n## 4      21 60  57.5  62.5      5\n## 5      13 65  62.5  67.5      5\n## 6      13 70  67.5  72.5      5\n## 7      42 75  72.5  77.5      5\n## 8      58 80  77.5  82.5      5\n## 9      38 85  82.5  87.5      5\n## 10     17 90  87.5  92.5      5\n## 11      4 95  92.5  97.5      5\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Combine the solution to the first challenge with layer_rects() to build a histogram\n\nfaithful \n%>%\n\n  compute_bin\n(\n~\nwaiting\n,\n width \n=\n \n5\n)\n \n%>%\n\n  ggvis\n(\nx \n=\n \n~\nxmin_\n,\n x2 \n=\n \n~\nxmax_\n,\n y \n=\n \n0\n,\n y2 \n=\n \n~\ncount_\n)\n \n%>%\n\n  layer_rects\n()\n\n\n\n\n\n\n\n\n\nDensity plots\n\n\n1\n2\n3\n4\n5\n# Combine compute_density() with layer_lines() to make a density plot of the waiting variable.\n\nfaithful \n%>%\n\n  compute_density\n(\n~\nwaiting\n)\n \n%>%\n\n  ggvis\n(\n~\npred_\n,\n~\nresp_\n)\n \n%>%\n\n  layer_lines\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Build a density plot directly using layer_densities. Use the correct variables and properties.\n\nfaithful \n%>%\n\n  ggvis\n(\n~\nwaiting\n)\n \n%>%\n\n  layer_densities\n(\nfill \n:=\n \n\"green\"\n)\n\n\n\n\n\n\n\n\n\nShortcuts\n\n\n1\n2\n3\n4\n# Complete the code to plot a bar graph of the cyl factor.\n\nmtcars \n%>%\n\n  ggvis\n(\n~\nfactor\n(\ncyl\n))\n \n%>%\n\n  layer_bars\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# Adapt the solution to the first challenge to just calculate the count values. No plotting!\n\nmtcars \n%>%\n\n  compute_count\n(\n~\nfactor\n(\ncyl\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n##   count_ x_\n## 1     11  4\n## 2      7  6\n## 3     14  8\n\n\n\n\n\n\nggvis\n and \ngroup_by\n\n\n1\n2\n3\n4\nmtcars \n%>%\n\n  group_by\n(\nam\n)\n \n%>%\n\n  ggvis\n(\n~\nmpg\n,\n \n~\nwt\n,\n stroke \n=\n \n~\nfactor\n(\nam\n))\n \n%>%\n\n  layer_smooths\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Change the code to plot a unique smooth line for each value of the cyl variable.\n\nmtcars \n%>%\n\n  group_by\n(\ncyl\n)\n \n%>%\n\n  ggvis\n(\n~\nmpg\n,\n \n~\nwt\n,\n stroke \n=\n \n~\nfactor\n(\ncyl\n))\n \n%>%\n\n  layer_smooths\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Adapt the graph to contain a separate density for each value of cyl.\n\nmtcars \n%>%\n\n  group_by\n(\ncyl\n)\n \n%>%\n\n  ggvis\n(\n~\nmpg\n)\n \n%>%\n\n  layer_densities\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Copy and alter the solution to the second challenge to map the fill property to a categorical version of cyl.\n\nmtcars \n%>%\n\n  group_by\n(\ncyl\n)\n \n%>%\n\n  ggvis\n(\n~\nmpg\n)\n \n%>%\n\n  layer_densities\n(\nfill \n=\n \n~\nfactor\n(\ncyl\n))\n\n\n\n\n\n\n\n\n\ngroup_by()\n versus \ninteraction()\n\n\n1\n2\n3\n4\nmtcars \n%>%\n\n  group_by\n(\ncyl\n)\n \n%>%\n\n  ggvis\n(\n~\nmpg\n)\n \n%>%\n\n  layer_densities\n(\nfill \n=\n \n~\nfactor\n(\ncyl\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# Alter the graph: separate density for each unique combination of 'cyl' and 'am'.\n\nmtcars \n%>%\n\n  group_by\n(\ncyl\n,\nam\n)\n \n%>%\n\n  ggvis\n(\n~\nmpg\n,\n fill \n=\n \n~\nfactor\n(\ncyl\n))\n \n%>%\n\n  layer_densities\n()\n\n\n\n#factor(cyl),factor(am)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\nmtcars \n%>%\n\n  group_by\n(\ncyl\n,\n am\n)\n \n%>%\n\n  ggvis\n(\n~\nmpg\n,\n fill \n=\n \n~\nfactor\n(\ncyl\n))\n \n%>%\n\n  layer_densities\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Update the graph to map `fill` to the unique combinations of the grouping variables.\n\nmtcars \n%>%\n\n  group_by\n(\ncyl\n,\n am\n)\n \n%>%\n\n  ggvis\n(\n~\nmpg\n,\n fill \n=\n \n~\ninteraction\n(\ncyl\n,\nam\n))\n \n%>%\n \n  layer_densities\n()\n\n\n\n\n\n\n\n\n\nChaining is a virtue\n\n\n1\n2\n3\n4\nmtcars \n%>%\n\n    group_by\n(\ncyl\n,\n am\n)\n \n%>%\n\n    ggvis\n(\n~\nmpg\n,\n fill \n=\n \n~\ninteraction\n(\ncyl\n,\n am\n))\n \n%>%\n\n    layer_densities\n()\n\n\n\n\n\n\n\n\n\nThis call is exactly equivalent to the following piece of code that is very hard to read:\n\n\n1\nlayer_densities\n(\nggvis\n(\ngroup_by\n(\nmtcars\n,\n cyl\n,\n am\n),\n \n~\nmpg\n,\n fill \n=\n \n~\ninteraction\n(\ncyl\n,\n am\n)))\n\n\n\n\n\n\n\n\n\nInteractivity and Layers\n\u00b6\n\n\nThe basics of interactive plots\n\n\n1\n2\n3\n4\n5\n# Run this code and inspect the output. Follow the link in the instructions for the interactive version\n\nfaithful \n%>%\n \n  ggvis\n(\n~\nwaiting\n,\n \n~\neruptions\n,\n fillOpacity \n:=\n \n0.5\n,\n \n        shape \n:=\n input_select\n(\nlabel \n=\n \n\"Choose shape:\"\n,\n choices \n=\n \nc\n(\n\"circle\"\n,\n \n\"square\"\n,\n \n\"cross\"\n,\n \n\"diamond\"\n,\n \n\"triangle-up\"\n,\n \n\"triangle-down\"\n)))\n \n%>%\n \n  layer_points\n()\n\n\n\n\n\n\n\n.gif file:\n\n\n\n\n1\n2\n3\n4\n# Copy the first code chunk and alter the code to make the fill property interactive using a select box\n\nfaithful \n%>%\n\n  ggvis\n(\n~\nwaiting\n,\n \n~\neruptions\n,\n fillOpacity \n:=\n \n0.5\n,\n shape \n:=\n input_select\n(\nlabel \n=\n \n\"Choose shape:\"\n,\n choices \n=\n \nc\n(\n\"circle\"\n,\n \n\"square\"\n,\n \n\"cross\"\n,\n \n\"diamond\"\n,\n \n\"triangle-up\"\n,\n \n\"triangle-down\"\n)),\n fill \n:=\n input_select\n(\nlabel \n=\n \n\"Choose color:\"\n,\n choices \n=\n \nc\n(\n\"black\"\n,\n\"red\"\n,\n\"blue\"\n,\n\"green\"\n)))\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n.gif file:\n\n\n\n\n1\n2\n3\n4\n# Add radio buttons to control the fill of the plot\n\nmtcars \n%>%\n \n  ggvis\n(\n~\nmpg\n,\n \n~\nwt\n,\n fill \n:=\n input_radiobuttons\n(\nlabel \n=\n \n\"Choose color:\"\n,\n choices \n=\n \nc\n(\n\"black\"\n,\n\"red\"\n,\n\"blue\"\n,\n\"green\"\n)))\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n.gif file:\n\n\n\n\nInput widgets in more detail\n\n\n1\n2\n3\nmtcars \n%>%\n\n  ggvis\n(\n~\nmpg\n,\n \n~\nwt\n,\n fill \n:=\n input_radiobuttons\n(\nlabel \n=\n \n\"Choose color:\"\n,\n choices \n=\n \nc\n(\n\"black\"\n,\n \n\"red\"\n,\n \n\"blue\"\n,\n \n\"green\"\n)))\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n.gif file:\n\n\n\n\n1\n2\n3\n4\n# Change the radiobuttons widget to a text widget \n\nmtcars \n%>%\n\n  ggvis\n(\n~\nmpg\n,\n \n~\nwt\n,\n fill \n:=\n input_text\n(\nlabel \n=\n \n\"Choose color:\"\n,\n value \n=\n \nc\n(\n\"black\"\n)))\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n.gif file:\n\n\n\n\n1\n2\n3\nmtcars \n%>%\n\n  ggvis\n(\n~\nmpg\n,\n \n~\nwt\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Map the fill property to a select box that returns variable names\n\nmtcars \n%>%\n\n  ggvis\n(\n~\nmpg\n,\n \n~\nwt\n,\n fill \n=\n input_select\n(\nlabel \n=\n \n\"Choose fill variable:\"\n,\n choices \n=\n \nnames\n(\nmtcars\n),\n map \n=\n \nas.name\n))\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n.gif file:\n\n\n\n\nInput widgets in more detail (2)\n\n\n1\n2\n3\n4\n# Map the fill property to a select box that returns variable names\n\nmtcars \n%>%\n\n  ggvis\n(\n~\nmpg\n,\n \n~\nwt\n,\n fill \n=\n input_select\n(\nlabel \n=\n \n\"Choose fill variable:\"\n,\n choices \n=\n \nnames\n(\nmtcars\n),\n map \n=\n \nas.name\n))\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n.gif file:\n\n\n\n\nControl parameters and values\n\n\n1\n2\n3\n4\n# Map the bindwidth to a numeric field (\"Choose a binwidth:\")\n\nmtcars \n%>%\n\n  ggvis\n(\n~\nmpg\n)\n \n%>%\n\n  layer_histograms\n(\nwidth \n=\n input_numeric\n(\nvalue \n=\n \n1\n,\n label \n=\n \n\"Choose a binwidth:\"\n))\n\n\n\n\n\n\n\n.gif file:\n\n\n\n\n1\n2\n3\n4\n# Map the binwidth to a slider bar (\"Choose a binwidth:\") with the correct specifications\n\nmtcars \n%>%\n\n  ggvis\n(\n~\nmpg\n)\n \n%>%\n\n  layer_histograms\n(\nwidth \n=\n input_slider\n(\nmin \n=\n \n1\n,\n max \n=\n \n20\n,\n label \n=\n \n\"Choose a binwidth:\"\n))\n\n\n\n\n\n\n\n.gif file:\n\n\n\n\nMulti-layered plots and their properties\n\n\n1\n2\n3\n4\n# Add a layer of points to the graph below.\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n stroke \n:=\n \n\"skyblue\"\n)\n \n%>%\n layer_lines\n()\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Copy and adapt the solution to the first instruction below so that only the lines layer uses a skyblue stroke.\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n)\n \n%>%\n\n  layer_lines\n(\nstroke \n:=\n \n\"skyblue\"\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Rewrite the code below so that only the points layer uses the shape property.\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n)\n \n%>%\n\n  layer_lines\n(\nstroke \n:=\n \n\"skyblue\"\n)\n \n%>%\n\n  layer_points\n(\nshape \n:=\n \n\"triangle-up\"\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Refactor the code for the graph below to make it as concise as possible\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n stroke \n:=\n \n\"skyblue\"\n,\n strokeOpacity \n:=\n \n0.5\n,\n strokeWidth \n:=\n \n5\n)\n \n%>%\n\n  layer_lines\n()\n \n%>%\n\n  layer_points\n(\nfill \n=\n \n~\ntemperature\n,\n shape \n:=\n \n\"triangle-up\"\n,\n size \n:=\n \n300\n)\n\n\n\n\n\n\n\n\n\nMulti-layered plots and their properties (2)\n\n\n1\n2\n3\n4\n5\n# Rewrite the code below so that only the points layer uses the shape property.\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n)\n \n%>%\n\n  layer_lines\n(\nstroke \n:=\n \n\"skyblue\"\n)\n \n%>%\n\n  layer_points\n(\nshape \n:=\n \n\"triangle-up\"\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# Refactor the code for the graph below to make it as concise as possible\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n stroke \n:=\n \n\"skyblue\"\n,\n\n        strokeOpacity \n:=\n \n0.5\n,\n strokeWidth \n:=\n \n5\n)\n \n%>%\n\n  layer_lines\n()\n \n%>%\n\n  layer_points\n(\nfill \n=\n \n~\ntemperature\n,\n shape \n:=\n \n\"triangle-up\"\n,\n size \n:=\n \n300\n)\n\n\n\n\n\n\n\n\n\nThere is no limit on the number of layers!\n\n\n1\n2\n3\n4\n5\n6\n7\n# Create a graph containing a scatterplot, a linear model and a smooth line.\n\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n)\n \n%>%\n\n  layer_points\n()\n \n%>%\n\n  layer_lines\n(\nstroke \n:=\n \n\"black\"\n,\n opacity \n:=\n \n0.5\n)\n \n%>%\n\n  layer_model_predictions\n(\nmodel \n=\n \n\"lm\"\n,\n stroke \n:=\n \n\"navy\"\n)\n \n%>%\n\n  layer_smooths\n(\nstroke \n:=\n \n\"skyblue\"\n)\n\n\n\n\n\n\n\n\n\nTaking local and global to the next level\n\n\n1\n2\n3\n4\n5\npressure \n%>%\n\n  ggvis\n(\n~\ntemperature\n,\n \n~\npressure\n,\n stroke \n:=\n \n\"darkred\"\n)\n \n%>%\n\n  layer_lines\n(\nstroke \n:=\n \n\"orange\"\n,\n strokeDash \n:=\n \n5\n,\n strokeWidth \n:=\n \n5\n)\n \n%>%\n\n  layer_points\n(\nshape \n:=\n \n\"circle\"\n,\n size \n:=\n \n100\n,\n fill \n:=\n \n\"lightgreen\"\n)\n \n%>%\n\n  layer_smooths\n()\n\n\n\n\n\n\n\n\n\nCustomizing Axes, Legends, and Scales\n\u00b6\n\n\nAxes\n\n\n1\n2\n3\n4\n5\n6\n# add the title of the x axis: \"Time since previous eruption (m)\"\n\nfaithful \n%>%\n \n  ggvis\n(\n~\nwaiting\n,\n \n~\neruptions\n)\n \n%>%\n \n  layer_points\n()\n \n%>%\n \n  add_axis\n(\n\"y\"\n,\n title \n=\n \n\"Duration of eruption (m)\"\n)\n \n%>%\n\n  add_axis\n(\n\"x\"\n,\n title \n=\n \n\"Time since previous eruption (m)\"\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# Add to the code to place a labelled tick mark at 50, 60, 70, 80, 90 on the x axis.\n\nfaithful \n%>%\n \n  ggvis\n(\n~\nwaiting\n,\n \n~\neruptions\n)\n \n%>%\n \n  layer_points\n()\n \n%>%\n \n  add_axis\n(\n\"y\"\n,\n title \n=\n \n\"Duration of eruption (m)\"\n,\n values \n=\n \nc\n(\n2\n,\n \n3\n,\n \n4\n,\n \n5\n),\n subdivide \n=\n \n9\n)\n \n%>%\n \n  add_axis\n(\n\"x\"\n,\n title \n=\n \n\"Time since previous eruption (m)\"\n,\n values \n=\n \nc\n(\n50\n,\n \n60\n,\n \n70\n,\n \n80\n,\n \n90\n),\n subdivide \n=\n \n9\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Add to the code below to change the location of the y axis\n\nfaithful \n%>%\n \n  ggvis\n(\n~\nwaiting\n,\n \n~\neruptions\n)\n \n%>%\n \n  layer_points\n()\n \n%>%\n\n  add_axis\n(\n\"x\"\n,\n orient \n=\n \n\"top\"\n)\n \n%>%\n add_axis\n(\n\"y\"\n,\n orient \n=\n \n\"right\"\n)\n\n\n\n\n\n\n\n\n\nLegends\n\n\n1\n2\n3\n4\n5\n# Add a legend to the plot below: use the correct title and orientation\n\nfaithful \n%>%\n \n  ggvis\n(\n~\nwaiting\n,\n \n~\neruptions\n,\n opacity \n:=\n \n0.6\n,\n fill \n=\n \n~\nfactor\n(\nround\n(\neruptions\n)))\n \n%>%\n \n  layer_points\n()\n \n%>%\n\n  add_legend\n(\n\"fill\"\n,\n title \n=\n \n\"~ duration (m)\"\n,\n orient \n=\n \n\"left\"\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n#add_legend(vis, scales = NULL, orient = \"right\", title = NULL, format = NULL, values = NULL, properties = NULL)\n\n\n\n# Use add_legend() to combine the legends in the plot below. Adjust its properties as instructed.\n\nfaithful \n%>%\n \n  ggvis\n(\n~\nwaiting\n,\n \n~\neruptions\n,\n opacity \n:=\n \n0.6\n,\n fill \n=\n \n~\nfactor\n(\nround\n(\neruptions\n)),\n shape \n=\n \n~\nfactor\n(\nround\n(\neruptions\n)),\n \n        size \n=\n \n~\nround\n(\neruptions\n))\n  \n%>%\n\n  layer_points\n()\n \n%>%\n\n  add_legend\n(\nc\n(\n\"fill\"\n,\n \n\"shape\"\n,\n \n\"size\"\n),\n title \n=\n \n\"~ duration (m)\"\n,\n values \n=\n \nc\n(\n2\n,\n3\n,\n4\n,\n5\n))\n\n\n\n\n\n\n\n\n\nLegends (2)\n\n\n1\n2\n3\n4\n5\n6\n# Fix the legend\n\nfaithful \n%>%\n \n  ggvis\n(\n~\nwaiting\n,\n \n~\neruptions\n,\n opacity \n:=\n \n0.6\n,\n fill \n=\n \n~\nfactor\n(\nround\n(\neruptions\n)),\n shape \n=\n \n~\nfactor\n(\nround\n(\neruptions\n)),\n \n        size \n=\n \n~\nround\n(\neruptions\n))\n \n%>%\n \n    layer_points\n()\n \n%>%\n \n    add_legend\n(\nc\n(\n\"fill\"\n,\n \n\"shape\"\n,\n \n\"size\"\n),\n title \n=\n \n\"~ duration (m)\"\n)\n\n\n\n\n\n\n\n\n\nScale types\n\n\n1\n2\n3\n4\n5\n# Add to the code below to make the stroke color range from \"darkred\" to \"orange\".\n\nmtcars \n%>%\n \n  ggvis\n(\n~\nwt\n,\n \n~\nmpg\n,\n fill \n=\n \n~\ndisp\n,\n stroke \n=\n \n~\ndisp\n,\n strokeWidth \n:=\n \n2\n)\n \n%>%\n\n  layer_points\n()\n \n%>%\n\n  scale_numeric\n(\n\"fill\"\n,\n range \n=\n \nc\n(\n\"red\"\n,\n \n\"yellow\"\n))\n \n%>%\n scale_numeric\n(\n\"stroke\"\n,\n range \n=\n \nc\n(\n\"darkred\"\n,\n \n\"orange\"\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# Change the graph below to make the fill colors range from green to beige.\n\nmtcars \n%>%\n ggvis\n(\n~\nwt\n,\n \n~\nmpg\n,\n fill \n=\n \n~\nhp\n)\n \n%>%\n\n  layer_points\n()\n \n%>%\n scale_numeric\n(\n\"fill\"\n,\n range \n=\n \nc\n(\n\"green\"\n,\n \n\"beige\"\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n# Create a scale that will map `factor(cyl)` to a new range of colors: purple, blue, and green. \n\nmtcars \n%>%\n ggvis\n(\n~\nwt\n,\n \n~\nmpg\n,\n fill \n=\n \n~\nfactor\n(\ncyl\n))\n \n%>%\n\n  layer_points\n()\n \n%>%\n scale_nominal\n(\n\"fill\"\n,\n range \n=\n \nc\n(\n\"purple\"\n,\n \n\"blue\"\n,\n \n\"green\"\n))\n\n\n\n\n\n\n\n\n\nAdjust any visual property\n\n\n1\n2\n3\n4\n5\n# Add a scale that limits the range of opacity from 0.2 to 1. \n\nmtcars \n%>%\n\n  ggvis\n(\nx \n=\n \n~\nwt\n,\n y \n=\n \n~\nmpg\n,\n fill \n=\n \n~\nfactor\n(\ncyl\n),\n opacity \n=\n \n~\nhp\n)\n \n%>%\n\n  layer_points\n()\n \n%>%\n\n  scale_numeric\n(\n\"opacity\"\n,\n range \n=\n \nc\n(\n0.2\n,\n1\n))\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# Add a second scale that will expand the x axis to cover data values from 0 to 6.\n\nmtcars \n%>%\n\n  ggvis\n(\n~\nwt\n,\n \n~\nmpg\n,\n fill \n=\n \n~\ndisp\n)\n \n%>%\n\n  layer_points\n()\n \n%>%\n\n  scale_numeric\n(\n\"y\"\n,\n domain \n=\n \nc\n(\n0\n,\n \nNA\n))\n \n%>%\n\n  scale_numeric\n(\n\"x\"\n,\n domain \n=\n \nc\n(\n0\n,\n \n6\n))\n\n\n\n\n\n\n\n\n\nAdjust any visual property (2)\n\n\n1\n2\n3\n4\n5\n6\n# Add a second scale to set domain for x\n\nmtcars \n%>%\n\n  ggvis\n(\n~\nwt\n,\n \n~\nmpg\n,\n fill \n=\n \n~\ndisp\n)\n \n%>%\n\n  layer_points\n()\n \n%>%\n\n  scale_numeric\n(\n\"y\"\n,\n domain \n=\n \nc\n(\n0\n,\n \nNA\n))\n \n%>%\n\n  scale_numeric\n(\n\"x\"\n,\n domain \n=\n \nc\n(\n0\n,\n \n6\n))\n\n\n\n\n\n\n\n\n\n\u201c\n=\n\u201d versus \u201c\n:=\n\u201c\n\n\n1\n2\n3\n4\n5\n6\n# Set the fill value to the color variable instead of mapping it, and see what happens\n\nmtcars\n$\ncolor \n<-\n \nc\n(\n\"red\"\n,\n \n\"teal\"\n,\n \n\"#cccccc\"\n,\n \n\"tan\"\n)\n\n\nmtcars \n%>%\n\n  ggvis\n(\nx \n=\n \n~\nwt\n,\n y \n=\n \n~\nmpg\n,\n fill \n=\n \n~\ncolor\n)\n \n%>%\n\n  layer_points\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\nmtcars \n%>%\n\n  ggvis\n(\nx \n=\n \n~\nwt\n,\n y \n=\n \n~\nmpg\n,\n fill \n:=\n \n~\ncolor\n)\n \n%>%\n\n  layer_points\n()",
            "title": "Plot Snippets - ggvis"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#dataset",
            "text": "For most examples, we use the  mtcars ,  pressure ,  faithful  datasets.",
            "title": "Dataset"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#the-ggvis-package",
            "text": "1 library ( ggvis )",
            "title": "The ggvis Package"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#the-grammar-of-graphics",
            "text": "Start to explore  1\n2\n3\n4 # change the code below to plot the disp variable of mtcars on the x axis \nmtcars  %>% \n  ggvis ( ~ disp ,   ~ mpg )   %>% \n  layer_points ()     ggvis  and its capabilities  1\n2\n3\n4 # Change the code below to make a graph with red points \nmtcars  %>% \n  ggvis ( ~ wt ,   ~ mpg ,  fill  :=   \"red\" )   %>% \n  layer_points ()     1\n2\n3\n4 # Change the code below draw smooths instead of points \nmtcars  %>% \n  ggvis ( ~ wt ,   ~ mpg )   %>% \n  layer_smooths ()     1\n2\n3\n4\n5 # Change the code below to make a graph containing both points and a smoothed summary line \nmtcars  %>% \n  ggvis ( ~ wt ,   ~ mpg )   %>% \n  layer_points ()   %>% \n  layer_smooths ()     ggvis  grammar  1\n2\n3\n4 # Make a scatterplot of the pressure dataset \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure )   %>% \n  layer_points    1\n2\n3\n4 # Adapt the code you wrote for the first challenge: show bars instead of points \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure )   %>% \n  layer_bars    1\n2\n3\n4 # Adapt the code you wrote for the first challenge: show lines instead of points \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure )   %>% \n  layer_lines    1\n2\n3\n4 # Adapt the code you wrote for the first challenge: map the fill property to the temperature variable \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure ,  fill  =   ~ temperature )   %>% \n  layer_points    1\n2\n3\n4 # Extend the code you wrote for the previous challenge: map the size property to the pressure variable \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure ,  fill  =   ~ temperature ,  size  =   ~ pressure )   %>% \n  layer_points    4 essential components of a graph  1\n2\n3\n4\n5\n6 faithful  %>% \n    ggvis ( ~ waiting ,   ~ eruptions ,  fill  :=   \"red\" )   %>% \n    layer_points ()   %>% \n    add_axis ( \"y\" ,  title  =   \"Duration of eruption (m)\" , \n             values  =   c ( 2 ,   3 ,   4 ,   5 ),  subdivide  =   9 )   %>% \n    add_axis ( \"x\" ,  title  =   \"Time since previous eruption (m)\" )",
            "title": "The Grammar of Graphics"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#lines-and-syntax",
            "text": "Three operators:  %>% ,  =  and  :=  1 layer_points ( ggvis ( faithful ,   ~ waiting ,   ~ eruptions ))     1\n2\n3\n4 # Rewrite the code with the pipe operator      \nfaithful  %>% \n  ggvis ( ~ waiting ,   ~ eruptions )   %>% \n  layer_points ()     1\n2\n3 pressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure )   %>% \n  layer_points ()     1\n2\n3\n4 # Modify this graph to map the size property to the pressure variable \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure ,  size  =   ~ pressure )   %>% \n  layer_points ()     1\n2\n3 pressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure )   %>% \n  layer_points ()     1\n2\n3\n4 # Modify this graph by setting the size property \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure ,  size  :=   100 )   %>% \n  layer_points ()     1\n2\n3 pressure  %>%  \n  ggvis ( ~ temperature ,   ~ pressure ,  fill  =   \"red\" )   %>% \n  layer_points ()     1\n2\n3\n4 # Fix this code to set the fill property to red \npressure  %>%  \n  ggvis ( ~ temperature ,   ~ pressure ,  fill  :=   \"red\" )   %>% \n  layer_points ()     Referring to different objects  1\n2\n3\n4\n5\n6 red  <-   \"green\" \npressure $ red  <-  pressure $ temperature\n\npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure ,  fill  :=  red )   %>% \n  layer_points ()     Properties for points  1\n2\n3\n4 # Change the code to set the fills using pressure$black \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure ,  fill  :=   ~ 'black' )   %>% \n  layer_points ()     1\n2\n3\n4 # Plot the faithful data as described in the second instruction \nfaithful  %>%  \n  ggvis ( ~ waiting ,   ~ eruptions ,  size  =   ~ eruptions ,  opacity  :=   0.5 ,  fill  :=   \"blue\" ,  stroke  :=   \"black\" )   %>%  \n  layer_points ()     1\n2\n3\n4 # Plot the faithful data as described in the third instruction \nfaithful  %>%  \n  ggvis ( ~ waiting ,   ~ eruptions ,  size  :=   100 ,  fill  :=   \"red\" ,  fillOpacity  =   ~ eruptions ,  stroke  :=   \"red\" ,  shape  :=   \"cross\" )   %>% \n  layer_points ()     Properties for lines  1\n2\n3\n4 # Change the code below to use the lines mark \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure )   %>% \n  layer_lines ()     1\n2\n3\n4 # Set the properties described in the second instruction in the graph below \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure ,  stroke  :=   \"red\" ,  strokeWidth  :=   2 ,  strokeDash  :=   6 )   %>% \n  layer_lines ()     Display model fits  layer_lines()  will always connect the points in your plot from the leftmost point to the rightmost point. This can be undesirable if you are trying to plot a specific shape.  layer_paths() : this mark connects the points in the order that they appear in the data set. So the paths mark will connect the point that corresponds to the first row of the data to the point that corresponds to the second row of data, and so on - no matter where those points appear in the graph.  1\n2\n3\n4\n5\n6\n7\n8\n9 # change the third line of code to plot a map of Texas  library ( maps )  library ( ggplot2 ) \n\ntexas  <-  ggplot2 :: map_data ( \"state\" ,  region  =   \"texas\" ) \n\ntexas  %>% \n  ggvis ( ~ long ,   ~ lat )   %>% \n  layer_paths ()     1\n2\n3\n4 # Same plot, but set the fill property of the texas map to dark orange \ntexas  %>% \n  ggvis ( ~ long ,   ~ lat ,  fill  :=   \"darkorange\" )   %>% \n  layer_paths ()     compute_smooth()  to simplify model fits  compute_model_prediction()  is a useful function to use with line graphs. It takes a data frame as input and returns a new data frame as output. The new data frame will contain the x and y values of a line fitted to the data in the original data frame.  Generate the x and y coordinates for a LOESS smooth line.  1\n2\n3 faithful  %>% \n  compute_model_prediction ( eruptions  ~  waiting ,  model  =   \"lm\" )   %>% \n   head ( 10 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ##       pred_    resp_\n## 1  43.00000 1.377986\n## 2  43.67089 1.428724\n## 3  44.34177 1.479461\n## 4  45.01266 1.530199\n## 5  45.68354 1.580937\n## 6  46.35443 1.631674\n## 7  47.02532 1.682412\n## 8  47.69620 1.733150\n## 9  48.36709 1.783888\n## 10 49.03797 1.834625   1\n2\n3\n4 # Compute the x and y coordinates for a loess smooth line that predicts mpg with the wt \nmtcars  %>% \n  compute_smooth ( mpg  ~  wt )   %>% \n   head ( 10 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ##       pred_    resp_\n## 1  1.513000 32.08897\n## 2  1.562506 31.68786\n## 3  1.612013 31.28163\n## 4  1.661519 30.87037\n## 5  1.711025 30.45419\n## 6  1.760532 30.03318\n## 7  1.810038 29.60745\n## 8  1.859544 29.17711\n## 9  1.909051 28.74224\n## 10 1.958557 28.30017   1 #model = \"loess\" is set by default",
            "title": "Lines and Syntax"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#transformations",
            "text": "Histograms (1)  1\n2\n3\n4 # Build a histogram of the waiting variable of the faithful data set. \nfaithful  %>% \n  ggvis ( ~ waiting )   %>% \n  layer_histograms ()     1\n2\n3\n4 # Build the same histogram, but with a binwidth (width argument) of 5 units \nfaithful  %>% \n  ggvis ( ~ waiting )   %>% \n  layer_histograms ( width  =   5 )     Histograms (2)  1\n2\n3 faithful  %>% \n  ggvis ( ~ waiting )   %>% \n  layer_histograms ( width  =   5 )     1\n2\n3 # Transform the code: just compute the bins instead of plotting a histogram \nfaithful  %>% \n  compute_bin ( ~ waiting ,  width  =   5 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ##    count_ x_ xmin_ xmax_ width_\n## 1      13 45  42.5  47.5      5\n## 2      24 50  47.5  52.5      5\n## 3      29 55  52.5  57.5      5\n## 4      21 60  57.5  62.5      5\n## 5      13 65  62.5  67.5      5\n## 6      13 70  67.5  72.5      5\n## 7      42 75  72.5  77.5      5\n## 8      58 80  77.5  82.5      5\n## 9      38 85  82.5  87.5      5\n## 10     17 90  87.5  92.5      5\n## 11      4 95  92.5  97.5      5   1\n2\n3\n4\n5 # Combine the solution to the first challenge with layer_rects() to build a histogram \nfaithful  %>% \n  compute_bin ( ~ waiting ,  width  =   5 )   %>% \n  ggvis ( x  =   ~ xmin_ ,  x2  =   ~ xmax_ ,  y  =   0 ,  y2  =   ~ count_ )   %>% \n  layer_rects ()     Density plots  1\n2\n3\n4\n5 # Combine compute_density() with layer_lines() to make a density plot of the waiting variable. \nfaithful  %>% \n  compute_density ( ~ waiting )   %>% \n  ggvis ( ~ pred_ , ~ resp_ )   %>% \n  layer_lines ()     1\n2\n3\n4 # Build a density plot directly using layer_densities. Use the correct variables and properties. \nfaithful  %>% \n  ggvis ( ~ waiting )   %>% \n  layer_densities ( fill  :=   \"green\" )     Shortcuts  1\n2\n3\n4 # Complete the code to plot a bar graph of the cyl factor. \nmtcars  %>% \n  ggvis ( ~ factor ( cyl ))   %>% \n  layer_bars ()     1\n2\n3 # Adapt the solution to the first challenge to just calculate the count values. No plotting! \nmtcars  %>% \n  compute_count ( ~ factor ( cyl ))    1\n2\n3\n4 ##   count_ x_\n## 1     11  4\n## 2      7  6\n## 3     14  8   ggvis  and  group_by  1\n2\n3\n4 mtcars  %>% \n  group_by ( am )   %>% \n  ggvis ( ~ mpg ,   ~ wt ,  stroke  =   ~ factor ( am ))   %>% \n  layer_smooths ()     1\n2\n3\n4\n5 # Change the code to plot a unique smooth line for each value of the cyl variable. \nmtcars  %>% \n  group_by ( cyl )   %>% \n  ggvis ( ~ mpg ,   ~ wt ,  stroke  =   ~ factor ( cyl ))   %>% \n  layer_smooths ()     1\n2\n3\n4\n5 # Adapt the graph to contain a separate density for each value of cyl. \nmtcars  %>% \n  group_by ( cyl )   %>% \n  ggvis ( ~ mpg )   %>% \n  layer_densities ()     1\n2\n3\n4\n5 # Copy and alter the solution to the second challenge to map the fill property to a categorical version of cyl. \nmtcars  %>% \n  group_by ( cyl )   %>% \n  ggvis ( ~ mpg )   %>% \n  layer_densities ( fill  =   ~ factor ( cyl ))     group_by()  versus  interaction()  1\n2\n3\n4 mtcars  %>% \n  group_by ( cyl )   %>% \n  ggvis ( ~ mpg )   %>% \n  layer_densities ( fill  =   ~ factor ( cyl ))     1\n2\n3\n4\n5\n6\n7 # Alter the graph: separate density for each unique combination of 'cyl' and 'am'. \nmtcars  %>% \n  group_by ( cyl , am )   %>% \n  ggvis ( ~ mpg ,  fill  =   ~ factor ( cyl ))   %>% \n  layer_densities ()  #factor(cyl),factor(am)     1\n2\n3\n4 mtcars  %>% \n  group_by ( cyl ,  am )   %>% \n  ggvis ( ~ mpg ,  fill  =   ~ factor ( cyl ))   %>% \n  layer_densities ()     1\n2\n3\n4\n5 # Update the graph to map `fill` to the unique combinations of the grouping variables. \nmtcars  %>% \n  group_by ( cyl ,  am )   %>% \n  ggvis ( ~ mpg ,  fill  =   ~ interaction ( cyl , am ))   %>%  \n  layer_densities ()     Chaining is a virtue  1\n2\n3\n4 mtcars  %>% \n    group_by ( cyl ,  am )   %>% \n    ggvis ( ~ mpg ,  fill  =   ~ interaction ( cyl ,  am ))   %>% \n    layer_densities ()     This call is exactly equivalent to the following piece of code that is very hard to read:  1 layer_densities ( ggvis ( group_by ( mtcars ,  cyl ,  am ),   ~ mpg ,  fill  =   ~ interaction ( cyl ,  am )))",
            "title": "Transformations"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#interactivity-and-layers",
            "text": "The basics of interactive plots  1\n2\n3\n4\n5 # Run this code and inspect the output. Follow the link in the instructions for the interactive version \nfaithful  %>%  \n  ggvis ( ~ waiting ,   ~ eruptions ,  fillOpacity  :=   0.5 ,  \n        shape  :=  input_select ( label  =   \"Choose shape:\" ,  choices  =   c ( \"circle\" ,   \"square\" ,   \"cross\" ,   \"diamond\" ,   \"triangle-up\" ,   \"triangle-down\" )))   %>%  \n  layer_points ()    .gif file:   1\n2\n3\n4 # Copy the first code chunk and alter the code to make the fill property interactive using a select box \nfaithful  %>% \n  ggvis ( ~ waiting ,   ~ eruptions ,  fillOpacity  :=   0.5 ,  shape  :=  input_select ( label  =   \"Choose shape:\" ,  choices  =   c ( \"circle\" ,   \"square\" ,   \"cross\" ,   \"diamond\" ,   \"triangle-up\" ,   \"triangle-down\" )),  fill  :=  input_select ( label  =   \"Choose color:\" ,  choices  =   c ( \"black\" , \"red\" , \"blue\" , \"green\" )))   %>% \n  layer_points ()    .gif file:   1\n2\n3\n4 # Add radio buttons to control the fill of the plot \nmtcars  %>%  \n  ggvis ( ~ mpg ,   ~ wt ,  fill  :=  input_radiobuttons ( label  =   \"Choose color:\" ,  choices  =   c ( \"black\" , \"red\" , \"blue\" , \"green\" )))   %>% \n  layer_points ()    .gif file:   Input widgets in more detail  1\n2\n3 mtcars  %>% \n  ggvis ( ~ mpg ,   ~ wt ,  fill  :=  input_radiobuttons ( label  =   \"Choose color:\" ,  choices  =   c ( \"black\" ,   \"red\" ,   \"blue\" ,   \"green\" )))   %>% \n  layer_points ()    .gif file:   1\n2\n3\n4 # Change the radiobuttons widget to a text widget  \nmtcars  %>% \n  ggvis ( ~ mpg ,   ~ wt ,  fill  :=  input_text ( label  =   \"Choose color:\" ,  value  =   c ( \"black\" )))   %>% \n  layer_points ()    .gif file:   1\n2\n3 mtcars  %>% \n  ggvis ( ~ mpg ,   ~ wt )   %>% \n  layer_points ()     1\n2\n3\n4 # Map the fill property to a select box that returns variable names \nmtcars  %>% \n  ggvis ( ~ mpg ,   ~ wt ,  fill  =  input_select ( label  =   \"Choose fill variable:\" ,  choices  =   names ( mtcars ),  map  =   as.name ))   %>% \n  layer_points ()    .gif file:   Input widgets in more detail (2)  1\n2\n3\n4 # Map the fill property to a select box that returns variable names \nmtcars  %>% \n  ggvis ( ~ mpg ,   ~ wt ,  fill  =  input_select ( label  =   \"Choose fill variable:\" ,  choices  =   names ( mtcars ),  map  =   as.name ))   %>% \n  layer_points ()    .gif file:   Control parameters and values  1\n2\n3\n4 # Map the bindwidth to a numeric field (\"Choose a binwidth:\") \nmtcars  %>% \n  ggvis ( ~ mpg )   %>% \n  layer_histograms ( width  =  input_numeric ( value  =   1 ,  label  =   \"Choose a binwidth:\" ))    .gif file:   1\n2\n3\n4 # Map the binwidth to a slider bar (\"Choose a binwidth:\") with the correct specifications \nmtcars  %>% \n  ggvis ( ~ mpg )   %>% \n  layer_histograms ( width  =  input_slider ( min  =   1 ,  max  =   20 ,  label  =   \"Choose a binwidth:\" ))    .gif file:   Multi-layered plots and their properties  1\n2\n3\n4 # Add a layer of points to the graph below. \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure ,  stroke  :=   \"skyblue\" )   %>%  layer_lines ()   %>% \n  layer_points ()     1\n2\n3\n4\n5 # Copy and adapt the solution to the first instruction below so that only the lines layer uses a skyblue stroke. \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure )   %>% \n  layer_lines ( stroke  :=   \"skyblue\" )   %>% \n  layer_points ()     1\n2\n3\n4\n5 # Rewrite the code below so that only the points layer uses the shape property. \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure )   %>% \n  layer_lines ( stroke  :=   \"skyblue\" )   %>% \n  layer_points ( shape  :=   \"triangle-up\" )     1\n2\n3\n4\n5 # Refactor the code for the graph below to make it as concise as possible \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure ,  stroke  :=   \"skyblue\" ,  strokeOpacity  :=   0.5 ,  strokeWidth  :=   5 )   %>% \n  layer_lines ()   %>% \n  layer_points ( fill  =   ~ temperature ,  shape  :=   \"triangle-up\" ,  size  :=   300 )     Multi-layered plots and their properties (2)  1\n2\n3\n4\n5 # Rewrite the code below so that only the points layer uses the shape property. \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure )   %>% \n  layer_lines ( stroke  :=   \"skyblue\" )   %>% \n  layer_points ( shape  :=   \"triangle-up\" )     1\n2\n3\n4\n5\n6 # Refactor the code for the graph below to make it as concise as possible \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure ,  stroke  :=   \"skyblue\" , \n        strokeOpacity  :=   0.5 ,  strokeWidth  :=   5 )   %>% \n  layer_lines ()   %>% \n  layer_points ( fill  =   ~ temperature ,  shape  :=   \"triangle-up\" ,  size  :=   300 )     There is no limit on the number of layers!  1\n2\n3\n4\n5\n6\n7 # Create a graph containing a scatterplot, a linear model and a smooth line. \npressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure )   %>% \n  layer_points ()   %>% \n  layer_lines ( stroke  :=   \"black\" ,  opacity  :=   0.5 )   %>% \n  layer_model_predictions ( model  =   \"lm\" ,  stroke  :=   \"navy\" )   %>% \n  layer_smooths ( stroke  :=   \"skyblue\" )     Taking local and global to the next level  1\n2\n3\n4\n5 pressure  %>% \n  ggvis ( ~ temperature ,   ~ pressure ,  stroke  :=   \"darkred\" )   %>% \n  layer_lines ( stroke  :=   \"orange\" ,  strokeDash  :=   5 ,  strokeWidth  :=   5 )   %>% \n  layer_points ( shape  :=   \"circle\" ,  size  :=   100 ,  fill  :=   \"lightgreen\" )   %>% \n  layer_smooths ()",
            "title": "Interactivity and Layers"
        },
        {
            "location": "/Plot_snippets_-_ggvis/#customizing-axes-legends-and-scales",
            "text": "Axes  1\n2\n3\n4\n5\n6 # add the title of the x axis: \"Time since previous eruption (m)\" \nfaithful  %>%  \n  ggvis ( ~ waiting ,   ~ eruptions )   %>%  \n  layer_points ()   %>%  \n  add_axis ( \"y\" ,  title  =   \"Duration of eruption (m)\" )   %>% \n  add_axis ( \"x\" ,  title  =   \"Time since previous eruption (m)\" )     1\n2\n3\n4\n5\n6 # Add to the code to place a labelled tick mark at 50, 60, 70, 80, 90 on the x axis. \nfaithful  %>%  \n  ggvis ( ~ waiting ,   ~ eruptions )   %>%  \n  layer_points ()   %>%  \n  add_axis ( \"y\" ,  title  =   \"Duration of eruption (m)\" ,  values  =   c ( 2 ,   3 ,   4 ,   5 ),  subdivide  =   9 )   %>%  \n  add_axis ( \"x\" ,  title  =   \"Time since previous eruption (m)\" ,  values  =   c ( 50 ,   60 ,   70 ,   80 ,   90 ),  subdivide  =   9 )     1\n2\n3\n4\n5 # Add to the code below to change the location of the y axis \nfaithful  %>%  \n  ggvis ( ~ waiting ,   ~ eruptions )   %>%  \n  layer_points ()   %>% \n  add_axis ( \"x\" ,  orient  =   \"top\" )   %>%  add_axis ( \"y\" ,  orient  =   \"right\" )     Legends  1\n2\n3\n4\n5 # Add a legend to the plot below: use the correct title and orientation \nfaithful  %>%  \n  ggvis ( ~ waiting ,   ~ eruptions ,  opacity  :=   0.6 ,  fill  =   ~ factor ( round ( eruptions )))   %>%  \n  layer_points ()   %>% \n  add_legend ( \"fill\" ,  title  =   \"~ duration (m)\" ,  orient  =   \"left\" )     1\n2\n3\n4\n5\n6\n7\n8 #add_legend(vis, scales = NULL, orient = \"right\", title = NULL, format = NULL, values = NULL, properties = NULL)  # Use add_legend() to combine the legends in the plot below. Adjust its properties as instructed. \nfaithful  %>%  \n  ggvis ( ~ waiting ,   ~ eruptions ,  opacity  :=   0.6 ,  fill  =   ~ factor ( round ( eruptions )),  shape  =   ~ factor ( round ( eruptions )),  \n        size  =   ~ round ( eruptions ))    %>% \n  layer_points ()   %>% \n  add_legend ( c ( \"fill\" ,   \"shape\" ,   \"size\" ),  title  =   \"~ duration (m)\" ,  values  =   c ( 2 , 3 , 4 , 5 ))     Legends (2)  1\n2\n3\n4\n5\n6 # Fix the legend \nfaithful  %>%  \n  ggvis ( ~ waiting ,   ~ eruptions ,  opacity  :=   0.6 ,  fill  =   ~ factor ( round ( eruptions )),  shape  =   ~ factor ( round ( eruptions )),  \n        size  =   ~ round ( eruptions ))   %>%  \n    layer_points ()   %>%  \n    add_legend ( c ( \"fill\" ,   \"shape\" ,   \"size\" ),  title  =   \"~ duration (m)\" )     Scale types  1\n2\n3\n4\n5 # Add to the code below to make the stroke color range from \"darkred\" to \"orange\". \nmtcars  %>%  \n  ggvis ( ~ wt ,   ~ mpg ,  fill  =   ~ disp ,  stroke  =   ~ disp ,  strokeWidth  :=   2 )   %>% \n  layer_points ()   %>% \n  scale_numeric ( \"fill\" ,  range  =   c ( \"red\" ,   \"yellow\" ))   %>%  scale_numeric ( \"stroke\" ,  range  =   c ( \"darkred\" ,   \"orange\" ))     1\n2\n3 # Change the graph below to make the fill colors range from green to beige. \nmtcars  %>%  ggvis ( ~ wt ,   ~ mpg ,  fill  =   ~ hp )   %>% \n  layer_points ()   %>%  scale_numeric ( \"fill\" ,  range  =   c ( \"green\" ,   \"beige\" ))     1\n2\n3 # Create a scale that will map `factor(cyl)` to a new range of colors: purple, blue, and green.  \nmtcars  %>%  ggvis ( ~ wt ,   ~ mpg ,  fill  =   ~ factor ( cyl ))   %>% \n  layer_points ()   %>%  scale_nominal ( \"fill\" ,  range  =   c ( \"purple\" ,   \"blue\" ,   \"green\" ))     Adjust any visual property  1\n2\n3\n4\n5 # Add a scale that limits the range of opacity from 0.2 to 1.  \nmtcars  %>% \n  ggvis ( x  =   ~ wt ,  y  =   ~ mpg ,  fill  =   ~ factor ( cyl ),  opacity  =   ~ hp )   %>% \n  layer_points ()   %>% \n  scale_numeric ( \"opacity\" ,  range  =   c ( 0.2 , 1 ))     1\n2\n3\n4\n5\n6 # Add a second scale that will expand the x axis to cover data values from 0 to 6. \nmtcars  %>% \n  ggvis ( ~ wt ,   ~ mpg ,  fill  =   ~ disp )   %>% \n  layer_points ()   %>% \n  scale_numeric ( \"y\" ,  domain  =   c ( 0 ,   NA ))   %>% \n  scale_numeric ( \"x\" ,  domain  =   c ( 0 ,   6 ))     Adjust any visual property (2)  1\n2\n3\n4\n5\n6 # Add a second scale to set domain for x \nmtcars  %>% \n  ggvis ( ~ wt ,   ~ mpg ,  fill  =   ~ disp )   %>% \n  layer_points ()   %>% \n  scale_numeric ( \"y\" ,  domain  =   c ( 0 ,   NA ))   %>% \n  scale_numeric ( \"x\" ,  domain  =   c ( 0 ,   6 ))     \u201c = \u201d versus \u201c := \u201c  1\n2\n3\n4\n5\n6 # Set the fill value to the color variable instead of mapping it, and see what happens \nmtcars $ color  <-   c ( \"red\" ,   \"teal\" ,   \"#cccccc\" ,   \"tan\" ) \n\nmtcars  %>% \n  ggvis ( x  =   ~ wt ,  y  =   ~ mpg ,  fill  =   ~ color )   %>% \n  layer_points ()     1\n2\n3 mtcars  %>% \n  ggvis ( x  =   ~ wt ,  y  =   ~ mpg ,  fill  :=   ~ color )   %>% \n  layer_points ()",
            "title": "Customizing Axes, Legends, and Scales"
        },
        {
            "location": "/Plot_snippets_-_Colours/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nDefault colours\n\u00b6\n\n\n1\n2\n3\n# Set\n\npalette\n(\n'default'\n)\n\npalette\n()\n\n\n\n\n\n\n\n1\n2\n## [1] \"black\"   \"red\"     \"green3\"  \"blue\"    \"cyan\"    \"magenta\" \"yellow\" \n## [8] \"gray\"\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# Show\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n2\n))\n\n\nn \n<-\n \n8\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n \nFALSE\n,\n main \n=\n \n'colourless'\n)\n\nn \n<-\n \n8\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n palette\n(),\n main \n=\n \n'default'\n)\n\n\n\n\n\n\n\n\n\n1\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n1\n))\n\n\n\n\n\n\n\nBasic colours\n\u00b6\n\n\n1\n2\n3\n# 8 types times 9 tones\n\nn \n<-\n \n9\n\nrainbow\n(\nn\n,\n s \n=\n \n1\n,\n v \n=\n \n1\n,\n start \n=\n \n0\n,\n end \n=\n \nmax\n(\n1\n,\n n \n-\n \n1\n)\n/\nn\n,\n alpha \n=\n \n1\n)\n\n\n\n\n\n\n\n1\n2\n## [1] \"#FF0000FF\" \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\"\n## [7] \"#0000FFFF\" \"#AA00FFFF\" \"#FF00AAFF\"\n\n\n\n\n\n\n1\n2\nm \n<-\n \n(\n1\n:\nn\n)\n/\nn\ngray\n(\nm\n)\n\n\n\n\n\n\n\n1\n2\n## [1] \"#1C1C1C\" \"#393939\" \"#555555\" \"#717171\" \"#8E8E8E\" \"#AAAAAA\" \"#C6C6C6\"\n## [8] \"#E3E3E3\" \"#FFFFFF\"\n\n\n\n\n\n\n1\nhsv\n(\nm\n,\n s \n=\n \n1\n,\n v \n=\n \n1\n,\n alpha \n=\n \n1\n)\n\n\n\n\n\n\n\n1\n2\n## [1] \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\" \"#0000FFFF\"\n## [7] \"#AA00FFFF\" \"#FF00AAFF\" \"#FF0000FF\"\n\n\n\n\n\n\n1\nblues9\n\n\n\n\n\n\n1\n2\n## [1] \"#F7FBFF\" \"#DEEBF7\" \"#C6DBEF\" \"#9ECAE1\" \"#6BAED6\" \"#4292C6\" \"#2171B5\"\n## [8] \"#08519C\" \"#08306B\"\n\n\n\n\n\n\n1\nheat.colors\n(\nn\n,\n alpha \n=\n \n1\n)\n\n\n\n\n\n\n\n1\n2\n## [1] \"#FF0000FF\" \"#FF2A00FF\" \"#FF5500FF\" \"#FF8000FF\" \"#FFAA00FF\" \"#FFD500FF\"\n## [7] \"#FFFF00FF\" \"#FFFF40FF\" \"#FFFFBFFF\"\n\n\n\n\n\n\n1\nterrain.colors\n(\nn\n,\n alpha \n=\n \n1\n)\n\n\n\n\n\n\n\n1\n2\n## [1] \"#00A600FF\" \"#3EBB00FF\" \"#8BD000FF\" \"#E6E600FF\" \"#E8C32EFF\" \"#EBB25EFF\"\n## [7] \"#EDB48EFF\" \"#F0C9C0FF\" \"#F2F2F2FF\"\n\n\n\n\n\n\n1\ncm.colors\n(\nn\n,\n alpha \n=\n \n1\n)\n\n\n\n\n\n\n\n1\n2\n## [1] \"#80FFFFFF\" \"#9FFFFFFF\" \"#BFFFFFFF\" \"#DFFFFFFF\" \"#FFFFFFFF\" \"#FFDFFFFF\"\n## [7] \"#FFBFFFFF\" \"#FF9FFFFF\" \"#FF80FFFF\"\n\n\n\n\n\n\n1\ntopo.colors\n(\nn\n,\n alpha \n=\n \n1\n)\n\n\n\n\n\n\n\n1\n2\n## [1] \"#4C00FFFF\" \"#004CFFFF\" \"#00E5FFFF\" \"#00FF4DFF\" \"#4DFF00FF\" \"#E6FF00FF\"\n## [7] \"#FFFF00FF\" \"#FFDE59FF\" \"#FFE0B3FF\"\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# Show\n\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n \n2\n))\n\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n rainbow\n(\nn\n,\n alpha \n=\n \n1\n),\n main \n=\n \n'rainbow'\n)\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n gray\n(\nm\n),\n main \n=\n \n'gray'\n)\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n hsv\n(\nm\n,\n alpha \n=\n \n1\n),\n main \n=\n \n'hsv'\n)\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n blues9\n,\n main \n=\n \n'blues9'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n heat.colors\n(\nn\n,\n alpha \n=\n \n1\n),\n main \n=\n \n'heat'\n)\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n terrain.colors\n(\nn\n,\n alpha \n=\n \n1\n),\n main \n=\n \n'terrain'\n)\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n cm.colors\n(\nn\n),\n main \n=\n \n'cm.colors'\n)\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n topo.colors\n(\nn\n,\n alpha \n=\n \n1\n),\n main \n=\n \n'topo'\n)\n\n\n\n\n\n\n\n\n\n1\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n1\n))\n\n\n\n\n\n\n\nRColorBrewer examples\n\u00b6\n\n\n1\n2\n3\n4\nlibrary\n(\nRColorBrewer\n)\n\n\n\n# Show all\n\ndisplay.brewer.all\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Pick a palette\n\nn \n<-\n \n8\n\ncolors \n<-\n brewer.pal\n(\nn\n,\n \n\"BuPu\"\n)\n\ncolors\n\n\n\n\n\n\n1\n2\n## [1] \"#F7FCFD\" \"#E0ECF4\" \"#BFD3E6\" \"#9EBCDA\" \"#8C96C6\" \"#8C6BB1\" \"#88419D\"\n## [8] \"#6E016B\"\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n2\n))\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n colors\n,\n main \n=\n \n'Sequential RdPu'\n)\n\n\n\n# Interpolate these colors\n\npal \n<-\n colorRampPalette\n(\nbrewer.pal\n(\nn\n,\n \n'RdPu'\n))\n\npal\n(\n8\n)\n\n\n\n\n\n\n\n1\n2\n## [1] \"#FFF7F3\" \"#FDE0DD\" \"#FCC5C0\" \"#FA9FB5\" \"#F768A1\" \"#DD3497\" \"#AE017E\"\n## [8] \"#7A0177\"\n\n\n\n\n\n\n1\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n pal\n(\n8\n),\n main \n=\n \n'Interpolated RdPu'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Apply\n\ndata\n(\nvolcano\n)\n\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n \n1\n))\n\nimage\n(\nvolcano\n,\n col \n=\n pal\n(\n8\n))\n\nimage\n(\nvolcano\n,\n col \n=\n pal\n(\n30\n))\n\n\n\n\n\n\n\n\n\n1\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n1\n))\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Show samples\n\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n \n2\n))\n\n\nn \n=\n \n9\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n brewer.pal\n(\nn\n,\n \n'RdPu'\n),\n main \n=\n \n'Sequential RdPu'\n)\n\nn \n=\n \n9\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n brewer.pal\n(\nn\n,\n \n'Set1'\n),\n main \n=\n \n'Qualitative Set1'\n)\n\nn \n=\n \n12\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n brewer.pal\n(\nn\n,\n \n'Paired'\n),\n main \n=\n \n'Qualitative Paired'\n)\n\nn \n=\n \n11\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n brewer.pal\n(\nn\n,\n \n'RdBu'\n),\n main \n=\n \n'Divergent RdBu'\n)\n\n\n\n\n\n\n\n\n\n1\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n1\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Show\n\nn \n=\n \n8\n\ndarkcols \n<-\n brewer.pal\n(\nn\n,\n \n'Dark2'\n)\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n darkcols\n,\n main \n=\n \n'Dark2'\n)\n\n\n\n\n\n\n\n\n\nBuilding a palette\n\u00b6\n\n\n1\n2\n# All\n\n\nhead\n(\ncolors\n())\n\n\n\n\n\n\n\n1\n2\n## [1] \"white\"         \"aliceblue\"     \"antiquewhite\"  \"antiquewhite1\"\n## [5] \"antiquewhite2\" \"antiquewhite3\"\n\n\n\n\n\n\n1\nlength\n(\ncolors\n())\n \n# 657\n\n\n\n\n\n\n\n1\n## [1] 657\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Create\n\nmycols \n<-\n colors\n()[\nc\n(\n8\n,\n \n5\n,\n \n30\n,\n \n53\n,\n \n118\n,\n \n72\n)]\n \n#\n\n\n# or\n\n\n# mycols <- c('aquamarine', 'antiquewhite2', 'blue4', 'chocolate1', 'deeppink2', 'cyan4')\n\n\n\n# Show\n\nn \n=\n \n6\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n mycols\n,\n main \n=\n \n'mycols'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Generate randomly\n\ncl \n<-\n colors\n(\ndistinct \n=\n \nTRUE\n)\n\n\nset.seed\n(\n15887\n)\n \n# to set random generator seed\n\nmycols2 \n<-\n \nsample\n(\ncl\n,\n \n7\n)\n\n\n\n# Show\n\nn \n=\n \n7\n\npie\n(\nrep\n(\n1\n,\n n\n),\n col \n=\n mycols2\n,\n main \n=\n \n'mycols2 (random)'\n)\n\n\n\n\n\n\n\n\n\nGrabing colours\n\u00b6\n\n\n\n\nGrab Website Colors\n.\n\n\nRGB Color Codes Chart\n.",
            "title": "Plot Snippets - Colours"
        },
        {
            "location": "/Plot_snippets_-_Colours/#basic-colours",
            "text": "1\n2\n3 # 8 types times 9 tones \nn  <-   9 \nrainbow ( n ,  s  =   1 ,  v  =   1 ,  start  =   0 ,  end  =   max ( 1 ,  n  -   1 ) / n ,  alpha  =   1 )    1\n2 ## [1] \"#FF0000FF\" \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\"\n## [7] \"#0000FFFF\" \"#AA00FFFF\" \"#FF00AAFF\"   1\n2 m  <-   ( 1 : n ) / n\ngray ( m )    1\n2 ## [1] \"#1C1C1C\" \"#393939\" \"#555555\" \"#717171\" \"#8E8E8E\" \"#AAAAAA\" \"#C6C6C6\"\n## [8] \"#E3E3E3\" \"#FFFFFF\"   1 hsv ( m ,  s  =   1 ,  v  =   1 ,  alpha  =   1 )    1\n2 ## [1] \"#FFAA00FF\" \"#AAFF00FF\" \"#00FF00FF\" \"#00FFAAFF\" \"#00AAFFFF\" \"#0000FFFF\"\n## [7] \"#AA00FFFF\" \"#FF00AAFF\" \"#FF0000FF\"   1 blues9   1\n2 ## [1] \"#F7FBFF\" \"#DEEBF7\" \"#C6DBEF\" \"#9ECAE1\" \"#6BAED6\" \"#4292C6\" \"#2171B5\"\n## [8] \"#08519C\" \"#08306B\"   1 heat.colors ( n ,  alpha  =   1 )    1\n2 ## [1] \"#FF0000FF\" \"#FF2A00FF\" \"#FF5500FF\" \"#FF8000FF\" \"#FFAA00FF\" \"#FFD500FF\"\n## [7] \"#FFFF00FF\" \"#FFFF40FF\" \"#FFFFBFFF\"   1 terrain.colors ( n ,  alpha  =   1 )    1\n2 ## [1] \"#00A600FF\" \"#3EBB00FF\" \"#8BD000FF\" \"#E6E600FF\" \"#E8C32EFF\" \"#EBB25EFF\"\n## [7] \"#EDB48EFF\" \"#F0C9C0FF\" \"#F2F2F2FF\"   1 cm.colors ( n ,  alpha  =   1 )    1\n2 ## [1] \"#80FFFFFF\" \"#9FFFFFFF\" \"#BFFFFFFF\" \"#DFFFFFFF\" \"#FFFFFFFF\" \"#FFDFFFFF\"\n## [7] \"#FFBFFFFF\" \"#FF9FFFFF\" \"#FF80FFFF\"   1 topo.colors ( n ,  alpha  =   1 )    1\n2 ## [1] \"#4C00FFFF\" \"#004CFFFF\" \"#00E5FFFF\" \"#00FF4DFF\" \"#4DFF00FF\" \"#E6FF00FF\"\n## [7] \"#FFFF00FF\" \"#FFDE59FF\" \"#FFE0B3FF\"   1\n2\n3\n4\n5\n6\n7 # Show \npar ( mfrow  =   c ( 2 ,   2 )) \n\npie ( rep ( 1 ,  n ),  col  =  rainbow ( n ,  alpha  =   1 ),  main  =   'rainbow' ) \npie ( rep ( 1 ,  n ),  col  =  gray ( m ),  main  =   'gray' ) \npie ( rep ( 1 ,  n ),  col  =  hsv ( m ,  alpha  =   1 ),  main  =   'hsv' ) \npie ( rep ( 1 ,  n ),  col  =  blues9 ,  main  =   'blues9' )     1\n2\n3\n4 pie ( rep ( 1 ,  n ),  col  =  heat.colors ( n ,  alpha  =   1 ),  main  =   'heat' ) \npie ( rep ( 1 ,  n ),  col  =  terrain.colors ( n ,  alpha  =   1 ),  main  =   'terrain' ) \npie ( rep ( 1 ,  n ),  col  =  cm.colors ( n ),  main  =   'cm.colors' ) \npie ( rep ( 1 ,  n ),  col  =  topo.colors ( n ,  alpha  =   1 ),  main  =   'topo' )     1 par ( mfrow  =   c ( 1 ,   1 ))",
            "title": "Basic colours"
        },
        {
            "location": "/Plot_snippets_-_Colours/#rcolorbrewer-examples",
            "text": "1\n2\n3\n4 library ( RColorBrewer )  # Show all \ndisplay.brewer.all ()     1\n2\n3\n4 # Pick a palette \nn  <-   8 \ncolors  <-  brewer.pal ( n ,   \"BuPu\" ) \ncolors   1\n2 ## [1] \"#F7FCFD\" \"#E0ECF4\" \"#BFD3E6\" \"#9EBCDA\" \"#8C96C6\" \"#8C6BB1\" \"#88419D\"\n## [8] \"#6E016B\"   1\n2\n3\n4\n5\n6 par ( mfrow  =   c ( 1 ,   2 )) \npie ( rep ( 1 ,  n ),  col  =  colors ,  main  =   'Sequential RdPu' )  # Interpolate these colors \npal  <-  colorRampPalette ( brewer.pal ( n ,   'RdPu' )) \npal ( 8 )    1\n2 ## [1] \"#FFF7F3\" \"#FDE0DD\" \"#FCC5C0\" \"#FA9FB5\" \"#F768A1\" \"#DD3497\" \"#AE017E\"\n## [8] \"#7A0177\"   1 pie ( rep ( 1 ,  n ),  col  =  pal ( 8 ),  main  =   'Interpolated RdPu' )     1\n2\n3\n4\n5 # Apply \ndata ( volcano ) \npar ( mfrow  =   c ( 2 ,   1 )) \nimage ( volcano ,  col  =  pal ( 8 )) \nimage ( volcano ,  col  =  pal ( 30 ))     1 par ( mfrow  =   c ( 1 ,   1 ))     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Show samples \npar ( mfrow  =   c ( 2 ,   2 )) \n\nn  =   9 \npie ( rep ( 1 ,  n ),  col  =  brewer.pal ( n ,   'RdPu' ),  main  =   'Sequential RdPu' ) \nn  =   9 \npie ( rep ( 1 ,  n ),  col  =  brewer.pal ( n ,   'Set1' ),  main  =   'Qualitative Set1' ) \nn  =   12 \npie ( rep ( 1 ,  n ),  col  =  brewer.pal ( n ,   'Paired' ),  main  =   'Qualitative Paired' ) \nn  =   11 \npie ( rep ( 1 ,  n ),  col  =  brewer.pal ( n ,   'RdBu' ),  main  =   'Divergent RdBu' )     1 par ( mfrow  =   c ( 1 ,   1 ))    1\n2\n3\n4 # Show \nn  =   8 \ndarkcols  <-  brewer.pal ( n ,   'Dark2' ) \npie ( rep ( 1 ,  n ),  col  =  darkcols ,  main  =   'Dark2' )",
            "title": "RColorBrewer examples"
        },
        {
            "location": "/Plot_snippets_-_Colours/#building-a-palette",
            "text": "1\n2 # All  head ( colors ())    1\n2 ## [1] \"white\"         \"aliceblue\"     \"antiquewhite\"  \"antiquewhite1\"\n## [5] \"antiquewhite2\" \"antiquewhite3\"   1 length ( colors ())   # 657    1 ## [1] 657   1\n2\n3\n4\n5\n6\n7\n8 # Create \nmycols  <-  colors ()[ c ( 8 ,   5 ,   30 ,   53 ,   118 ,   72 )]   #  # or  # mycols <- c('aquamarine', 'antiquewhite2', 'blue4', 'chocolate1', 'deeppink2', 'cyan4')  # Show \nn  =   6 \npie ( rep ( 1 ,  n ),  col  =  mycols ,  main  =   'mycols' )     1\n2\n3\n4\n5\n6\n7\n8 # Generate randomly \ncl  <-  colors ( distinct  =   TRUE )  set.seed ( 15887 )   # to set random generator seed \nmycols2  <-   sample ( cl ,   7 )  # Show \nn  =   7 \npie ( rep ( 1 ,  n ),  col  =  mycols2 ,  main  =   'mycols2 (random)' )",
            "title": "Building a palette"
        },
        {
            "location": "/Plot_snippets_-_Colours/#grabing-colours",
            "text": "Grab Website Colors .  RGB Color Codes Chart .",
            "title": "Grabing colours"
        },
        {
            "location": "/googleVis/",
            "text": "Foreword\n\n\nNotes, snippets, and results.\n\n\n\n\nThe \ngoogleVis\n package\n\u00b6\n\n\nThe package provides an interface to Google\u2019s chart tools, allowing users to create interactive charts based on data frames. It included maps.\n\n\nThe interactive maps are displayed in a browser. We can plot a complete set of interactive graphs and embed them into a web page.\n\n\nSome motion charts cannot be displays in tablets and mobile phones (using HTML5) because they are rendered with Flash; Flash has to be installed on a PC.\n\n\n\n\nExamples\n.\n\n\nCharts: line, bar, column, area, stepped area, combo, scatter, bubble, customizing, candlestick (or boxplot), pie, gauge, annotation, Sankey, histogram, and motion (GapMinder-like)\n\n\nMaps: intensity, geo, choropleth, marker, Google Maps.\n\n\nTable, organizational chart, tree map, calendar, timeline, merging.\n\n\nMotion charts and some maps only work in Flash, not in HTML5 as with tablets and mobile phones).\n\n\n\n\n\n\nGallery\n.\n\n\nDocumentation\n.\n\n\nAs above.\n\n\n\n\n\n\nIntroduction\n.\n\n\nRoles\n.\n\n\nTrendlines\n.\n\n\nMarkdown\n.\n\n\nIn R, run a demo with \ndemo(googleVis)\n.\n\n\n\n\nAlways cite the package:\n\n\n1\ncitation\n(\n\"googleVis\"\n)\n\n\n\n\n\n\n\nSuppress the message\n\u00b6\n\n\nWe normally load the library\u2026\n\n\n1\nlibrary\n(\ngoogleVis\n)\n\n\n\n\n\n\n\n\u2026but we can also suppress the message when loading the package.\n\n\n1\nsuppressPackageStartupMessages\n(\nlibrary\n(\ngoogleVis\n))\n\n\n\n\n\n\n\nPrinting\n\u00b6\n\n\n\n\nGenerate the chart.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nlibrary\n(\ngoogleVis\n)\n\n\n\nhead\n(\nFruits\n,\n \n3\n)\n\n\nM \n<-\n gvisMotionChart\n(\nFruits\n,\n\n                     idvar\n=\n'Fruit'\n,\n\n                     timevar\n=\n'Year'\n,\n\n                     options\n=\nlist\n(\nwidth\n=\n400\n,\n height\n=\n350\n))\n\n\n\n# xvar =\n\n\n# yvar =\n\n\n# colorvar =\n\n\n# sizevar =\n\n\n# date.format =\n\n\n\n\n\n\n\n\n\nEmulate the chart\u2026\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n# In the browser, with a tag underneath\n\nplot\n(\nM\n)\n\n\n\n# In the console (code only), with a tag underneath\n\nM\n\n\n# Header part\n\n\n# Basic html and formatting tags\n\n\nprint\n(\nM\n,\n tag\n=\n'header'\n)\n\n\n\n# Actual Google visualization code\n\n\n# Can be copy-paste in a markdown or html document\n\n\nprint\n(\nM\n,\n tag\n=\n'chart'\n)\n\n\n\n# Header + visualization code = what we see in the browser\n\n\n\n# Components of the chart\n\n\nprint\n(\nM\n,\n tag\n=\n'jsChart'\n)\n\n\n\n# Basic chart caption and html footer (what is underneath)\n\n\nprint\n(\nM\n,\n tag\n=\n'caption'\n)\n\n\n\n# Save it locally\n\n\nprint\n(\nM\n,\n file\n=\n\"GoogleVis/M.html\"\n)\n\n\n# Or\n\n\n#cat(M$html$chart, file = \"GoogleVis/M.html\")\n\n\n\n\n\n\n\nEmbedding a chart/map into a static website\n\u00b6\n\n\nThe chart can be standing alone as an image file; even be embedded within a text.\n\n\n\n\nIn RStudio, set the working directory with \nsetwd(' ')\n.\n\n\nSet the option to print the output in a html file.\n\n\n\n\n1\n2\n3\n4\n{\nr\n,\n message\n=\nFALSE\n}\n\n\nlibrary\n(\ngoogleVis\n)\n\n\nop \n<-\n \noptions\n(\ngvis.plot.tag\n=\n'chart'\n)\n\n\n\n\n\n\n\n\n\nSet options back to original options.\n\n\n\n\n1\noptions\n(\nop\n)\n\n\n\n\n\n\n\n\n\nGenerate the chart.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nlibrary\n(\ngoogleVis\n)\n\n\n\n# Set the option to print here (alternative)\n\n\noptions\n(\ngvis.plot.tag\n=\n'chart'\n)\n\n\n\n# Create the chart\n\nM \n<-\n gvisMotionChart\n(\nFruits\n,\n\n                     idvar\n=\n'Fruit'\n,\n\n                     timevar\n=\n'Year'\n,\n\n                     options\n=\nlist\n(\nwidth\n=\n400\n,\n height\n=\n350\n))\n\n\n\n# Do not print the chart in the browser\n\n\n#plot(M) # in the browser\n\n\n\n# Print the chart as a html file\n\n\ncat\n(\nM\n$\nhtml\n$\nchart\n,\n file \n=\n \n\"M.html\"\n)\n\n\n\n\n\n\n\n\n\nThe file has several parts: JavaScript and HTML. The part that displays the chart is in the end.\n\n\n\n\n1\n2\n3\n4\n5\n<!-- divChart -->\n\n\n\n<\ndiv\n \nid\n=\n\"MotionChartID2cd02805862d\"\n \n     \nstyle\n=\n\"width: 400; height: 350;\"\n>\n\n\n</\ndiv\n>\n\n\n\n\n\n\n\n\n\nFollowing this bullet, in the Markdown document, paste the M.html code. First, open the HTML file to copy the code.\n\n\nModify the markup language with more styling parameters.\n\n\n\n\n1\n2\n3\n<\ndiv\n \nid\n=\n\"MotionChartID2cd02805862d\"\n \n\nstyle\n=\n\"width: 400; height: 350; float:left;\"\n;\n>\n\n\n</\ndiv\n>\n\n\n\n\n\n\n\n\n\nThe HTML snippet must follow the JavaScript snippet in the document. Move the HTML snippet within a text (the text can separate both snippets). \n\n\n\n\n1\n2\n3\n4\n5\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n\n<\ndiv\n \nid\n=\n\"MotionChartID2cd02805862d\"\n \n\nstyle\n=\n\"width: 400; height: 350; float:left;\"\n;\n>\n\n\n</\ndiv\n>\n\nDuis aute irure dolor...\n\n\n\n\n\n\nHere is the result:\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataMotionChartID2cd02805862d () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Apples\",\n2008,\n\"West\",\n98,\n78,\n20,\n\"2008-12-31\"\n],\n[\n\"Apples\",\n2009,\n\"West\",\n111,\n79,\n32,\n\"2009-12-31\"\n],\n[\n\"Apples\",\n2010,\n\"West\",\n89,\n76,\n13,\n\"2010-12-31\"\n],\n[\n\"Oranges\",\n2008,\n\"East\",\n96,\n81,\n15,\n\"2008-12-31\"\n],\n[\n\"Bananas\",\n2008,\n\"East\",\n85,\n76,\n9,\n\"2008-12-31\"\n],\n[\n\"Oranges\",\n2009,\n\"East\",\n93,\n80,\n13,\n\"2009-12-31\"\n],\n[\n\"Bananas\",\n2009,\n\"East\",\n94,\n78,\n16,\n\"2009-12-31\"\n],\n[\n\"Oranges\",\n2010,\n\"East\",\n98,\n91,\n7,\n\"2010-12-31\"\n],\n[\n\"Bananas\",\n2010,\n\"East\",\n81,\n71,\n10,\n\"2010-12-31\"\n] \n];\ndata.addColumn('string','Fruit');\ndata.addColumn('number','Year');\ndata.addColumn('string','Location');\ndata.addColumn('number','Sales');\ndata.addColumn('number','Expenses');\ndata.addColumn('number','Profit');\ndata.addColumn('string','Date');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartMotionChartID2cd02805862d() {\nvar data = gvisDataMotionChartID2cd02805862d();\nvar options = {};\noptions[\"width\"] = 400;\noptions[\"height\"] = 350;\noptions[\"state\"] = \"\";\n\n\n    var chart = new google.visualization.MotionChart(\n    document.getElementById('MotionChartID2cd02805862d')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"motionchart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartMotionChartID2cd02805862d);\n})();\nfunction displayChartMotionChartID2cd02805862d() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n\n\n\n\n\n\n\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem.\n\n\nOther static websites\n\u00b6\n\n\nThere is a procedure for embedding graphics in:\n\n\n\n\nWordPress.\n\n\nGoogle Sites.\n\n\nBlogger.\n\n\nGoogle Code wiki pages.\n\n\nWikipedia.\n\n\nothers websites.\n\n\n\n\nDynamic websites\n\u00b6\n\n\n\n\nThe \nR.rsp\n package allows the integration of R code into html code. \n\n\nThe \nrApache\n and \nbrew\n packages support web application development using R and the Apache HTTP server.\n\n\nRook is a lightweight web server interface for R.\n\n\nThe \nshiny\n package builds interactive web application with R.\n\n\n\n\nCharts\n\u00b6\n\n\nConsult the examples (further above); charts are similar to what we find in other packages. \n\n\ngvisMotionChart\n is exclusive to \ngoogleVis\n.\n\n\ngvisMotionChart\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n1\nhead\n(\nFruits\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Output\n   Fruit Year Location Sales Expenses Profit       Date\n1 Apples 2008     West    98       78     20 2008-12-31\n2 Apples 2009     West   111       79     32 2009-12-31\n3 Apples 2010     West    89       76     13 2010-12-31\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nM \n<-\n gvisMotionChart\n(\nFruits\n,\n\n                     idvar\n=\n'Fruit'\n,\n\n                     timevar\n=\n'Year'\n,\n\n                     options\n=\nlist\n(\nwidth\n=\n400\n,\n height\n=\n350\n))\n\n\n\n# xvar =\n\n\n# yvar =\n\n\n# colorvar =\n\n\n# sizevar =\n\n\n# date.format =\n\n\nplot\n(\nM\n)\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataMotionChartID336e668836ab () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Apples\",\n2008,\n\"West\",\n98,\n78,\n20,\n\"2008-12-31\"\n],\n[\n\"Apples\",\n2009,\n\"West\",\n111,\n79,\n32,\n\"2009-12-31\"\n],\n[\n\"Apples\",\n2010,\n\"West\",\n89,\n76,\n13,\n\"2010-12-31\"\n],\n[\n\"Oranges\",\n2008,\n\"East\",\n96,\n81,\n15,\n\"2008-12-31\"\n],\n[\n\"Bananas\",\n2008,\n\"East\",\n85,\n76,\n9,\n\"2008-12-31\"\n],\n[\n\"Oranges\",\n2009,\n\"East\",\n93,\n80,\n13,\n\"2009-12-31\"\n],\n[\n\"Bananas\",\n2009,\n\"East\",\n94,\n78,\n16,\n\"2009-12-31\"\n],\n[\n\"Oranges\",\n2010,\n\"East\",\n98,\n91,\n7,\n\"2010-12-31\"\n],\n[\n\"Bananas\",\n2010,\n\"East\",\n81,\n71,\n10,\n\"2010-12-31\"\n] \n];\ndata.addColumn('string','Fruit');\ndata.addColumn('number','Year');\ndata.addColumn('string','Location');\ndata.addColumn('number','Sales');\ndata.addColumn('number','Expenses');\ndata.addColumn('number','Profit');\ndata.addColumn('string','Date');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartMotionChartID336e668836ab() {\nvar data = gvisDataMotionChartID336e668836ab();\nvar options = {};\noptions[\"width\"] = 400;\noptions[\"height\"] = 350;\noptions[\"state\"] = \"\";\n\n\n    var chart = new google.visualization.MotionChart(\n    document.getElementById('MotionChartID336e668836ab')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"motionchart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartMotionChartID336e668836ab);\n})();\nfunction displayChartMotionChartID336e668836ab() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmbedding with \n<iframe>\n or \n<embed>\n\u00b6\n\n\nAfter we generate the HTML chart or map, save the object as a HTML file.\n\n\n1\n2\n3\n4\nlibrary\n(\nhtmlwidgets\n)\n \n\nlibrary\n(\nDT\n)\n \n\nsaveWidget\n(\nobject\n,\n \n\"googleVis.html\"\n)\n\n\n\n\n\n\n\nAdd these HTML snippets to the Markdown/HTML document.\n\n\n1\n2\n<\niframe\n \nseamless\n \nsrc\n=\n\"../googleVis.html\"\n \nwidth\n=\n600px\n \n\nheight\n=\n400px\n \n></\niframe\n>\n\n\n\n\n\n\n\n\n\n\n1\n<\nembed\n \nseamless\n \nsrc\n=\n\"../googleVis.html\"\n \nwidth\n=\n600px\n \nheight\n=\n400px\n \n></\nembed\n>\n\n\n\n\n\n\n\n\n\nIn other words, we can either write the code or embed a file into the HTML/Markdown document.\n\n\nInformation from the object\n\u00b6\n\n\n1\n2\nM\n$\ntype\nM\n$\nchartid\n\n\n\n\n\n\n1\n2\n\"MotionChart\"\n\"MotionChartID336e668836ab\"\n\n\n\n\n\n\nMaps\n\u00b6\n\n\ngvisGeoChart\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n1\nhead\n(\nExports\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Output\n        Country Profit Online\n1       Germany      3   TRUE\n2        Brazil      4  FALSE\n3 United States      5   TRUE\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nGeo \n<-\n gvisGeoChart\n(\nExports\n,\n \n                    locationvar \n=\n \n\"Country\"\n,\n\n                    colorvar \n=\n \n\"Profit\"\n,\n\n                    sizevar \n=\n \n\"\"\n,\n \n# size of markers\n\n                    hovervar \n=\n \n\"\"\n,\n \n# text\n\n                    options \n=\n \nlist\n(\nprojection\n=\n\"kavrayskiy-vii\"\n))\n\n\n\n# locationvar can be lat:long or address, country name, region, state, city\n\n\nplot\n(\nGeo\n)\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID49617fef373 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49617fef373() {\nvar data = gvisDataGeoChartID49617fef373();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 347;\noptions[\"projection\"] = \"kavrayskiy-vii\";\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID49617fef373')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49617fef373);\n})();\nfunction displayChartGeoChartID49617fef373() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nhead\n(\nCityPopularity\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Output\n      City Popularity\n1 New York        200\n2   Boston        300\n3    Miami        400\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nGeo2 \n<-\n gvisGeoChart\n(\nCityPopularity\n,\n \n                     locationvar\n=\n'City'\n,\n\n                     colorvar\n=\n'Popularity'\n,\n\n                     options\n=\nlist\n(\nregion\n=\n'US'\n,\n\n                                  height\n=\n350\n,\n\n                                  displayMode\n=\n'markers'\n,\n \n                                  colorAxis\n=\n\"{values:[200,400,600,800], colors:[\\'red', \\'pink\\', \\'orange',\\'green']}\"\n))\n\n\nplot\n(\nGeo2\n)\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID4961741b9385 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"New York\",\n200\n],\n[\n\"Boston\",\n300\n],\n[\n\"Miami\",\n400\n],\n[\n\"Chicago\",\n500\n],\n[\n\"Los Angeles\",\n600\n],\n[\n\"Houston\",\n700\n] \n];\ndata.addColumn('string','City');\ndata.addColumn('number','Popularity');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID4961741b9385() {\nvar data = gvisDataGeoChartID4961741b9385();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 350;\noptions[\"region\"] = \"US\";\noptions[\"displayMode\"] = \"markers\";\noptions[\"colorAxis\"] = {values:[200,400,600,800], colors:['red', 'pink', 'orange','green']};\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID4961741b9385')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID4961741b9385);\n})();\nfunction displayChartGeoChartID4961741b9385() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nCityPopularity3 \n<-\n \ndata.frame\n(\nCity \n=\n \nc\n(\n'Montreal'\n,\n \n'Toronto'\n),\n\n                              Popularity \n=\n \nc\n(\n400\n,\n \n200\n))\n\n\nGeo3 \n<-\n gvisGeoChart\n(\nCityPopularity3\n,\n \n                     locationvar\n=\n'City'\n,\n\n                     colorvar\n=\n'Popularity'\n,\n\n                     options\n=\nlist\n(\nregion \n=\n \n'CA'\n,\n\n                                  height\n=\n350\n,\n\n                                  displayMode\n=\n'markers'\n,\n \n                                  colorAxis\n=\n\"{values:[200,400,600,800], colors:[\\'red', \\'pink\\', \\'orange',\\'green']}\"\n))\n\n\nplot\n(\nGeo3\n)\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID49614f24acff () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Montreal\",\n700\n],\n[\n\"Toronto\",\n200\n] \n];\ndata.addColumn('string','City');\ndata.addColumn('number','Popularity');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49614f24acff() {\nvar data = gvisDataGeoChartID49614f24acff();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 350;\noptions[\"region\"] = \"CA\";\noptions[\"displayMode\"] = \"markers\";\noptions[\"colorAxis\"] = {values:[200,400,600,800], colors:['red', 'pink', 'orange','green']};\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID49614f24acff')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49614f24acff);\n})();\nfunction displayChartGeoChartID49614f24acff() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nhead\n(\nAndrew\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Output\n\n        \nDate\n/\nTime\n \nUTC\n  \nLat\n  \nLong\n \nPressure_mb\n \nSpeed_kt\n            \nCategory\n\n\n1\n \n1992\n-\n08\n-\n16\n \n18\n:\n00\n:\n00\n \n10.8\n \n-\n35.5\n        \n1010\n       \n25\n \nTropical\n \nDepression\n\n\n2\n \n1992\n-\n08\n-\n17\n \n00\n:\n00\n:\n00\n \n11.2\n \n-\n37.4\n        \n1009\n       \n30\n \nTropical\n \nDepression\n\n\n3\n \n1992\n-\n08\n-\n17\n \n06\n:\n00\n:\n00\n \n11.7\n \n-\n39.6\n        \n1008\n       \n30\n \nTropical\n \nDepression\n\n     \nLatLong\n                                              \nTip\n\n\n1\n \n10.8\n:-\n35.5\n \nTropical\n \nDepression\n<\nBR\n>\nPressure\n=\n1010\n<\nBR\n>\nSpeed\n=\n25\n\n\n2\n \n11.2\n:-\n37.4\n \nTropical\n \nDepression\n<\nBR\n>\nPressure\n=\n1009\n<\nBR\n>\nSpeed\n=\n30\n\n\n3\n \n11.7\n:-\n39.6\n \nTropical\n \nDepression\n<\nBR\n>\nPressure\n=\n1008\n<\nBR\n>\nSpeed\n=\n30\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\nGeoMarker \n<-\n gvisGeoChart\n(\nAndrew\n,\n\n                          locationvar\n=\n\"LatLong\"\n,\n \n                          sizevar\n=\n'Speed_kt'\n,\n\n                          colorvar\n=\n\"Pressure_mb\"\n,\n \n                          options\n=\nlist\n(\nregion\n=\n\"US\"\n))\n\n\nplot\n(\nGeoMarker\n)\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID2cd02f295f7 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n10.8,\n-35.5,\n1010,\n25\n],\n[\n11.2,\n-37.4,\n1009,\n30\n],\n[\n11.7,\n-39.6,\n1008,\n30\n],\n[\n12.3,\n-42,\n1006,\n35\n],\n[\n13.1,\n-44.2,\n1003,\n35\n],\n[\n13.6,\n-46.2,\n1002,\n40\n],\n[\n14.1,\n-48,\n1001,\n45\n],\n[\n14.6,\n-49.9,\n1000,\n45\n],\n[\n15.4,\n-51.8,\n1000,\n45\n],\n[\n16.3,\n-53.5,\n1001,\n45\n],\n[\n17.2,\n-55.3,\n1002,\n45\n],\n[\n18,\n-56.9,\n1005,\n45\n],\n[\n18.8,\n-58.3,\n1007,\n45\n],\n[\n19.8,\n-59.3,\n1011,\n40\n],\n[\n20.7,\n-60,\n1013,\n40\n],\n[\n21.7,\n-60.7,\n1015,\n40\n],\n[\n22.5,\n-61.5,\n1014,\n40\n],\n[\n23.2,\n-62.4,\n1014,\n45\n],\n[\n23.9,\n-63.3,\n1010,\n45\n],\n[\n24.4,\n-64.2,\n1007,\n50\n],\n[\n24.8,\n-64.9,\n1004,\n50\n],\n[\n25.3,\n-65.9,\n1000,\n55\n],\n[\n25.6,\n-67,\n994,\n60\n],\n[\n25.8,\n-68.3,\n981,\n70\n],\n[\n25.7,\n-69.7,\n969,\n80\n],\n[\n25.6,\n-71.1,\n961,\n90\n],\n[\n25.5,\n-72.5,\n947,\n105\n],\n[\n25.4,\n-74.2,\n933,\n120\n],\n[\n25.4,\n-75.8,\n922,\n135\n],\n[\n25.4,\n-77.5,\n930,\n125\n],\n[\n25.4,\n-79.3,\n937,\n120\n],\n[\n25.6,\n-81.2,\n951,\n110\n],\n[\n25.8,\n-83.1,\n947,\n115\n],\n[\n26.2,\n-85,\n943,\n115\n],\n[\n26.6,\n-86.7,\n948,\n115\n],\n[\n27.2,\n-88.2,\n946,\n115\n],\n[\n27.8,\n-89.6,\n941,\n120\n],\n[\n28.5,\n-90.5,\n937,\n120\n],\n[\n29.2,\n-91.3,\n955,\n115\n],\n[\n30.1,\n-91.7,\n973,\n80\n],\n[\n30.9,\n-91.6,\n991,\n50\n],\n[\n31.5,\n-91.1,\n995,\n35\n],\n[\n32.1,\n-90.5,\n997,\n30\n],\n[\n32.8,\n-89.6,\n998,\n30\n],\n[\n33.6,\n-88.4,\n999,\n25\n],\n[\n34.4,\n-86.7,\n1000,\n20\n],\n[\n35.4,\n-84,\n1000,\n20\n] \n];\ndata.addColumn('number','Latitude');\ndata.addColumn('number','Longitude');\ndata.addColumn('number','Pressure_mb');\ndata.addColumn('number','Speed_kt');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID2cd02f295f7() {\nvar data = gvisDataGeoChartID2cd02f295f7();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 347;\noptions[\"region\"] = \"US\";\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID2cd02f295f7')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID2cd02f295f7);\n})();\nfunction displayChartGeoChartID2cd02f295f7() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngvisMap\n or Google Maps\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nAndrewMap \n<-\n gvisMap\n(\nAndrew\n,\n\n                     locationvar\n=\n\"LatLong\"\n \n,\n\n                     tipvar\n=\n\"Tip\"\n,\n \n#  text displayed over the tip icon\n\n                     options\n=\nlist\n(\nshowTip\n=\nTRUE\n,\n \n                                  showLine\n=\nTRUE\n,\n \n                                  enableScrollWheel\n=\nTRUE\n,\n\n                                  mapType\n=\n'terrain'\n,\n \n                                  useMapTypeControl\n=\nTRUE\n))\n\n\nplot\n(\nAndrewMap\n)\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataMapID496129f7ada () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n10.8,\n-35.5,\n\"Tropical Depression<BR>Pressure=1010<BR>Speed=25\"\n],\n[\n11.2,\n-37.4,\n\"Tropical Depression<BR>Pressure=1009<BR>Speed=30\"\n],\n[\n11.7,\n-39.6,\n\"Tropical Depression<BR>Pressure=1008<BR>Speed=30\"\n],\n[\n12.3,\n-42,\n\"Tropical Storm<BR>Pressure=1006<BR>Speed=35\"\n],\n[\n13.1,\n-44.2,\n\"Tropical Storm<BR>Pressure=1003<BR>Speed=35\"\n],\n[\n13.6,\n-46.2,\n\"Tropical Storm<BR>Pressure=1002<BR>Speed=40\"\n],\n[\n14.1,\n-48,\n\"Tropical Storm<BR>Pressure=1001<BR>Speed=45\"\n],\n[\n14.6,\n-49.9,\n\"Tropical Storm<BR>Pressure=1000<BR>Speed=45\"\n],\n[\n15.4,\n-51.8,\n\"Tropical Storm<BR>Pressure=1000<BR>Speed=45\"\n],\n[\n16.3,\n-53.5,\n\"Tropical Storm<BR>Pressure=1001<BR>Speed=45\"\n],\n[\n17.2,\n-55.3,\n\"Tropical Storm<BR>Pressure=1002<BR>Speed=45\"\n],\n[\n18,\n-56.9,\n\"Tropical Storm<BR>Pressure=1005<BR>Speed=45\"\n],\n[\n18.8,\n-58.3,\n\"Tropical Storm<BR>Pressure=1007<BR>Speed=45\"\n],\n[\n19.8,\n-59.3,\n\"Tropical Storm<BR>Pressure=1011<BR>Speed=40\"\n],\n[\n20.7,\n-60,\n\"Tropical Storm<BR>Pressure=1013<BR>Speed=40\"\n],\n[\n21.7,\n-60.7,\n\"Tropical Storm<BR>Pressure=1015<BR>Speed=40\"\n],\n[\n22.5,\n-61.5,\n\"Tropical Storm<BR>Pressure=1014<BR>Speed=40\"\n],\n[\n23.2,\n-62.4,\n\"Tropical Storm<BR>Pressure=1014<BR>Speed=45\"\n],\n[\n23.9,\n-63.3,\n\"Tropical Storm<BR>Pressure=1010<BR>Speed=45\"\n],\n[\n24.4,\n-64.2,\n\"Tropical Storm<BR>Pressure=1007<BR>Speed=50\"\n],\n[\n24.8,\n-64.9,\n\"Tropical Storm<BR>Pressure=1004<BR>Speed=50\"\n],\n[\n25.3,\n-65.9,\n\"Tropical Storm<BR>Pressure=1000<BR>Speed=55\"\n],\n[\n25.6,\n-67,\n\"Tropical Storm<BR>Pressure=994<BR>Speed=60\"\n],\n[\n25.8,\n-68.3,\n\"Hurricane<BR>Pressure=981<BR>Speed=70\"\n],\n[\n25.7,\n-69.7,\n\"Hurricane<BR>Pressure=969<BR>Speed=80\"\n],\n[\n25.6,\n-71.1,\n\"Hurricane<BR>Pressure=961<BR>Speed=90\"\n],\n[\n25.5,\n-72.5,\n\"Hurricane<BR>Pressure=947<BR>Speed=105\"\n],\n[\n25.4,\n-74.2,\n\"Hurricane<BR>Pressure=933<BR>Speed=120\"\n],\n[\n25.4,\n-75.8,\n\"Hurricane<BR>Pressure=922<BR>Speed=135\"\n],\n[\n25.4,\n-77.5,\n\"Hurricane<BR>Pressure=930<BR>Speed=125\"\n],\n[\n25.4,\n-79.3,\n\"Hurricane<BR>Pressure=937<BR>Speed=120\"\n],\n[\n25.6,\n-81.2,\n\"Hurricane<BR>Pressure=951<BR>Speed=110\"\n],\n[\n25.8,\n-83.1,\n\"Hurricane<BR>Pressure=947<BR>Speed=115\"\n],\n[\n26.2,\n-85,\n\"Hurricane<BR>Pressure=943<BR>Speed=115\"\n],\n[\n26.6,\n-86.7,\n\"Hurricane<BR>Pressure=948<BR>Speed=115\"\n],\n[\n27.2,\n-88.2,\n\"Hurricane<BR>Pressure=946<BR>Speed=115\"\n],\n[\n27.8,\n-89.6,\n\"Hurricane<BR>Pressure=941<BR>Speed=120\"\n],\n[\n28.5,\n-90.5,\n\"Hurricane<BR>Pressure=937<BR>Speed=120\"\n],\n[\n29.2,\n-91.3,\n\"Hurricane<BR>Pressure=955<BR>Speed=115\"\n],\n[\n30.1,\n-91.7,\n\"Tropical Storm<BR>Pressure=973<BR>Speed=80\"\n],\n[\n30.9,\n-91.6,\n\"Tropical Storm<BR>Pressure=991<BR>Speed=50\"\n],\n[\n31.5,\n-91.1,\n\"Tropical Depression<BR>Pressure=995<BR>Speed=35\"\n],\n[\n32.1,\n-90.5,\n\"Tropical Depression<BR>Pressure=997<BR>Speed=30\"\n],\n[\n32.8,\n-89.6,\n\"Tropical Depression<BR>Pressure=998<BR>Speed=30\"\n],\n[\n33.6,\n-88.4,\n\"Tropical Depression<BR>Pressure=999<BR>Speed=25\"\n],\n[\n34.4,\n-86.7,\n\"Tropical Depression<BR>Pressure=1000<BR>Speed=20\"\n],\n[\n35.4,\n-84,\n\"Tropical Depression<BR>Pressure=1000<BR>Speed=20\"\n] \n];\ndata.addColumn('number','Latitude');\ndata.addColumn('number','Longitude');\ndata.addColumn('string','Tip');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartMapID496129f7ada() {\nvar data = gvisDataMapID496129f7ada();\nvar options = {};\noptions[\"showTip\"] = true;\noptions[\"showLine\"] = true;\noptions[\"enableScrollWheel\"] = true;\noptions[\"mapType\"] = \"terrain\";\noptions[\"useMapTypeControl\"] = true;\n\n\n    var chart = new google.visualization.Map(\n    document.getElementById('MapID496129f7ada')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"map\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartMapID496129f7ada);\n})();\nfunction displayChartMapID496129f7ada() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEditor\n\u00b6\n\n\nThis is a fantastic option that let us start with one chart or map and change everything with a menu.\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\nEditor \n<-\n gvisGeoChart\n(\nExports\n,\n \n                       locationvar\n=\n\"Country\"\n,\n\n                       colorvar\n=\n\"Profit\"\n,\n\n                       options\n=\nlist\n(\ngvis.editor\n=\n'Edit me!'\n))\n\n\nplot\n(\nEditor\n)\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID49613aa5c54a () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49613aa5c54a() {\nvar data = gvisDataGeoChartID49613aa5c54a();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 347;\n\n\n    chartGeoChartID49613aa5c54a = new google.visualization.ChartWrapper({\n    dataTable: data,       \n    chartType: 'GeoChart',\n    containerId: 'GeoChartID49613aa5c54a',\n    options: options\n    });\n    chartGeoChartID49613aa5c54a.draw();\n\n\n}\n\n  function openEditorGeoChartID49613aa5c54a() {\n  var editor = new google.visualization.ChartEditor();\n  google.visualization.events.addListener(editor, 'ok',\n  function() { \n  chartGeoChartID49613aa5c54a = editor.getChartWrapper();  \n  chartGeoChartID49613aa5c54a.draw(document.getElementById('GeoChartID49613aa5c54a')); \n  }); \n  editor.openDialog(chartGeoChartID49613aa5c54a);\n  }\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"charteditor\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49613aa5c54a);\n})();\nfunction displayChartGeoChartID49613aa5c54a() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTables with \ngvisTable\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n1\nhead\n(\nStock\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Output\n        Date  Device Value Title Annotation\n1 2008-01-01 Pencils  3000  <NA>       <NA>\n2 2008-01-02 Pencils 14045  <NA>       <NA>\n3 2008-01-03 Pencils  5502  <NA>       <NA>\n\n\n\n\n\n\n1\n2\n3\n4\nTable \n<-\n gvisTable\n(\nStock\n,\n \n                   formats\n=\nlist\n(\nValue\n=\n\"#,###\"\n))\n\n\nplot\n(\nTable\n)\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataTableID2cd03b735929 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\nnew Date(2008,0,1),\n\"Pencils\",\n3000,\nnull,\nnull\n],\n[\nnew Date(2008,0,2),\n\"Pencils\",\n14045,\nnull,\nnull\n],\n[\nnew Date(2008,0,3),\n\"Pencils\",\n5502,\nnull,\nnull\n],\n[\nnew Date(2008,0,4),\n\"Pencils\",\n75284,\nnull,\nnull\n],\n[\nnew Date(2008,0,5),\n\"Pencils\",\n41476,\n\"Bought pencils\",\n\"Bought 200k pencils\"\n],\n[\nnew Date(2008,0,6),\n\"Pencils\",\n333222,\nnull,\nnull\n],\n[\nnew Date(2008,0,1),\n\"Pens\",\n40645,\nnull,\nnull\n],\n[\nnew Date(2008,0,2),\n\"Pens\",\n20374,\nnull,\nnull\n],\n[\nnew Date(2008,0,3),\n\"Pens\",\n50766,\nnull,\nnull\n],\n[\nnew Date(2008,0,4),\n\"Pens\",\n14334,\n\"Out of stock\",\n\"Ran out of stock of pens at 4pm\"\n],\n[\nnew Date(2008,0,5),\n\"Pens\",\n66467,\nnull,\nnull\n],\n[\nnew Date(2008,0,6),\n\"Pens\",\n39463,\nnull,\nnull\n] \n];\ndata.addColumn('date','Date');\ndata.addColumn('string','Device');\ndata.addColumn('number','Value');\ndata.addColumn('string','Title');\ndata.addColumn('string','Annotation');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartTableID2cd03b735929() {\nvar data = gvisDataTableID2cd03b735929();\nvar options = {};\noptions[\"allowHtml\"] = true;\n\n  var dataFormat1 = new google.visualization.NumberFormat({pattern:\"#,###\"});\n  dataFormat1.format(data, 2);\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID2cd03b735929')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID2cd03b735929);\n})();\nfunction displayChartTableID2cd03b735929() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nhead\n(\nPopulation\n,\n \n3\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n# Output\n                                                                                 Rank       Country Population % of World Population\n1    1         China 1339940000                0.1950\n2    2         India 1188650000                0.1730\n3    3 United States  310438000                0.0452\n                                                                                                                                                                     Flag\n1 <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_the_People%27s_Republic_of_China.svg/22px-Flag_of_the_People%27s_Republic_of_China.svg.png\">\n2                                                       <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_India.svg/22px-Flag_of_India.svg.png\">\n3                               <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/22px-Flag_of_the_United_States.svg.png\">\n  Mode       Date\n1 TRUE 2010-10-09\n2 TRUE 2010-10-09\n3 TRUE 2010-10-09\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\nPopTable \n<-\n gvisTable\n(\nPopulation\n,\n \n                      formats\n=\nlist\n(\nPopulation\n=\n\"#,###\"\n,\n\n                                   \n'% of World Population'\n=\n'#.#%'\n),\n\n                      options\n=\nlist\n(\npage\n=\n'enable'\n))\n\n\nplot\n(\nPopTable\n)\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataTableID2cd013050362 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"1\",\n\"China\",\n1339940000,\n0.195,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_the_People%27s_Republic_of_China.svg/22px-Flag_of_the_People%27s_Republic_of_China.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"2\",\n\"India\",\n1188650000,\n0.173,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_India.svg/22px-Flag_of_India.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"3\",\n\"United States\",\n310438000,\n0.0452,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/22px-Flag_of_the_United_States.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"4\",\n\"Indonesia\",\n237556363,\n0.0346,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_Indonesia.svg/22px-Flag_of_Indonesia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"5\",\n\"Brazil\",\n193626000,\n0.0282,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Brazil.svg/22px-Flag_of_Brazil.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"6\",\n\"Pakistan\",\n170745000,\n0.0248,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Flag_of_Pakistan.svg/22px-Flag_of_Pakistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"7\",\n\"Bangladesh\",\n164425000,\n0.0239,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f9/Flag_of_Bangladesh.svg/22px-Flag_of_Bangladesh.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"8\",\n\"Nigeria\",\n158259000,\n0.023,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Flag_of_Nigeria.svg/22px-Flag_of_Nigeria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"9\",\n\"Russia\",\n141927297,\n0.0206,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Russia.svg/22px-Flag_of_Russia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"10\",\n\"Japan\",\n127390000,\n0.0185,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Flag_of_Japan.svg/22px-Flag_of_Japan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"11\",\n\"Mexico\",\n108396211,\n0.0158,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fc/Flag_of_Mexico.svg/22px-Flag_of_Mexico.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"12\",\n\"Philippines\",\n94013200,\n0.0137,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Flag_of_the_Philippines.svg/22px-Flag_of_the_Philippines.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"13\",\n\"Vietnam\",\n85846997,\n0.0125,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Flag_of_Vietnam.svg/22px-Flag_of_Vietnam.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"14\",\n\"Ethiopia\",\n84976000,\n0.0124,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Flag_of_Ethiopia.svg/22px-Flag_of_Ethiopia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"15\",\n\"Germany\",\n81802257,\n0.0119,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/22px-Flag_of_Germany.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"16\",\n\"Egypt\",\n79135000,\n0.0115,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Egypt.svg/22px-Flag_of_Egypt.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"17\",\n\"Iran\",\n75078000,\n0.0109,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Flag_of_Iran.svg/22px-Flag_of_Iran.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"18\",\n\"Turkey\",\n72561312,\n0.0106,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Flag_of_Turkey.svg/22px-Flag_of_Turkey.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"19\",\n\"Dem. Rep. of Congo\",\n67827000,\n0.0099,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Flag_of_the_Democratic_Republic_of_the_Congo.svg/22px-Flag_of_the_Democratic_Republic_of_the_Congo.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"20\",\n\"Thailand\",\n67070000,\n0.01,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Flag_of_Thailand.svg/22px-Flag_of_Thailand.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"21\",\n\"France\",\n65447374,\n0.0095,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/22px-Flag_of_France.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"22\",\n\"United Kingdom\",\n62008049,\n0.009,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Flag_of_the_United_Kingdom.svg/22px-Flag_of_the_United_Kingdom.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"23\",\n\"Italy\",\n60402499,\n0.0088,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/03/Flag_of_Italy.svg/22px-Flag_of_Italy.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"24\",\n\"Myanmar\",\n50496000,\n0.0073,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Flag_of_Myanmar.svg/22px-Flag_of_Myanmar.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"25\",\n\"South Africa\",\n49991300,\n0.0073,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Flag_of_South_Africa.svg/22px-Flag_of_South_Africa.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"26\",\n\"South Korea\",\n49773145,\n0.0072,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Flag_of_South_Korea.svg/22px-Flag_of_South_Korea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"27\",\n\"Spain\",\n46072834,\n0.0067,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Spain.svg/22px-Flag_of_Spain.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"28\",\n\"Ukraine\",\n45871738,\n0.0067,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Flag_of_Ukraine.svg/22px-Flag_of_Ukraine.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"29\",\n\"Colombia\",\n45655000,\n0.0066,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Flag_of_Colombia.svg/22px-Flag_of_Colombia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"30\",\n\"Tanzania\",\n45040000,\n0.0066,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Tanzania.svg/22px-Flag_of_Tanzania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"31\",\n\"Sudan\",\n43192000,\n0.0063,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_Sudan.svg/22px-Flag_of_Sudan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"32\",\n\"Argentina\",\n40518951,\n0.0059,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Flag_of_Argentina.svg/22px-Flag_of_Argentina.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"33\",\n\"Kenya\",\n38610097,\n0.0056,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Flag_of_Kenya.svg/22px-Flag_of_Kenya.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"34\",\n\"Poland\",\n38167329,\n0.0056,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Flag_of_Poland.svg/22px-Flag_of_Poland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"35\",\n\"Algeria\",\n35423000,\n0.0052,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_Algeria.svg/22px-Flag_of_Algeria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"36\",\n\"Canada\",\n34272000,\n0.005,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Flag_of_Canada.svg/22px-Flag_of_Canada.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"37\",\n\"Uganda\",\n33796000,\n0.0049,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Flag_of_Uganda.svg/22px-Flag_of_Uganda.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"38\",\n\"Morocco\",\n31944000,\n0.0046,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Morocco.svg/22px-Flag_of_Morocco.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"39\",\n\"Iraq\",\n31467000,\n0.0046,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Flag_of_Iraq.svg/22px-Flag_of_Iraq.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"40\",\n\"Nepal\",\n29853000,\n0.0043,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Flag_of_Nepal.svg/16px-Flag_of_Nepal.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"41\",\n\"Peru\",\n29461933,\n0.0043,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Flag_of_Peru.svg/22px-Flag_of_Peru.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"42\",\n\"Afghanistan\",\n29117000,\n0.0042,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Afghanistan.svg/22px-Flag_of_Afghanistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"43\",\n\"Venezuela\",\n28958000,\n0.0042,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Flag_of_Venezuela.svg/22px-Flag_of_Venezuela.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"44\",\n\"Malaysia\",\n28250500,\n0.0041,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Flag_of_Malaysia.svg/22px-Flag_of_Malaysia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"45\",\n\"Uzbekistan\",\n27794000,\n0.004,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Flag_of_Uzbekistan.svg/22px-Flag_of_Uzbekistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"46\",\n\"Saudi Arabia\",\n27136977,\n0.0039,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Flag_of_Saudi_Arabia.svg/22px-Flag_of_Saudi_Arabia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"47\",\n\"Ghana\",\n24333000,\n0.0035,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Ghana.svg/22px-Flag_of_Ghana.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"48\",\n\"Yemen\",\n24256000,\n0.0035,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Flag_of_Yemen.svg/22px-Flag_of_Yemen.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"49\",\n\"North Korea\",\n23991000,\n0.0035,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/51/Flag_of_North_Korea.svg/22px-Flag_of_North_Korea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"50\",\n\"Mozambique\",\n23406000,\n0.0034,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flag_of_Mozambique.svg/22px-Flag_of_Mozambique.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"52\",\n\"Syria\",\n22505000,\n0.0033,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Flag_of_Syria.svg/22px-Flag_of_Syria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"53\",\n\"Australia\",\n22483305,\n0.0033,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Flag_of_Australia.svg/22px-Flag_of_Australia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"54\",\n\"Cote d'Ivoire\",\n21571000,\n0.0031,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Flag_of_Cote_d%27Ivoire.svg/22px-Flag_of_Cote_d%27Ivoire.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"55\",\n\"Romania\",\n21466174,\n0.0031,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Flag_of_Romania.svg/22px-Flag_of_Romania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"56\",\n\"Sri Lanka\",\n20410000,\n0.003,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Flag_of_Sri_Lanka.svg/22px-Flag_of_Sri_Lanka.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"57\",\n\"Madagascar\",\n20146000,\n0.0029,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Madagascar.svg/22px-Flag_of_Madagascar.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"58\",\n\"Cameroon\",\n19958000,\n0.0029,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Flag_of_Cameroon.svg/22px-Flag_of_Cameroon.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"59\",\n\"Angola\",\n18993000,\n0.0028,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Angola.svg/22px-Flag_of_Angola.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"60\",\n\"Chile\",\n17140000,\n0.0025,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Flag_of_Chile.svg/22px-Flag_of_Chile.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"61\",\n\"Netherlands\",\n16619500,\n0.00242,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/22px-Flag_of_the_Netherlands.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"62\",\n\"Burkina Faso\",\n16287000,\n0.0024,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Burkina_Faso.svg/22px-Flag_of_Burkina_Faso.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"63\",\n\"Kazakhstan\",\n16197000,\n0.0024,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Flag_of_Kazakhstan.svg/22px-Flag_of_Kazakhstan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"64\",\n\"Niger\",\n15891000,\n0.0023,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Flag_of_Niger.svg/22px-Flag_of_Niger.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"65\",\n\"Malawi\",\n15692000,\n0.0023,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Flag_of_Malawi.svg/22px-Flag_of_Malawi.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"66\",\n\"Mali\",\n14517176,\n0.0021,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Mali.svg/22px-Flag_of_Mali.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"67\",\n\"Guatemala\",\n14377000,\n0.0021,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Flag_of_Guatemala.svg/22px-Flag_of_Guatemala.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"68\",\n\"Ecuador\",\n14259000,\n0.0021,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Flag_of_Ecuador.svg/22px-Flag_of_Ecuador.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"69\",\n\"Cambodia\",\n13395682,\n0.0019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Flag_of_Cambodia.svg/22px-Flag_of_Cambodia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"70\",\n\"Zambia\",\n13257000,\n0.0019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Flag_of_Zambia.svg/22px-Flag_of_Zambia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"71\",\n\"Senegal\",\n12861000,\n0.0019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Flag_of_Senegal.svg/22px-Flag_of_Senegal.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"72\",\n\"Zimbabwe\",\n12644000,\n0.0018,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/Flag_of_Zimbabwe.svg/22px-Flag_of_Zimbabwe.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"73\",\n\"Chad\",\n11506000,\n0.0017,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Flag_of_Chad.svg/22px-Flag_of_Chad.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"74\",\n\"Greece\",\n11306183,\n0.0016,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Greece.svg/22px-Flag_of_Greece.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"75\",\n\"Cuba\",\n11204000,\n0.0016,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Flag_of_Cuba.svg/22px-Flag_of_Cuba.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"76\",\n\"Belgium\",\n10827519,\n0.0016,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Belgium_%28civil%29.svg/22px-Flag_of_Belgium_%28civil%29.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"77\",\n\"Portugal\",\n10636888,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Portugal.svg/22px-Flag_of_Portugal.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"78\",\n\"Czech Republic\",\n10512397,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Flag_of_the_Czech_Republic.svg/22px-Flag_of_the_Czech_Republic.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"79\",\n\"Tunisia\",\n10432500,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Flag_of_Tunisia.svg/22px-Flag_of_Tunisia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"80\",\n\"Guinea\",\n10324000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Flag_of_Guinea.svg/22px-Flag_of_Guinea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"81\",\n\"Rwanda\",\n10277000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Rwanda.svg/22px-Flag_of_Rwanda.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"82\",\n\"Dominican Republic\",\n10225000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_the_Dominican_Republic.svg/22px-Flag_of_the_Dominican_Republic.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"83\",\n\"Haiti\",\n10188000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Flag_of_Haiti.svg/22px-Flag_of_Haiti.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"84\",\n\"Bolivia\",\n10031000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Bolivia.svg/22px-Flag_of_Bolivia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"85\",\n\"Hungary\",\n10013628,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c1/Flag_of_Hungary.svg/22px-Flag_of_Hungary.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"86\",\n\"Serbia\",\n9856000,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Flag_of_Serbia.svg/22px-Flag_of_Serbia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"87\",\n\"Belarus\",\n9467700,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Flag_of_Belarus.svg/22px-Flag_of_Belarus.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"88\",\n\"Sweden\",\n9393648,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Sweden.svg/22px-Flag_of_Sweden.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"89\",\n\"Somalia\",\n9359000,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Flag_of_Somalia.svg/22px-Flag_of_Somalia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"90\",\n\"Benin\",\n9212000,\n0.0013,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flag_of_Benin.svg/22px-Flag_of_Benin.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"91\",\n\"Azerbaijan\",\n8997400,\n0.0013,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Flag_of_Azerbaijan.svg/22px-Flag_of_Azerbaijan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"92\",\n\"Burundi\",\n8519000,\n0.0012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Flag_of_Burundi.svg/22px-Flag_of_Burundi.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"93\",\n\"Austria\",\n8372930,\n0.0012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_Austria.svg/22px-Flag_of_Austria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"94\",\n\"Switzerland\",\n7782900,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Switzerland.svg/20px-Flag_of_Switzerland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"95\",\n\"Israel\",\n7640800,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Flag_of_Israel.svg/22px-Flag_of_Israel.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"96\",\n\"Honduras\",\n7616000,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Flag_of_Honduras.svg/22px-Flag_of_Honduras.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"97\",\n\"Bulgaria\",\n7576751,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Bulgaria.svg/22px-Flag_of_Bulgaria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"98\",\n\"Tajikistan\",\n7075000,\n0.00103,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flag_of_Tajikistan.svg/22px-Flag_of_Tajikistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"100\",\n\"Papua New Guinea\",\n6888000,\n0.001,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Flag_of_Papua_New_Guinea.svg/22px-Flag_of_Papua_New_Guinea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"101\",\n\"Togo\",\n6780000,\n0.00099,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Flag_of_Togo.svg/22px-Flag_of_Togo.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"102\",\n\"Libya\",\n6546000,\n0.00095,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Libya.svg/22px-Flag_of_Libya.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"103\",\n\"Jordan\",\n6472000,\n0.00094,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Flag_of_Jordan.svg/22px-Flag_of_Jordan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"104\",\n\"Paraguay\",\n6460000,\n0.00094,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Flag_of_Paraguay.svg/22px-Flag_of_Paraguay.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"105\",\n\"Laos\",\n6436000,\n0.00094,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Flag_of_Laos.svg/22px-Flag_of_Laos.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"106\",\n\"El Salvador\",\n6194000,\n9e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Flag_of_El_Salvador.svg/22px-Flag_of_El_Salvador.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"107\",\n\"Sierra Leone\",\n5836000,\n0.00085,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Sierra_Leone.svg/22px-Flag_of_Sierra_Leone.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"108\",\n\"Nicaragua\",\n5822000,\n0.00085,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Nicaragua.svg/22px-Flag_of_Nicaragua.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"109\",\n\"Kyrgyzstan\",\n5550000,\n0.00081,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Flag_of_Kyrgyzstan.svg/22px-Flag_of_Kyrgyzstan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"110\",\n\"Denmark\",\n5543819,\n8e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Flag_of_Denmark.svg/22px-Flag_of_Denmark.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"111\",\n\"Slovakia\",\n5429763,\n0.00079,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Flag_of_Slovakia.svg/22px-Flag_of_Slovakia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"112\",\n\"Finland\",\n5370000,\n0.00078,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Finland.svg/22px-Flag_of_Finland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"113\",\n\"Eritrea\",\n5224000,\n0.00076,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Flag_of_Eritrea.svg/22px-Flag_of_Eritrea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"114\",\n\"Turkmenistan\",\n5177000,\n0.00075,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Flag_of_Turkmenistan.svg/22px-Flag_of_Turkmenistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"115\",\n\"Singapore\",\n5076700,\n0.00074,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Singapore.svg/22px-Flag_of_Singapore.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"116\",\n\"Norway\",\n4906500,\n0.00071,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Flag_of_Norway.svg/22px-Flag_of_Norway.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"117\",\n\"United Arab Emirates\",\n4707000,\n0.00068,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Flag_of_the_United_Arab_Emirates.svg/22px-Flag_of_the_United_Arab_Emirates.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"118\",\n\"Costa Rica\",\n4640000,\n0.00068,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f2/Flag_of_Costa_Rica.svg/22px-Flag_of_Costa_Rica.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"119\",\n\"Central African Republic\",\n4506000,\n0.00066,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Flag_of_the_Central_African_Republic.svg/22px-Flag_of_the_Central_African_Republic.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"120\",\n\"Ireland\",\n4470700,\n0.00064,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Flag_of_Ireland.svg/22px-Flag_of_Ireland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"121\",\n\"Georgia\",\n4436000,\n0.00065,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Flag_of_Georgia.svg/22px-Flag_of_Georgia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"122\",\n\"Croatia\",\n4435056,\n0.00065,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Flag_of_Croatia.svg/22px-Flag_of_Croatia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"123\",\n\"New Zealand\",\n4393000,\n0.00064,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Flag_of_New_Zealand.svg/22px-Flag_of_New_Zealand.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"124\",\n\"Lebanon\",\n4255000,\n0.00062,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Flag_of_Lebanon.svg/22px-Flag_of_Lebanon.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"125\",\n\"Liberia\",\n4102000,\n6e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/Flag_of_Liberia.svg/22px-Flag_of_Liberia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"127\",\n\"Palestinian territories\",\n3935249,\n0.00055,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Palestine.svg/22px-Flag_of_Palestine.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"128\",\n\"Bosnia and Herzegovina\",\n3760000,\n0.00055,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Flag_of_Bosnia_and_Herzegovina.svg/22px-Flag_of_Bosnia_and_Herzegovina.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"129\",\n\"Republic of the Congo\",\n3759000,\n0.00055,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_the_Republic_of_the_Congo.svg/22px-Flag_of_the_Republic_of_the_Congo.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"130\",\n\"Moldova\",\n3563800,\n0.00052,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Flag_of_Moldova.svg/22px-Flag_of_Moldova.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"131\",\n\"Uruguay\",\n3372000,\n0.00049,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Uruguay.svg/22px-Flag_of_Uruguay.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"132\",\n\"Mauritania\",\n3366000,\n0.00049,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Flag_of_Mauritania.svg/22px-Flag_of_Mauritania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"133\",\n\"Lithuania\",\n3329227,\n0.00048,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Flag_of_Lithuania.svg/22px-Flag_of_Lithuania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"134\",\n\"Panama\",\n3322576,\n0.00048,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/Flag_of_Panama.svg/22px-Flag_of_Panama.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"135\",\n\"Armenia\",\n3238000,\n0.00047,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Flag_of_Armenia.svg/22px-Flag_of_Armenia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"136\",\n\"Albania\",\n3195000,\n0.00046,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/36/Flag_of_Albania.svg/22px-Flag_of_Albania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"137\",\n\"Kuwait\",\n3051000,\n0.00044,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Flag_of_Kuwait.svg/22px-Flag_of_Kuwait.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"138\",\n\"Oman\",\n2905000,\n0.00042,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Flag_of_Oman.svg/22px-Flag_of_Oman.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"139\",\n\"Mongolia\",\n2776500,\n4e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Mongolia.svg/22px-Flag_of_Mongolia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"140\",\n\"Jamaica\",\n2730000,\n4e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flag_of_Jamaica.svg/22px-Flag_of_Jamaica.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"141\",\n\"Latvia\",\n2236300,\n0.00033,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Flag_of_Latvia.svg/22px-Flag_of_Latvia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"142\",\n\"Namibia\",\n2212000,\n0.00032,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Namibia.svg/22px-Flag_of_Namibia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"143\",\n\"Lesotho\",\n2084000,\n3e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Flag_of_Lesotho.svg/22px-Flag_of_Lesotho.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"144\",\n\"Slovenia\",\n2065720,\n3e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f0/Flag_of_Slovenia.svg/22px-Flag_of_Slovenia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"145\",\n\"Republic of Macedonia\",\n2048620,\n3e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Flag_of_Macedonia.svg/22px-Flag_of_Macedonia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"146\",\n\"Botswana\",\n1978000,\n0.00029,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_Botswana.svg/22px-Flag_of_Botswana.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"147\",\n\"Gambia\",\n1751000,\n0.00025,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_The_Gambia.svg/22px-Flag_of_The_Gambia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"148\",\n\"Qatar\",\n1696563,\n0.00025,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Flag_of_Qatar.svg/22px-Flag_of_Qatar.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"149\",\n\"Guinea-Bissau\",\n1647000,\n0.00024,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_Guinea-Bissau.svg/22px-Flag_of_Guinea-Bissau.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"150\",\n\"Gabon\",\n1501000,\n0.00022,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Flag_of_Gabon.svg/22px-Flag_of_Gabon.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"151\",\n\"Trinidad and Tobago\",\n1344000,\n2e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Flag_of_Trinidad_and_Tobago.svg/22px-Flag_of_Trinidad_and_Tobago.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"152\",\n\"Estonia\",\n1340127,\n0.00019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Flag_of_Estonia.svg/22px-Flag_of_Estonia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"153\",\n\"Mauritius\",\n1297000,\n0.00019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_Mauritius.svg/22px-Flag_of_Mauritius.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"154\",\n\"Swaziland\",\n1202000,\n0.00017,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Flag_of_Swaziland.svg/22px-Flag_of_Swaziland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"155\",\n\"East Timor\",\n1171000,\n0.00017,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Flag_of_East_Timor.svg/22px-Flag_of_East_Timor.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"156\",\n\"Djibouti\",\n879000,\n0.00013,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Flag_of_Djibouti.svg/22px-Flag_of_Djibouti.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"157\",\n\"Fiji\",\n854000,\n0.00012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Fiji.svg/22px-Flag_of_Fiji.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"158\",\n\"Bahrain\",\n807000,\n0.00012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Bahrain.svg/22px-Flag_of_Bahrain.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"159\",\n\"Cyprus\",\n801851,\n0.00012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Flag_of_Cyprus.svg/22px-Flag_of_Cyprus.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"160\",\n\"Guyana\",\n761000,\n0.00011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Flag_of_Guyana.svg/22px-Flag_of_Guyana.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"161\",\n\"Bhutan\",\n708000,\n1e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Flag_of_Bhutan.svg/22px-Flag_of_Bhutan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"162\",\n\"Equatorial Guinea\",\n693000,\n1e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Equatorial_Guinea.svg/22px-Flag_of_Equatorial_Guinea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"163\",\n\"Comoros\",\n691000,\n1e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Flag_of_the_Comoros.svg/22px-Flag_of_the_Comoros.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"164\",\n\"Montenegro\",\n626000,\n9e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Flag_of_Montenegro.svg/22px-Flag_of_Montenegro.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"166\",\n\"Solomon Islands\",\n536000,\n8e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Flag_of_the_Solomon_Islands.svg/22px-Flag_of_the_Solomon_Islands.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"167\",\n\"Western Sahara\",\n530000,\n8e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Flag_of_Western_Sahara.svg/22px-Flag_of_Western_Sahara.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"168\",\n\"Suriname\",\n524000,\n8e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Flag_of_Suriname.svg/22px-Flag_of_Suriname.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"169\",\n\"Cape Verde\",\n513000,\n7e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Cape_Verde.svg/22px-Flag_of_Cape_Verde.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"170\",\n\"Luxembourg\",\n502207,\n7e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Flag_of_Luxembourg.svg/22px-Flag_of_Luxembourg.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"171\",\n\"Malta\",\n416333,\n6e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Flag_of_Malta.svg/22px-Flag_of_Malta.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"172\",\n\"Brunei\",\n407000,\n6e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Flag_of_Brunei.svg/22px-Flag_of_Brunei.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"173\",\n\"Bahamas\",\n346000,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Flag_of_the_Bahamas.svg/22px-Flag_of_the_Bahamas.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"174\",\n\"Belize\",\n322100,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Flag_of_Belize.svg/22px-Flag_of_Belize.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"175\",\n\"Iceland\",\n318006,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Flag_of_Iceland.svg/22px-Flag_of_Iceland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"176\",\n\"Maldives\",\n314000,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Flag_of_Maldives.svg/22px-Flag_of_Maldives.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"177\",\n\"Barbados\",\n257000,\n4e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Flag_of_Barbados.svg/22px-Flag_of_Barbados.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"178\",\n\"Vanuatu\",\n246000,\n4e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Vanuatu.svg/22px-Flag_of_Vanuatu.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"181\",\n\"Samoa\",\n179000,\n3e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Samoa.svg/22px-Flag_of_Samoa.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"182\",\n\"Saint Lucia\",\n174000,\n3e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_Saint_Lucia.svg/22px-Flag_of_Saint_Lucia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"183\",\n\"Sao Tome and Principe\",\n165000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Flag_of_Sao_Tome_and_Principe.svg/22px-Flag_of_Sao_Tome_and_Principe.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"184\",\n\"Federated States of Micronesia\",\n111000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Federated_States_of_Micronesia.svg/22px-Flag_of_Federated_States_of_Micronesia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"186\",\n\"Saint Vincent and the Grenadines\",\n109000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Flag_of_Saint_Vincent_and_the_Grenadines.svg/22px-Flag_of_Saint_Vincent_and_the_Grenadines.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"188\",\n\"Grenada\",\n104000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Grenada.svg/22px-Flag_of_Grenada.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"189\",\n\"Tonga\",\n104000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Tonga.svg/22px-Flag_of_Tonga.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"190\",\n\"Kiribati\",\n100000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Flag_of_Kiribati.svg/22px-Flag_of_Kiribati.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"192\",\n\"Antigua and Barbuda\",\n89000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Flag_of_Antigua_and_Barbuda.svg/22px-Flag_of_Antigua_and_Barbuda.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"194\",\n\"Seychelles\",\n85000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_the_Seychelles.svg/22px-Flag_of_the_Seychelles.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"195\",\n\"Andorra\",\n84082,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Andorra.svg/22px-Flag_of_Andorra.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"198\",\n\"Dominica\",\n67000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Flag_of_Dominica.svg/22px-Flag_of_Dominica.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"200\",\n\"Marshall Islands\",\n63000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/Flag_of_the_Marshall_Islands.svg/22px-Flag_of_the_Marshall_Islands.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"204\",\n\"Saint Kitts and Nevis\",\n52000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Saint_Kitts_and_Nevis.svg/22px-Flag_of_Saint_Kitts_and_Nevis.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"206\",\n\"Liechtenstein\",\n35904,\n5e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Flag_of_Liechtenstein.svg/22px-Flag_of_Liechtenstein.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"207\",\n\"Monaco\",\n33000,\n5e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/22px-Flag_of_Monaco.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"209\",\n\"San Marino\",\n31794,\n5e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Flag_of_San_Marino.svg/22px-Flag_of_San_Marino.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"213\",\n\"Palau\",\n20000,\n3e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Palau.svg/22px-Flag_of_Palau.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"215\",\n\"Tuvalu\",\n10000,\n1e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Tuvalu.svg/22px-Flag_of_Tuvalu.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"216\",\n\"Nauru\",\n10000,\n1e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/30/Flag_of_Nauru.svg/22px-Flag_of_Nauru.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"222\",\n\"Vatican City\",\n800,\n2e-07,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_the_Vatican_City.svg/20px-Flag_of_the_Vatican_City.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n] \n];\ndata.addColumn('string','Rank');\ndata.addColumn('string','Country');\ndata.addColumn('number','Population');\ndata.addColumn('number','% of World Population');\ndata.addColumn('string','Flag');\ndata.addColumn('boolean','Mode');\ndata.addColumn('date','Date');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartTableID2cd013050362() {\nvar data = gvisDataTableID2cd013050362();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"page\"] = \"enable\";\n\n  var dataFormat1 = new google.visualization.NumberFormat({pattern:\"#,###\"});\n  dataFormat1.format(data, 2);\n  var dataFormat2 = new google.visualization.NumberFormat({pattern:\"#.#%\"});\n  dataFormat2.format(data, 3);\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID2cd013050362')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID2cd013050362);\n})();\nfunction displayChartTableID2cd013050362() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDashboards with \ngvisMerge\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\nG \n<-\n gvisGeoChart\n(\nExports\n,\n\n                  locationvar\n=\n\"Country\"\n,\n\n                  colorvar\n=\n\"Profit\"\n,\n\n                  options\n=\nlist\n(\nwidth\n=\n300\n,\n height\n=\n200\n))\n\n\n\nT\n \n<-\n gvisTable\n(\nExports\n,\n\n               options\n=\nlist\n(\nwidth\n=\n300\n,\n height\n=\n370\n))\n\n\n\n\n\n\n\n1\n2\n3\nGT \n<-\n gvisMerge\n(\nG\n,\n \nT\n,\n horizontal\n=\nFALSE\n)\n\n\nplot\n(\nGT\n)\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID49611ffdda49 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n\n// jsData \nfunction gvisDataTableID496155ab963a () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3,\ntrue\n],\n[\n\"Brazil\",\n4,\nfalse\n],\n[\n\"United States\",\n5,\ntrue\n],\n[\n\"France\",\n4,\ntrue\n],\n[\n\"Hungary\",\n3,\nfalse\n],\n[\n\"India\",\n2,\ntrue\n],\n[\n\"Iceland\",\n1,\nfalse\n],\n[\n\"Norway\",\n4,\ntrue\n],\n[\n\"Spain\",\n5,\ntrue\n],\n[\n\"Turkey\",\n1,\nfalse\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addColumn('boolean','Online');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49611ffdda49() {\nvar data = gvisDataGeoChartID49611ffdda49();\nvar options = {};\noptions[\"width\"] = 300;\noptions[\"height\"] = 200;\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID49611ffdda49')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n\n// jsDrawChart\nfunction drawChartTableID496155ab963a() {\nvar data = gvisDataTableID496155ab963a();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"width\"] = 300;\noptions[\"height\"] = 370;\n\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID496155ab963a')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49611ffdda49);\n})();\nfunction displayChartGeoChartID49611ffdda49() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID496155ab963a);\n})();\nfunction displayChartTableID496155ab963a() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\nG \n<-\n gvisGeoChart\n(\nExports\n,\n\n                  locationvar\n=\n\"Country\"\n,\n\n                  colorvar\n=\n\"Profit\"\n,\n\n                  options\n=\nlist\n(\nwidth\n=\n300\n,\n height\n=\n370\n))\n\n\n\n\n\n\n\n1\n2\n3\nGT \n<-\n gvisMerge\n(\nG\n,\n \nT\n,\n horizontal\n=\nTRUE\n)\n\n\nplot\n(\nGT\n)\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataGeoChartID114f1fa5c040 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n\n// jsData \nfunction gvisDataTableID114f79fb112f () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3,\ntrue\n],\n[\n\"Brazil\",\n4,\nfalse\n],\n[\n\"United States\",\n5,\ntrue\n],\n[\n\"France\",\n4,\ntrue\n],\n[\n\"Hungary\",\n3,\nfalse\n],\n[\n\"India\",\n2,\ntrue\n],\n[\n\"Iceland\",\n1,\nfalse\n],\n[\n\"Norway\",\n4,\ntrue\n],\n[\n\"Spain\",\n5,\ntrue\n],\n[\n\"Turkey\",\n1,\nfalse\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addColumn('boolean','Online');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID114f1fa5c040() {\nvar data = gvisDataGeoChartID114f1fa5c040();\nvar options = {};\noptions[\"width\"] = 300;\noptions[\"height\"] = 370;\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID114f1fa5c040')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n\n// jsDrawChart\nfunction drawChartTableID114f79fb112f() {\nvar data = gvisDataTableID114f79fb112f();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"width\"] = 200;\noptions[\"height\"] = 270;\n\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID114f79fb112f')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID114f1fa5c040);\n})();\nfunction displayChartGeoChartID114f1fa5c040() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID114f79fb112f);\n})();\nfunction displayChartTableID114f79fb112f() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptions\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\ndf \n<-\n \ndata.frame\n(\ncountry\n=\nc\n(\n\"US\"\n,\n \n\"GB\"\n,\n \n\"BR\"\n),\n\n                 val1\n=\nc\n(\n1\n,\n3\n,\n4\n),\n\n                 val2\n=\nc\n(\n23\n,\n12\n,\n32\n))\n\n\nLine \n<-\n gvisLineChart\n(\ndf\n,\n\n                      xvar\n=\n\"country\"\n,\n\n                      yvar\n=\nc\n(\n\"val1\"\n,\n\"val2\"\n),\n\n                      options\n=\nlist\n(\n\n                        title\n=\n\"Hello World\"\n,\n\n                        titleTextStyle\n=\n\"{color:'red',\n\n\n                                         fontName:'Courier',\n\n\n                                         fontSize:16}\"\n,\n\n                        backgroundColor\n=\n\"#D3D3D3\"\n,\n\n                        vAxis\n=\n\"{gridlines:{color:'red', count:3}}\"\n,\n\n                        hAxis\n=\n\"{title:'Country',\n\n\n                                titleTextStyle:{color:'blue'}}\"\n,\n\n                        series\n=\n\"[{color:'green',\n\n\n                                  targetAxisIndex: 0},\n\n\n                                 {color: 'orange',targetAxisIndex:1}]\"\n,\n\n                        vAxes\n=\n\"[{title:'val1'}, {title:'val2'}]\"\n,\n\n                        legend\n=\n\"bottom\"\n,\n\n                        curveType\n=\n\"function\"\n,\n\n                        width\n=\n500\n,\n\n                        height\n=\n300\n))\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataLineChartID584f5d6f811a () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"US\",\n1,\n23\n],\n[\n\"GB\",\n3,\n12\n],\n[\n\"BR\",\n4,\n32\n] \n];\ndata.addColumn('string','country');\ndata.addColumn('number','val1');\ndata.addColumn('number','val2');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartLineChartID584f5d6f811a() {\nvar data = gvisDataLineChartID584f5d6f811a();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"title\"] = \"Hello World\";\noptions[\"titleTextStyle\"] = {color:'red',\n                                         fontName:'Courier',\n                                         fontSize:16};\noptions[\"backgroundColor\"] = \"#D3D3D3\";\noptions[\"vAxis\"] = {gridlines:{color:'red', count:3}};\noptions[\"hAxis\"] = {title:'Country',\n                                titleTextStyle:{color:'blue'}};\noptions[\"series\"] = [{color:'green',\n                                  targetAxisIndex: 0},\n                                 {color: 'orange',targetAxisIndex:1}];\noptions[\"vAxes\"] = [{title:'val1'}, {title:'val2'}];\noptions[\"legend\"] = \"bottom\";\noptions[\"curveType\"] = \"function\";\noptions[\"width\"] = 500;\noptions[\"height\"] = 300;\n\n\n    var chart = new google.visualization.LineChart(\n    document.getElementById('LineChartID584f5d6f811a')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"corechart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartLineChartID584f5d6f811a);\n})();\nfunction displayChartLineChartID584f5d6f811a() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApostrophes\n\u00b6\n\n\n\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\ndf \n<-\n \ndata.frame\n(\n\"Year\"\n=\nc\n(\n2009\n,\n2010\n),\n\n                 \n\"Lloyd\\\\'s\"\n=\nc\n(\n86.1\n,\n \n93.3\n),\n\n                 \n\"Munich Re\\\\'s R/I\"\n=\nc\n(\n95.3\n,\n \n100.5\n),\n check.names\n=\nFALSE\n)\n\n\nR \n<-\n gvisColumnChart\n(\ndf\n,\n \n                     options\n=\nlist\n(\nvAxis\n=\n'{baseline:0}'\n,\n\n                                  title\n=\n\"Combined Ratio %\"\n,\n\n                                  legend\n=\n\"{position:'bottom'}\"\n))\n\n\n\ncat\n(\nR\n$\nhtml\n$\nchart\n,\n file \n=\n \n\"GoogleVis/R.html\"\n)\n \n# save\n\n\n\n\n\n\n\n\n\n\n\n\n// jsData \nfunction gvisDataColumnChartID336e54fe1b42 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n2009,\n86.1,\n95.3\n],\n[\n2010,\n93.3,\n100.5\n] \n];\ndata.addColumn('number','Year');\ndata.addColumn('number','Lloyd\\'s');\ndata.addColumn('number','Munich Re\\'s R/I');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartColumnChartID336e54fe1b42() {\nvar data = gvisDataColumnChartID336e54fe1b42();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"vAxis\"] = {baseline:0};\noptions[\"title\"] = \"Combined Ratio %\";\noptions[\"legend\"] = {position:'bottom'};\n\n\n    var chart = new google.visualization.ColumnChart(\n    document.getElementById('ColumnChartID336e54fe1b42')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"corechart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartColumnChartID336e54fe1b42);\n})();\nfunction displayChartColumnChartID336e54fe1b42() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "googleVis (embed HTML plots)"
        },
        {
            "location": "/googleVis/#suppress-the-message",
            "text": "We normally load the library\u2026  1 library ( googleVis )    \u2026but we can also suppress the message when loading the package.  1 suppressPackageStartupMessages ( library ( googleVis ))",
            "title": "Suppress the message"
        },
        {
            "location": "/googleVis/#printing",
            "text": "Generate the chart.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 library ( googleVis )  head ( Fruits ,   3 ) \n\nM  <-  gvisMotionChart ( Fruits , \n                     idvar = 'Fruit' , \n                     timevar = 'Year' , \n                     options = list ( width = 400 ,  height = 350 ))  # xvar =  # yvar =  # colorvar =  # sizevar =  # date.format =     Emulate the chart\u2026    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 # In the browser, with a tag underneath \nplot ( M )  # In the console (code only), with a tag underneath \nM # Header part  # Basic html and formatting tags  print ( M ,  tag = 'header' )  # Actual Google visualization code  # Can be copy-paste in a markdown or html document  print ( M ,  tag = 'chart' )  # Header + visualization code = what we see in the browser  # Components of the chart  print ( M ,  tag = 'jsChart' )  # Basic chart caption and html footer (what is underneath)  print ( M ,  tag = 'caption' )  # Save it locally  print ( M ,  file = \"GoogleVis/M.html\" )  # Or  #cat(M$html$chart, file = \"GoogleVis/M.html\")",
            "title": "Printing"
        },
        {
            "location": "/googleVis/#embedding-a-chartmap-into-a-static-website",
            "text": "The chart can be standing alone as an image file; even be embedded within a text.   In RStudio, set the working directory with  setwd(' ') .  Set the option to print the output in a html file.   1\n2\n3\n4 { r ,  message = FALSE }  library ( googleVis ) \n\nop  <-   options ( gvis.plot.tag = 'chart' )     Set options back to original options.   1 options ( op )     Generate the chart.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 library ( googleVis )  # Set the option to print here (alternative)  options ( gvis.plot.tag = 'chart' )  # Create the chart \nM  <-  gvisMotionChart ( Fruits , \n                     idvar = 'Fruit' , \n                     timevar = 'Year' , \n                     options = list ( width = 400 ,  height = 350 ))  # Do not print the chart in the browser  #plot(M) # in the browser  # Print the chart as a html file  cat ( M $ html $ chart ,  file  =   \"M.html\" )     The file has several parts: JavaScript and HTML. The part that displays the chart is in the end.   1\n2\n3\n4\n5 <!-- divChart -->  < div   id = \"MotionChartID2cd02805862d\"  \n      style = \"width: 400; height: 350;\" >  </ div >     Following this bullet, in the Markdown document, paste the M.html code. First, open the HTML file to copy the code.  Modify the markup language with more styling parameters.   1\n2\n3 < div   id = \"MotionChartID2cd02805862d\"   style = \"width: 400; height: 350; float:left;\" ; >  </ div >     The HTML snippet must follow the JavaScript snippet in the document. Move the HTML snippet within a text (the text can separate both snippets).    1\n2\n3\n4\n5 Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. < div   id = \"MotionChartID2cd02805862d\"   style = \"width: 400; height: 350; float:left;\" ; >  </ div > \nDuis aute irure dolor...   Here is the result:   \n\n// jsData \nfunction gvisDataMotionChartID2cd02805862d () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Apples\",\n2008,\n\"West\",\n98,\n78,\n20,\n\"2008-12-31\"\n],\n[\n\"Apples\",\n2009,\n\"West\",\n111,\n79,\n32,\n\"2009-12-31\"\n],\n[\n\"Apples\",\n2010,\n\"West\",\n89,\n76,\n13,\n\"2010-12-31\"\n],\n[\n\"Oranges\",\n2008,\n\"East\",\n96,\n81,\n15,\n\"2008-12-31\"\n],\n[\n\"Bananas\",\n2008,\n\"East\",\n85,\n76,\n9,\n\"2008-12-31\"\n],\n[\n\"Oranges\",\n2009,\n\"East\",\n93,\n80,\n13,\n\"2009-12-31\"\n],\n[\n\"Bananas\",\n2009,\n\"East\",\n94,\n78,\n16,\n\"2009-12-31\"\n],\n[\n\"Oranges\",\n2010,\n\"East\",\n98,\n91,\n7,\n\"2010-12-31\"\n],\n[\n\"Bananas\",\n2010,\n\"East\",\n81,\n71,\n10,\n\"2010-12-31\"\n] \n];\ndata.addColumn('string','Fruit');\ndata.addColumn('number','Year');\ndata.addColumn('string','Location');\ndata.addColumn('number','Sales');\ndata.addColumn('number','Expenses');\ndata.addColumn('number','Profit');\ndata.addColumn('string','Date');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartMotionChartID2cd02805862d() {\nvar data = gvisDataMotionChartID2cd02805862d();\nvar options = {};\noptions[\"width\"] = 400;\noptions[\"height\"] = 350;\noptions[\"state\"] = \"\";\n\n\n    var chart = new google.visualization.MotionChart(\n    document.getElementById('MotionChartID2cd02805862d')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"motionchart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartMotionChartID2cd02805862d);\n})();\nfunction displayChartMotionChartID2cd02805862d() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter     Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.    Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem.",
            "title": "Embedding a chart/map into a static website"
        },
        {
            "location": "/googleVis/#other-static-websites",
            "text": "There is a procedure for embedding graphics in:   WordPress.  Google Sites.  Blogger.  Google Code wiki pages.  Wikipedia.  others websites.",
            "title": "Other static websites"
        },
        {
            "location": "/googleVis/#dynamic-websites",
            "text": "The  R.rsp  package allows the integration of R code into html code.   The  rApache  and  brew  packages support web application development using R and the Apache HTTP server.  Rook is a lightweight web server interface for R.  The  shiny  package builds interactive web application with R.",
            "title": "Dynamic websites"
        },
        {
            "location": "/googleVis/#charts",
            "text": "Consult the examples (further above); charts are similar to what we find in other packages.   gvisMotionChart  is exclusive to  googleVis .",
            "title": "Charts"
        },
        {
            "location": "/googleVis/#gvismotionchart",
            "text": "1 head ( Fruits ,   3 )    1\n2\n3\n4\n5 # Output\n   Fruit Year Location Sales Expenses Profit       Date\n1 Apples 2008     West    98       78     20 2008-12-31\n2 Apples 2009     West   111       79     32 2009-12-31\n3 Apples 2010     West    89       76     13 2010-12-31    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 M  <-  gvisMotionChart ( Fruits , \n                     idvar = 'Fruit' , \n                     timevar = 'Year' , \n                     options = list ( width = 400 ,  height = 350 ))  # xvar =  # yvar =  # colorvar =  # sizevar =  # date.format = \n\nplot ( M )     \n\n// jsData \nfunction gvisDataMotionChartID336e668836ab () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Apples\",\n2008,\n\"West\",\n98,\n78,\n20,\n\"2008-12-31\"\n],\n[\n\"Apples\",\n2009,\n\"West\",\n111,\n79,\n32,\n\"2009-12-31\"\n],\n[\n\"Apples\",\n2010,\n\"West\",\n89,\n76,\n13,\n\"2010-12-31\"\n],\n[\n\"Oranges\",\n2008,\n\"East\",\n96,\n81,\n15,\n\"2008-12-31\"\n],\n[\n\"Bananas\",\n2008,\n\"East\",\n85,\n76,\n9,\n\"2008-12-31\"\n],\n[\n\"Oranges\",\n2009,\n\"East\",\n93,\n80,\n13,\n\"2009-12-31\"\n],\n[\n\"Bananas\",\n2009,\n\"East\",\n94,\n78,\n16,\n\"2009-12-31\"\n],\n[\n\"Oranges\",\n2010,\n\"East\",\n98,\n91,\n7,\n\"2010-12-31\"\n],\n[\n\"Bananas\",\n2010,\n\"East\",\n81,\n71,\n10,\n\"2010-12-31\"\n] \n];\ndata.addColumn('string','Fruit');\ndata.addColumn('number','Year');\ndata.addColumn('string','Location');\ndata.addColumn('number','Sales');\ndata.addColumn('number','Expenses');\ndata.addColumn('number','Profit');\ndata.addColumn('string','Date');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartMotionChartID336e668836ab() {\nvar data = gvisDataMotionChartID336e668836ab();\nvar options = {};\noptions[\"width\"] = 400;\noptions[\"height\"] = 350;\noptions[\"state\"] = \"\";\n\n\n    var chart = new google.visualization.MotionChart(\n    document.getElementById('MotionChartID336e668836ab')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"motionchart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartMotionChartID336e668836ab);\n})();\nfunction displayChartMotionChartID336e668836ab() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "gvisMotionChart"
        },
        {
            "location": "/googleVis/#embedding-with-iframe-or-embed",
            "text": "After we generate the HTML chart or map, save the object as a HTML file.  1\n2\n3\n4 library ( htmlwidgets )   library ( DT )  \n\nsaveWidget ( object ,   \"googleVis.html\" )    Add these HTML snippets to the Markdown/HTML document.  1\n2 < iframe   seamless   src = \"../googleVis.html\"   width = 600px   height = 400px   ></ iframe >     1 < embed   seamless   src = \"../googleVis.html\"   width = 600px   height = 400px   ></ embed >     In other words, we can either write the code or embed a file into the HTML/Markdown document.",
            "title": "Embedding with &lt;iframe&gt; or &lt;embed&gt;"
        },
        {
            "location": "/googleVis/#information-from-the-object",
            "text": "1\n2 M $ type\nM $ chartid   1\n2 \"MotionChart\"\n\"MotionChartID336e668836ab\"",
            "title": "Information from the object"
        },
        {
            "location": "/googleVis/#maps",
            "text": "",
            "title": "Maps"
        },
        {
            "location": "/googleVis/#gvisgeochart",
            "text": "1 head ( Exports ,   3 )    1\n2\n3\n4\n5 # Output\n        Country Profit Online\n1       Germany      3   TRUE\n2        Brazil      4  FALSE\n3 United States      5   TRUE    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 Geo  <-  gvisGeoChart ( Exports ,  \n                    locationvar  =   \"Country\" , \n                    colorvar  =   \"Profit\" , \n                    sizevar  =   \"\" ,   # size of markers \n                    hovervar  =   \"\" ,   # text \n                    options  =   list ( projection = \"kavrayskiy-vii\" ))  # locationvar can be lat:long or address, country name, region, state, city \n\nplot ( Geo )     \n\n// jsData \nfunction gvisDataGeoChartID49617fef373 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49617fef373() {\nvar data = gvisDataGeoChartID49617fef373();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 347;\noptions[\"projection\"] = \"kavrayskiy-vii\";\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID49617fef373')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49617fef373);\n})();\nfunction displayChartGeoChartID49617fef373() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter          1 head ( CityPopularity ,   3 )    1\n2\n3\n4\n5 # Output\n      City Popularity\n1 New York        200\n2   Boston        300\n3    Miami        400   1\n2\n3\n4\n5\n6\n7\n8\n9 Geo2  <-  gvisGeoChart ( CityPopularity ,  \n                     locationvar = 'City' , \n                     colorvar = 'Popularity' , \n                     options = list ( region = 'US' , \n                                  height = 350 , \n                                  displayMode = 'markers' ,  \n                                  colorAxis = \"{values:[200,400,600,800], colors:[\\'red', \\'pink\\', \\'orange',\\'green']}\" )) \n\nplot ( Geo2 )     \n\n// jsData \nfunction gvisDataGeoChartID4961741b9385 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"New York\",\n200\n],\n[\n\"Boston\",\n300\n],\n[\n\"Miami\",\n400\n],\n[\n\"Chicago\",\n500\n],\n[\n\"Los Angeles\",\n600\n],\n[\n\"Houston\",\n700\n] \n];\ndata.addColumn('string','City');\ndata.addColumn('number','Popularity');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID4961741b9385() {\nvar data = gvisDataGeoChartID4961741b9385();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 350;\noptions[\"region\"] = \"US\";\noptions[\"displayMode\"] = \"markers\";\noptions[\"colorAxis\"] = {values:[200,400,600,800], colors:['red', 'pink', 'orange','green']};\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID4961741b9385')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID4961741b9385);\n})();\nfunction displayChartGeoChartID4961741b9385() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter           1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 CityPopularity3  <-   data.frame ( City  =   c ( 'Montreal' ,   'Toronto' ), \n                              Popularity  =   c ( 400 ,   200 )) \n\nGeo3  <-  gvisGeoChart ( CityPopularity3 ,  \n                     locationvar = 'City' , \n                     colorvar = 'Popularity' , \n                     options = list ( region  =   'CA' , \n                                  height = 350 , \n                                  displayMode = 'markers' ,  \n                                  colorAxis = \"{values:[200,400,600,800], colors:[\\'red', \\'pink\\', \\'orange',\\'green']}\" )) \n\nplot ( Geo3 )     \n\n// jsData \nfunction gvisDataGeoChartID49614f24acff () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Montreal\",\n700\n],\n[\n\"Toronto\",\n200\n] \n];\ndata.addColumn('string','City');\ndata.addColumn('number','Popularity');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49614f24acff() {\nvar data = gvisDataGeoChartID49614f24acff();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 350;\noptions[\"region\"] = \"CA\";\noptions[\"displayMode\"] = \"markers\";\noptions[\"colorAxis\"] = {values:[200,400,600,800], colors:['red', 'pink', 'orange','green']};\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID49614f24acff')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49614f24acff);\n})();\nfunction displayChartGeoChartID49614f24acff() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter          1 head ( Andrew ,   3 )    1\n2\n3\n4\n5\n6\n7\n8\n9 # Output \n         Date / Time   UTC    Lat    Long   Pressure_mb   Speed_kt              Category  1   1992 - 08 - 16   18 : 00 : 00   10.8   - 35.5          1010         25   Tropical   Depression  2   1992 - 08 - 17   00 : 00 : 00   11.2   - 37.4          1009         30   Tropical   Depression  3   1992 - 08 - 17   06 : 00 : 00   11.7   - 39.6          1008         30   Tropical   Depression \n      LatLong                                                Tip  1   10.8 :- 35.5   Tropical   Depression < BR > Pressure = 1010 < BR > Speed = 25  2   11.2 :- 37.4   Tropical   Depression < BR > Pressure = 1009 < BR > Speed = 30  3   11.7 :- 39.6   Tropical   Depression < BR > Pressure = 1008 < BR > Speed = 30    1\n2\n3\n4\n5\n6\n7 GeoMarker  <-  gvisGeoChart ( Andrew , \n                          locationvar = \"LatLong\" ,  \n                          sizevar = 'Speed_kt' , \n                          colorvar = \"Pressure_mb\" ,  \n                          options = list ( region = \"US\" )) \n\nplot ( GeoMarker )     \n\n// jsData \nfunction gvisDataGeoChartID2cd02f295f7 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n10.8,\n-35.5,\n1010,\n25\n],\n[\n11.2,\n-37.4,\n1009,\n30\n],\n[\n11.7,\n-39.6,\n1008,\n30\n],\n[\n12.3,\n-42,\n1006,\n35\n],\n[\n13.1,\n-44.2,\n1003,\n35\n],\n[\n13.6,\n-46.2,\n1002,\n40\n],\n[\n14.1,\n-48,\n1001,\n45\n],\n[\n14.6,\n-49.9,\n1000,\n45\n],\n[\n15.4,\n-51.8,\n1000,\n45\n],\n[\n16.3,\n-53.5,\n1001,\n45\n],\n[\n17.2,\n-55.3,\n1002,\n45\n],\n[\n18,\n-56.9,\n1005,\n45\n],\n[\n18.8,\n-58.3,\n1007,\n45\n],\n[\n19.8,\n-59.3,\n1011,\n40\n],\n[\n20.7,\n-60,\n1013,\n40\n],\n[\n21.7,\n-60.7,\n1015,\n40\n],\n[\n22.5,\n-61.5,\n1014,\n40\n],\n[\n23.2,\n-62.4,\n1014,\n45\n],\n[\n23.9,\n-63.3,\n1010,\n45\n],\n[\n24.4,\n-64.2,\n1007,\n50\n],\n[\n24.8,\n-64.9,\n1004,\n50\n],\n[\n25.3,\n-65.9,\n1000,\n55\n],\n[\n25.6,\n-67,\n994,\n60\n],\n[\n25.8,\n-68.3,\n981,\n70\n],\n[\n25.7,\n-69.7,\n969,\n80\n],\n[\n25.6,\n-71.1,\n961,\n90\n],\n[\n25.5,\n-72.5,\n947,\n105\n],\n[\n25.4,\n-74.2,\n933,\n120\n],\n[\n25.4,\n-75.8,\n922,\n135\n],\n[\n25.4,\n-77.5,\n930,\n125\n],\n[\n25.4,\n-79.3,\n937,\n120\n],\n[\n25.6,\n-81.2,\n951,\n110\n],\n[\n25.8,\n-83.1,\n947,\n115\n],\n[\n26.2,\n-85,\n943,\n115\n],\n[\n26.6,\n-86.7,\n948,\n115\n],\n[\n27.2,\n-88.2,\n946,\n115\n],\n[\n27.8,\n-89.6,\n941,\n120\n],\n[\n28.5,\n-90.5,\n937,\n120\n],\n[\n29.2,\n-91.3,\n955,\n115\n],\n[\n30.1,\n-91.7,\n973,\n80\n],\n[\n30.9,\n-91.6,\n991,\n50\n],\n[\n31.5,\n-91.1,\n995,\n35\n],\n[\n32.1,\n-90.5,\n997,\n30\n],\n[\n32.8,\n-89.6,\n998,\n30\n],\n[\n33.6,\n-88.4,\n999,\n25\n],\n[\n34.4,\n-86.7,\n1000,\n20\n],\n[\n35.4,\n-84,\n1000,\n20\n] \n];\ndata.addColumn('number','Latitude');\ndata.addColumn('number','Longitude');\ndata.addColumn('number','Pressure_mb');\ndata.addColumn('number','Speed_kt');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID2cd02f295f7() {\nvar data = gvisDataGeoChartID2cd02f295f7();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 347;\noptions[\"region\"] = \"US\";\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID2cd02f295f7')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID2cd02f295f7);\n})();\nfunction displayChartGeoChartID2cd02f295f7() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "gvisGeoChart"
        },
        {
            "location": "/googleVis/#gvismap-or-google-maps",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 AndrewMap  <-  gvisMap ( Andrew , \n                     locationvar = \"LatLong\"   , \n                     tipvar = \"Tip\" ,   #  text displayed over the tip icon \n                     options = list ( showTip = TRUE ,  \n                                  showLine = TRUE ,  \n                                  enableScrollWheel = TRUE , \n                                  mapType = 'terrain' ,  \n                                  useMapTypeControl = TRUE )) \n\nplot ( AndrewMap )     \n\n// jsData \nfunction gvisDataMapID496129f7ada () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n10.8,\n-35.5,\n\"Tropical Depression<BR>Pressure=1010<BR>Speed=25\"\n],\n[\n11.2,\n-37.4,\n\"Tropical Depression<BR>Pressure=1009<BR>Speed=30\"\n],\n[\n11.7,\n-39.6,\n\"Tropical Depression<BR>Pressure=1008<BR>Speed=30\"\n],\n[\n12.3,\n-42,\n\"Tropical Storm<BR>Pressure=1006<BR>Speed=35\"\n],\n[\n13.1,\n-44.2,\n\"Tropical Storm<BR>Pressure=1003<BR>Speed=35\"\n],\n[\n13.6,\n-46.2,\n\"Tropical Storm<BR>Pressure=1002<BR>Speed=40\"\n],\n[\n14.1,\n-48,\n\"Tropical Storm<BR>Pressure=1001<BR>Speed=45\"\n],\n[\n14.6,\n-49.9,\n\"Tropical Storm<BR>Pressure=1000<BR>Speed=45\"\n],\n[\n15.4,\n-51.8,\n\"Tropical Storm<BR>Pressure=1000<BR>Speed=45\"\n],\n[\n16.3,\n-53.5,\n\"Tropical Storm<BR>Pressure=1001<BR>Speed=45\"\n],\n[\n17.2,\n-55.3,\n\"Tropical Storm<BR>Pressure=1002<BR>Speed=45\"\n],\n[\n18,\n-56.9,\n\"Tropical Storm<BR>Pressure=1005<BR>Speed=45\"\n],\n[\n18.8,\n-58.3,\n\"Tropical Storm<BR>Pressure=1007<BR>Speed=45\"\n],\n[\n19.8,\n-59.3,\n\"Tropical Storm<BR>Pressure=1011<BR>Speed=40\"\n],\n[\n20.7,\n-60,\n\"Tropical Storm<BR>Pressure=1013<BR>Speed=40\"\n],\n[\n21.7,\n-60.7,\n\"Tropical Storm<BR>Pressure=1015<BR>Speed=40\"\n],\n[\n22.5,\n-61.5,\n\"Tropical Storm<BR>Pressure=1014<BR>Speed=40\"\n],\n[\n23.2,\n-62.4,\n\"Tropical Storm<BR>Pressure=1014<BR>Speed=45\"\n],\n[\n23.9,\n-63.3,\n\"Tropical Storm<BR>Pressure=1010<BR>Speed=45\"\n],\n[\n24.4,\n-64.2,\n\"Tropical Storm<BR>Pressure=1007<BR>Speed=50\"\n],\n[\n24.8,\n-64.9,\n\"Tropical Storm<BR>Pressure=1004<BR>Speed=50\"\n],\n[\n25.3,\n-65.9,\n\"Tropical Storm<BR>Pressure=1000<BR>Speed=55\"\n],\n[\n25.6,\n-67,\n\"Tropical Storm<BR>Pressure=994<BR>Speed=60\"\n],\n[\n25.8,\n-68.3,\n\"Hurricane<BR>Pressure=981<BR>Speed=70\"\n],\n[\n25.7,\n-69.7,\n\"Hurricane<BR>Pressure=969<BR>Speed=80\"\n],\n[\n25.6,\n-71.1,\n\"Hurricane<BR>Pressure=961<BR>Speed=90\"\n],\n[\n25.5,\n-72.5,\n\"Hurricane<BR>Pressure=947<BR>Speed=105\"\n],\n[\n25.4,\n-74.2,\n\"Hurricane<BR>Pressure=933<BR>Speed=120\"\n],\n[\n25.4,\n-75.8,\n\"Hurricane<BR>Pressure=922<BR>Speed=135\"\n],\n[\n25.4,\n-77.5,\n\"Hurricane<BR>Pressure=930<BR>Speed=125\"\n],\n[\n25.4,\n-79.3,\n\"Hurricane<BR>Pressure=937<BR>Speed=120\"\n],\n[\n25.6,\n-81.2,\n\"Hurricane<BR>Pressure=951<BR>Speed=110\"\n],\n[\n25.8,\n-83.1,\n\"Hurricane<BR>Pressure=947<BR>Speed=115\"\n],\n[\n26.2,\n-85,\n\"Hurricane<BR>Pressure=943<BR>Speed=115\"\n],\n[\n26.6,\n-86.7,\n\"Hurricane<BR>Pressure=948<BR>Speed=115\"\n],\n[\n27.2,\n-88.2,\n\"Hurricane<BR>Pressure=946<BR>Speed=115\"\n],\n[\n27.8,\n-89.6,\n\"Hurricane<BR>Pressure=941<BR>Speed=120\"\n],\n[\n28.5,\n-90.5,\n\"Hurricane<BR>Pressure=937<BR>Speed=120\"\n],\n[\n29.2,\n-91.3,\n\"Hurricane<BR>Pressure=955<BR>Speed=115\"\n],\n[\n30.1,\n-91.7,\n\"Tropical Storm<BR>Pressure=973<BR>Speed=80\"\n],\n[\n30.9,\n-91.6,\n\"Tropical Storm<BR>Pressure=991<BR>Speed=50\"\n],\n[\n31.5,\n-91.1,\n\"Tropical Depression<BR>Pressure=995<BR>Speed=35\"\n],\n[\n32.1,\n-90.5,\n\"Tropical Depression<BR>Pressure=997<BR>Speed=30\"\n],\n[\n32.8,\n-89.6,\n\"Tropical Depression<BR>Pressure=998<BR>Speed=30\"\n],\n[\n33.6,\n-88.4,\n\"Tropical Depression<BR>Pressure=999<BR>Speed=25\"\n],\n[\n34.4,\n-86.7,\n\"Tropical Depression<BR>Pressure=1000<BR>Speed=20\"\n],\n[\n35.4,\n-84,\n\"Tropical Depression<BR>Pressure=1000<BR>Speed=20\"\n] \n];\ndata.addColumn('number','Latitude');\ndata.addColumn('number','Longitude');\ndata.addColumn('string','Tip');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartMapID496129f7ada() {\nvar data = gvisDataMapID496129f7ada();\nvar options = {};\noptions[\"showTip\"] = true;\noptions[\"showLine\"] = true;\noptions[\"enableScrollWheel\"] = true;\noptions[\"mapType\"] = \"terrain\";\noptions[\"useMapTypeControl\"] = true;\n\n\n    var chart = new google.visualization.Map(\n    document.getElementById('MapID496129f7ada')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"map\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartMapID496129f7ada);\n})();\nfunction displayChartMapID496129f7ada() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "gvisMap or Google Maps"
        },
        {
            "location": "/googleVis/#editor",
            "text": "This is a fantastic option that let us start with one chart or map and change everything with a menu.     1\n2\n3\n4\n5\n6 Editor  <-  gvisGeoChart ( Exports ,  \n                       locationvar = \"Country\" , \n                       colorvar = \"Profit\" , \n                       options = list ( gvis.editor = 'Edit me!' )) \n\nplot ( Editor )     \n\n// jsData \nfunction gvisDataGeoChartID49613aa5c54a () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49613aa5c54a() {\nvar data = gvisDataGeoChartID49613aa5c54a();\nvar options = {};\noptions[\"width\"] = 556;\noptions[\"height\"] = 347;\n\n\n    chartGeoChartID49613aa5c54a = new google.visualization.ChartWrapper({\n    dataTable: data,       \n    chartType: 'GeoChart',\n    containerId: 'GeoChartID49613aa5c54a',\n    options: options\n    });\n    chartGeoChartID49613aa5c54a.draw();\n\n\n}\n\n  function openEditorGeoChartID49613aa5c54a() {\n  var editor = new google.visualization.ChartEditor();\n  google.visualization.events.addListener(editor, 'ok',\n  function() { \n  chartGeoChartID49613aa5c54a = editor.getChartWrapper();  \n  chartGeoChartID49613aa5c54a.draw(document.getElementById('GeoChartID49613aa5c54a')); \n  }); \n  editor.openDialog(chartGeoChartID49613aa5c54a);\n  }\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"charteditor\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49613aa5c54a);\n})();\nfunction displayChartGeoChartID49613aa5c54a() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "Editor"
        },
        {
            "location": "/googleVis/#tables-with-gvistable",
            "text": "1 head ( Stock ,   3 )    1\n2\n3\n4\n5 # Output\n        Date  Device Value Title Annotation\n1 2008-01-01 Pencils  3000  <NA>       <NA>\n2 2008-01-02 Pencils 14045  <NA>       <NA>\n3 2008-01-03 Pencils  5502  <NA>       <NA>   1\n2\n3\n4 Table  <-  gvisTable ( Stock ,  \n                   formats = list ( Value = \"#,###\" )) \n\nplot ( Table )     \n\n// jsData \nfunction gvisDataTableID2cd03b735929 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\nnew Date(2008,0,1),\n\"Pencils\",\n3000,\nnull,\nnull\n],\n[\nnew Date(2008,0,2),\n\"Pencils\",\n14045,\nnull,\nnull\n],\n[\nnew Date(2008,0,3),\n\"Pencils\",\n5502,\nnull,\nnull\n],\n[\nnew Date(2008,0,4),\n\"Pencils\",\n75284,\nnull,\nnull\n],\n[\nnew Date(2008,0,5),\n\"Pencils\",\n41476,\n\"Bought pencils\",\n\"Bought 200k pencils\"\n],\n[\nnew Date(2008,0,6),\n\"Pencils\",\n333222,\nnull,\nnull\n],\n[\nnew Date(2008,0,1),\n\"Pens\",\n40645,\nnull,\nnull\n],\n[\nnew Date(2008,0,2),\n\"Pens\",\n20374,\nnull,\nnull\n],\n[\nnew Date(2008,0,3),\n\"Pens\",\n50766,\nnull,\nnull\n],\n[\nnew Date(2008,0,4),\n\"Pens\",\n14334,\n\"Out of stock\",\n\"Ran out of stock of pens at 4pm\"\n],\n[\nnew Date(2008,0,5),\n\"Pens\",\n66467,\nnull,\nnull\n],\n[\nnew Date(2008,0,6),\n\"Pens\",\n39463,\nnull,\nnull\n] \n];\ndata.addColumn('date','Date');\ndata.addColumn('string','Device');\ndata.addColumn('number','Value');\ndata.addColumn('string','Title');\ndata.addColumn('string','Annotation');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartTableID2cd03b735929() {\nvar data = gvisDataTableID2cd03b735929();\nvar options = {};\noptions[\"allowHtml\"] = true;\n\n  var dataFormat1 = new google.visualization.NumberFormat({pattern:\"#,###\"});\n  dataFormat1.format(data, 2);\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID2cd03b735929')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID2cd03b735929);\n})();\nfunction displayChartTableID2cd03b735929() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter           1 head ( Population ,   3 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 # Output\n                                                                                 Rank       Country Population % of World Population\n1    1         China 1339940000                0.1950\n2    2         India 1188650000                0.1730\n3    3 United States  310438000                0.0452\n                                                                                                                                                                     Flag\n1 <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_the_People%27s_Republic_of_China.svg/22px-Flag_of_the_People%27s_Republic_of_China.svg.png\">\n2                                                       <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_India.svg/22px-Flag_of_India.svg.png\">\n3                               <img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/22px-Flag_of_the_United_States.svg.png\">\n  Mode       Date\n1 TRUE 2010-10-09\n2 TRUE 2010-10-09\n3 TRUE 2010-10-09   1\n2\n3\n4\n5\n6 PopTable  <-  gvisTable ( Population ,  \n                      formats = list ( Population = \"#,###\" , \n                                    '% of World Population' = '#.#%' ), \n                      options = list ( page = 'enable' )) \n\nplot ( PopTable )     \n\n// jsData \nfunction gvisDataTableID2cd013050362 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"1\",\n\"China\",\n1339940000,\n0.195,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_the_People%27s_Republic_of_China.svg/22px-Flag_of_the_People%27s_Republic_of_China.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"2\",\n\"India\",\n1188650000,\n0.173,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_India.svg/22px-Flag_of_India.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"3\",\n\"United States\",\n310438000,\n0.0452,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Flag_of_the_United_States.svg/22px-Flag_of_the_United_States.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"4\",\n\"Indonesia\",\n237556363,\n0.0346,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_Indonesia.svg/22px-Flag_of_Indonesia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"5\",\n\"Brazil\",\n193626000,\n0.0282,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Brazil.svg/22px-Flag_of_Brazil.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"6\",\n\"Pakistan\",\n170745000,\n0.0248,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Flag_of_Pakistan.svg/22px-Flag_of_Pakistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"7\",\n\"Bangladesh\",\n164425000,\n0.0239,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f9/Flag_of_Bangladesh.svg/22px-Flag_of_Bangladesh.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"8\",\n\"Nigeria\",\n158259000,\n0.023,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Flag_of_Nigeria.svg/22px-Flag_of_Nigeria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"9\",\n\"Russia\",\n141927297,\n0.0206,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Russia.svg/22px-Flag_of_Russia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"10\",\n\"Japan\",\n127390000,\n0.0185,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Flag_of_Japan.svg/22px-Flag_of_Japan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"11\",\n\"Mexico\",\n108396211,\n0.0158,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fc/Flag_of_Mexico.svg/22px-Flag_of_Mexico.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"12\",\n\"Philippines\",\n94013200,\n0.0137,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Flag_of_the_Philippines.svg/22px-Flag_of_the_Philippines.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"13\",\n\"Vietnam\",\n85846997,\n0.0125,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Flag_of_Vietnam.svg/22px-Flag_of_Vietnam.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"14\",\n\"Ethiopia\",\n84976000,\n0.0124,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Flag_of_Ethiopia.svg/22px-Flag_of_Ethiopia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"15\",\n\"Germany\",\n81802257,\n0.0119,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Germany.svg/22px-Flag_of_Germany.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"16\",\n\"Egypt\",\n79135000,\n0.0115,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Egypt.svg/22px-Flag_of_Egypt.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"17\",\n\"Iran\",\n75078000,\n0.0109,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Flag_of_Iran.svg/22px-Flag_of_Iran.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"18\",\n\"Turkey\",\n72561312,\n0.0106,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Flag_of_Turkey.svg/22px-Flag_of_Turkey.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"19\",\n\"Dem. Rep. of Congo\",\n67827000,\n0.0099,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Flag_of_the_Democratic_Republic_of_the_Congo.svg/22px-Flag_of_the_Democratic_Republic_of_the_Congo.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"20\",\n\"Thailand\",\n67070000,\n0.01,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Flag_of_Thailand.svg/22px-Flag_of_Thailand.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"21\",\n\"France\",\n65447374,\n0.0095,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Flag_of_France.svg/22px-Flag_of_France.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"22\",\n\"United Kingdom\",\n62008049,\n0.009,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Flag_of_the_United_Kingdom.svg/22px-Flag_of_the_United_Kingdom.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"23\",\n\"Italy\",\n60402499,\n0.0088,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/03/Flag_of_Italy.svg/22px-Flag_of_Italy.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"24\",\n\"Myanmar\",\n50496000,\n0.0073,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Flag_of_Myanmar.svg/22px-Flag_of_Myanmar.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"25\",\n\"South Africa\",\n49991300,\n0.0073,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Flag_of_South_Africa.svg/22px-Flag_of_South_Africa.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"26\",\n\"South Korea\",\n49773145,\n0.0072,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Flag_of_South_Korea.svg/22px-Flag_of_South_Korea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"27\",\n\"Spain\",\n46072834,\n0.0067,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Spain.svg/22px-Flag_of_Spain.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"28\",\n\"Ukraine\",\n45871738,\n0.0067,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Flag_of_Ukraine.svg/22px-Flag_of_Ukraine.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"29\",\n\"Colombia\",\n45655000,\n0.0066,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Flag_of_Colombia.svg/22px-Flag_of_Colombia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"30\",\n\"Tanzania\",\n45040000,\n0.0066,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Tanzania.svg/22px-Flag_of_Tanzania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"31\",\n\"Sudan\",\n43192000,\n0.0063,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_Sudan.svg/22px-Flag_of_Sudan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"32\",\n\"Argentina\",\n40518951,\n0.0059,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Flag_of_Argentina.svg/22px-Flag_of_Argentina.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"33\",\n\"Kenya\",\n38610097,\n0.0056,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Flag_of_Kenya.svg/22px-Flag_of_Kenya.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"34\",\n\"Poland\",\n38167329,\n0.0056,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Flag_of_Poland.svg/22px-Flag_of_Poland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"35\",\n\"Algeria\",\n35423000,\n0.0052,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_Algeria.svg/22px-Flag_of_Algeria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"36\",\n\"Canada\",\n34272000,\n0.005,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Flag_of_Canada.svg/22px-Flag_of_Canada.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"37\",\n\"Uganda\",\n33796000,\n0.0049,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Flag_of_Uganda.svg/22px-Flag_of_Uganda.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"38\",\n\"Morocco\",\n31944000,\n0.0046,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Morocco.svg/22px-Flag_of_Morocco.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"39\",\n\"Iraq\",\n31467000,\n0.0046,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Flag_of_Iraq.svg/22px-Flag_of_Iraq.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"40\",\n\"Nepal\",\n29853000,\n0.0043,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Flag_of_Nepal.svg/16px-Flag_of_Nepal.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"41\",\n\"Peru\",\n29461933,\n0.0043,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Flag_of_Peru.svg/22px-Flag_of_Peru.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"42\",\n\"Afghanistan\",\n29117000,\n0.0042,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Afghanistan.svg/22px-Flag_of_Afghanistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"43\",\n\"Venezuela\",\n28958000,\n0.0042,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Flag_of_Venezuela.svg/22px-Flag_of_Venezuela.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"44\",\n\"Malaysia\",\n28250500,\n0.0041,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Flag_of_Malaysia.svg/22px-Flag_of_Malaysia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"45\",\n\"Uzbekistan\",\n27794000,\n0.004,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Flag_of_Uzbekistan.svg/22px-Flag_of_Uzbekistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"46\",\n\"Saudi Arabia\",\n27136977,\n0.0039,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Flag_of_Saudi_Arabia.svg/22px-Flag_of_Saudi_Arabia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"47\",\n\"Ghana\",\n24333000,\n0.0035,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Ghana.svg/22px-Flag_of_Ghana.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"48\",\n\"Yemen\",\n24256000,\n0.0035,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Flag_of_Yemen.svg/22px-Flag_of_Yemen.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"49\",\n\"North Korea\",\n23991000,\n0.0035,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/51/Flag_of_North_Korea.svg/22px-Flag_of_North_Korea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"50\",\n\"Mozambique\",\n23406000,\n0.0034,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flag_of_Mozambique.svg/22px-Flag_of_Mozambique.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"52\",\n\"Syria\",\n22505000,\n0.0033,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Flag_of_Syria.svg/22px-Flag_of_Syria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"53\",\n\"Australia\",\n22483305,\n0.0033,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Flag_of_Australia.svg/22px-Flag_of_Australia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"54\",\n\"Cote d'Ivoire\",\n21571000,\n0.0031,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Flag_of_Cote_d%27Ivoire.svg/22px-Flag_of_Cote_d%27Ivoire.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"55\",\n\"Romania\",\n21466174,\n0.0031,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Flag_of_Romania.svg/22px-Flag_of_Romania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"56\",\n\"Sri Lanka\",\n20410000,\n0.003,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Flag_of_Sri_Lanka.svg/22px-Flag_of_Sri_Lanka.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"57\",\n\"Madagascar\",\n20146000,\n0.0029,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Madagascar.svg/22px-Flag_of_Madagascar.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"58\",\n\"Cameroon\",\n19958000,\n0.0029,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Flag_of_Cameroon.svg/22px-Flag_of_Cameroon.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"59\",\n\"Angola\",\n18993000,\n0.0028,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Angola.svg/22px-Flag_of_Angola.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"60\",\n\"Chile\",\n17140000,\n0.0025,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Flag_of_Chile.svg/22px-Flag_of_Chile.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"61\",\n\"Netherlands\",\n16619500,\n0.00242,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Flag_of_the_Netherlands.svg/22px-Flag_of_the_Netherlands.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"62\",\n\"Burkina Faso\",\n16287000,\n0.0024,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Burkina_Faso.svg/22px-Flag_of_Burkina_Faso.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"63\",\n\"Kazakhstan\",\n16197000,\n0.0024,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Flag_of_Kazakhstan.svg/22px-Flag_of_Kazakhstan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"64\",\n\"Niger\",\n15891000,\n0.0023,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Flag_of_Niger.svg/22px-Flag_of_Niger.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"65\",\n\"Malawi\",\n15692000,\n0.0023,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Flag_of_Malawi.svg/22px-Flag_of_Malawi.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"66\",\n\"Mali\",\n14517176,\n0.0021,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Mali.svg/22px-Flag_of_Mali.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"67\",\n\"Guatemala\",\n14377000,\n0.0021,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Flag_of_Guatemala.svg/22px-Flag_of_Guatemala.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"68\",\n\"Ecuador\",\n14259000,\n0.0021,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Flag_of_Ecuador.svg/22px-Flag_of_Ecuador.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"69\",\n\"Cambodia\",\n13395682,\n0.0019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Flag_of_Cambodia.svg/22px-Flag_of_Cambodia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"70\",\n\"Zambia\",\n13257000,\n0.0019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Flag_of_Zambia.svg/22px-Flag_of_Zambia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"71\",\n\"Senegal\",\n12861000,\n0.0019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Flag_of_Senegal.svg/22px-Flag_of_Senegal.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"72\",\n\"Zimbabwe\",\n12644000,\n0.0018,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/Flag_of_Zimbabwe.svg/22px-Flag_of_Zimbabwe.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"73\",\n\"Chad\",\n11506000,\n0.0017,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Flag_of_Chad.svg/22px-Flag_of_Chad.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"74\",\n\"Greece\",\n11306183,\n0.0016,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Greece.svg/22px-Flag_of_Greece.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"75\",\n\"Cuba\",\n11204000,\n0.0016,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Flag_of_Cuba.svg/22px-Flag_of_Cuba.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"76\",\n\"Belgium\",\n10827519,\n0.0016,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_Belgium_%28civil%29.svg/22px-Flag_of_Belgium_%28civil%29.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"77\",\n\"Portugal\",\n10636888,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Portugal.svg/22px-Flag_of_Portugal.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"78\",\n\"Czech Republic\",\n10512397,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Flag_of_the_Czech_Republic.svg/22px-Flag_of_the_Czech_Republic.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"79\",\n\"Tunisia\",\n10432500,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Flag_of_Tunisia.svg/22px-Flag_of_Tunisia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"80\",\n\"Guinea\",\n10324000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Flag_of_Guinea.svg/22px-Flag_of_Guinea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"81\",\n\"Rwanda\",\n10277000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Rwanda.svg/22px-Flag_of_Rwanda.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"82\",\n\"Dominican Republic\",\n10225000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_the_Dominican_Republic.svg/22px-Flag_of_the_Dominican_Republic.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"83\",\n\"Haiti\",\n10188000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Flag_of_Haiti.svg/22px-Flag_of_Haiti.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"84\",\n\"Bolivia\",\n10031000,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Bolivia.svg/22px-Flag_of_Bolivia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"85\",\n\"Hungary\",\n10013628,\n0.0015,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c1/Flag_of_Hungary.svg/22px-Flag_of_Hungary.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"86\",\n\"Serbia\",\n9856000,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Flag_of_Serbia.svg/22px-Flag_of_Serbia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"87\",\n\"Belarus\",\n9467700,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Flag_of_Belarus.svg/22px-Flag_of_Belarus.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"88\",\n\"Sweden\",\n9393648,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Sweden.svg/22px-Flag_of_Sweden.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"89\",\n\"Somalia\",\n9359000,\n0.0014,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Flag_of_Somalia.svg/22px-Flag_of_Somalia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"90\",\n\"Benin\",\n9212000,\n0.0013,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flag_of_Benin.svg/22px-Flag_of_Benin.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"91\",\n\"Azerbaijan\",\n8997400,\n0.0013,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Flag_of_Azerbaijan.svg/22px-Flag_of_Azerbaijan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"92\",\n\"Burundi\",\n8519000,\n0.0012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Flag_of_Burundi.svg/22px-Flag_of_Burundi.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"93\",\n\"Austria\",\n8372930,\n0.0012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Flag_of_Austria.svg/22px-Flag_of_Austria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"94\",\n\"Switzerland\",\n7782900,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Flag_of_Switzerland.svg/20px-Flag_of_Switzerland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"95\",\n\"Israel\",\n7640800,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Flag_of_Israel.svg/22px-Flag_of_Israel.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"96\",\n\"Honduras\",\n7616000,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/82/Flag_of_Honduras.svg/22px-Flag_of_Honduras.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"97\",\n\"Bulgaria\",\n7576751,\n0.0011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Bulgaria.svg/22px-Flag_of_Bulgaria.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"98\",\n\"Tajikistan\",\n7075000,\n0.00103,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flag_of_Tajikistan.svg/22px-Flag_of_Tajikistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"100\",\n\"Papua New Guinea\",\n6888000,\n0.001,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Flag_of_Papua_New_Guinea.svg/22px-Flag_of_Papua_New_Guinea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"101\",\n\"Togo\",\n6780000,\n0.00099,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Flag_of_Togo.svg/22px-Flag_of_Togo.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"102\",\n\"Libya\",\n6546000,\n0.00095,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Libya.svg/22px-Flag_of_Libya.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"103\",\n\"Jordan\",\n6472000,\n0.00094,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Flag_of_Jordan.svg/22px-Flag_of_Jordan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"104\",\n\"Paraguay\",\n6460000,\n0.00094,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Flag_of_Paraguay.svg/22px-Flag_of_Paraguay.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"105\",\n\"Laos\",\n6436000,\n0.00094,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Flag_of_Laos.svg/22px-Flag_of_Laos.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"106\",\n\"El Salvador\",\n6194000,\n9e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Flag_of_El_Salvador.svg/22px-Flag_of_El_Salvador.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"107\",\n\"Sierra Leone\",\n5836000,\n0.00085,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Flag_of_Sierra_Leone.svg/22px-Flag_of_Sierra_Leone.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"108\",\n\"Nicaragua\",\n5822000,\n0.00085,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Nicaragua.svg/22px-Flag_of_Nicaragua.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"109\",\n\"Kyrgyzstan\",\n5550000,\n0.00081,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Flag_of_Kyrgyzstan.svg/22px-Flag_of_Kyrgyzstan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"110\",\n\"Denmark\",\n5543819,\n8e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Flag_of_Denmark.svg/22px-Flag_of_Denmark.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"111\",\n\"Slovakia\",\n5429763,\n0.00079,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Flag_of_Slovakia.svg/22px-Flag_of_Slovakia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"112\",\n\"Finland\",\n5370000,\n0.00078,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Finland.svg/22px-Flag_of_Finland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"113\",\n\"Eritrea\",\n5224000,\n0.00076,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Flag_of_Eritrea.svg/22px-Flag_of_Eritrea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"114\",\n\"Turkmenistan\",\n5177000,\n0.00075,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Flag_of_Turkmenistan.svg/22px-Flag_of_Turkmenistan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"115\",\n\"Singapore\",\n5076700,\n0.00074,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Singapore.svg/22px-Flag_of_Singapore.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"116\",\n\"Norway\",\n4906500,\n0.00071,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Flag_of_Norway.svg/22px-Flag_of_Norway.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"117\",\n\"United Arab Emirates\",\n4707000,\n0.00068,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Flag_of_the_United_Arab_Emirates.svg/22px-Flag_of_the_United_Arab_Emirates.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"118\",\n\"Costa Rica\",\n4640000,\n0.00068,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f2/Flag_of_Costa_Rica.svg/22px-Flag_of_Costa_Rica.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"119\",\n\"Central African Republic\",\n4506000,\n0.00066,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Flag_of_the_Central_African_Republic.svg/22px-Flag_of_the_Central_African_Republic.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"120\",\n\"Ireland\",\n4470700,\n0.00064,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Flag_of_Ireland.svg/22px-Flag_of_Ireland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"121\",\n\"Georgia\",\n4436000,\n0.00065,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Flag_of_Georgia.svg/22px-Flag_of_Georgia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"122\",\n\"Croatia\",\n4435056,\n0.00065,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Flag_of_Croatia.svg/22px-Flag_of_Croatia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"123\",\n\"New Zealand\",\n4393000,\n0.00064,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Flag_of_New_Zealand.svg/22px-Flag_of_New_Zealand.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"124\",\n\"Lebanon\",\n4255000,\n0.00062,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Flag_of_Lebanon.svg/22px-Flag_of_Lebanon.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"125\",\n\"Liberia\",\n4102000,\n6e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/Flag_of_Liberia.svg/22px-Flag_of_Liberia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"127\",\n\"Palestinian territories\",\n3935249,\n0.00055,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Palestine.svg/22px-Flag_of_Palestine.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"128\",\n\"Bosnia and Herzegovina\",\n3760000,\n0.00055,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Flag_of_Bosnia_and_Herzegovina.svg/22px-Flag_of_Bosnia_and_Herzegovina.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"129\",\n\"Republic of the Congo\",\n3759000,\n0.00055,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_the_Republic_of_the_Congo.svg/22px-Flag_of_the_Republic_of_the_Congo.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"130\",\n\"Moldova\",\n3563800,\n0.00052,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Flag_of_Moldova.svg/22px-Flag_of_Moldova.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"131\",\n\"Uruguay\",\n3372000,\n0.00049,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Uruguay.svg/22px-Flag_of_Uruguay.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"132\",\n\"Mauritania\",\n3366000,\n0.00049,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Flag_of_Mauritania.svg/22px-Flag_of_Mauritania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"133\",\n\"Lithuania\",\n3329227,\n0.00048,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Flag_of_Lithuania.svg/22px-Flag_of_Lithuania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"134\",\n\"Panama\",\n3322576,\n0.00048,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/Flag_of_Panama.svg/22px-Flag_of_Panama.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"135\",\n\"Armenia\",\n3238000,\n0.00047,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Flag_of_Armenia.svg/22px-Flag_of_Armenia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"136\",\n\"Albania\",\n3195000,\n0.00046,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/36/Flag_of_Albania.svg/22px-Flag_of_Albania.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"137\",\n\"Kuwait\",\n3051000,\n0.00044,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Flag_of_Kuwait.svg/22px-Flag_of_Kuwait.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"138\",\n\"Oman\",\n2905000,\n0.00042,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Flag_of_Oman.svg/22px-Flag_of_Oman.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"139\",\n\"Mongolia\",\n2776500,\n4e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Mongolia.svg/22px-Flag_of_Mongolia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"140\",\n\"Jamaica\",\n2730000,\n4e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flag_of_Jamaica.svg/22px-Flag_of_Jamaica.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"141\",\n\"Latvia\",\n2236300,\n0.00033,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Flag_of_Latvia.svg/22px-Flag_of_Latvia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"142\",\n\"Namibia\",\n2212000,\n0.00032,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_Namibia.svg/22px-Flag_of_Namibia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"143\",\n\"Lesotho\",\n2084000,\n3e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Flag_of_Lesotho.svg/22px-Flag_of_Lesotho.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"144\",\n\"Slovenia\",\n2065720,\n3e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f0/Flag_of_Slovenia.svg/22px-Flag_of_Slovenia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"145\",\n\"Republic of Macedonia\",\n2048620,\n3e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Flag_of_Macedonia.svg/22px-Flag_of_Macedonia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"146\",\n\"Botswana\",\n1978000,\n0.00029,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Flag_of_Botswana.svg/22px-Flag_of_Botswana.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"147\",\n\"Gambia\",\n1751000,\n0.00025,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_The_Gambia.svg/22px-Flag_of_The_Gambia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"148\",\n\"Qatar\",\n1696563,\n0.00025,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Flag_of_Qatar.svg/22px-Flag_of_Qatar.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"149\",\n\"Guinea-Bissau\",\n1647000,\n0.00024,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_Guinea-Bissau.svg/22px-Flag_of_Guinea-Bissau.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"150\",\n\"Gabon\",\n1501000,\n0.00022,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Flag_of_Gabon.svg/22px-Flag_of_Gabon.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"151\",\n\"Trinidad and Tobago\",\n1344000,\n2e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Flag_of_Trinidad_and_Tobago.svg/22px-Flag_of_Trinidad_and_Tobago.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"152\",\n\"Estonia\",\n1340127,\n0.00019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Flag_of_Estonia.svg/22px-Flag_of_Estonia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"153\",\n\"Mauritius\",\n1297000,\n0.00019,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Flag_of_Mauritius.svg/22px-Flag_of_Mauritius.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"154\",\n\"Swaziland\",\n1202000,\n0.00017,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Flag_of_Swaziland.svg/22px-Flag_of_Swaziland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"155\",\n\"East Timor\",\n1171000,\n0.00017,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Flag_of_East_Timor.svg/22px-Flag_of_East_Timor.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"156\",\n\"Djibouti\",\n879000,\n0.00013,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Flag_of_Djibouti.svg/22px-Flag_of_Djibouti.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"157\",\n\"Fiji\",\n854000,\n0.00012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Flag_of_Fiji.svg/22px-Flag_of_Fiji.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"158\",\n\"Bahrain\",\n807000,\n0.00012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Flag_of_Bahrain.svg/22px-Flag_of_Bahrain.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"159\",\n\"Cyprus\",\n801851,\n0.00012,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Flag_of_Cyprus.svg/22px-Flag_of_Cyprus.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"160\",\n\"Guyana\",\n761000,\n0.00011,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Flag_of_Guyana.svg/22px-Flag_of_Guyana.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"161\",\n\"Bhutan\",\n708000,\n1e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Flag_of_Bhutan.svg/22px-Flag_of_Bhutan.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"162\",\n\"Equatorial Guinea\",\n693000,\n1e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Equatorial_Guinea.svg/22px-Flag_of_Equatorial_Guinea.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"163\",\n\"Comoros\",\n691000,\n1e-04,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/94/Flag_of_the_Comoros.svg/22px-Flag_of_the_Comoros.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"164\",\n\"Montenegro\",\n626000,\n9e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Flag_of_Montenegro.svg/22px-Flag_of_Montenegro.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"166\",\n\"Solomon Islands\",\n536000,\n8e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Flag_of_the_Solomon_Islands.svg/22px-Flag_of_the_Solomon_Islands.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"167\",\n\"Western Sahara\",\n530000,\n8e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Flag_of_Western_Sahara.svg/22px-Flag_of_Western_Sahara.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"168\",\n\"Suriname\",\n524000,\n8e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Flag_of_Suriname.svg/22px-Flag_of_Suriname.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"169\",\n\"Cape Verde\",\n513000,\n7e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Cape_Verde.svg/22px-Flag_of_Cape_Verde.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"170\",\n\"Luxembourg\",\n502207,\n7e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Flag_of_Luxembourg.svg/22px-Flag_of_Luxembourg.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"171\",\n\"Malta\",\n416333,\n6e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Flag_of_Malta.svg/22px-Flag_of_Malta.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"172\",\n\"Brunei\",\n407000,\n6e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Flag_of_Brunei.svg/22px-Flag_of_Brunei.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"173\",\n\"Bahamas\",\n346000,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Flag_of_the_Bahamas.svg/22px-Flag_of_the_Bahamas.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"174\",\n\"Belize\",\n322100,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Flag_of_Belize.svg/22px-Flag_of_Belize.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"175\",\n\"Iceland\",\n318006,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Flag_of_Iceland.svg/22px-Flag_of_Iceland.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"176\",\n\"Maldives\",\n314000,\n5e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Flag_of_Maldives.svg/22px-Flag_of_Maldives.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"177\",\n\"Barbados\",\n257000,\n4e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Flag_of_Barbados.svg/22px-Flag_of_Barbados.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"178\",\n\"Vanuatu\",\n246000,\n4e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Vanuatu.svg/22px-Flag_of_Vanuatu.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"181\",\n\"Samoa\",\n179000,\n3e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/31/Flag_of_Samoa.svg/22px-Flag_of_Samoa.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"182\",\n\"Saint Lucia\",\n174000,\n3e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Flag_of_Saint_Lucia.svg/22px-Flag_of_Saint_Lucia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"183\",\n\"Sao Tome and Principe\",\n165000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Flag_of_Sao_Tome_and_Principe.svg/22px-Flag_of_Sao_Tome_and_Principe.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"184\",\n\"Federated States of Micronesia\",\n111000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Flag_of_Federated_States_of_Micronesia.svg/22px-Flag_of_Federated_States_of_Micronesia.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"186\",\n\"Saint Vincent and the Grenadines\",\n109000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Flag_of_Saint_Vincent_and_the_Grenadines.svg/22px-Flag_of_Saint_Vincent_and_the_Grenadines.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"188\",\n\"Grenada\",\n104000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Flag_of_Grenada.svg/22px-Flag_of_Grenada.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"189\",\n\"Tonga\",\n104000,\n2e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Flag_of_Tonga.svg/22px-Flag_of_Tonga.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"190\",\n\"Kiribati\",\n100000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Flag_of_Kiribati.svg/22px-Flag_of_Kiribati.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"192\",\n\"Antigua and Barbuda\",\n89000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Flag_of_Antigua_and_Barbuda.svg/22px-Flag_of_Antigua_and_Barbuda.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"194\",\n\"Seychelles\",\n85000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Flag_of_the_Seychelles.svg/22px-Flag_of_the_Seychelles.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"195\",\n\"Andorra\",\n84082,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Flag_of_Andorra.svg/22px-Flag_of_Andorra.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"198\",\n\"Dominica\",\n67000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Flag_of_Dominica.svg/22px-Flag_of_Dominica.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"200\",\n\"Marshall Islands\",\n63000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/Flag_of_the_Marshall_Islands.svg/22px-Flag_of_the_Marshall_Islands.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"204\",\n\"Saint Kitts and Nevis\",\n52000,\n1e-05,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Flag_of_Saint_Kitts_and_Nevis.svg/22px-Flag_of_Saint_Kitts_and_Nevis.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"206\",\n\"Liechtenstein\",\n35904,\n5e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Flag_of_Liechtenstein.svg/22px-Flag_of_Liechtenstein.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"207\",\n\"Monaco\",\n33000,\n5e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Flag_of_Monaco.svg/22px-Flag_of_Monaco.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"209\",\n\"San Marino\",\n31794,\n5e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Flag_of_San_Marino.svg/22px-Flag_of_San_Marino.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"213\",\n\"Palau\",\n20000,\n3e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Flag_of_Palau.svg/22px-Flag_of_Palau.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"215\",\n\"Tuvalu\",\n10000,\n1e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Flag_of_Tuvalu.svg/22px-Flag_of_Tuvalu.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"216\",\n\"Nauru\",\n10000,\n1e-06,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/3/30/Flag_of_Nauru.svg/22px-Flag_of_Nauru.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n],\n[\n\"222\",\n\"Vatican City\",\n800,\n2e-07,\n\"<img src=\\\"http://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Flag_of_the_Vatican_City.svg/20px-Flag_of_the_Vatican_City.svg.png\\\">\",\ntrue,\nnew Date(2010,9,9)\n] \n];\ndata.addColumn('string','Rank');\ndata.addColumn('string','Country');\ndata.addColumn('number','Population');\ndata.addColumn('number','% of World Population');\ndata.addColumn('string','Flag');\ndata.addColumn('boolean','Mode');\ndata.addColumn('date','Date');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartTableID2cd013050362() {\nvar data = gvisDataTableID2cd013050362();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"page\"] = \"enable\";\n\n  var dataFormat1 = new google.visualization.NumberFormat({pattern:\"#,###\"});\n  dataFormat1.format(data, 2);\n  var dataFormat2 = new google.visualization.NumberFormat({pattern:\"#.#%\"});\n  dataFormat2.format(data, 3);\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID2cd013050362')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID2cd013050362);\n})();\nfunction displayChartTableID2cd013050362() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "Tables with gvisTable"
        },
        {
            "location": "/googleVis/#dashboards-with-gvismerge",
            "text": "1\n2\n3\n4\n5\n6\n7 G  <-  gvisGeoChart ( Exports , \n                  locationvar = \"Country\" , \n                  colorvar = \"Profit\" , \n                  options = list ( width = 300 ,  height = 200 ))  T   <-  gvisTable ( Exports , \n               options = list ( width = 300 ,  height = 370 ))    1\n2\n3 GT  <-  gvisMerge ( G ,   T ,  horizontal = FALSE ) \n\nplot ( GT )     \n\n// jsData \nfunction gvisDataGeoChartID49611ffdda49 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n\n// jsData \nfunction gvisDataTableID496155ab963a () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3,\ntrue\n],\n[\n\"Brazil\",\n4,\nfalse\n],\n[\n\"United States\",\n5,\ntrue\n],\n[\n\"France\",\n4,\ntrue\n],\n[\n\"Hungary\",\n3,\nfalse\n],\n[\n\"India\",\n2,\ntrue\n],\n[\n\"Iceland\",\n1,\nfalse\n],\n[\n\"Norway\",\n4,\ntrue\n],\n[\n\"Spain\",\n5,\ntrue\n],\n[\n\"Turkey\",\n1,\nfalse\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addColumn('boolean','Online');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID49611ffdda49() {\nvar data = gvisDataGeoChartID49611ffdda49();\nvar options = {};\noptions[\"width\"] = 300;\noptions[\"height\"] = 200;\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID49611ffdda49')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n\n// jsDrawChart\nfunction drawChartTableID496155ab963a() {\nvar data = gvisDataTableID496155ab963a();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"width\"] = 300;\noptions[\"height\"] = 370;\n\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID496155ab963a')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID49611ffdda49);\n})();\nfunction displayChartGeoChartID49611ffdda49() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID496155ab963a);\n})();\nfunction displayChartTableID496155ab963a() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter                       1\n2\n3\n4 G  <-  gvisGeoChart ( Exports , \n                  locationvar = \"Country\" , \n                  colorvar = \"Profit\" , \n                  options = list ( width = 300 ,  height = 370 ))    1\n2\n3 GT  <-  gvisMerge ( G ,   T ,  horizontal = TRUE ) \n\nplot ( GT )     \n\n// jsData \nfunction gvisDataGeoChartID114f1fa5c040 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3\n],\n[\n\"Brazil\",\n4\n],\n[\n\"United States\",\n5\n],\n[\n\"France\",\n4\n],\n[\n\"Hungary\",\n3\n],\n[\n\"India\",\n2\n],\n[\n\"Iceland\",\n1\n],\n[\n\"Norway\",\n4\n],\n[\n\"Spain\",\n5\n],\n[\n\"Turkey\",\n1\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addRows(datajson);\nreturn(data);\n}\n\n\n// jsData \nfunction gvisDataTableID114f79fb112f () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"Germany\",\n3,\ntrue\n],\n[\n\"Brazil\",\n4,\nfalse\n],\n[\n\"United States\",\n5,\ntrue\n],\n[\n\"France\",\n4,\ntrue\n],\n[\n\"Hungary\",\n3,\nfalse\n],\n[\n\"India\",\n2,\ntrue\n],\n[\n\"Iceland\",\n1,\nfalse\n],\n[\n\"Norway\",\n4,\ntrue\n],\n[\n\"Spain\",\n5,\ntrue\n],\n[\n\"Turkey\",\n1,\nfalse\n] \n];\ndata.addColumn('string','Country');\ndata.addColumn('number','Profit');\ndata.addColumn('boolean','Online');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartGeoChartID114f1fa5c040() {\nvar data = gvisDataGeoChartID114f1fa5c040();\nvar options = {};\noptions[\"width\"] = 300;\noptions[\"height\"] = 370;\n\n\n    var chart = new google.visualization.GeoChart(\n    document.getElementById('GeoChartID114f1fa5c040')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n\n// jsDrawChart\nfunction drawChartTableID114f79fb112f() {\nvar data = gvisDataTableID114f79fb112f();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"width\"] = 200;\noptions[\"height\"] = 270;\n\n\n    var chart = new google.visualization.Table(\n    document.getElementById('TableID114f79fb112f')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"geochart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartGeoChartID114f1fa5c040);\n})();\nfunction displayChartGeoChartID114f1fa5c040() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"table\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartTableID114f79fb112f);\n})();\nfunction displayChartTableID114f79fb112f() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "Dashboards with gvisMerge"
        },
        {
            "location": "/googleVis/#options",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 df  <-   data.frame ( country = c ( \"US\" ,   \"GB\" ,   \"BR\" ), \n                 val1 = c ( 1 , 3 , 4 ), \n                 val2 = c ( 23 , 12 , 32 )) \n\nLine  <-  gvisLineChart ( df , \n                      xvar = \"country\" , \n                      yvar = c ( \"val1\" , \"val2\" ), \n                      options = list ( \n                        title = \"Hello World\" , \n                        titleTextStyle = \"{color:'red',                                           fontName:'Courier',                                           fontSize:16}\" , \n                        backgroundColor = \"#D3D3D3\" , \n                        vAxis = \"{gridlines:{color:'red', count:3}}\" , \n                        hAxis = \"{title:'Country',                                  titleTextStyle:{color:'blue'}}\" , \n                        series = \"[{color:'green',                                    targetAxisIndex: 0},                                   {color: 'orange',targetAxisIndex:1}]\" , \n                        vAxes = \"[{title:'val1'}, {title:'val2'}]\" , \n                        legend = \"bottom\" , \n                        curveType = \"function\" , \n                        width = 500 , \n                        height = 300 ))     \n\n// jsData \nfunction gvisDataLineChartID584f5d6f811a () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n\"US\",\n1,\n23\n],\n[\n\"GB\",\n3,\n12\n],\n[\n\"BR\",\n4,\n32\n] \n];\ndata.addColumn('string','country');\ndata.addColumn('number','val1');\ndata.addColumn('number','val2');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartLineChartID584f5d6f811a() {\nvar data = gvisDataLineChartID584f5d6f811a();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"title\"] = \"Hello World\";\noptions[\"titleTextStyle\"] = {color:'red',\n                                         fontName:'Courier',\n                                         fontSize:16};\noptions[\"backgroundColor\"] = \"#D3D3D3\";\noptions[\"vAxis\"] = {gridlines:{color:'red', count:3}};\noptions[\"hAxis\"] = {title:'Country',\n                                titleTextStyle:{color:'blue'}};\noptions[\"series\"] = [{color:'green',\n                                  targetAxisIndex: 0},\n                                 {color: 'orange',targetAxisIndex:1}];\noptions[\"vAxes\"] = [{title:'val1'}, {title:'val2'}];\noptions[\"legend\"] = \"bottom\";\noptions[\"curveType\"] = \"function\";\noptions[\"width\"] = 500;\noptions[\"height\"] = 300;\n\n\n    var chart = new google.visualization.LineChart(\n    document.getElementById('LineChartID584f5d6f811a')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"corechart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartLineChartID584f5d6f811a);\n})();\nfunction displayChartLineChartID584f5d6f811a() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "Options"
        },
        {
            "location": "/googleVis/#apostrophes",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 df  <-   data.frame ( \"Year\" = c ( 2009 , 2010 ), \n                  \"Lloyd\\\\'s\" = c ( 86.1 ,   93.3 ), \n                  \"Munich Re\\\\'s R/I\" = c ( 95.3 ,   100.5 ),  check.names = FALSE ) \n\nR  <-  gvisColumnChart ( df ,  \n                     options = list ( vAxis = '{baseline:0}' , \n                                  title = \"Combined Ratio %\" , \n                                  legend = \"{position:'bottom'}\" ))  cat ( R $ html $ chart ,  file  =   \"GoogleVis/R.html\" )   # save     \n\n// jsData \nfunction gvisDataColumnChartID336e54fe1b42 () {\nvar data = new google.visualization.DataTable();\nvar datajson =\n[\n [\n2009,\n86.1,\n95.3\n],\n[\n2010,\n93.3,\n100.5\n] \n];\ndata.addColumn('number','Year');\ndata.addColumn('number','Lloyd\\'s');\ndata.addColumn('number','Munich Re\\'s R/I');\ndata.addRows(datajson);\nreturn(data);\n}\n\n// jsDrawChart\nfunction drawChartColumnChartID336e54fe1b42() {\nvar data = gvisDataColumnChartID336e54fe1b42();\nvar options = {};\noptions[\"allowHtml\"] = true;\noptions[\"vAxis\"] = {baseline:0};\noptions[\"title\"] = \"Combined Ratio %\";\noptions[\"legend\"] = {position:'bottom'};\n\n\n    var chart = new google.visualization.ColumnChart(\n    document.getElementById('ColumnChartID336e54fe1b42')\n    );\n    chart.draw(data,options);\n\n\n}\n\n\n// jsDisplayChart\n(function() {\nvar pkgs = window.__gvisPackages = window.__gvisPackages || [];\nvar callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\nvar chartid = \"corechart\";\n\n// Manually see if chartid is in pkgs (not all browsers support Array.indexOf)\nvar i, newPackage = true;\nfor (i = 0; newPackage && i < pkgs.length; i++) {\nif (pkgs[i] === chartid)\nnewPackage = false;\n}\nif (newPackage)\n  pkgs.push(chartid);\n\n// Add the drawChart function to the global list of callbacks\ncallbacks.push(drawChartColumnChartID336e54fe1b42);\n})();\nfunction displayChartColumnChartID336e54fe1b42() {\n  var pkgs = window.__gvisPackages = window.__gvisPackages || [];\n  var callbacks = window.__gvisCallbacks = window.__gvisCallbacks || [];\n  window.clearTimeout(window.__gvisLoad);\n  // The timeout is set to 100 because otherwise the container div we are\n  // targeting might not be part of the document yet\n  window.__gvisLoad = setTimeout(function() {\n  var pkgCount = pkgs.length;\n  google.load(\"visualization\", \"1\", { packages:pkgs, callback: function() {\n  if (pkgCount != pkgs.length) {\n  // Race condition where another setTimeout call snuck in after us; if\n  // that call added a package, we must not shift its callback\n  return;\n}\nwhile (callbacks.length > 0)\ncallbacks.shift()();\n} });\n}, 100);\n}\n\n// jsFooter",
            "title": "Apostrophes"
        },
        {
            "location": "/gganimate/",
            "text": "Foreword\n\n\n\n\nSnippets and results.\n\n\n\n\n\n\nThe \ngganimate\n package\n\u00b6\n\n\nThe \npackage\n does not make interactive charts, but introduces a time dimension in the display of static charts.\n\n\nStatic chart\n\u00b6\n\n\nWe add a time dimension with \nframe = year\n. However, we are not using it and the data points look bunched up!\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nlibrary\n(\ngapminder\n)\n\n\nlibrary\n(\nggplot2\n)\n\ntheme_set\n(\ntheme_bw\n())\n\n\np \n<-\n ggplot\n(\ngapminder\n,\n aes\n(\ngdpPercap\n,\n lifeExp\n,\n size \n=\n pop\n,\n color \n=\n continent\n,\n frame \n=\n year\n))\n \n+\n\n  geom_point\n()\n \n+\n\n  scale_x_log10\n()\n\n\np\n\n\n\n\n\n\n\n\nDynamising a static chart\n\u00b6\n\n\nMoving frames can turn into a motion pictures! Let\u2019s use \nframe = year\n.\n\n\n1\n2\n3\nlibrary(gganimate)\n\ngganimate(p)\n\n\n\n\n\n\nWe can see the results further down.\n\n\nSaving the animation\n\u00b6\n\n\n1\n2\n3\n4\n5\ngganimate(p, \"img/gganimate/output.mp4\")\ngganimate(p, \"img/gganimate/output.swf\")\ngganimate(p, \"img/gganimate/output.html\")\n\ngganimate(p, \"output.gif\")\n\n\n\n\n\n\nRendering the animation (.gif):\n\n\n\n\nAnd again\u2026\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nlibrary\n(\ngoogleVis\n)\n\n\n\nhead\n(\nFruits\n,\n \n3\n)\n\n\n\n##    Fruit Year Location Sales Expenses Profit       Date\n\n\n## 1 Apples 2008     West    98       78     20 2008-12-31\n\n\n## 2 Apples 2009     West   111       79     32 2009-12-31\n\n\n## 3 Apples 2010     West    89       76     13 2010-12-31\n\n\n\nlibrary\n(\nggplot2\n)\n\n\nlibrary\n(\ngganimate\n)\n\n\nFruits_a \n<-\n ggplot\n(\nFruits\n,\n aes\n(\nx\n=\nSales\n,\n y\n=\nExpenses\n,\n size \n=\n Profit\n,\n color \n=\n Fruit\n,\n frame \n=\n Year\n))\n \n+\n\n  geom_point\n()\n \n+\n\n  geom_path\n(\naes\n(\ncumulative \n=\n \nTRUE\n,\n group \n=\n Fruit\n))\n \n+\n\n  facet_wrap\n(\n~\nFruit\n)\n\n\nFruits_a\n\n\n\n\n\n\n\n\n1\n2\n3\ngganimate(Fruits_a, interval = 5)\n\ngganimate(Fruits_a, \"Fruits_a.gif\")\n\n\n\n\n\n\nRendering the animation (.gif):",
            "title": "gganimate"
        },
        {
            "location": "/gganimate/#static-chart",
            "text": "We add a time dimension with  frame = year . However, we are not using it and the data points look bunched up!  1\n2\n3\n4\n5\n6\n7\n8\n9 library ( gapminder )  library ( ggplot2 ) \ntheme_set ( theme_bw ()) \n\np  <-  ggplot ( gapminder ,  aes ( gdpPercap ,  lifeExp ,  size  =  pop ,  color  =  continent ,  frame  =  year ))   + \n  geom_point ()   + \n  scale_x_log10 () \n\np",
            "title": "Static chart"
        },
        {
            "location": "/gganimate/#dynamising-a-static-chart",
            "text": "Moving frames can turn into a motion pictures! Let\u2019s use  frame = year .  1\n2\n3 library(gganimate)\n\ngganimate(p)   We can see the results further down.",
            "title": "Dynamising a static chart"
        },
        {
            "location": "/gganimate/#saving-the-animation",
            "text": "1\n2\n3\n4\n5 gganimate(p, \"img/gganimate/output.mp4\")\ngganimate(p, \"img/gganimate/output.swf\")\ngganimate(p, \"img/gganimate/output.html\")\n\ngganimate(p, \"output.gif\")   Rendering the animation (.gif):",
            "title": "Saving the animation"
        },
        {
            "location": "/gganimate/#and-again",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 library ( googleVis )  head ( Fruits ,   3 )  ##    Fruit Year Location Sales Expenses Profit       Date  ## 1 Apples 2008     West    98       78     20 2008-12-31  ## 2 Apples 2009     West   111       79     32 2009-12-31  ## 3 Apples 2010     West    89       76     13 2010-12-31  library ( ggplot2 )  library ( gganimate ) \n\nFruits_a  <-  ggplot ( Fruits ,  aes ( x = Sales ,  y = Expenses ,  size  =  Profit ,  color  =  Fruit ,  frame  =  Year ))   + \n  geom_point ()   + \n  geom_path ( aes ( cumulative  =   TRUE ,  group  =  Fruit ))   + \n  facet_wrap ( ~ Fruit ) \n\nFruits_a    1\n2\n3 gganimate(Fruits_a, interval = 5)\n\ngganimate(Fruits_a, \"Fruits_a.gif\")   Rendering the animation (.gif):",
            "title": "And again..."
        },
        {
            "location": "/Fast-and-Frugal_Decision_Trees_in_R_with_FFTrees/",
            "text": "Foreword\n\n\n\n\nCode snippets and excerpts from the tutorial. From DataCamp.\n\n\nOpen the \nHTML file\n in a new tab.\n\n\n\n\n\n\nWhy?\n\u00b6\n\n\nQuickly decide: use an algorithm that can quickly understand and apply with minimal effort. For this reason, complex algorithms such as random forests, and even regression are not viable.\n\n\nA fast-and-frugal tree is an extremely simple decision tree that anyone can easily understand, learn, and use to make fast decisions with minimal effort.\n\n\nExplore the Heart Disease Data\n\u00b6\n\n\nThe \nFFTrees\n package contains two datasets: \nheart.train\n to create (aka. train) fast-and-frugal trees, and \nheart.test\n to test their prediction performance.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Load the package\nlibrary(\"FFTrees\")\n\n# Print the first few rows of the training dataframe\nhead(heart.train, 3)\n\n##     age sex cp trestbps chol fbs     restecg thalach exang oldpeak slope\n## 94   44   0 np      108  141   0      normal     175     0     0.6  flat\n## 78   51   0 np      140  308   0 hypertrophy     142     0     1.5    up\n## 167  52   1 np      138  223   0      normal     169     0     0.0    up\n##     ca   thal diagnosis\n## 94   0 normal         0\n## 78   1 normal         0\n## 167  1 normal         0\n\n\n\n\n\n\nThe train dataframe contains data from several patients, each categorized by demographic features such as age and sex, as well as the results of medical tests and measures such as their cholesterol level (\nchol\n) and their type of chest pain (\ncp\n). The key variable you want to predict is \ndiagnosis\n, which is 1 for patients who are truly having heart attacks, and 0 for those who are not. The goal of your fast-and-frugal tree will be to find a few key variables in the data that can quickly and accurately predict diagnosis.\n\n\nCreate an \nFFTrees\n object\n\u00b6\n\n\nAs with any algorithm, we need a function (formula), a train set, and a test set. We add a title and labels to the results.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n# Create an FFTrees object called `heart_FFT`\n\nheart_FFT \n<-\n FFTrees\n(\nformula \n=\n diagnosis \n~\n \n.\n,\n\n                     data \n=\n heart.train\n,\n\n                     data.test \n=\n heart.test\n,\n\n                     main \n=\n \n\"ER Decisions\"\n,\n\n                     decision.labels \n=\n \nc\n(\n\"Stable\"\n,\n \n\"H Attack\"\n))\n\n\n\n# Print the summary statistics\n\nheart_FFT\n\n\n## ER Decisions\n\n\n## FFT #1 predicts diagnosis using 3 cues: {thal,cp,ca}\n\n\n## \n\n\n## [1] If thal = {rd,fd}, predict H Attack.\n\n\n## [2] If cp != {a}, predict Stable.\n\n\n## [3] If ca <= 0, predict Stable, otherwise, predict H Attack.\n\n\n## \n\n\n##                    train   test\n\n\n## cases       :n    150.00 153.00\n\n\n## speed       :mcu    1.74   1.73\n\n\n## frugality   :pci    0.88   0.88\n\n\n## accuracy    :acc    0.80   0.82\n\n\n## weighted    :wacc   0.80   0.82\n\n\n## sensitivity :sens   0.82   0.88\n\n\n## specificity :spec   0.79   0.76\n\n\n## \n\n\n## pars: algorithm = 'ifan', goal = 'wacc', goal.chase = 'bacc', sens.w = 0.5, max.levels = 4\n\n\n\n\n\n\n\nthal\n is either \nrd\n or \nfd\n, the dependent variable \nH Attack\n. \ncp\n is not equal to \na\n and the model has converged towards an optimum (\nStable\n). The tree used an average of 1.74 pieces of information to classify cases with the train set. The tree has then an accuracy of 82% with the test set.\n\n\nPlotting Fast-And-Frugal Trees\n\u00b6\n\n\nThe visualization greatly improves the understanding.\n\n\n1\n2\n# Visualise the tree applied to the test data heart.test\nplot(heart_FFT, data = \"test\")\n\n\n\n\n\n\n\n\n\n\nOn the top row, you can see that there were 153 patients (cases) were in the training data, where 73 patients were truly having heart attacks (48%), and 80 patients were not (52%).\n\n\nIn the middle row, you see exactly how the tree makes decisions for each of the patients using easy\u2013to\u2013understand icon arrays. For example, you see that 72 patients suspected of having heart attacks were (virtually) sent to the CCU after the first question, where 18 were not having heart attacks (false\u2013alarms), and 54 were having heart attacks (hits).\n\n\nIn the bottom row of the plot, you can see aggregate summary statistics for the tree. On the bottom row, you have a 2 x 2 confusion matrix, which shows you a summary of how well the tree was able to classify patients, levels indicating overall summary statistics, and an ROC curve which compares the accuracy of the tree to other algorithms such as logistic regression (LR) and random forests (RF). Here, where the fast-and-frugal tree is represented by the green circle \u201c1\u201d, you can see that the fast-and-frugal tree had a higher sensitivity than logistic regression and random forests, but at a cost of a lower specificity.\n\n\n\n\nCreating and Testing a Custom Tree\n\u00b6\n\n\nWe can easily describe fast-and-frugal trees \u2018in words\u2019: \u201cA better rule would be to use the cues cholesterol, age, and slope.\u201d\n\n\n\n\nIf cholesterol > 300, decide Heart Attack.\n\n\nIf age < 50, decide Stable.\n\n\nIf slope is either up or flat, predict Attack, otherwise,\n\n    predict Stable.\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n# Create Heidi's custom FFT\n\ncustom_FFT \n<-\n FFTrees\n(\nformula \n=\n diagnosis \n~\n \n.\n,\n               \n                      data \n=\n heart.train\n,\n                    \n                      data.test \n=\n heart.test\n,\n                \n                      main \n=\n \n\"Heidi's Tree\"\n,\n                  \n                      decision.labels \n=\n \nc\n(\n\"Stable\"\n,\n \n\"Attack\"\n),\n\n                      my.tree \n=\n \n\"If chol > 300, predict Attack.\n\n\n                                 If age < 50, predict Stable.\n\n\n                                 If slope = {up, flat} predict Attack. Otherwise, predict Stable.\"\n)\n\n\n\n# Print the summary statistics\n\ncustom_FFT\n\n\n## Heidi's Tree\n\n\n## FFT #1 predicts diagnosis using 3 cues: {chol,age,slope}\n\n\n## \n\n\n## [1] If chol > 300, predict Attack.\n\n\n## [2] If age < 50, predict Stable.\n\n\n## [3] If slope != {up,flat}, predict Stable, otherwise, predict Attack.\n\n\n## \n\n\n##                    train   test\n\n\n## cases       :n    150.00 153.00\n\n\n## speed       :mcu    2.41   2.49\n\n\n## frugality   :pci    0.83   0.82\n\n\n## accuracy    :acc    0.59   0.54\n\n\n## weighted    :wacc   0.61   0.55\n\n\n## sensitivity :sens   0.80   0.77\n\n\n## specificity :spec   0.42   0.34\n\n\n## \n\n\n## pars: algorithm = 'ifan', goal = 'wacc', goal.chase = 'bacc', sens.w = 0.5, max.levels = 4\n\n\n\n# Plot Heidi's tree and accuracy statistics\n\nplot\n(\ncustom_FFT\n,\n data \n=\n \n\"test\"\n)\n\n\n\n\n\n\n\n\n\nThe new tree is much, much worse than the tree the internal algorithm came up with. While the tree generated by FFTrees had an overall accuracy of 82%, the new tree is only 54% accurate!\n\n\nMoreover, you can see very few patients (only 21) are classified as having a heart attack after the first node based on their cholesterol level, and of those, only 12 / 21 (57%) were really having heart attacks. In contrast, for the tree created by \nFFTrees\n, a full 72 patients are classified after the first node based on their value of \nthal\n, and of these, 75% were truly having heart attacks.\n\n\nMore\n\u00b6\n\n\nWe can use trees to predict classes (and their probabilities) for new datasets, and create trees that minimise different classification error costs (for example, when the cost of a miss is much higher than the cost of a false alarm).\n\n\nCheck out the package vignette by running \nFFTrees.guide()\n.\n\n\n1\nFFTrees.guide()\n\n\n\n\n\n\nSummary\n\u00b6\n\n\nFast-and-frugal decision trees are great options when you need a simple, transparent decision algorithm that can easily be communicated and applied, either by a person or a computer.\n\n\nAlthough fast-and-frugal trees are great for medical decisions (Green & Mehr, 1997), they can be created from any dataset with a binary criterion, from predicting whether or not a bank will fail (Neth et al., 2014), to predicting a judge\u2019s bailing decisions (Dhami & Ayton, 2001).\n\n\n\n\nDhami, Mandeep K, and Peter Ayton. 2001. \u201cBailing and Jailing the Fast and Frugal Way.\u201d Journal of Behavioral Decision Making 14 (2). Wiley Online Library: 141 - 168.\n\n\nGalesic, Mirta, Rocio Garcia-Retamero, and Gerd Gigerenzer. 2009. \u201cUsing Icon Arrays to Communicate Medical Risks: Overcoming Low Numeracy.\u201d Health Psychology 28 (2). American Psychological Association: 210.\n\n\nGreen, Lee, and David R Mehr. 1997. \u201cWhat Alters Physicians\u2019 Decisions to Admit to the Coronary Care Unit\u201d. Journal of Family Practice 45 (3). [New York, Appleton-Century-Crofts]: 219 - 226.\n\n\nMartignon, Laura, Oliver Vitouch, Masanori Takezawa, and Malcolm R Forster. 2003. \u201cNaive and yet Enlightened: From Natural Frequencies to Fast and Frugal Decision Trees.\u201d Thinking: Pychological Perspectives on Reasoning, Judgment and Decision Making. John Wiley & Sons, Ltd, 189 - 211.\n\n\nNeth, Hansj\u00f6rg, Bj\u00f6rn Meder, Amit Kothiyal, and Gerd Gigerenzer. 2014. \u201cHomo Heuristicus in the Financial World: From Risk Management to Managing Uncertainty.\u201d Journal of Risk Management in Financial Institutions 7 (2). Henry Stewart Publications: 134 - 144.\n\n\nPhillips, Nathaniel D, Hansj\u00f6rg Neth, Jan K Woike, and Wolfgang Gaissmaier. 2017. \u201cFFTrees: A Toolbox to Create, Visualize, and Evaluate Fast-and-Frugal Decision Trees.\u201d Judgment and Decision Making 12 (4). Society for Judgment & Decision Making: 344.",
            "title": "Fast-and-Frugal Decision Trees"
        },
        {
            "location": "/Fast-and-Frugal_Decision_Trees_in_R_with_FFTrees/#explore-the-heart-disease-data",
            "text": "The  FFTrees  package contains two datasets:  heart.train  to create (aka. train) fast-and-frugal trees, and  heart.test  to test their prediction performance.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Load the package\nlibrary(\"FFTrees\")\n\n# Print the first few rows of the training dataframe\nhead(heart.train, 3)\n\n##     age sex cp trestbps chol fbs     restecg thalach exang oldpeak slope\n## 94   44   0 np      108  141   0      normal     175     0     0.6  flat\n## 78   51   0 np      140  308   0 hypertrophy     142     0     1.5    up\n## 167  52   1 np      138  223   0      normal     169     0     0.0    up\n##     ca   thal diagnosis\n## 94   0 normal         0\n## 78   1 normal         0\n## 167  1 normal         0   The train dataframe contains data from several patients, each categorized by demographic features such as age and sex, as well as the results of medical tests and measures such as their cholesterol level ( chol ) and their type of chest pain ( cp ). The key variable you want to predict is  diagnosis , which is 1 for patients who are truly having heart attacks, and 0 for those who are not. The goal of your fast-and-frugal tree will be to find a few key variables in the data that can quickly and accurately predict diagnosis.",
            "title": "Explore the Heart Disease Data"
        },
        {
            "location": "/Fast-and-Frugal_Decision_Trees_in_R_with_FFTrees/#create-an-fftrees-object",
            "text": "As with any algorithm, we need a function (formula), a train set, and a test set. We add a title and labels to the results.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27 # Create an FFTrees object called `heart_FFT` \nheart_FFT  <-  FFTrees ( formula  =  diagnosis  ~   . , \n                     data  =  heart.train , \n                     data.test  =  heart.test , \n                     main  =   \"ER Decisions\" , \n                     decision.labels  =   c ( \"Stable\" ,   \"H Attack\" ))  # Print the summary statistics \nheart_FFT ## ER Decisions  ## FFT #1 predicts diagnosis using 3 cues: {thal,cp,ca}  ##   ## [1] If thal = {rd,fd}, predict H Attack.  ## [2] If cp != {a}, predict Stable.  ## [3] If ca <= 0, predict Stable, otherwise, predict H Attack.  ##   ##                    train   test  ## cases       :n    150.00 153.00  ## speed       :mcu    1.74   1.73  ## frugality   :pci    0.88   0.88  ## accuracy    :acc    0.80   0.82  ## weighted    :wacc   0.80   0.82  ## sensitivity :sens   0.82   0.88  ## specificity :spec   0.79   0.76  ##   ## pars: algorithm = 'ifan', goal = 'wacc', goal.chase = 'bacc', sens.w = 0.5, max.levels = 4    thal  is either  rd  or  fd , the dependent variable  H Attack .  cp  is not equal to  a  and the model has converged towards an optimum ( Stable ). The tree used an average of 1.74 pieces of information to classify cases with the train set. The tree has then an accuracy of 82% with the test set.",
            "title": "Create an FFTrees object"
        },
        {
            "location": "/Fast-and-Frugal_Decision_Trees_in_R_with_FFTrees/#plotting-fast-and-frugal-trees",
            "text": "The visualization greatly improves the understanding.  1\n2 # Visualise the tree applied to the test data heart.test\nplot(heart_FFT, data = \"test\")     On the top row, you can see that there were 153 patients (cases) were in the training data, where 73 patients were truly having heart attacks (48%), and 80 patients were not (52%).  In the middle row, you see exactly how the tree makes decisions for each of the patients using easy\u2013to\u2013understand icon arrays. For example, you see that 72 patients suspected of having heart attacks were (virtually) sent to the CCU after the first question, where 18 were not having heart attacks (false\u2013alarms), and 54 were having heart attacks (hits).  In the bottom row of the plot, you can see aggregate summary statistics for the tree. On the bottom row, you have a 2 x 2 confusion matrix, which shows you a summary of how well the tree was able to classify patients, levels indicating overall summary statistics, and an ROC curve which compares the accuracy of the tree to other algorithms such as logistic regression (LR) and random forests (RF). Here, where the fast-and-frugal tree is represented by the green circle \u201c1\u201d, you can see that the fast-and-frugal tree had a higher sensitivity than logistic regression and random forests, but at a cost of a lower specificity.",
            "title": "Plotting Fast-And-Frugal Trees"
        },
        {
            "location": "/Fast-and-Frugal_Decision_Trees_in_R_with_FFTrees/#creating-and-testing-a-custom-tree",
            "text": "We can easily describe fast-and-frugal trees \u2018in words\u2019: \u201cA better rule would be to use the cues cholesterol, age, and slope.\u201d   If cholesterol > 300, decide Heart Attack.  If age < 50, decide Stable.  If slope is either up or flat, predict Attack, otherwise, \n    predict Stable.     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33 # Create Heidi's custom FFT \ncustom_FFT  <-  FFTrees ( formula  =  diagnosis  ~   . ,                \n                      data  =  heart.train ,                     \n                      data.test  =  heart.test ,                 \n                      main  =   \"Heidi's Tree\" ,                   \n                      decision.labels  =   c ( \"Stable\" ,   \"Attack\" ), \n                      my.tree  =   \"If chol > 300, predict Attack.                                   If age < 50, predict Stable.                                   If slope = {up, flat} predict Attack. Otherwise, predict Stable.\" )  # Print the summary statistics \ncustom_FFT ## Heidi's Tree  ## FFT #1 predicts diagnosis using 3 cues: {chol,age,slope}  ##   ## [1] If chol > 300, predict Attack.  ## [2] If age < 50, predict Stable.  ## [3] If slope != {up,flat}, predict Stable, otherwise, predict Attack.  ##   ##                    train   test  ## cases       :n    150.00 153.00  ## speed       :mcu    2.41   2.49  ## frugality   :pci    0.83   0.82  ## accuracy    :acc    0.59   0.54  ## weighted    :wacc   0.61   0.55  ## sensitivity :sens   0.80   0.77  ## specificity :spec   0.42   0.34  ##   ## pars: algorithm = 'ifan', goal = 'wacc', goal.chase = 'bacc', sens.w = 0.5, max.levels = 4  # Plot Heidi's tree and accuracy statistics \nplot ( custom_FFT ,  data  =   \"test\" )     The new tree is much, much worse than the tree the internal algorithm came up with. While the tree generated by FFTrees had an overall accuracy of 82%, the new tree is only 54% accurate!  Moreover, you can see very few patients (only 21) are classified as having a heart attack after the first node based on their cholesterol level, and of those, only 12 / 21 (57%) were really having heart attacks. In contrast, for the tree created by  FFTrees , a full 72 patients are classified after the first node based on their value of  thal , and of these, 75% were truly having heart attacks.",
            "title": "Creating and Testing a Custom Tree"
        },
        {
            "location": "/Fast-and-Frugal_Decision_Trees_in_R_with_FFTrees/#more",
            "text": "We can use trees to predict classes (and their probabilities) for new datasets, and create trees that minimise different classification error costs (for example, when the cost of a miss is much higher than the cost of a false alarm).  Check out the package vignette by running  FFTrees.guide() .  1 FFTrees.guide()",
            "title": "More"
        },
        {
            "location": "/Fast-and-Frugal_Decision_Trees_in_R_with_FFTrees/#summary",
            "text": "Fast-and-frugal decision trees are great options when you need a simple, transparent decision algorithm that can easily be communicated and applied, either by a person or a computer.  Although fast-and-frugal trees are great for medical decisions (Green & Mehr, 1997), they can be created from any dataset with a binary criterion, from predicting whether or not a bank will fail (Neth et al., 2014), to predicting a judge\u2019s bailing decisions (Dhami & Ayton, 2001).   Dhami, Mandeep K, and Peter Ayton. 2001. \u201cBailing and Jailing the Fast and Frugal Way.\u201d Journal of Behavioral Decision Making 14 (2). Wiley Online Library: 141 - 168.  Galesic, Mirta, Rocio Garcia-Retamero, and Gerd Gigerenzer. 2009. \u201cUsing Icon Arrays to Communicate Medical Risks: Overcoming Low Numeracy.\u201d Health Psychology 28 (2). American Psychological Association: 210.  Green, Lee, and David R Mehr. 1997. \u201cWhat Alters Physicians\u2019 Decisions to Admit to the Coronary Care Unit\u201d. Journal of Family Practice 45 (3). [New York, Appleton-Century-Crofts]: 219 - 226.  Martignon, Laura, Oliver Vitouch, Masanori Takezawa, and Malcolm R Forster. 2003. \u201cNaive and yet Enlightened: From Natural Frequencies to Fast and Frugal Decision Trees.\u201d Thinking: Pychological Perspectives on Reasoning, Judgment and Decision Making. John Wiley & Sons, Ltd, 189 - 211.  Neth, Hansj\u00f6rg, Bj\u00f6rn Meder, Amit Kothiyal, and Gerd Gigerenzer. 2014. \u201cHomo Heuristicus in the Financial World: From Risk Management to Managing Uncertainty.\u201d Journal of Risk Management in Financial Institutions 7 (2). Henry Stewart Publications: 134 - 144.  Phillips, Nathaniel D, Hansj\u00f6rg Neth, Jan K Woike, and Wolfgang Gaissmaier. 2017. \u201cFFTrees: A Toolbox to Create, Visualize, and Evaluate Fast-and-Frugal Decision Trees.\u201d Judgment and Decision Making 12 (4). Society for Judgment & Decision Making: 344.",
            "title": "Summary"
        },
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets only.\n\n\n\n\n\n\nDocumentation\n\u00b6\n\n\n\n\nMicrosoft R Server: previously called Revolution R Enterprise for Hadoop, Linux and Teradata and included new Microsoft enterprise support and purchasing options. Microsoft R Server was further made available to students through the Microsoft DreamSpark programme.\n\n\nMicrosoft R Server Developer Edition: a gratis version for developers that with a feature set akin to the commercial edition.\n\n\nMicrosoft Data Science Virtual Machine: an analytics tool developed by the Revolution Analytics division premiered in January 2015.\n\n\nMicrosoft R Open: a rebranded version of Revolution R Open.\n\n\n\n\nIntroduction\n\u00b6\n\n\nImporting data with \nrxImport\n function\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# Declare the file paths for the csv and xdf files\n\n\n# find the path or directory where the file is, load the path variable\n\nmyAirlineCsv \n<-\n \nfile.path\n(\nrxGetOption\n(\n'sampleDataDir'\n),\n \n'2007_subset.csv'\n)\n\n\n\n# fin the data in this directory and load the data variable\n\nmyAirlineXdf \n<-\n \n'2007_subset.xdf'\n\n\n\n# Use rxImport to import the data into xdf format\n\n\n# rxImport(inData = myAirlineCsv, outFile = myAirlineXdf, overwrite = TRUE)\n\n\n# or\n\n\n# function within a function for more stats\n\n\nsystem.time\n(\nrxImport\n(\ninData \n=\n myAirlineCsv\n,\n \n                     outFile \n=\n myAirlineXdf\n,\n \n                     overwrite \n=\n \nTRUE\n))\n\n\nlist.files\n()\n\n\n\n\n\n\n\nFunctions for summarizing data\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n# Get basic information about your data\n\nrxGetInfo\n(\ndata \n=\n myAirlineXdf\n,\n \n          getVarInfo \n=\n \nTRUE\n,\n \n          numRows \n=\n \n10\n)\n\n\n\n# Summarize the variables corresponding to actual elapsed time, time in the air, departure delay, flight Distance\n\nrxSummary\n(\nformula \n=\n \n~\n ActualElapsedTime \n+\n AirTime \n+\n DepDelay \n+\n Distance\n,\n \n          data \n=\n myAirlineXdf\n)\n\n\n\n# Histogram of departure delays\n\nrxHistogram\n(\nformula \n=\n \n~\nDepDelay\n,\n \n            data \n=\n myAirlineXdf\n)\n\n\n\n# Use parameters similar to a regular histogram to zero in on the interesting area\n\nrxHistogram\n(\nformula \n=\n \n~\nDepDelay\n,\n \n            data \n=\n myAirlineXdf\n,\n \n            xAxisMinMax \n=\n \nc\n(\n-100\n,\n \n400\n),\n \n            numBreaks \n=\n \n500\n,\n\n            xNumTicks \n=\n \n10\n)\n\n\n\n\n\n\n\nCreating new variables using \nrxDataStep\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n# Calculate an additional variable: airspeed (distance traveled / time in the air)\n\nrxDataStep\n(\ninData \n=\n myAirlineXdf\n,\n \n           outFile \n=\n myAirlineXdf\n,\n \n           varsToKeep \n=\n \nc\n(\n'Distance'\n,\n \n'AirTime'\n),\n\n           transforms \n=\n \nlist\n(\nairSpeed \n=\n Distance \n/\n Airtime\n),\n\n           append \n=\n \n'cols'\n,\n\n           overwrite \n=\n \nTRUE\n)\n\n\n\n# Get Variable Information for airspeed\n\nrxGetInfo\n(\ndata \n=\n myAirlineXdf\n,\n \n          getVarInfo \n=\n \nTRUE\n,\n\n          varsToKeep \n=\n \n'airSpeed'\n)\n\n\n\n# Summary for the airspeed variable\n\nrxSummary\n(\n~\nairSpeed\n,\n \n          data \n=\n myAirlineXdf\n)\n\n\n\n# Construct a histogtam for airspeed\n\n\n# We can use the xAxisMinMax argument to limit the X-axis\n\nrxHistogram\n(\n~\nairSpeed\n,\n \n            data \n=\n myAirlineXdf\n)\n\n\nrxHistogram\n(\n~\nairSpeed\n,\n \n            data \n=\n myAirlineXdf\n,\n\n            xNumTicks \n=\n \n10\n,\n\n            numBreaks \n=\n \n1500\n,\n\n            xAxisMinMax \n=\n \nc\n(\n0\n,\n12\n))\n\n\n\n\n\n\n\nTransforming variables using \nrxDataStep\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Conversion to miles per hour\n\nrxDataStep\n(\ninData \n=\n myAirlineXdf\n,\n \n         outFile \n=\n myAirlineXdf\n,\n \n         varsToKeep \n=\n \nc\n(\n'airSpeed'\n),\n\n           transforms \n=\n \nlist\n(\nairSpeed \n=\n airSpeed \n*\n \n60\n),\n\n         overwrite \n=\n \nTRUE\n)\n\n\n\n# Histogram for airspeed after conversion\n\nrxHistogram\n(\n~\nairSpeed\n,\n \n            data \n=\n myAirlineXdf\n)\n\n\n\n\n\n\n\nCorrelations\n\n\n1\n2\n3\n4\n# Correlation for departure delay, arrival delay, and air speed\n\nrxCor\n(\nformula \n=\n \n~\n DepDelay \n+\n ArrDelay \n+\n airSpeed\n,\n\n      data \n=\n myAirlineXdf\n,\n\n      rowSelection \n=\n \n(\nairSpeed \n>\n \n50\n)\n \n&\n \n(\nairSpeed \n<\n \n800\n))\n\n\n\n\n\n\n\nLinear regression\n\n\n1\n2\n3\n4\n5\n6\n# Regression for airSpeed based on departure delay\n\nmyLMobj \n<-\n rxLinMod\n(\nformula \n=\n airSpeed \n~\n DepDelay\n,\n \n         data \n=\n myAirlineXdf\n,\n\n         rowSelection \n=\n \n(\nairSpeed \n>\n \n50\n)\n \n&\n \n(\nairSpeed \n<\n \n800\n))\n\n\n\nsummary\n(\nmyLMobj\n)\n\n\n\n\n\n\n\nData Exploration\n\u00b6\n\n\nRevoScaleR\n options\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Extract the names of the possible options\n\n\nnames\n(\nrxOptions\n())\n\n\n\n# Extract the sample data directory\n\nrxGetOption\n(\n'sampleDataDir'\n)\n\n\n\n# View the current value of the reportProgress option\n\nrxGetOption\n(\n'reportProgress'\n)\n\n\n\n# Set the value of the reportProgress option to 0\n\nrxOptions\n(\nreportProgress \n=\n \n0\n)\n\n\n\n\n\n\n\nImport and explore Dow Jones data\n\n\n1\n2\n3\n4\n5\n# Set up the variable that has the address of the relevant data file\n\ndjiXdf \n<-\n \nfile.path\n(\nrxGetOption\n(\n'sampleDataDir'\n),\n \n'DJIAdaily.xdf'\n)\n\n\n\n# Get information about that dataset\n\nrxGetInfo\n(\ndjiXdf\n,\n getVarInfo \n=\n \nTRUE\n)\n\n\n\n\n\n\n\nExtracting meta data about a variable using \nrxGetVarInfo\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# Get variable information for the dataset\n\ndjiVarInfo \n<-\n rxGetVarInfo\n(\ndjiXdf\n)\n\n\nnames\n(\ndjiVarInfo\n)\n\n\n\n# Extract information about the closing cost variable\n\n\n(\ncloseVarInfo \n<-\n djiVarInfo\n[[\n'Close'\n]])\n\n\n\n# Get the class of the closeVarInfo object\n\n\nclass\n(\ncloseVarInfo\n)\n\n\n\n# Examine the structure of the closeVarInfo object\n\nstr\n(\ncloseVarInfo\n)\n\n\n\n# Extract the global maximum of the closing cost variable\n\ncloseMax \n<-\n closeVarInfo\n[[\n'high'\n]]\n\n\n\n\n\n\n\nSummarizing variables with \nrxSummary\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# Basic summary statistics\n\nrxSummary\n(\n~\n DayOfWeek \n+\n Close \n+\n Volume\n,\n \n          data \n=\n djiXdf\n)\n\n\n\n# Frequency weighted\n\nrxSummary\n(\n~\n DayOfWeek \n+\n Close\n,\n \n          data \n=\n djiXdf\n,\n \n          fweights \n=\n \n'Volume'\n)\n\n\n\n# Basic frequency count\n\nrxCrossTabs\n(\n~\n DayOfWeek\n,\n \n            data \n=\n djiXdf\n)\n\n\n\n\n\n\n\nExploring a distribution with \nrxHistogram\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# Numeric Variables\n\nrxHistogram\n(\n~\n Close\n,\n \n            data \n=\n djiXdf\n)\n\n\n\n# Categorical Variable\n\nrxHistogram\n(\n~\n DayOfWeek\n,\n \n            data \n=\n djiXdf\n)\n\n\n\n# Different panels for different days of the week\n\nrxHistogram\n(\n~\n Close \n|\n DayOfWeek\n,\n \n            data \n=\n djiXdf\n)\n\n\n\n# Numeric Variables with a frequency weighting\n\nrxHistogram\n(\n~\n Close\n,\n data \n=\n djiXdf\n,\n \n            fweights \n=\n \n'Volume'\n)\n\n\n\n\n\n\n\nPlotting bivariate relationships with \nrxLinePlot\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n# Simple bivariate line plot\n\nrxLinePlot\n(\nClose \n~\n DaysSince1928\n,\n \n           data \n=\n djiXdf\n)\n\n\n\n# Using different panels for different days of the week\n\nrxLinePlot\n(\nClose \n~\n DaysSince1928 \n|\n DayOfWeek\n,\n \n           data \n=\n djiXdf\n)\n\n\n\n# Using different groups\n\nrxLinePlot\n(\nClose \n~\n DaysSince1928\n,\n \n           groups \n=\n DayOfWeek\n,\n \n           data \n=\n djiXdf\n)\n\n\n\n# Simple bivariate line plot, after taking the log() of the ordinate (y) variable\n\nrxLinePlot\n(\nlog\n(\nClose\n)\n \n~\n DaysSince1928\n,\n \n           data \n=\n djiXdf\n)\n\n\n\n\n\n\n\nSummarzing variables with \nrxCrossTabs\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# Compute the the summed volume for each day of the week\n\nrxCrossTabs\n(\nformula \n=\n Volume \n~\n DayOfWeek\n,\n \n            data \n=\n djiXdf\n)\n\n\n\n# Compute the the summed volume for each day of the week for each month\n\nrxCrossTabs\n(\nformula \n=\n Volume \n~\n \nF\n(\nMonth\n)\n:\nDayOfWeek\n,\n \n            data \n=\n djiXdf\n)\n\n\n\n# Compute the the average volume for each day of the week for each month\n\nrxCrossTabs\n(\nformula \n=\n Volume \n~\n \nF\n(\nMonth\n)\n:\nDayOfWeek\n,\n \n            data \n=\n djiXdf\n,\n \n            means \n=\n \nTRUE\n)\n\n\n\n# Compute the the average closing price for each day of the week for each month, using volume as frequency weights\n\nrxCrossTabs\n(\nformula \n=\n Close \n~\n \nF\n(\nMonth\n)\n:\nDayOfWeek\n,\n \n            data \n=\n djiXdf\n,\n \n            means \n=\n \nTRUE\n,\n \n            fweights \n=\n \n'Volume'\n)\n\n\n\n\n\n\n\nSummarzing variables with \nrxCube\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n# Compute the the summed volume for each day of the week\n\nrxCrossTabs\n(\nVolume \n~\n DayOfWeek\n,\n \n            data \n=\n djiXdf\n)\n\n\nrxCube\n(\nVolume \n~\n DayOfWeek\n,\n \n       data \n=\n djiXdf\n,\n \n       means \n=\n \nFALSE\n)\n\n\n\n# Compute the the summed volume for each day of the week for each month\n\nrxCrossTabs\n(\nVolume \n~\n \nF\n(\nMonth\n)\n:\nDayOfWeek\n,\n \n            data \n=\n djiXdf\n)\n\n\nrxCube\n(\nVolume \n~\n \nF\n(\nMonth\n)\n:\nDayOfWeek\n,\n \n       data \n=\n djiXdf\n,\n \n       means \n=\n \nFALSE\n)\n\n\n\n# Compute the the average volume for each day of the week for each month\n\nrxCube\n(\nVolume \n~\n \nF\n(\nMonth\n)\n:\nDayOfWeek\n,\n \n       data \n=\n djiXdf\n)\n\n\n\n# Compute the the average closing price for each day of the week for each month, using volume as frequency weights\n\nrxCube\n(\nClose \n~\n DayOfWeek\n,\n \n       data \n=\n djiXdf\n,\n \n       fweights \n=\n \n'Volume'\n)\n\n\n\n\n\n\n\nData Manipulation\n\u00b6\n\n\nUsing \nrxDataStep\n to transform data\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n# Get information on mortData\n\nrxGetInfo\n(\nmortData\n)\n\n\n\n## Set up my personal copy of the data\n\nmyMortData \n<-\n \n'myMD.xdf'\n\n\n\n# Create the transform\n\nrxDataStep\n(\ninData \n=\n mortData\n,\n \n           outFile \n=\n myMortData\n,\n \n           transforms \n=\n \nlist\n(\nhighDebtRow \n=\n ccDebt \n>\n \n8000\n),\n \n           overwrite \n=\n \nTRUE\n)\n\n\n\n#rxDataStep(inData = mortData, outFile = myMortData, transforms = list(highDebtRow = ccDebt > 8000))\n\n\n# Get the variable information\n\nrxGetVarInfo\n(\nmyMortData\n)\n\n\n\n# Get the proportion of values that are 1\n\nrxSummary\n(\n \n~\n highDebtRow\n,\n \n           data \n=\n myMortData\n)\n\n\n\n# Compute multiple transforms!\n\nrxDataStep\n(\ninData \n=\n myMortData\n,\n outFile \n=\n myMortData\n,\n\n           transforms \n=\n \nlist\n(\n\n             newHouse \n=\n houseAge \n<\n \n10\n,\n\n             ccsXhd \n=\n creditScore \n*\n highDebtRow\n),\n\n           append \n=\n \n'cols'\n,\n\n           overwrite \n=\n \nTRUE\n)\n\n\n\n\n\n\n\nMore complex transforms using \ntransformFuncs\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n# Compute the summary statistics\n\n\n(\ncsSummary \n<-\n rxSummary\n(\n~\n creditScore\n,\n data \n=\n mortData\n))\n\n\n\n# Extract the mean and std. deviation\n\nmeanCS \n<-\n csSummary\n$\nsDataFrame\n$\nMean\n[\n1\n]\n\nsdCS \n<-\n csSummary\n$\nsDataFrame\n$\nStdDev\n[\n1\n]\n\n\n\n# Create a function to compute the scaled variable\n\nscaleCS \n<-\n \nfunction\n(\nmylist\n){\n\n  mylist\n[[\n'scaledCreditScore'\n]]\n \n<-\n \n(\nmylist\n[[\n'creditScore'\n]]\n \n-\n myCenter\n)\n \n/\n myScale\n  \nreturn\n(\nmylist\n)\n\n\n}\n\n\n\n# Run it with rxDataStep (A above in B below)\n\nmyMortData \n<-\n \n'myMD.xdf'\n\nrxDataStep\n(\ninData \n=\n mortData\n,\n outFile \n=\n myMortData\n,\n\n           transformFunc \n=\n scaleCS\n,\n\n           transformObjects \n=\n \nlist\n(\nmyCenter \n=\n meanCS\n,\n myScale \n=\n sdCS\n))\n\n\n\n# Check the new variable\n\nrxGetVarInfo\n(\nmyMortData\n)\n\nrxSummary\n(\n~\n scaledCreditScore\n,\n \n          data \n=\n myMortData\n)\n\n\n\n\n\n\n\nData Analysis\n\u00b6\n\n\nPreparing data for analysis: import\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n# Declare the file paths for the csv and xdf files\n\nmyAirlineCsv \n<-\n \nfile.path\n(\nrxGetOption\n(\n'sampleDataDir'\n),\n \n'AirlineDemoSmall.csv'\n)\n\nmyAirlineXdf \n<-\n \n'ADS.xdf'\n\n\n\n# Use rxImport to import the data into xdf format\n\nrxImport\n(\ninData \n=\n myAirlineCsv\n,\n \n         outFile \n=\n myAirlineXdf\n,\n \n         overwrite \n=\n \nTRUE\n,\n\n         colInfo \n=\n \nlist\n(\n \n           DayOfWeek \n=\n \nlist\n(\n\n            type \n=\n \n'factor'\n,\n \n            levels \n=\n \nc\n(\n'Monday'\n,\n \n'Tuesday'\n,\n \n'Wednesday'\n,\n \n                       \n'Thursday'\n,\n \n'Friday'\n,\n \n'Saturday'\n,\n \n'Sunday'\n))))\n\n\n\n\n\n\n\nPreparing data Ffor analysis: exploration\n\n\n1\n2\n3\n4\n5\n6\n7\n# Summarize arrival delay for each day of the week\n\nrxSummary\n(\nformula \n=\n ArrDelay \n~\n DayOfWeek\n,\n \n          data \n=\n myAirlineXdf\n)\n\n\n\n# Vizualize the arrival delay histogram\n\nrxHistogram\n(\nformula \n=\n \n~\nArrDelay\n,\n \n            data \n=\n myAirlineXdf\n)\n\n\n\n\n\n\n\nConstruct a linear model\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n# predict arrival delay by day of the week\n\nmyLM1 \n<-\n rxLinMod\n(\nArrDelay \n~\n DayOfWeek\n,\n \n                  data \n=\n myAirlineXdf\n)\n\n\n\n# summarize the model\n\nsmmary\n(\nmyLM1\n)\n\n\n\n# Use the transforms argument to create a factor variable associated with departure time 'on the fly,'\n\n\n# predict Arrival Delay by the interaction between Day of the week and that new factor variable\n\nmyLM2 \n<-\n rxLinMod\n(\nArrDelay \n~\n DayOfWeek\n,\n \n                  data \n=\n myAirlineXdf\n,\n\n                  transforms \n=\n \nlist\n(\n\n                    catDepTime \n=\n \ncut\n(\nCRSDepTime\n,\n breaks \n=\n \nseq\n(\nfrom \n=\n \n5\n,\n to \n=\n \n23\n,\n by \n=\n \n2\n))),\n\n                    cube \n=\n \nTRUE\n)\n\n\n\n# summarize the model\n\n\nsummary\n(\nmyLM2\n)\n\n\n\n\n\n\n\nGenerating predictions and residuals\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n# Summarize model first\n\n\nsummary\n(\nmyLM2\n)\n\n\n\n# Path to new dataset storing predictions\n\nmyNewADS \n<-\n \n'myNEWADS.xdf'\n\n\n\n# Generate predictions\n\nrxPredict\n(\nmodelObject \n=\n myLM2\n,\n \n          data \n=\n myAirlineXdf\n,\n \n          outData \n=\n myNewADS\n,\n \n          writeModelVars \n=\n \nTRUE\n)\n\n\n\n# Get information on the new dataset\n\nrxGetInfo\n(\nmyNewADS\n,\n getVarInfo \n=\n \nTRUE\n)\n\n\n\n# Generate residuals.\n\nrxPredict\n(\nmodelObject \n=\n myLM2\n,\n \n          data \n=\n myAirlineXdf\n,\n \n          outData \n=\n myNewADS\n,\n \n          writeModelVars \n=\n \nTRUE\n,\n \n          computeResiduals \n=\n \nTRUE\n,\n \n          overwrite \n=\n \nTRUE\n)\n\n\n\n# Get information on the new dataset\n\nrxGetInfo\n(\nmyNewADS\n,\n getVarInfo \n=\n \nTRUE\n)\n\n\n\n\n\n\n\nLogistic regression\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# look at the meta data\n\n\nls\n()\n\nrxGetInfo\n(\ndata \n=\n mortData\n,\n getVarInfo \n=\n \nTRUE\n)\n\n\n\n# Construct the logit model\n\nlogitModel \n<-\n rxLogit\n(\nformula \n=\n default \n~\n houseAge \n+\n \nF\n(\nyear\n)\n \n+\n ccDebt \n+\n creditScore \n+\n yearsEmploy\n,\n \n                      data \n=\n mortData\n)\n\n\n\n# Summarize the result contained in logitModel\n\n\nsummary\n(\nlogitModel\n\n\n\n\n\n\nIndividual mortgage information\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Summarize the model\n\n\nsummary\n(\nlogitModel\n)\n\n\n\n# view the first few rows\n\n\nhead\n(\nnewData\n)\n\n\n\n# Make predictions\n\ndataWithPredictions \n<-\n rxPredict\n(\nmodelObject \n=\n logitModel\n,\n \n                                 data \n=\n newData\n,\n \n                                 outData \n=\n newData\n,\n \n                                 type \n=\n \n'response'\n)\n\n\n\n# view the predictions\n\ndataWithPredictions\n\n\n\n\n\n\nComputing k-means with \nrxKmeans\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n# Examine the mortData dataset\n\nrxGetInfo\n(\nmortData\n,\n getVarInfo \n=\n \nTRUE\n)\n\n\n\n# Set up a path to a new xdf file\n\nmyNewMortData \n=\n \n'myMDwithKMeans.xdf'\n\n\n\n# Run k-means\n\nKMout \n<-\n rxKmeans\n(\nformula \n=\n \n~\n ccDebt \n+\n creditScore \n+\n houseAge\n,\n \n         data \n=\n mortData\n,\n\n         outFile \n=\n myNewMortData\n,\n\n         rowSelection \n=\n year \n==\n \n2000\n,\n\n         numClusters \n=\n \n4\n,\n\n         writeModelVars \n=\n \nTRUE\n)\n\n\n\nprint\n(\nKMout\n)\n\n\n\n# Examine the variables in the new dataset:\n\nrxGetInfo\n(\nmyNewMortData\n,\n getVarInfo \n=\n \nTRUE\n)\n\n\n\n# Summarize the cluster variable:\n\nrxSummary\n(\n~\n \nF\n(\n.\nrxCluster\n),\n data \n=\n myNewMortData\n)\n\n\n\n# Read into memory 10% of the data:\n\nmydf \n<-\n rxXdfToDataFrame\n(\nmyNewMortData\n,\n\n                         rowSelection \n=\n randSamp \n==\n \n1\n,\n\n                         varsToDrop \n=\n \n'year'\n,\n\n                         transforms \n=\n \nlist\n(\nrandSamp \n=\n \nsample\n(\n10\n,\n size \n=\n \n.\nrxNumRows\n,\n replace \n=\n \nTRUE\n)))\n\n\n\n## Visualize the clusters\n\nplot\n(\nmydf\n[\n-1\n],\n col \n=\n mydf\n$\n.\nrxCluster\n)\n\n\n\n\n\n\n\nCreate some decision trees\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n# regression tree\n\nregTreeOut \n<-\n rxDTree\n(\ndefault \n~\n creditScore \n+\n ccDebt \n+\n yearsEmploy \n+\n houseAge\n,\n \n                      rowSelection \n=\n year \n==\n \n2000\n,\n \n                      data \n=\n mortData\n,\n maxdepth \n=\n \n5\n)\n\n\n\n# print out the object\n\n\nprint\n(\nregTreeOut\n)\n\n\n\n# plot a dendrogram, and add node labels\n\nplot\n(\nrxAddInheritance\n(\nregTreeOut\n))\n\ntext\n(\nrxAddInheritance\n(\nregTreeOut\n))\n\n\n\n# Another visualization\n\n\n#library(RevoTreeView)\n\n\n#createTreeView(regTreeOut)\n\n\n# predict values\n\nmyNewData \n=\n \n'myNewMortData.xdf'\n\n\nrxPredict\n(\nregTreeOut\n,\n\n          data \n=\n mortData\n,\n\n          outData \n=\n myNewData\n,\n\n          writeModelVars \n=\n \nTRUE\n,\n\n          predVarNames \n=\n \n'default_RegPred'\n)\n\n\n\n# visualize ROC curve\n\nrxRocCurve\n(\nactualVarName \n=\n \n'default'\n,\n \n           predVarNames \n=\n \n'default_RegPred'\n,\n \n           data \n=\n myNewData\n)",
            "title": "Big Data Analysis with Revolution R Enterprise"
        },
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/#introduction",
            "text": "Importing data with  rxImport  function   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 # Declare the file paths for the csv and xdf files  # find the path or directory where the file is, load the path variable \nmyAirlineCsv  <-   file.path ( rxGetOption ( 'sampleDataDir' ),   '2007_subset.csv' )  # fin the data in this directory and load the data variable \nmyAirlineXdf  <-   '2007_subset.xdf'  # Use rxImport to import the data into xdf format  # rxImport(inData = myAirlineCsv, outFile = myAirlineXdf, overwrite = TRUE)  # or  # function within a function for more stats  system.time ( rxImport ( inData  =  myAirlineCsv ,  \n                     outFile  =  myAirlineXdf ,  \n                     overwrite  =   TRUE ))  list.files ()    Functions for summarizing data   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 # Get basic information about your data \nrxGetInfo ( data  =  myAirlineXdf ,  \n          getVarInfo  =   TRUE ,  \n          numRows  =   10 )  # Summarize the variables corresponding to actual elapsed time, time in the air, departure delay, flight Distance \nrxSummary ( formula  =   ~  ActualElapsedTime  +  AirTime  +  DepDelay  +  Distance ,  \n          data  =  myAirlineXdf )  # Histogram of departure delays \nrxHistogram ( formula  =   ~ DepDelay ,  \n            data  =  myAirlineXdf )  # Use parameters similar to a regular histogram to zero in on the interesting area \nrxHistogram ( formula  =   ~ DepDelay ,  \n            data  =  myAirlineXdf ,  \n            xAxisMinMax  =   c ( -100 ,   400 ),  \n            numBreaks  =   500 , \n            xNumTicks  =   10 )    Creating new variables using  rxDataStep   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27 # Calculate an additional variable: airspeed (distance traveled / time in the air) \nrxDataStep ( inData  =  myAirlineXdf ,  \n           outFile  =  myAirlineXdf ,  \n           varsToKeep  =   c ( 'Distance' ,   'AirTime' ), \n           transforms  =   list ( airSpeed  =  Distance  /  Airtime ), \n           append  =   'cols' , \n           overwrite  =   TRUE )  # Get Variable Information for airspeed \nrxGetInfo ( data  =  myAirlineXdf ,  \n          getVarInfo  =   TRUE , \n          varsToKeep  =   'airSpeed' )  # Summary for the airspeed variable \nrxSummary ( ~ airSpeed ,  \n          data  =  myAirlineXdf )  # Construct a histogtam for airspeed  # We can use the xAxisMinMax argument to limit the X-axis \nrxHistogram ( ~ airSpeed ,  \n            data  =  myAirlineXdf ) \n\nrxHistogram ( ~ airSpeed ,  \n            data  =  myAirlineXdf , \n            xNumTicks  =   10 , \n            numBreaks  =   1500 , \n            xAxisMinMax  =   c ( 0 , 12 ))    Transforming variables using  rxDataStep   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Conversion to miles per hour \nrxDataStep ( inData  =  myAirlineXdf ,  \n         outFile  =  myAirlineXdf ,  \n         varsToKeep  =   c ( 'airSpeed' ), \n           transforms  =   list ( airSpeed  =  airSpeed  *   60 ), \n         overwrite  =   TRUE )  # Histogram for airspeed after conversion \nrxHistogram ( ~ airSpeed ,  \n            data  =  myAirlineXdf )    Correlations  1\n2\n3\n4 # Correlation for departure delay, arrival delay, and air speed \nrxCor ( formula  =   ~  DepDelay  +  ArrDelay  +  airSpeed , \n      data  =  myAirlineXdf , \n      rowSelection  =   ( airSpeed  >   50 )   &   ( airSpeed  <   800 ))    Linear regression  1\n2\n3\n4\n5\n6 # Regression for airSpeed based on departure delay \nmyLMobj  <-  rxLinMod ( formula  =  airSpeed  ~  DepDelay ,  \n         data  =  myAirlineXdf , \n         rowSelection  =   ( airSpeed  >   50 )   &   ( airSpeed  <   800 ))  summary ( myLMobj )",
            "title": "Introduction"
        },
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/#data-exploration",
            "text": "RevoScaleR  options   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Extract the names of the possible options  names ( rxOptions ())  # Extract the sample data directory \nrxGetOption ( 'sampleDataDir' )  # View the current value of the reportProgress option \nrxGetOption ( 'reportProgress' )  # Set the value of the reportProgress option to 0 \nrxOptions ( reportProgress  =   0 )    Import and explore Dow Jones data  1\n2\n3\n4\n5 # Set up the variable that has the address of the relevant data file \ndjiXdf  <-   file.path ( rxGetOption ( 'sampleDataDir' ),   'DJIAdaily.xdf' )  # Get information about that dataset \nrxGetInfo ( djiXdf ,  getVarInfo  =   TRUE )    Extracting meta data about a variable using  rxGetVarInfo   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 # Get variable information for the dataset \ndjiVarInfo  <-  rxGetVarInfo ( djiXdf )  names ( djiVarInfo )  # Extract information about the closing cost variable  ( closeVarInfo  <-  djiVarInfo [[ 'Close' ]])  # Get the class of the closeVarInfo object  class ( closeVarInfo )  # Examine the structure of the closeVarInfo object \nstr ( closeVarInfo )  # Extract the global maximum of the closing cost variable \ncloseMax  <-  closeVarInfo [[ 'high' ]]    Summarizing variables with  rxSummary   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # Basic summary statistics \nrxSummary ( ~  DayOfWeek  +  Close  +  Volume ,  \n          data  =  djiXdf )  # Frequency weighted \nrxSummary ( ~  DayOfWeek  +  Close ,  \n          data  =  djiXdf ,  \n          fweights  =   'Volume' )  # Basic frequency count \nrxCrossTabs ( ~  DayOfWeek ,  \n            data  =  djiXdf )    Exploring a distribution with  rxHistogram   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 # Numeric Variables \nrxHistogram ( ~  Close ,  \n            data  =  djiXdf )  # Categorical Variable \nrxHistogram ( ~  DayOfWeek ,  \n            data  =  djiXdf )  # Different panels for different days of the week \nrxHistogram ( ~  Close  |  DayOfWeek ,  \n            data  =  djiXdf )  # Numeric Variables with a frequency weighting \nrxHistogram ( ~  Close ,  data  =  djiXdf ,  \n            fweights  =   'Volume' )    Plotting bivariate relationships with  rxLinePlot   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 # Simple bivariate line plot \nrxLinePlot ( Close  ~  DaysSince1928 ,  \n           data  =  djiXdf )  # Using different panels for different days of the week \nrxLinePlot ( Close  ~  DaysSince1928  |  DayOfWeek ,  \n           data  =  djiXdf )  # Using different groups \nrxLinePlot ( Close  ~  DaysSince1928 ,  \n           groups  =  DayOfWeek ,  \n           data  =  djiXdf )  # Simple bivariate line plot, after taking the log() of the ordinate (y) variable \nrxLinePlot ( log ( Close )   ~  DaysSince1928 ,  \n           data  =  djiXdf )    Summarzing variables with  rxCrossTabs   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # Compute the the summed volume for each day of the week \nrxCrossTabs ( formula  =  Volume  ~  DayOfWeek ,  \n            data  =  djiXdf )  # Compute the the summed volume for each day of the week for each month \nrxCrossTabs ( formula  =  Volume  ~   F ( Month ) : DayOfWeek ,  \n            data  =  djiXdf )  # Compute the the average volume for each day of the week for each month \nrxCrossTabs ( formula  =  Volume  ~   F ( Month ) : DayOfWeek ,  \n            data  =  djiXdf ,  \n            means  =   TRUE )  # Compute the the average closing price for each day of the week for each month, using volume as frequency weights \nrxCrossTabs ( formula  =  Close  ~   F ( Month ) : DayOfWeek ,  \n            data  =  djiXdf ,  \n            means  =   TRUE ,  \n            fweights  =   'Volume' )    Summarzing variables with  rxCube   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 # Compute the the summed volume for each day of the week \nrxCrossTabs ( Volume  ~  DayOfWeek ,  \n            data  =  djiXdf ) \n\nrxCube ( Volume  ~  DayOfWeek ,  \n       data  =  djiXdf ,  \n       means  =   FALSE )  # Compute the the summed volume for each day of the week for each month \nrxCrossTabs ( Volume  ~   F ( Month ) : DayOfWeek ,  \n            data  =  djiXdf ) \n\nrxCube ( Volume  ~   F ( Month ) : DayOfWeek ,  \n       data  =  djiXdf ,  \n       means  =   FALSE )  # Compute the the average volume for each day of the week for each month \nrxCube ( Volume  ~   F ( Month ) : DayOfWeek ,  \n       data  =  djiXdf )  # Compute the the average closing price for each day of the week for each month, using volume as frequency weights \nrxCube ( Close  ~  DayOfWeek ,  \n       data  =  djiXdf ,  \n       fweights  =   'Volume' )",
            "title": "Data Exploration"
        },
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/#data-manipulation",
            "text": "Using  rxDataStep  to transform data   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27 # Get information on mortData \nrxGetInfo ( mortData )  ## Set up my personal copy of the data \nmyMortData  <-   'myMD.xdf'  # Create the transform \nrxDataStep ( inData  =  mortData ,  \n           outFile  =  myMortData ,  \n           transforms  =   list ( highDebtRow  =  ccDebt  >   8000 ),  \n           overwrite  =   TRUE )  #rxDataStep(inData = mortData, outFile = myMortData, transforms = list(highDebtRow = ccDebt > 8000))  # Get the variable information \nrxGetVarInfo ( myMortData )  # Get the proportion of values that are 1 \nrxSummary (   ~  highDebtRow ,  \n           data  =  myMortData )  # Compute multiple transforms! \nrxDataStep ( inData  =  myMortData ,  outFile  =  myMortData , \n           transforms  =   list ( \n             newHouse  =  houseAge  <   10 , \n             ccsXhd  =  creditScore  *  highDebtRow ), \n           append  =   'cols' , \n           overwrite  =   TRUE )    More complex transforms using  transformFuncs   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 # Compute the summary statistics  ( csSummary  <-  rxSummary ( ~  creditScore ,  data  =  mortData ))  # Extract the mean and std. deviation \nmeanCS  <-  csSummary $ sDataFrame $ Mean [ 1 ] \nsdCS  <-  csSummary $ sDataFrame $ StdDev [ 1 ]  # Create a function to compute the scaled variable \nscaleCS  <-   function ( mylist ){ \n  mylist [[ 'scaledCreditScore' ]]   <-   ( mylist [[ 'creditScore' ]]   -  myCenter )   /  myScale\n   return ( mylist )  }  # Run it with rxDataStep (A above in B below) \nmyMortData  <-   'myMD.xdf' \nrxDataStep ( inData  =  mortData ,  outFile  =  myMortData , \n           transformFunc  =  scaleCS , \n           transformObjects  =   list ( myCenter  =  meanCS ,  myScale  =  sdCS ))  # Check the new variable \nrxGetVarInfo ( myMortData ) \nrxSummary ( ~  scaledCreditScore ,  \n          data  =  myMortData )",
            "title": "Data Manipulation"
        },
        {
            "location": "/Big_Data_Analysis_with_Revolution_R_Enterprise/#data-analysis",
            "text": "Preparing data for analysis: import   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 # Declare the file paths for the csv and xdf files \nmyAirlineCsv  <-   file.path ( rxGetOption ( 'sampleDataDir' ),   'AirlineDemoSmall.csv' ) \nmyAirlineXdf  <-   'ADS.xdf'  # Use rxImport to import the data into xdf format \nrxImport ( inData  =  myAirlineCsv ,  \n         outFile  =  myAirlineXdf ,  \n         overwrite  =   TRUE , \n         colInfo  =   list (  \n           DayOfWeek  =   list ( \n            type  =   'factor' ,  \n            levels  =   c ( 'Monday' ,   'Tuesday' ,   'Wednesday' ,  \n                        'Thursday' ,   'Friday' ,   'Saturday' ,   'Sunday' ))))    Preparing data Ffor analysis: exploration  1\n2\n3\n4\n5\n6\n7 # Summarize arrival delay for each day of the week \nrxSummary ( formula  =  ArrDelay  ~  DayOfWeek ,  \n          data  =  myAirlineXdf )  # Vizualize the arrival delay histogram \nrxHistogram ( formula  =   ~ ArrDelay ,  \n            data  =  myAirlineXdf )    Construct a linear model   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 # predict arrival delay by day of the week \nmyLM1  <-  rxLinMod ( ArrDelay  ~  DayOfWeek ,  \n                  data  =  myAirlineXdf )  # summarize the model \nsmmary ( myLM1 )  # Use the transforms argument to create a factor variable associated with departure time 'on the fly,'  # predict Arrival Delay by the interaction between Day of the week and that new factor variable \nmyLM2  <-  rxLinMod ( ArrDelay  ~  DayOfWeek ,  \n                  data  =  myAirlineXdf , \n                  transforms  =   list ( \n                    catDepTime  =   cut ( CRSDepTime ,  breaks  =   seq ( from  =   5 ,  to  =   23 ,  by  =   2 ))), \n                    cube  =   TRUE )  # summarize the model  summary ( myLM2 )    Generating predictions and residuals   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25 # Summarize model first  summary ( myLM2 )  # Path to new dataset storing predictions \nmyNewADS  <-   'myNEWADS.xdf'  # Generate predictions \nrxPredict ( modelObject  =  myLM2 ,  \n          data  =  myAirlineXdf ,  \n          outData  =  myNewADS ,  \n          writeModelVars  =   TRUE )  # Get information on the new dataset \nrxGetInfo ( myNewADS ,  getVarInfo  =   TRUE )  # Generate residuals. \nrxPredict ( modelObject  =  myLM2 ,  \n          data  =  myAirlineXdf ,  \n          outData  =  myNewADS ,  \n          writeModelVars  =   TRUE ,  \n          computeResiduals  =   TRUE ,  \n          overwrite  =   TRUE )  # Get information on the new dataset \nrxGetInfo ( myNewADS ,  getVarInfo  =   TRUE )    Logistic regression   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # look at the meta data  ls () \nrxGetInfo ( data  =  mortData ,  getVarInfo  =   TRUE )  # Construct the logit model \nlogitModel  <-  rxLogit ( formula  =  default  ~  houseAge  +   F ( year )   +  ccDebt  +  creditScore  +  yearsEmploy ,  \n                      data  =  mortData )  # Summarize the result contained in logitModel  summary ( logitModel   Individual mortgage information   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Summarize the model  summary ( logitModel )  # view the first few rows  head ( newData )  # Make predictions \ndataWithPredictions  <-  rxPredict ( modelObject  =  logitModel ,  \n                                 data  =  newData ,  \n                                 outData  =  newData ,  \n                                 type  =   'response' )  # view the predictions \ndataWithPredictions   Computing k-means with  rxKmeans   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30 # Examine the mortData dataset \nrxGetInfo ( mortData ,  getVarInfo  =   TRUE )  # Set up a path to a new xdf file \nmyNewMortData  =   'myMDwithKMeans.xdf'  # Run k-means \nKMout  <-  rxKmeans ( formula  =   ~  ccDebt  +  creditScore  +  houseAge ,  \n         data  =  mortData , \n         outFile  =  myNewMortData , \n         rowSelection  =  year  ==   2000 , \n         numClusters  =   4 , \n         writeModelVars  =   TRUE )  print ( KMout )  # Examine the variables in the new dataset: \nrxGetInfo ( myNewMortData ,  getVarInfo  =   TRUE )  # Summarize the cluster variable: \nrxSummary ( ~   F ( . rxCluster ),  data  =  myNewMortData )  # Read into memory 10% of the data: \nmydf  <-  rxXdfToDataFrame ( myNewMortData , \n                         rowSelection  =  randSamp  ==   1 , \n                         varsToDrop  =   'year' , \n                         transforms  =   list ( randSamp  =   sample ( 10 ,  size  =   . rxNumRows ,  replace  =   TRUE )))  ## Visualize the clusters \nplot ( mydf [ -1 ],  col  =  mydf $ . rxCluster )    Create some decision trees   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 # regression tree \nregTreeOut  <-  rxDTree ( default  ~  creditScore  +  ccDebt  +  yearsEmploy  +  houseAge ,  \n                      rowSelection  =  year  ==   2000 ,  \n                      data  =  mortData ,  maxdepth  =   5 )  # print out the object  print ( regTreeOut )  # plot a dendrogram, and add node labels \nplot ( rxAddInheritance ( regTreeOut )) \ntext ( rxAddInheritance ( regTreeOut ))  # Another visualization  #library(RevoTreeView)  #createTreeView(regTreeOut)  # predict values \nmyNewData  =   'myNewMortData.xdf' \n\nrxPredict ( regTreeOut , \n          data  =  mortData , \n          outData  =  myNewData , \n          writeModelVars  =   TRUE , \n          predVarNames  =   'default_RegPred' )  # visualize ROC curve \nrxRocCurve ( actualVarName  =   'default' ,  \n           predVarNames  =   'default_RegPred' ,  \n           data  =  myNewData )",
            "title": "Data Analysis"
        },
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/",
            "text": "Foreword\n\n\nCourse notes and snippets.\n\n\n\n\nIntroduction to eXtensible Time\n\u00b6\n\n\nWhat is an \nxts\n object\n\n\nxts\n, a constructor or a subclass that inherits behavior from parents. \nxts\n (as a subclass) extends the popular \nzoo\n class (as a parent). Most \nzoo\n methods work for \nxts\n. \n\n\nxts\n is a matrix objects; subsets always preserve the matrix form. \n\n\nxts\n are indexed by a formal time object. You can time-stamp the data.\n\n\nThe main \nxts\n constructor two most important arguments are \nx\n for the data and \norder.by\n for the index. \nx\n must be a vector or matrix. \norder.by\n is a vector of the same length or number of rows of \nx\n; it must be a proper time or date object and it must be in increasing order.\n\n\nxts\n also allows you to bind arbitrary key-value attributes to your data. This lets you keep metadata about your object inside your object.\n\n\n1\n2\n# Load xts\n\n\nlibrary\n(\nxts\n)\n\n\n\n\n\n\n\nA first \nxts\n object\n\n\n1\n2\n3\n4\n5\n6\n7\n8\ncore \n<-\n \nmatrix\n(\nc\n(\nrep\n(\n1\n,\n3\n),\n \nrep\n(\n2\n,\n3\n)),\n nrow \n=\n \n3\n,\n ncol \n=\n \n2\n)\n\ncore\n\nidx \n<-\n \nas.Date\n(\nc\n(\n\"2016-06-01\"\n,\n\"2016-06-02\"\n,\n \n\"2016-06-03\"\n))\n\nidx\n\nex_matrix \n<-\n xts\n(\ncore\n,\n order.by \n=\n idx\n)\n\nex_matrix\n\n\n\n\n\n\n\n\n\n    \n1\n2\n\n    \n1\n2\n\n    \n1\n2\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n[1] \"2016-06-01\" \"2016-06-02\" \"2016-06-03\"\n\n\n\n           [,1] [,2]\n2016-06-01    1    2\n2016-06-02    1    2\n2016-06-03    1    2\n\n\n\n\n\n\nMore than a matrix\n\n\nxts\n objects are normal R matrices, but with special powers.\n\n\n1\n2\n# View the structure of ex_matrix\n\nstr\n(\nex_matrix\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\nAn 'xts' object on 2016-06-01/2016-06-03 containing:\n  Data: num [1:3, 1:2] 1 1 1 2 2 2\n  Indexed by objects of class: [Date] TZ: UTC\n  xts Attributes:  \n NULL\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Extract the 3rd observation of the 2nd column of ex_matrix\n\nex_matrix\n[\n3\n,\n \n2\n]\n\n\n\n# Extract the 3rd observation of the 2nd column of core \n\ncore\n[\n3\n,\n \n2\n]\n\n\n\n\n\n\n\n1\n2\n           [,1]\n2016-06-03    2\n\n\n\n\n\n\nYour first \nxts\n object\n\n\nVector must be of the same length or number of rows as x and, this is very important: it must be a proper time or date object and it must be in increasing order.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n# Create the object data using 5 random numbers\n\ndata \n<-\n rnorm\n(\n5\n)\n\ndata\n\n\n# Create dates as a Date class object starting from 2016-01-01\n\ndates \n<-\n \nseq\n(\nas.Date\n(\n\"2016-01-01\"\n),\n length \n=\n \n5\n,\n by \n=\n \n\"days\"\n)\n\ndates\n\n\n# Use xts() to create smith\n\nsmith \n<-\n xts\n(\nx \n=\n data\n,\n order.by \n=\n dates\n)\n\nsmith\n\n\n# Create bday (1899-05-08) using a POSIXct date class object\n\nbday \n<-\n \nas.POSIXct\n(\n\"1899-05-08\"\n)\n\nbday\n\n\n# Create hayek and add a new attribute called born\n\nhayek \n<-\n xts\n(\nx \n=\n data\n,\n order.by \n=\n dates\n,\n born \n=\n bday\n)\n\nhayek\n\n\n\n\n\n\n\n    \n0.384581498733952\n\n    \n-0.423869463715667\n\n    \n-0.49549963387221\n\n    \n1.49946058244083\n\n    \n-0.92906780054414\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n[1] \"2016-01-01\" \"2016-01-02\" \"2016-01-03\" \"2016-01-04\" \"2016-01-05\"\n\n\n\n                 [,1]\n2016-01-01  0.3845815\n2016-01-02 -0.4238695\n2016-01-03 -0.4954996\n2016-01-04  1.4994606\n2016-01-05 -0.9290678\n\n\n\n[1] \"1899-05-08 EST\"\n\n\n\n                 [,1]\n2016-01-01  0.3845815\n2016-01-02 -0.4238695\n2016-01-03 -0.4954996\n2016-01-04  1.4994606\n2016-01-05 -0.9290678\n\n\n\n\n\n\nDeconstructing \nxts\n\n\nSeparate a time series into its core data and index attributes for additional analysis and manipulation.\n\n\nFunctions are methods from the \nzoo\n class.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Extract the core data of hayek\n\nhayek_core \n<-\n coredata\n(\nhayek\n)\n\n\n\n# View the class of hayek_core\n\n\nclass\n(\nhayek_core\n)\n\n\n\n# Extract the index of hayek\n\nhayek_index \n<-\n index\n(\nhayek\n)\n\n\n\n# View the class of hayek_index\n\n\nclass\n(\nhayek_index\n)\n\n\n\n\n\n\n\n\u2018matrix\u2019\n\n\n\u2018Date\u2019\n\n\nTime-based indices\n\n\nxts\n objects get their power from the index attribute that holds the time dimension. One major difference between \nxts\n and most other time series objects in R is the ability to use any one of various classes that are used to represent time. Whether \nPOSIXct\n, \nDate\n, or some other class, xts will convert this into an internal form to make subsetting as natural to the user as possible.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Create dates\n\ndates \n<-\n \nas.Date\n(\n\"2016-01-01\"\n)\n \n+\n \n0\n:\n4\n\n\n\n# Create ts_a\n\nts_a \n<-\n xts\n(\nx \n=\n \n1\n:\n5\n,\n order.by \n=\n dates\n)\n\n\n\n# Create ts_b\n\nts_b \n<-\n xts\n(\nx \n=\n \n1\n:\n5\n,\n order.by \n=\n \nas.POSIXct\n(\ndates\n))\n\n\n\n# Extract the rows of ts_a using the index of ts_b\n\nts_a\n[\nindex\n(\nts_a\n)]\n\n\n\n# Extract the rows of ts_b using the index of ts_a\n\nts_b\n[\nindex\n(\nts_a\n)]\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n           [,1]\n2016-01-01    1\n2016-01-02    2\n2016-01-03    3\n2016-01-04    4\n2016-01-05    5\n\n\n\n     [,1]\n\n\n\n\n\n\nFirst Order of Business - Basic Manipulations\n\u00b6\n\n\nConverting \nxts\n objects\n\n\nxts\n provides methods to convert all of the major objects you are likely to come across - suitable native R types like matrix, data.frame, and ts are supported, as well as contributed ones such as \ntimeSeries\n, \nfts\n and of course \nzoo\n. \nas.xts\n is the workhorse function to do the conversions to \nxts\n, and similar functions will provide the reverse behavior.\n\n\n1\n2\n# using a R dataset\n\n\nhead\n(\naustres\n)\n\n\n\n\n\n\n\n\n    \n13067.3\n\n    \n13130.5\n\n    \n13198.4\n\n    \n13254.2\n\n    \n13303.7\n\n    \n13353.9\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Convert austres to an xts object called au\n\nau \n<-\n as.xts\n(\naustres\n)\n\n\nhead\n(\nau\n,\n \n5\n)\n\n\n\n# Convert your xts object (au) into a matrix am\n\nam \n<-\n \nas.matrix\n(\nau\n)\n\n\nhead\n(\nam\n,\n \n5\n)\n\n\n\n# Convert the original austres into a matrix am2\n\nam2 \n<-\n \nas.matrix\n(\naustres\n)\n\n\nhead\n(\nam2\n,\n \n5\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n           [,1]\n1971 Q2 13067.3\n1971 Q3 13130.5\n1971 Q4 13198.4\n1972 Q1 13254.2\n1972 Q2 13303.7\n\n\n\n\n\n\n\n\n\n    \n1971 Q2\n13067.3\n\n    \n1971 Q3\n13130.5\n\n    \n1971 Q4\n13198.4\n\n    \n1972 Q1\n13254.2\n\n    \n1972 Q2\n13303.7\n\n\n\n\n\n\n\n\n\n\n    \n13067.3\n\n    \n13130.5\n\n    \n13198.4\n\n    \n13254.2\n\n    \n13303.7\n\n\n\n\n\n\n\nImporting data\n\n\nRead raw data from files on disk or the web and convert data to \nxts\n.\n\n\nRead the same data into a \nzoo\n object using read.zoo and convert the \nzoo\n into an \nxts\n object.\n\n\n1\n2\n3\n# Create dat by reading tmp_file\n\ndat \n<-\n \nas.matrix\n(\nread.csv\n(\n'tmp_file.csv'\n,\n sep \n=\n \n','\n,\n header \n=\n \nTRUE\n))\n\ndat\n\n\n\n\n\n\n\n\na\nb\n\n\n\n    \n1/02/2015\n1\n3\n\n    \n2/03/2015\n2\n4\n\n\n\n\n\n\n\n1\n2\n# Convert dat into xts\n\nxts\n(\ndat\n,\n order.by \n=\n \nas.Date\n(\nrownames\n(\ndat\n),\n \n\"%m%d%Y\"\n))\n\n\n\n\n\n\n\n1\n2\n3\n     a b\n<NA> 1 3\n<NA> 2 4\n\n\n\n\n\n\n1\n2\n3\n# Read tmp_file using read.zoo and as.xts\n\ndat_xts \n<-\n as.xts\n(\nread.zoo\n(\n'tmp_file.csv'\n,\n index.column \n=\n \n0\n,\n sep \n=\n \n\",\"\n,\n format \n=\n \n\"%m/%d/%Y\"\n))\n\ndat_xts\n\n\n\n\n\n\n1\n2\n3\n           a b\n2015-01-02 1 3\n2015-02-03 2 4\n\n\n\n\n\n\nExporting \nxts\n objects\n\n\n1\n2\n3\n4\nhead\n(\nsunspots\n,\n \n5\n)\n\n\n\n# Convert sunspots to xts\n\nsunspots_xts \n<-\n as.xts\n(\nsunspots\n)\n\n\n\n\n\n\n\n\n    \n58\n\n    \n62.6\n\n    \n70\n\n    \n55.7\n\n    \n85\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# Get the temporary file name\n\ntmp \n<-\n \ntempfile\n()\n\n\n\n# Write the xts object using zoo to tmp \n\nwrite.zoo\n(\nsunspots\n,\n sep \n=\n \n\",\"\n,\n file \n=\n tmp\n)\n\n\ntmp\n\n\n\n\n\n\n\u2018C:\\Users\\Dell\\AppData\\Local\\Temp\\RtmpEdYcAQ\\file1a706c8c732b\u2019\n\n\n1\n2\n3\n4\n5\n6\n7\n# Read the tmp file\n\n\n# FUN=as.yearmon will convert strings such as Jan 1749 into a proper time class\n\nsun \n<-\n read.zoo\n(\ntmp\n,\n sep \n=\n \n\",\"\n,\n FUN \n=\n as.yearmon\n)\n\n\n\n# Convert sun into xts\n\nsun_xts \n<-\n as.xts\n(\nsun\n)\n\n\nhead\n(\nsun_xts\n,\n \n5\n)\n\n\n\n\n\n\n\n1\n2\nError in eval(expr, envir, enclos): impossible de trouver la fonction \"read.zoo\"\nTraceback:\n\n\n\n\n\n\nThe ISO8601 Standard\n\n\nThe ISO8601 standard is the internationally recognized and accepted way to represent dates and times. From the biggest to the smallest, left to right, YYYY-MM-DDTHH:MM:SS.\n\n\nThe standard allows for a common format to not only describe dates, but also a way to represent ranges and repeating intervals:\n\n\n\n\n2004 or 2001/2015\n\n\n201402/03\n\n\n2014-02-22 08:30:00\n\n\nT08:00/T09:00\n\n\n\n\nQuerying for dates\n\n\nDate ranges can be extracted from xts objects by simply specifying the period(s) you want using special character strings in your subset.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nA\n[\n\"201601\"\n]\n         \n## Jan 2016\n\nA\n[\n\"20160125\"\n]\n       \n## Jan 25, 2016\n\nA\n[\n\"201203/201212\"\n]\n  \n## Feb to Dec 2012\n\n\n\n# Select all of 2016 from x\n\nx_2016 \n<-\n x\n[\n\"2016\"\n]\n\n\n\n# Select January 2016 to March 22\n\njan_march \n<-\n x\n[\n\"201601/20160322\"\n]\n\n\n\n\n\n\n\nExtracting recurring intraday intervals\n\n\nMost common data \u201cin the wild\u201d is daily. On ocassion you may find yourself working with intraday data, that is date plus times.\n\n\nYou can slice days easily by using special notation in the \ni =\n argument to the single bracket extraction (i.e. \n[i,j]\n).\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Intraday times for all days\n\nNYSE\n[\n\"T09:30/T16:00\"\n]\n\n\n\n# Extract all data between 8AM and 10AM\n\nmorn_2010 \n<-\n irreg\n[\n\"T8:00/T10:00\"\n]\n\n\n\n# Extract the observations for January 13th\n\nmorn_2010\n[\n\"2010-01-13\"\n]\n\n\n\n\n\n\n\nAlternating extraction techniques\n\n\nOften times you may need to subset an existing time series with a set of Dates, or time-based objects. These might be from \nas.Date()\n, or \nas.POSIXct()\n, or a variety of other classes.\n\n\n1\n2\n3\n4\n5\n# Subset x using the vector dates\n\nx\n[\ndates\n]\n\n\n\n# Subset x using dates as POSIXct\n\nx\n[\nas.POSIXct\n(\ndates\n)]\n\n\n\n\n\n\n\nExtracting recurring intraday intervals\n\n\nThe most common time series data is daily, intraday data, which contains both dates and times. \n\n\n1\n2\n3\n4\n5\n# Extract all data between 8AM and 10AM\n\nmorn_2010 \n<-\n irreg\n[\n\"T8:00/T10:00\"\n]\n\n\n\n# Extract the observations for January 13th\n\nmorn_2010\n[\n\"2010-01-13\"\n]\n\n\n\n\n\n\n\nRow selection with time objects\n\n\nSubset an existing time series with a set of Dates, or time-based objects.\n\n\n1\n2\n3\n4\n5\n# Subset x using the vector dates\n\nx\n[\ndates\n]\n\n\n\n# Subset x using dates as POSIXct\n\nx\n[\nas.POSIXct\n(\ndates\n)]\n\n\n\n\n\n\n\nUpdate and replace elements\n\n\nReplace known intervals or observations with an NA, say due to a malfunctioning sensor on a particular day or a set of outliers given a holiday.\n\n\n1\n2\n3\n4\n5\n# Replace all values in x on dates with NA\n\nx\n[\ndates\n]\n \n<-\n \nNA\n\n\n\n# Replace dates from 2016-06-09 and on with 0\n\nx\n[\n\"20160609/\"\n]\n \n<-\n \n0\n\n\n\n\n\n\n\nFind the \nfirst\n or \nlast\n period of time\n\n\nSometimes you need to locate data by relative time. Instead of using an absolute offset, you describe a position relative in time. A simple example would be something like the \nlast 3 weeks\n of a series, or the \nfirst day of current month\n.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Create lastweek with using the last 1 week of temps\n\nlastweek \n<-\n last\n(\ntemps\n,\n \n\"1 week\"\n)\n\n\n\n# Print the last 2 observations in lastweek\n\nlast\n(\nlastweek\n,\n \n2\n)\n\n\n\n# Extract all but the last two days of lastweek\n\nlast\n(\nlastweek\n,\n \n\"-2 day\"\n)\n\n\n\n\n\n\n\nCombining \nfirst\n and \nlast\n\n\n1\n2\n3\n4\n5\n# Last 3 days of first week\n\nlast\n(\nfirst\n(\nTemps\n,\n \n'1 week'\n),\n'3 days'\n)\n \n\n\n# Extract the first three days of the second week of temps\n\nfirst\n(\nlast\n(\nfirst\n(\ntemps\n,\n \n\"2 week\"\n),\n \n\"1 week\"\n),\n \n\"3 day\"\n)\n\n\n\n\n\n\n\nMatrix arithmetic - add, subtract, multiply and divide in time!\n\n\nWhen you perform any binary operation using two xts objects, these objects are first aligned using the intersection of the indexes to preserve the point-in-time aspect of your data, assuring that you don\u2019t introduce accidental look ahead (or look behind!) bias into your calculations.\n\n\nYour options include:\n\n\n\n\ncoredata() or as.numeric() (drop one to a matrix or vector).\n\n\nManually shift index values - i.e. use lag().\n\n\nReindex your data (before or after the calculation).\n\n\n\n\nxts\n respects time and will only return the intersection of times when doing various mathematical operations. First option:\n\n\n1\n2\n3\n4\n5\n# Add a and b\n\na \n+\n b\n\n\n# Add a with the numeric value of b\n\na \n+\n \nas.numeric\n(\nb\n)\n\n\n\n\n\n\n\nMath with non-overlapping indexes\n\n\nThis third way involves modifying the two series you want by aligning assuring you have some union of dates - the dates you require in your final output. This makes it possibly to preserve dimensionality of the data:\n\n\n1\n2\n3\n4\n5\n6\n7\nmerge\n(\nb\n,\n index\n(\na\n))\n\n\n\n# Add a to b, and fill all missing rows of b with 0\n\na \n+\n \nmerge\n(\nb\n,\n index\n(\na\n),\n fill \n=\n \n0\n)\n\n\n\n# Add a to b and fill NAs with the last observation\n\na \n+\n \nmerge\n(\nb\n,\n index\n(\na\n),\n fill \n=\n na.locf\n)\n\n\n\n\n\n\n\nMerging and modifying time series\n\u00b6\n\n\nCombining \nxts\n by column with \nmerge\n\n\nxts\n makes it easy to join data by column and row using a few different functions.  \n\n\nxts\n objects must be of identical type (e.g. integer + integer), or be POSIXct dates vector, or be atomic vectors of the same type (e.g. numeric), or be a single NA. It does not work on data.frames with various column types.\n\n\nOne of the most important functions to accomplish this is \nmerge\n. It works like a \ncbind\n or and SQL \njoin\n:\n\n\n\n\ninner join (and).\n\n\nouter join (or).\n\n\nleft join.\n\n\nright join.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Basic argument use\n\n\nmerge\n(\na\n,\n b\n,\n join \n=\n \n\"right\"\n,\n fill \n=\n \n9999\n)\n\n\n\n# Perform an inner join of a and b\n\n\nmerge\n(\na\n,\n b\n,\n join \n=\n \n\"inner\"\n)\n\n\n\n# Perform of a left-join of a and b, fill mising values with 0\n\n\nmerge\n(\na\n,\n b\n,\n join \n=\n \n\"left\"\n,\n fill \n=\n \n0\n)\n\n\n\n# fill = na.locf, fill = NA\n\n\n\n\n\n\n\nCombining \nxts\n by row with \nrbind\n\n\nEasy to add new rows to your data.\n\n\n1\n2\n3\n4\n5\n# Row bind temps_june30 to temps, assign this to temps2\n\ntemps2 \n<-\n \nrbind\n(\ntemps\n,\n temps_june30\n)\n\n\n\n# Row bind temps_july17 and temps_july18 to temps2, call this temps3\n\ntemps3 \n<-\n \nrbind\n(\ntemps2\n,\n temps_july17\n,\n temps_july18\n)\n\n\n\n\n\n\n\nFill missing values using last or previous observation\n\n\nThe \nna.locf\n function takes the last-observation-carried-forward approach.\n\n\nThe \nxts\n package leverages the power of \nzoo\n for help with this. \nzoo\n provides a variety of missing data handling functions which are usable by \nxts\n, and very handy.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# Last obs. carried forward\n\nna.locf\n(\nx\n)\n                \n\n\n# Next obs. carried backward\n\nna.locf\n(\nx\n,\n fromLast \n=\n \nTRUE\n)\n\n\n\n# Fill missing values using the last observation\n\nna.locf\n(\ntemps\n,\n na.rm \n=\n \nTRUE\n,\n\n                fromLast \n=\n \nTRUE\n)\n\n\n\n# Fill missing values using the next observation\n\nna.locf\n(\ntemps\n,\n na.rm \n=\n \nTRUE\n)\n\n\n\n# maxgap = Inf\n\n\n# rule = 2,\n\n\n\n\n\n\n\nNA interpolation\n\n\nFrom \nzoo\n.\n\n\nOn occassion, a simple carry forward approach to missingness isn\u2019t appropriate. It may be that a series is missing an observation due to a higher frequency sampling than the generating process. You might also encounter an observation that is in error, yet expected to be somewhere between the values of its neighboring observations.\n\n\nBoth of these cases are examples of where interpolation is useful.\n\n\n1\n2\n# Interpolate NAs using linear approximation\n\nna.approx\n(\nAirPass\n)\n\n\n\n\n\n\n\nCombine a leading and lagging time series\n\n\nAnother common modification for time series is the ability to lag a series.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n# Your final object\n\nlag\n(\nx\n,\n k \n=\n \n1\n,\n na.pad \n=\n \nTRUE\n)\n\n\n\n# k = -1  leading\n\n\n# k = 1   lagging\n\n\n# zoo uses the other way around\n\n\n\n# k = c(0, 1, 5)  multiple leads or lags\n\n\n\n# Create a leading object called lead_x\n\nlead_x \n<-\n lag\n(\nx\n,\n k \n=\n \n-1\n)\n\n\n\n# Create a lagging object called lag_x\n\nlag_x \n<-\n lag\n(\nx\n,\n k \n=\n \n1\n)\n\n\n\n# Merge your three series together and assign to z\n\nz \n<-\n \nmerge\n(\nlead_x\n,\n x\n,\n lag_x\n)\n\n\n\n\n\n\n\nCalculate a difference of a series with \ndiff\n\n\nAnother common operation on time series, typically on those that are non-stationary, is to take a difference of the series.\n\n\ndifferences\n is the order of the difference (how many times \ndiff\n is called). A simple way to view a single (or \nfirst order\n) difference is to see it as \nx(t) - x(t-k)\n where \nk\n is the number of lags to go back. Higher order differences are simply the reapplication of a difference to each prior result (like a second derivative or a difference of the difference).\n\n\n1\n2\n3\n4\n5\n6\n7\ndiff\n(\nx\n,\n lag \n=\n \n1\n,\n difference \n=\n \n1\n,\n log \n=\n \nFALSE\n,\n na.pad \n=\n \nTRUE\n)\n\n\n\n# calculate the first difference of AirPass using lag and subtraction\n\nAirPass \n-\n lag\n(\nAirPass\n,\n k \n=\n \n1\n)\n\n\n\n# calculate the first order 12-month difference if AirPass\n\n\ndiff\n(\nAirPass\n,\n lag \n=\n \n12\n,\n differences \n=\n \n1\n)\n\n\n\n\n\n\n\nWhat is the key difference in lag between xts and zoo? The \nk\n argument in \nzoo\n uses positive values for shifting past observations forward.\n\n\nApply and aggregate by time\n\u00b6\n\n\nFind intervals by time in \nxts\n\n\nThe main function in xts to facilitate this is endpoints. It takes a time series (or a vector of times) and returns the locations of the last observations in each interval.\n\n\nFor example, the code below locates the last observation of each year for the \nAirPass\n dataset:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nendpoints\n(\nAirPass\n,\n on \n=\n \n\"years\"\n)\n\n\n\n# The argument on supports a variety of periods, including \"years\", \"quarters\", \"months\", as well as intraday intervals such as \"hours\", and \"minutes\"\n\n\n\n# on = \"weeks\", k = 2, the result would be the final day of every other week in your data\n\n\n\n# Locate the final day of every week\n\nendpoints\n(\ntemps\n,\n on \n=\n \n\"weeks\"\n)\n\n\n\n# Locate the final day of every two weeks\n\nendpoints\n(\ntemps\n,\n on \n=\n \n\"weeks\"\n,\n k \n=\n \n2\n)\n\n\n\n\n\n\n\nApply a function by time period(s)\n\n\nxts\n provides \nperiod.apply\n, which takes a time series, an \nINDEX\n of endpoints, and a function to \napply\n:\n\n\n1\n2\n3\n4\n5\n6\n7\nperiod.apply\n(\nx\n,\n INDEX\n,\n FUN\n,\n \n...\n)\n\n\n\n# Calculate the weekly endpoints\n\nep \n<-\n endpoints\n(\ntemps\n,\n on \n=\n \n\"weeks\"\n)\n\n\n\n# Now calculate the weekly mean and display the results\n\nperiod.apply\n(\ntemps\n[,\n \n\"Temp.Mean\"\n],\n INDEX \n=\n ep\n,\n FUN \n=\n \nmean\n)\n\n\n\n\n\n\n\nUsing \nlapply\n and \nsplit\n to apply functions on intervals\n\n\nOften it is useful to physically split your data into disjoint chunks by time and perform some calculation on these periods.\n\n\n1\n2\n3\n4\n5\n# Split temps by week\n\ntemps_weekly \n<-\n \nsplit\n(\ntemps\n,\n f \n=\n \n\"weeks\"\n)\n\n\n\n# Create a list of weekly means\n\n\nlapply\n(\nX \n=\n temps_weekly\n,\n FUN \n=\n \nmean\n)\n\n\n\n\n\n\n\nSelection by \nendpoints\n vs. \nsplit-lapply-rbind\n\n\n1\n2\n3\n4\n5\n6\n7\n# use the proper combination of split, lapply and rbind.\n\n\n#T1 <- do.call(rbind, ___(___(Temps, \"weeks\"), function(w) last(w, n=\"___\")))\n\nT1 \n<-\n \ndo.call\n(\nrbind\n,\n \nlapply\n(\nsplit\n(\nTemps\n,\n \n\"weeks\"\n),\n \nfunction\n(\nw\n)\n last\n(\nw\n,\n n\n=\n \n\"1 day\"\n)))\n\n\n\n# now subset Temps using the results of 'endpoints'\n\nlast_day_of_weeks \n<-\n endpoints\n(\nTemps\n,\n on \n=\n \n\"weeks\"\n)\n\nT2 \n<-\n Temps\n[\nlast_day_of_weeks\n]\n\n\n\n\n\n\n\nConvert univariate series to OHLC data\n\n\nIn financial series it is common to find Open-High-Low-Close data calculated over some repeating and regular interval.\n\n\nAlso known as range bars, aggregating a series based on some regular window can make analysis easier amongst series that have varying frequencies. For example, a weekly economic series and a daily stock series can be compared more easily if the daily is converted to weekly.\n\n\nOHLC means Opening, High, Low, Closing values.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nto.period\n(\nx\n,\n\n          period \n=\n \n\"months\"\n,\n \n          k \n=\n \n1\n,\n \n          indexAt\n,\n \n          name\n=\nNULL\n,\n\n          OHLC \n=\n \nTRUE\n,\n\n          \n...\n)\n\n\n\n# Convert USDEUR to weekly\n\n\n# OHLC = FALSE by default\n\nUSDEUR_weekly \n<-\n to.period\n(\nUSDEUR\n,\n period \n=\n \n\"weeks\"\n)\n\n\nhead\n(\nUSDEUR\n)\n\n\nhead\n(\nUSDEUR_weekly\n)\n\n\n\n# Convert USDEUR_weekly to monthly\n\nUSDEUR_monthly \n<-\n to.period\n(\nUSDEUR_weekly\n,\n period \n=\n \n\"months\"\n)\n\n\n\n# Convert USDEUR_monthly to yearly univariate\n\nUSDEUR_yearly \n<-\n to.period\n(\nUSDEUR_monthly\n,\n period \n=\n \n\"years\"\n,\n OHLC \n=\n \nFALSE\n)\n\n\n\n\n\n\n\nConvert a series to a lower frequency\n\n\nBesides converting univariate time series to OHLC series, to.period() also lets you convert OHLC to lower regularized frequency - something like subsampling your data.\n\n\nFor example, when using the shortcut function \nto.quarterly()\n xts will convert your index to the \nyearqtr\n class to make periods more obvious.\n\n\n1\n2\n3\n4\n5\n6\n# Convert EqMktNeutral to quarterly OHLC\n\nmkt_quarterly \n<-\n to.period\n(\nEqMktNeutral\n,\n period \n=\n \n\"quarters\"\n)\n\n\n\n# Convert EqMktNeutral to quarterly using shortcut function\n\n\n# Change the base name of each OHLC column to EDHEC.Equity and change the index to \"firstof\"\n\nmkt_quarterly2 \n<-\n to.quarterly\n(\nEqMktNeutral\n,\n name \n=\n \n\"EDHEC.Equity\"\n,\n indexAt \n=\n \n\"first\"\n)\n\n\n\n\n\n\n\nCalculate basic rolling value of series by month\n\n\nRolling windows can be discrete: use \nlapply\n with (\ncumsum, cumprod, cummax, cummin\n) and \nsplit\n:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# split-lapply-rbind pattern\n\n\nx_split \n<-\n \nsplit\n(\nx\n,\n f \n=\n \n\"months\"\n)\n\nx_list \n<-\n \nlapply\n(\nx_split\n,\n \ncummax\n)\n\nx_list_rbind \n<-\n \ndo.call\n(\nrbind\n,\n x_list\n)\n\n\n\n# Split edhec into years\n\nedhec_years \n<-\n \nsplit\n(\nedhec \n,\n f \n=\n \n\"years\"\n)\n\n\n\n# Use lapply to calculate the cumsum for each year\n\nedhec_ytd \n<-\n \nlapply\n(\nedhec_years\n,\n FUN \n=\n \ncumsum\n)\n\n\n\n# Use do.call to rbind the results\n\nedhec_xts \n<-\n \ndo.call\n(\nrbind\n,\n edhec_ytd\n)\n\n\n\n\n\n\n\nCalculate the rolling standard deviation of a time series\n\n\nRolling windows can be continuous: use \nrollapply\n:\n\n\n1\n2\n3\n4\nrollapply\n(\nx\n,\n \n10\n,\n FUN \n=\n \nmax\n,\n na.rm \n=\n \nTRUE\n)\n\n\n\n# Use rollapply to calculate the rolling 3 period sd of EqMktNeutral\n\neq_monthly \n<-\n rollapply\n(\nEqMktNeutral\n,\n \n3\n,\n FUN \n=\n sd\n)\n\n\n\n\n\n\n\nExtra features of \nxts\n\u00b6\n\n\nIndex, attributes, and time zones\n\n\nAll time is stored in seconds since 1970-01-01. Time via \n.index\n()\n () and \nindex()\n (raw seconds since 1970-01-01).\n\n\nindex()\n returns the time of an \nxts\n object, as a vector of class \nindexClass\n.\n\n\nSys.setenv()\n.\n\n\nClass attributes \u2013 \ntclass\n, \ntzone\n and \ntformat\n\n\nxts\n objects are somewhat tricky when it comes to times. Internally, we have now seen that the index attribute is really a vector of numeric values corresponding to the seconds since the UNIX epoch (1970-01-01).\n\n\nHow these values are displayed on printing and how they are returned to the user when using the \nindex()\n function is dependent on a few key internal attributes.\n\n\nThe information that controls this behavior can be viewed and even changed through a set of accessor functions detailed below:\n\n\n\n\nThe index class using indexClass (e.g. from Date to chron)\n\n\nThe time zone using indexTZ (e.g. from America/Chicago to Europe/London)\n\n\nThe time format to be displayed via indexFormat (e.g. YYYY/MM/DD)\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Get the index class of temps\n\nindexClass\n(\ntemps\n)\n\n\n\n# Get the timezone of temps\n\nindexTZ\n(\ntemps\n)\n\n\n\n# Change the format of the time display\n\nindexFormat\n(\ntemps\n)\n \n<-\n \n\"M/DD/YYYY\"\n\n\n\n# Extract the new index\n\ntime\n(\ntemps\n)\n\n\n\n\n\n\n\nTime zones\n\n\nR provides time zone support in native classes \nPOSIXct\n and \nPOSIXlt\n. xts extends this power to the entire object, allowing you to have multiple timezones across various objects. One very important thing to remember is that some internal operation system functions require a timezone to do date math, and if not set one is chosen for you! Be careful to always set a timezone in your environment to prevent very hard to debug errors when working with dates and times. \n\n\nxts\n provides the function \ntzone\n. This function allows to you to extract, or set timezones. \ntzone\n(\nx\n)\n \n<-\n \n\"Time_Zone\"\n In this exercise you will work with an object called times to practice constructing your own xts objects with custom time zones.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Construct times_xts with TZ set to America/Chicago\n\ntimes_xts \n<-\n xts\n(\n1\n:\n10\n,\n order.by \n=\n times\n,\n tzone \n=\n \n\"America/Chicago\"\n)\n\n\n\n# Change the time zone to Asia/Hong_Kong\n\ntzone\n(\ntimes_xts\n)\n \n<-\n \n\"Asia/Hong_Kong\"\n\n\n\n# Extract the current timezone \n\nindexTZ\n(\ntimes_xts\n)\n\n\n\n\n\n\n\nPeriods, periodicity, and timestamps\n\n\nPeriods (yearly or intradays time index?): \nperiodicity()\n\n\nBroken downtime with \n.index\n*\n, \n.index\n()\n, \n.indexmday\n()\n, \nindexyday()\n, \nindexyear()\n.\n\n\nTimestamps.\n\n\nalign.time()\n round time stamps to another period.\n\n\nmake.index.unique()\n removes observations from duplicate time stamps.\n\n\nDetermining periodicity\n\n\nThe idea of periodicity is pretty simple; with what regularity does your data repeat? For stock market data, you might have hourly prices or maybe daily open-high-low-close bars. For macro economic series, it might be monthly or weekly survey numbers.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Calculate the periodicity of temps\n\nperiodicity\n(\ntemps\n)\n\n\n\n# Calculate the periodicity of edhec\n\nperiodicity\n(\nedhec\n)\n\n\n\n# Convert edhec to yearly\n\nedhec_yearly \n<-\n to.yearly\n(\nedhec\n,\n \n12\n)\n\n\n\n# Calculate the periodicity of edhec_yearly\n\nperiodicity\n(\nedhec_yearly\n)\n\n\n\n\n\n\n\nFind the number of periods in your data\n\n\nOften times it is handy to know not just the range of your time series index, but also have an idea of how many discrete irregular periods this covers. xts provides a set of functions to do just that. If you have a time series, it is now easy to see how many days, weeks or years your data contains.\n\n\nCount: \nnseconds()\n, \nnminutes()\n, \nnhours()\n, etc.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n`\n# Count the months\n\nnmonths\n(\nedhec\n)\n\n\n\n# Count the quarters\n\nnquarters\n(\nedhec\n)\n\n\n\n# Count the years\n\nnyears\n(\nedhec\n)\n  \n\n\n\n\n\n\nSecret index tools\n\n\nNormally you want to access the times you stored. \nindex()\n does this magically for you by using your \nindexClass\n. To get to the raw vector another function is provided, \n.index\n()\n. Note the critical dot before the function name.\n\n\nMore useful than extracting raw seconds is the ability to extract time components similar to the \nPOSIXlt\n class, which mirrors closely the underlying POSIX internal compiled structure \ntm\n. This is provided by a handful of functions such as \n.indexday\n()\n, \n.indexmon\n()\n, \n.indexyear\n()\n and more.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Explore underlying units of temps\n\n\n.i\nndex\n(\ntemps\n)\n \n# in seconds\n\n\n.i\nndexwday\n(\ntemps\n)\n \n# (0 for Sun, 1, 2,..., 6 for Sat) day of the week\n\n\n\n# Create an index using which (Sunday has a value of 0, and Saturday has a value of 6)\n\nindex \n<-\n \nwhich\n(\n.i\nndexwday\n(\ntemps\n)\n \n==\n \n0\n \n|\n \n.i\nndexwday\n(\ntemps\n)\n \n==\n \n6\n)\n\n\n\n# Select the index\n\ntemps\n[\nindex\n]\n\n\n\n\n\n\n\nModifying timestamps\n\n\nDepending on your field you might encounter higher frequency data - think intraday trading intervals, or sensor data from medical equipment.\n\n\nIf you find that you have observations with identical timestamps, it might be useful to perturb or remove these times to allow for uniqueness.\n\n\nOn other ocassions you might find your timestamps a bit too precise. In these instances it might be better to round up to some fixed interval, for example an observation may occur at any point in an hour, but you want to record the latest as of beginning of the next hour.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nmake.index.unique\n(\nx\n,\n eps \n=\n \n1e-4\n)\n  \n# Perturb\n\nmake.index.unique\n(\nx\n,\n drop \n=\n \nTRUE\n)\n \n# Drop duplicates\n\nalign.time\n(\nx\n,\n n \n=\n \n60\n)\n \n# Round to the minute\n\n\n\n# Make z have unique timestamps\n\nmake.index.unique\n(\nz\n,\n eps \n=\n \n1e-4\n)\n\n\n\n# Remove duplicate times in z\n\nmake.index.unique\n(\nz\n,\n drop \n=\n \nTRUE\n)\n\n\n\n# Round observations to the next time\n\nalign.time\n(\nz\n,\n n \n=\n \n3600\n)\n \n# next hour",
            "title": "Time Series in R, The Power of xts and zoo"
        },
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/#first-order-of-business-basic-manipulations",
            "text": "Converting  xts  objects  xts  provides methods to convert all of the major objects you are likely to come across - suitable native R types like matrix, data.frame, and ts are supported, as well as contributed ones such as  timeSeries ,  fts  and of course  zoo .  as.xts  is the workhorse function to do the conversions to  xts , and similar functions will provide the reverse behavior.  1\n2 # using a R dataset  head ( austres )    \n     13067.3 \n     13130.5 \n     13198.4 \n     13254.2 \n     13303.7 \n     13353.9    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Convert austres to an xts object called au \nau  <-  as.xts ( austres )  head ( au ,   5 )  # Convert your xts object (au) into a matrix am \nam  <-   as.matrix ( au )  head ( am ,   5 )  # Convert the original austres into a matrix am2 \nam2  <-   as.matrix ( austres )  head ( am2 ,   5 )    1\n2\n3\n4\n5\n6            [,1]\n1971 Q2 13067.3\n1971 Q3 13130.5\n1971 Q4 13198.4\n1972 Q1 13254.2\n1972 Q2 13303.7    \n     1971 Q2 13067.3 \n     1971 Q3 13130.5 \n     1971 Q4 13198.4 \n     1972 Q1 13254.2 \n     1972 Q2 13303.7     \n     13067.3 \n     13130.5 \n     13198.4 \n     13254.2 \n     13303.7    Importing data  Read raw data from files on disk or the web and convert data to  xts .  Read the same data into a  zoo  object using read.zoo and convert the  zoo  into an  xts  object.  1\n2\n3 # Create dat by reading tmp_file \ndat  <-   as.matrix ( read.csv ( 'tmp_file.csv' ,  sep  =   ',' ,  header  =   TRUE )) \ndat    a b  \n     1/02/2015 1 3 \n     2/03/2015 2 4    1\n2 # Convert dat into xts \nxts ( dat ,  order.by  =   as.Date ( rownames ( dat ),   \"%m%d%Y\" ))    1\n2\n3      a b\n<NA> 1 3\n<NA> 2 4   1\n2\n3 # Read tmp_file using read.zoo and as.xts \ndat_xts  <-  as.xts ( read.zoo ( 'tmp_file.csv' ,  index.column  =   0 ,  sep  =   \",\" ,  format  =   \"%m/%d/%Y\" )) \ndat_xts   1\n2\n3            a b\n2015-01-02 1 3\n2015-02-03 2 4   Exporting  xts  objects  1\n2\n3\n4 head ( sunspots ,   5 )  # Convert sunspots to xts \nsunspots_xts  <-  as.xts ( sunspots )    \n     58 \n     62.6 \n     70 \n     55.7 \n     85   1\n2\n3\n4\n5\n6\n7 # Get the temporary file name \ntmp  <-   tempfile ()  # Write the xts object using zoo to tmp  \nwrite.zoo ( sunspots ,  sep  =   \",\" ,  file  =  tmp ) \n\ntmp   \u2018C:\\Users\\Dell\\AppData\\Local\\Temp\\RtmpEdYcAQ\\file1a706c8c732b\u2019  1\n2\n3\n4\n5\n6\n7 # Read the tmp file  # FUN=as.yearmon will convert strings such as Jan 1749 into a proper time class \nsun  <-  read.zoo ( tmp ,  sep  =   \",\" ,  FUN  =  as.yearmon )  # Convert sun into xts \nsun_xts  <-  as.xts ( sun )  head ( sun_xts ,   5 )    1\n2 Error in eval(expr, envir, enclos): impossible de trouver la fonction \"read.zoo\"\nTraceback:   The ISO8601 Standard  The ISO8601 standard is the internationally recognized and accepted way to represent dates and times. From the biggest to the smallest, left to right, YYYY-MM-DDTHH:MM:SS.  The standard allows for a common format to not only describe dates, but also a way to represent ranges and repeating intervals:   2004 or 2001/2015  201402/03  2014-02-22 08:30:00  T08:00/T09:00   Querying for dates  Date ranges can be extracted from xts objects by simply specifying the period(s) you want using special character strings in your subset.  1\n2\n3\n4\n5\n6\n7\n8\n9 A [ \"201601\" ]           ## Jan 2016 \nA [ \"20160125\" ]         ## Jan 25, 2016 \nA [ \"201203/201212\" ]    ## Feb to Dec 2012  # Select all of 2016 from x \nx_2016  <-  x [ \"2016\" ]  # Select January 2016 to March 22 \njan_march  <-  x [ \"201601/20160322\" ]    Extracting recurring intraday intervals  Most common data \u201cin the wild\u201d is daily. On ocassion you may find yourself working with intraday data, that is date plus times.  You can slice days easily by using special notation in the  i =  argument to the single bracket extraction (i.e.  [i,j] ).  1\n2\n3\n4\n5\n6\n7\n8 # Intraday times for all days \nNYSE [ \"T09:30/T16:00\" ]  # Extract all data between 8AM and 10AM \nmorn_2010  <-  irreg [ \"T8:00/T10:00\" ]  # Extract the observations for January 13th \nmorn_2010 [ \"2010-01-13\" ]    Alternating extraction techniques  Often times you may need to subset an existing time series with a set of Dates, or time-based objects. These might be from  as.Date() , or  as.POSIXct() , or a variety of other classes.  1\n2\n3\n4\n5 # Subset x using the vector dates \nx [ dates ]  # Subset x using dates as POSIXct \nx [ as.POSIXct ( dates )]    Extracting recurring intraday intervals  The most common time series data is daily, intraday data, which contains both dates and times.   1\n2\n3\n4\n5 # Extract all data between 8AM and 10AM \nmorn_2010  <-  irreg [ \"T8:00/T10:00\" ]  # Extract the observations for January 13th \nmorn_2010 [ \"2010-01-13\" ]    Row selection with time objects  Subset an existing time series with a set of Dates, or time-based objects.  1\n2\n3\n4\n5 # Subset x using the vector dates \nx [ dates ]  # Subset x using dates as POSIXct \nx [ as.POSIXct ( dates )]    Update and replace elements  Replace known intervals or observations with an NA, say due to a malfunctioning sensor on a particular day or a set of outliers given a holiday.  1\n2\n3\n4\n5 # Replace all values in x on dates with NA \nx [ dates ]   <-   NA  # Replace dates from 2016-06-09 and on with 0 \nx [ \"20160609/\" ]   <-   0    Find the  first  or  last  period of time  Sometimes you need to locate data by relative time. Instead of using an absolute offset, you describe a position relative in time. A simple example would be something like the  last 3 weeks  of a series, or the  first day of current month .  1\n2\n3\n4\n5\n6\n7\n8 # Create lastweek with using the last 1 week of temps \nlastweek  <-  last ( temps ,   \"1 week\" )  # Print the last 2 observations in lastweek \nlast ( lastweek ,   2 )  # Extract all but the last two days of lastweek \nlast ( lastweek ,   \"-2 day\" )    Combining  first  and  last  1\n2\n3\n4\n5 # Last 3 days of first week \nlast ( first ( Temps ,   '1 week' ), '3 days' )   # Extract the first three days of the second week of temps \nfirst ( last ( first ( temps ,   \"2 week\" ),   \"1 week\" ),   \"3 day\" )    Matrix arithmetic - add, subtract, multiply and divide in time!  When you perform any binary operation using two xts objects, these objects are first aligned using the intersection of the indexes to preserve the point-in-time aspect of your data, assuring that you don\u2019t introduce accidental look ahead (or look behind!) bias into your calculations.  Your options include:   coredata() or as.numeric() (drop one to a matrix or vector).  Manually shift index values - i.e. use lag().  Reindex your data (before or after the calculation).   xts  respects time and will only return the intersection of times when doing various mathematical operations. First option:  1\n2\n3\n4\n5 # Add a and b \na  +  b # Add a with the numeric value of b \na  +   as.numeric ( b )    Math with non-overlapping indexes  This third way involves modifying the two series you want by aligning assuring you have some union of dates - the dates you require in your final output. This makes it possibly to preserve dimensionality of the data:  1\n2\n3\n4\n5\n6\n7 merge ( b ,  index ( a ))  # Add a to b, and fill all missing rows of b with 0 \na  +   merge ( b ,  index ( a ),  fill  =   0 )  # Add a to b and fill NAs with the last observation \na  +   merge ( b ,  index ( a ),  fill  =  na.locf )",
            "title": "First Order of Business - Basic Manipulations"
        },
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/#merging-and-modifying-time-series",
            "text": "Combining  xts  by column with  merge  xts  makes it easy to join data by column and row using a few different functions.    xts  objects must be of identical type (e.g. integer + integer), or be POSIXct dates vector, or be atomic vectors of the same type (e.g. numeric), or be a single NA. It does not work on data.frames with various column types.  One of the most important functions to accomplish this is  merge . It works like a  cbind  or and SQL  join :   inner join (and).  outer join (or).  left join.  right join.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Basic argument use  merge ( a ,  b ,  join  =   \"right\" ,  fill  =   9999 )  # Perform an inner join of a and b  merge ( a ,  b ,  join  =   \"inner\" )  # Perform of a left-join of a and b, fill mising values with 0  merge ( a ,  b ,  join  =   \"left\" ,  fill  =   0 )  # fill = na.locf, fill = NA    Combining  xts  by row with  rbind  Easy to add new rows to your data.  1\n2\n3\n4\n5 # Row bind temps_june30 to temps, assign this to temps2 \ntemps2  <-   rbind ( temps ,  temps_june30 )  # Row bind temps_july17 and temps_july18 to temps2, call this temps3 \ntemps3  <-   rbind ( temps2 ,  temps_july17 ,  temps_july18 )    Fill missing values using last or previous observation  The  na.locf  function takes the last-observation-carried-forward approach.  The  xts  package leverages the power of  zoo  for help with this.  zoo  provides a variety of missing data handling functions which are usable by  xts , and very handy.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 # Last obs. carried forward \nna.locf ( x )                  # Next obs. carried backward \nna.locf ( x ,  fromLast  =   TRUE )  # Fill missing values using the last observation \nna.locf ( temps ,  na.rm  =   TRUE , \n                fromLast  =   TRUE )  # Fill missing values using the next observation \nna.locf ( temps ,  na.rm  =   TRUE )  # maxgap = Inf  # rule = 2,    NA interpolation  From  zoo .  On occassion, a simple carry forward approach to missingness isn\u2019t appropriate. It may be that a series is missing an observation due to a higher frequency sampling than the generating process. You might also encounter an observation that is in error, yet expected to be somewhere between the values of its neighboring observations.  Both of these cases are examples of where interpolation is useful.  1\n2 # Interpolate NAs using linear approximation \nna.approx ( AirPass )    Combine a leading and lagging time series  Another common modification for time series is the ability to lag a series.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 # Your final object \nlag ( x ,  k  =   1 ,  na.pad  =   TRUE )  # k = -1  leading  # k = 1   lagging  # zoo uses the other way around  # k = c(0, 1, 5)  multiple leads or lags  # Create a leading object called lead_x \nlead_x  <-  lag ( x ,  k  =   -1 )  # Create a lagging object called lag_x \nlag_x  <-  lag ( x ,  k  =   1 )  # Merge your three series together and assign to z \nz  <-   merge ( lead_x ,  x ,  lag_x )    Calculate a difference of a series with  diff  Another common operation on time series, typically on those that are non-stationary, is to take a difference of the series.  differences  is the order of the difference (how many times  diff  is called). A simple way to view a single (or  first order ) difference is to see it as  x(t) - x(t-k)  where  k  is the number of lags to go back. Higher order differences are simply the reapplication of a difference to each prior result (like a second derivative or a difference of the difference).  1\n2\n3\n4\n5\n6\n7 diff ( x ,  lag  =   1 ,  difference  =   1 ,  log  =   FALSE ,  na.pad  =   TRUE )  # calculate the first difference of AirPass using lag and subtraction \nAirPass  -  lag ( AirPass ,  k  =   1 )  # calculate the first order 12-month difference if AirPass  diff ( AirPass ,  lag  =   12 ,  differences  =   1 )    What is the key difference in lag between xts and zoo? The  k  argument in  zoo  uses positive values for shifting past observations forward.",
            "title": "Merging and modifying time series"
        },
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/#apply-and-aggregate-by-time",
            "text": "Find intervals by time in  xts  The main function in xts to facilitate this is endpoints. It takes a time series (or a vector of times) and returns the locations of the last observations in each interval.  For example, the code below locates the last observation of each year for the  AirPass  dataset:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 endpoints ( AirPass ,  on  =   \"years\" )  # The argument on supports a variety of periods, including \"years\", \"quarters\", \"months\", as well as intraday intervals such as \"hours\", and \"minutes\"  # on = \"weeks\", k = 2, the result would be the final day of every other week in your data  # Locate the final day of every week \nendpoints ( temps ,  on  =   \"weeks\" )  # Locate the final day of every two weeks \nendpoints ( temps ,  on  =   \"weeks\" ,  k  =   2 )    Apply a function by time period(s)  xts  provides  period.apply , which takes a time series, an  INDEX  of endpoints, and a function to  apply :  1\n2\n3\n4\n5\n6\n7 period.apply ( x ,  INDEX ,  FUN ,   ... )  # Calculate the weekly endpoints \nep  <-  endpoints ( temps ,  on  =   \"weeks\" )  # Now calculate the weekly mean and display the results \nperiod.apply ( temps [,   \"Temp.Mean\" ],  INDEX  =  ep ,  FUN  =   mean )    Using  lapply  and  split  to apply functions on intervals  Often it is useful to physically split your data into disjoint chunks by time and perform some calculation on these periods.  1\n2\n3\n4\n5 # Split temps by week \ntemps_weekly  <-   split ( temps ,  f  =   \"weeks\" )  # Create a list of weekly means  lapply ( X  =  temps_weekly ,  FUN  =   mean )    Selection by  endpoints  vs.  split-lapply-rbind  1\n2\n3\n4\n5\n6\n7 # use the proper combination of split, lapply and rbind.  #T1 <- do.call(rbind, ___(___(Temps, \"weeks\"), function(w) last(w, n=\"___\"))) \nT1  <-   do.call ( rbind ,   lapply ( split ( Temps ,   \"weeks\" ),   function ( w )  last ( w ,  n =   \"1 day\" )))  # now subset Temps using the results of 'endpoints' \nlast_day_of_weeks  <-  endpoints ( Temps ,  on  =   \"weeks\" ) \nT2  <-  Temps [ last_day_of_weeks ]    Convert univariate series to OHLC data  In financial series it is common to find Open-High-Low-Close data calculated over some repeating and regular interval.  Also known as range bars, aggregating a series based on some regular window can make analysis easier amongst series that have varying frequencies. For example, a weekly economic series and a daily stock series can be compared more easily if the daily is converted to weekly.  OHLC means Opening, High, Low, Closing values.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 to.period ( x , \n          period  =   \"months\" ,  \n          k  =   1 ,  \n          indexAt ,  \n          name = NULL , \n          OHLC  =   TRUE , \n           ... )  # Convert USDEUR to weekly  # OHLC = FALSE by default \nUSDEUR_weekly  <-  to.period ( USDEUR ,  period  =   \"weeks\" )  head ( USDEUR )  head ( USDEUR_weekly )  # Convert USDEUR_weekly to monthly \nUSDEUR_monthly  <-  to.period ( USDEUR_weekly ,  period  =   \"months\" )  # Convert USDEUR_monthly to yearly univariate \nUSDEUR_yearly  <-  to.period ( USDEUR_monthly ,  period  =   \"years\" ,  OHLC  =   FALSE )    Convert a series to a lower frequency  Besides converting univariate time series to OHLC series, to.period() also lets you convert OHLC to lower regularized frequency - something like subsampling your data.  For example, when using the shortcut function  to.quarterly()  xts will convert your index to the  yearqtr  class to make periods more obvious.  1\n2\n3\n4\n5\n6 # Convert EqMktNeutral to quarterly OHLC \nmkt_quarterly  <-  to.period ( EqMktNeutral ,  period  =   \"quarters\" )  # Convert EqMktNeutral to quarterly using shortcut function  # Change the base name of each OHLC column to EDHEC.Equity and change the index to \"firstof\" \nmkt_quarterly2  <-  to.quarterly ( EqMktNeutral ,  name  =   \"EDHEC.Equity\" ,  indexAt  =   \"first\" )    Calculate basic rolling value of series by month  Rolling windows can be discrete: use  lapply  with ( cumsum, cumprod, cummax, cummin ) and  split :   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # split-lapply-rbind pattern \n\nx_split  <-   split ( x ,  f  =   \"months\" ) \nx_list  <-   lapply ( x_split ,   cummax ) \nx_list_rbind  <-   do.call ( rbind ,  x_list )  # Split edhec into years \nedhec_years  <-   split ( edhec  ,  f  =   \"years\" )  # Use lapply to calculate the cumsum for each year \nedhec_ytd  <-   lapply ( edhec_years ,  FUN  =   cumsum )  # Use do.call to rbind the results \nedhec_xts  <-   do.call ( rbind ,  edhec_ytd )    Calculate the rolling standard deviation of a time series  Rolling windows can be continuous: use  rollapply :  1\n2\n3\n4 rollapply ( x ,   10 ,  FUN  =   max ,  na.rm  =   TRUE )  # Use rollapply to calculate the rolling 3 period sd of EqMktNeutral \neq_monthly  <-  rollapply ( EqMktNeutral ,   3 ,  FUN  =  sd )",
            "title": "Apply and aggregate by time"
        },
        {
            "location": "/Time+Series+in+R+The+Power+of+xts+and+zoo/#extra-features-of-xts",
            "text": "Index, attributes, and time zones  All time is stored in seconds since 1970-01-01. Time via  .index ()  () and  index()  (raw seconds since 1970-01-01).  index()  returns the time of an  xts  object, as a vector of class  indexClass .  Sys.setenv() .  Class attributes \u2013  tclass ,  tzone  and  tformat  xts  objects are somewhat tricky when it comes to times. Internally, we have now seen that the index attribute is really a vector of numeric values corresponding to the seconds since the UNIX epoch (1970-01-01).  How these values are displayed on printing and how they are returned to the user when using the  index()  function is dependent on a few key internal attributes.  The information that controls this behavior can be viewed and even changed through a set of accessor functions detailed below:   The index class using indexClass (e.g. from Date to chron)  The time zone using indexTZ (e.g. from America/Chicago to Europe/London)  The time format to be displayed via indexFormat (e.g. YYYY/MM/DD)    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Get the index class of temps \nindexClass ( temps )  # Get the timezone of temps \nindexTZ ( temps )  # Change the format of the time display \nindexFormat ( temps )   <-   \"M/DD/YYYY\"  # Extract the new index \ntime ( temps )    Time zones  R provides time zone support in native classes  POSIXct  and  POSIXlt . xts extends this power to the entire object, allowing you to have multiple timezones across various objects. One very important thing to remember is that some internal operation system functions require a timezone to do date math, and if not set one is chosen for you! Be careful to always set a timezone in your environment to prevent very hard to debug errors when working with dates and times.   xts  provides the function  tzone . This function allows to you to extract, or set timezones.  tzone ( x )   <-   \"Time_Zone\"  In this exercise you will work with an object called times to practice constructing your own xts objects with custom time zones.  1\n2\n3\n4\n5\n6\n7\n8 # Construct times_xts with TZ set to America/Chicago \ntimes_xts  <-  xts ( 1 : 10 ,  order.by  =  times ,  tzone  =   \"America/Chicago\" )  # Change the time zone to Asia/Hong_Kong \ntzone ( times_xts )   <-   \"Asia/Hong_Kong\"  # Extract the current timezone  \nindexTZ ( times_xts )    Periods, periodicity, and timestamps  Periods (yearly or intradays time index?):  periodicity()  Broken downtime with  .index * ,  .index () ,  .indexmday () ,  indexyday() ,  indexyear() .  Timestamps.  align.time()  round time stamps to another period.  make.index.unique()  removes observations from duplicate time stamps.  Determining periodicity  The idea of periodicity is pretty simple; with what regularity does your data repeat? For stock market data, you might have hourly prices or maybe daily open-high-low-close bars. For macro economic series, it might be monthly or weekly survey numbers.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Calculate the periodicity of temps \nperiodicity ( temps )  # Calculate the periodicity of edhec \nperiodicity ( edhec )  # Convert edhec to yearly \nedhec_yearly  <-  to.yearly ( edhec ,   12 )  # Calculate the periodicity of edhec_yearly \nperiodicity ( edhec_yearly )    Find the number of periods in your data  Often times it is handy to know not just the range of your time series index, but also have an idea of how many discrete irregular periods this covers. xts provides a set of functions to do just that. If you have a time series, it is now easy to see how many days, weeks or years your data contains.  Count:  nseconds() ,  nminutes() ,  nhours() , etc.  1\n2\n3\n4\n5\n6\n7\n8 ` # Count the months \nnmonths ( edhec )  # Count the quarters \nnquarters ( edhec )  # Count the years \nnyears ( edhec )      Secret index tools  Normally you want to access the times you stored.  index()  does this magically for you by using your  indexClass . To get to the raw vector another function is provided,  .index () . Note the critical dot before the function name.  More useful than extracting raw seconds is the ability to extract time components similar to the  POSIXlt  class, which mirrors closely the underlying POSIX internal compiled structure  tm . This is provided by a handful of functions such as  .indexday () ,  .indexmon () ,  .indexyear ()  and more.  1\n2\n3\n4\n5\n6\n7\n8\n9 # Explore underlying units of temps  .i ndex ( temps )   # in seconds  .i ndexwday ( temps )   # (0 for Sun, 1, 2,..., 6 for Sat) day of the week  # Create an index using which (Sunday has a value of 0, and Saturday has a value of 6) \nindex  <-   which ( .i ndexwday ( temps )   ==   0   |   .i ndexwday ( temps )   ==   6 )  # Select the index \ntemps [ index ]    Modifying timestamps  Depending on your field you might encounter higher frequency data - think intraday trading intervals, or sensor data from medical equipment.  If you find that you have observations with identical timestamps, it might be useful to perturb or remove these times to allow for uniqueness.  On other ocassions you might find your timestamps a bit too precise. In these instances it might be better to round up to some fixed interval, for example an observation may occur at any point in an hour, but you want to record the latest as of beginning of the next hour.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 make.index.unique ( x ,  eps  =   1e-4 )    # Perturb \nmake.index.unique ( x ,  drop  =   TRUE )   # Drop duplicates \nalign.time ( x ,  n  =   60 )   # Round to the minute  # Make z have unique timestamps \nmake.index.unique ( z ,  eps  =   1e-4 )  # Remove duplicate times in z \nmake.index.unique ( z ,  drop  =   TRUE )  # Round observations to the next time \nalign.time ( z ,  n  =   3600 )   # next hour",
            "title": "Extra features of xts"
        },
        {
            "location": "/Parallel computing in R/",
            "text": "Foreword\n\n\nCode snippets and notes.\n\n\n\n\nUse of parallel computing\n\u00b6\n\n\nR uses in-memory calculations. All data need to be processed in the random-access memory (RAM). It is highly efficient and rapid, but can be burdensome as the size of the data increase and the RAM becomes limited. \n\n\nR is a single-threaded program. In the modern multi-core processors, R cannot effectively use all the computing cores. Even though a CPU had 260 computing core, R would only take 1/260 of the computing power!\n\n\nWe can distribute the task on multiple machines (multiple CPU-cache-RAM) and use several parallel single-threaded computations.\n\n\nApplications\n\u00b6\n\n\n\n\nLinear algebra transformations.\n\n\nBootstrapping, Monte Carlo, inferential computations.\n\n\nUsing the \ncaret\n package.\n\n\ncross-validation in machine learning (train-test sets).\n\n\nSVD.\n\n\nPCA.\n\n\nLDA.\n\n\nregressions with a binary or multinomial dependent variables.\n\n\nNeural networks and deep learning.\n\n\n\n\nImplicit mode\n\u00b6\n\n\nIt is the most suitable mode. It requires some investment in hardware and specialized packages.\n\n\nNowadays, it can be done in the cloud with no initial investment. Cloud computing is highly scalable (in terms of capacity and cost).\n\n\nExplicit mode\n\u00b6\n\n\nWe have to deal with more details: partition data, distribute tasks, collect results, etc. Some might prefer this control over the operations.\n\n\nIt can be done with a simple laptop on 4 cores for example. On Linux, with 4 cores, tests demonstrated that computations were 3 times faster than on a single thread. We can also build a cluster by bridging two or more machine together to add up more cores. One machine becomes the master and the other machines become the slave.\n\n\nFunctions\n\u00b6\n\n\nWe can use the \napply\n family functions alone.\n\n\nPackages\n\u00b6\n\n\n\n\ndata.table\n.\n\n\nsnowfall\n.\n\n\nparallel\n, \ndoParallel\n.\n\n\nRmpi\n.\n\n\nforeach\n.\n\n\nMulticore\n on Unix-based OS.\n\n\nRevoScaleR\n with \nMicrosoftML\n (\ndoc\n).\n\n\nand others.\n\n\n\n\nTests\n\u00b6\n\n\nWith the \napply\n functions, the \nparallel\n package, the \nforeach\n and \ndoParallel\n package and \nmore\n.\n\n\nImplementation 1\n\u00b6\n\n\nUsing the \nparallel\n package, then the \ndoParallel\n and \nforeach\n package.\n\n\nUsing a solver function\n.\n\n\nImplementation 2\n\u00b6\n\n\nUsing the \nparallel\n package.\n\n\nRunning a \nbinomial regression\n.",
            "title": "Parallel computing in R"
        },
        {
            "location": "/Parallel computing in R/#applications",
            "text": "Linear algebra transformations.  Bootstrapping, Monte Carlo, inferential computations.  Using the  caret  package.  cross-validation in machine learning (train-test sets).  SVD.  PCA.  LDA.  regressions with a binary or multinomial dependent variables.  Neural networks and deep learning.",
            "title": "Applications"
        },
        {
            "location": "/Parallel computing in R/#implicit-mode",
            "text": "It is the most suitable mode. It requires some investment in hardware and specialized packages.  Nowadays, it can be done in the cloud with no initial investment. Cloud computing is highly scalable (in terms of capacity and cost).",
            "title": "Implicit mode"
        },
        {
            "location": "/Parallel computing in R/#explicit-mode",
            "text": "We have to deal with more details: partition data, distribute tasks, collect results, etc. Some might prefer this control over the operations.  It can be done with a simple laptop on 4 cores for example. On Linux, with 4 cores, tests demonstrated that computations were 3 times faster than on a single thread. We can also build a cluster by bridging two or more machine together to add up more cores. One machine becomes the master and the other machines become the slave.",
            "title": "Explicit mode"
        },
        {
            "location": "/Parallel computing in R/#functions",
            "text": "We can use the  apply  family functions alone.",
            "title": "Functions"
        },
        {
            "location": "/Parallel computing in R/#packages",
            "text": "data.table .  snowfall .  parallel ,  doParallel .  Rmpi .  foreach .  Multicore  on Unix-based OS.  RevoScaleR  with  MicrosoftML  ( doc ).  and others.",
            "title": "Packages"
        },
        {
            "location": "/Parallel computing in R/#tests",
            "text": "With the  apply  functions, the  parallel  package, the  foreach  and  doParallel  package and  more .",
            "title": "Tests"
        },
        {
            "location": "/Parallel computing in R/#implementation-1",
            "text": "Using the  parallel  package, then the  doParallel  and  foreach  package.  Using a solver function .",
            "title": "Implementation 1"
        },
        {
            "location": "/Parallel computing in R/#implementation-2",
            "text": "Using the  parallel  package.  Running a  binomial regression .",
            "title": "Implementation 2"
        },
        {
            "location": "/Tables/",
            "text": "Foreword\n\n\n\n\nOutput options: \u2018pygments\u2019 syntax, the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nMarkdown tables\n\u00b6\n\n\nExample 1\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n|\n \nRight\n \n|\n \nLeft\n \n|\n \nDefault\n \n|\n \nCenter\n \n|\n\n\n|\n------:\n|:-\n----\n|\n---------\n|:-\n-----:\n|\n\n\n|\n   \n12\n  \n|\n  \n12\n  \n|\n    \n12\n   \n|\n    \n12\n  \n|\n\n\n|\n  \n123\n  \n|\n  \n123\n \n|\n   \n123\n   \n|\n   \n123\n  \n|\n\n\n|\n    \n1\n  \n|\n    \n1\n \n|\n     \n1\n   \n|\n     \n1\n  \n|\n\n\n\n:\n \nDemonstration\n \nof\n \npipe\n \ntable\n \nsyntax\n.\n\n\n\n\n\n\n\n\n\nDemonstration of pipe table syntax.\n\n\n\n\n\n\nRight\n\n\nLeft\n\n\nDefault\n\n\nCenter\n\n\n\n\n\n\n\n\n\n\n12\n\n\n12\n\n\n12\n\n\n12\n\n\n\n\n\n\n123\n\n\n123\n\n\n123\n\n\n123\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\n\nExample 2\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n: Sample grid table.\n\n+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| Bananas       | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - tasty            |\n+---------------+---------------+--------------------+\n\n\n\n\n\n\n\n\nSample grid table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFruit\n\n\nPrice\n\n\nAdvantages\n\n\n\n\n\n\n\n\n\n\nBananas\n\n\n$1.34\n\n\n\n\nbuilt-in wrapper\n\n\nbright color\n\n\n\n\n\n\n\n\nOranges\n\n\n$2.10\n\n\n\n\ncures scurvy\n\n\ntasty\n\n\n\n\n\n\n\n\n\n\n\nThe \nxtable\n package\n\u00b6\n\n\nThe output is in HTML.\n\n\nExample 1\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\nlibrary\n(\nxtable\n)\n\n\n\n# given the data in the first row\n\n\nprint\n(\nxtable\n(\noutput\n,\n\n             caption \n=\n \n'A test table'\n,\n \n             align \n=\n \nc\n(\n'l'\n,\n \n'c'\n,\n \n'r'\n)),\n\n      type \n=\n \n'html'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n A test table \n\n\n \n  \n \n 1st header \n \n 2nd header \n  \n\n  \n \n 1st row \n \n Content A \n \n Content B \n \n\n  \n \n 2nd row \n \n Content C \n \n Content D \n \n\n   \n\n\n\nThe \nknitr::kable\n function\n\u00b6\n\n\nThe output is in Markdown.\n\n\nExample 1\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\nlibrary\n(\nknitr\n)\n\n\n\n# given the data in the first row\n\nkable\n(\noutput\n,\n \n      caption \n=\n \n'A test table'\n,\n \n      align \n=\n \nc\n(\n'c'\n,\n \n'r'\n))\n\n\n\n\n\n\n\n\n\nA test table\n\n\n\n\n\n\n\n\n1st header\n\n\n2nd header\n\n\n\n\n\n\n\n\n\n\n1st row\n\n\nContent A\n\n\nContent B\n\n\n\n\n\n\n2nd row\n\n\nContent C\n\n\nContent D\n\n\n\n\n\n\n\n\n\nWe can also write \nknitr::kable()\n without calling \nlibrary(knitr)\n.\n\n\nThe \nhtmlTable\n package\n\u00b6\n\n\nhtmlTable\n on GitHub.\n\n\nExample 1\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\noutput \n<-\n \n  \nmatrix\n(\npaste\n(\n'Content'\n,\n \nLETTERS\n[\n1\n:\n16\n]),\n \n         ncol \n=\n \n4\n,\n byrow \n=\n \nTRUE\n)\n\n\n\nlibrary\n(\nhtmlTable\n)\n\n\nhtmlTable\n(\noutput\n,\n\n          header \n=\n \npaste\n(\nc\n(\n'1st'\n,\n \n'2nd'\n,\n \n'3rd'\n,\n \n'4th'\n),\n \n'header'\n),\n\n          rnames \n=\n \npaste\n(\nc\n(\n'1st'\n,\n \n'2nd'\n,\n \n'3rd'\n,\n \n'4th'\n),\n \n'row'\n),\n\n          rgroup \n=\n \nc\n(\n'Group A'\n,\n \n'Group B'\n),\n\n          n.rgroup \n=\n \nc\n(\n2\n,\n2\n),\n\n          cgroup \n=\n \nc\n(\n'Cgroup 1'\n,\n \n'Cgroup 2&dagger;'\n),\n\n          n.cgroup \n=\n \nc\n(\n2\n,\n2\n),\n \n          caption \n=\n \n'Basic table with both column spanners (groups) and row groups'\n,\n\n          tfoot \n=\n \n'&dagger; A table footer commment'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic table with both column spanners (groups) and row groups\n\n\n\n\n\n\n\n\n\n\n\n\nCgroup 1\n\n\n\n\n\u00a0\n\n\n\n\nCgroup 2\u2020\n\n\n\n\n\n\n\n\n\n\n\n\n1st header\n\n\n\n\n2nd header\n\n\n\n\n\u00a0\n\n\n\n\n3rd header\n\n\n\n\n4th header\n\n\n\n\n\n\n\n\n\n\n\n\nGroup A\n\n\n\n\n\n\n\n\n\u00a0\u00a01st row\n\n\n\n\nContent A\n\n\n\n\nContent B\n\n\n\n\n\u00a0\n\n\n\n\nContent C\n\n\n\n\nContent D\n\n\n\n\n\n\n\n\n\u00a0\u00a02nd row\n\n\n\n\nContent E\n\n\n\n\nContent F\n\n\n\n\n\u00a0\n\n\n\n\nContent G\n\n\n\n\nContent H\n\n\n\n\n\n\n\n\nGroup B\n\n\n\n\n\n\n\n\n\u00a0\u00a03rd row\n\n\n\n\nContent I\n\n\n\n\nContent J\n\n\n\n\n\u00a0\n\n\n\n\nContent K\n\n\n\n\nContent L\n\n\n\n\n\n\n\n\n\u00a0\u00a04th row\n\n\n\n\nContent M\n\n\n\n\nContent N\n\n\n\n\n\u00a0\n\n\n\n\nContent O\n\n\n\n\nContent P\n\n\n\n\n\n\n\n\n\n\n\n\n\u2020 A table footer commment\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nlibrary\n(\nhtmlTable\n)\n\n\n\n# given the data in the first row\n\nhtmlTable\n(\ntxtRound\n(\nmx\n,\n \n1\n),\n \n          cgroup \n=\n cgroup\n,\n\n          n.cgroup \n=\n n.cgroup\n,\n\n          rgroup \n=\n \nc\n(\n'First period'\n,\n \n'Second period'\n,\n \n'Third period'\n),\n\n          n.rgroup \n=\n \nrep\n(\n5\n,\n \n3\n),\n\n          tfoot \n=\n txtMergeLines\n(\n'&Delta;<sub>int</sub> correspnds to the change since start'\n,\n\n                                \n'&Delta;<sub>std</sub> corresponds to the change compared to national average'\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst period\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\nSecond period\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n1.5\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n-1.3\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n1.8\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n1.9\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\nThird period\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n2.0\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n2.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n2.5\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n-1.0\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nlibrary\n(\nhtmlTable\n)\n\n\n\n# given the data in the first row\n\nhtmlTable\n(\ntxtRound\n(\nmx\n,\n \n1\n),\n \n          align \n=\n \n'rrrr|r'\n,\n\n          cgroup \n=\n cgroup\n,\n\n          n.cgroup \n=\n n.cgroup\n,\n\n          rgroup \n=\n \nc\n(\n'First period'\n,\n \n'Second period'\n,\n \n'Third period'\n),\n\n          n.rgroup \n=\n \nrep\n(\n5\n,\n \n3\n),\n\n          tfoot \n=\n txtMergeLines\n(\n'&Delta;<sub>int</sub> correspnds to the change since start'\n,\n\n                                \n'&Delta;<sub>std</sub> corresponds to the change compared to national average'\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst period\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\nSecond period\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n1.5\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n-1.3\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n1.8\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n1.9\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\nThird period\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n2.0\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n2.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n2.5\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n-1.0\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 4\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nlibrary\n(\nhtmlTable\n)\n\n\n\n# given the data in the first row\n\nhtmlTable\n(\ntxtRound\n(\nmx\n,\n \n1\n),\n \n          col.columns \n=\n \nc\n(\nrep\n(\n'#E6E6F0'\n,\n \n4\n),\n\n                          \nrep\n(\n'none'\n,\n \nncol\n(\nmx\n)\n \n-\n \n4\n)),\n\n          align \n=\n \n'rrrr|r'\n,\n\n          cgroup \n=\n cgroup\n,\n\n          n.cgroup \n=\n n.cgroup\n,\n\n          rgroup \n=\n \nc\n(\n'First period'\n,\n \n'Second period'\n,\n \n'Third period'\n),\n\n          n.rgroup \n=\n \nrep\n(\n5\n,\n \n3\n),\n\n                    tfoot \n=\n txtMergeLines\n(\n'&Delta;<sub>int</sub> correspnds to the change since start'\n,\n\n                                          \n'&Delta;<sub>std</sub> corresponds to the change compared to national average'\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst period\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\nSecond period\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n1.5\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n-1.3\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n1.8\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n1.9\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\nThird period\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n2.0\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n2.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n2.5\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n-1.0\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nlibrary\n(\nhtmlTable\n)\n\n\n\n# given the data in the first row\n\nhtmlTable\n(\ntxtRound\n(\nmx\n,\n \n1\n),\n\n          col.rgroup \n=\n \nc\n(\n'none'\n,\n \n'#FFFFCC'\n),\n\n          col.columns \n=\n \nc\n(\nrep\n(\n'#EFEFF0'\n,\n \n4\n),\n\n                          \nrep\n(\n'none'\n,\n \nncol\n(\nmx\n)\n \n-\n \n4\n)),\n\n          align \n=\n \n'rrrr|r'\n,\n\n          cgroup \n=\n cgroup\n,\n\n          n.cgroup \n=\n n.cgroup\n,\n\n          \n# I use the &nbsp; - the no breaking space as I don't want to have a\n\n          \n# row break in the row group. This adds a little space in the table\n\n          \n# when used together with the cspan.rgroup=1.\n\n          rgroup \n=\n \nc\n(\n'1st&nbsp;period'\n,\n \n'2nd&nbsp;period'\n,\n \n'3rd&nbsp;period'\n),\n\n          n.rgroup \n=\n \nrep\n(\n5\n,\n \n3\n),\n\n          tfoot \n=\n txtMergeLines\n(\n'&Delta;<sub>int</sub> correspnds to the change since start'\n,\n\n                                \n'&Delta;<sub>std</sub> corresponds to the change compared to national average'\n),\n\n          cspan.rgroup \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\n1st\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.6\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n2nd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n-1.5\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.8\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n-1.7\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n-1.4\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n1.5\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n1.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n-1.9\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n-1.3\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n1.7\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n1.8\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n1.9\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n-2.0\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n-1.2\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n3rd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n2.0\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n2.1\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n-2.1\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n2.3\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n2.4\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n-1.1\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n2.2\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n2.5\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n-2.2\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n-2.3\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n-1.0\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 6\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nlibrary\n(\nhtmlTable\n)\n\n\n\n# given the data in the first row\n\nhtmlTable\n(\nout_mx\n,\n\n          caption \n=\n \n'Average age in Sweden counties over a period of\n\n\n                     15 years. The Norbotten county is typically known\n\n\n                     for having a negative migration pattern compared to\n\n\n                     Stockholm, while Uppsala has a proportionally large \n\n\n                     population of students.'\n,\n\n          pos.rowlabel \n=\n \n'bottom'\n,\n\n          rowlabel\n=\n'Year'\n,\n \n          col.rgroup \n=\n \nc\n(\n'none'\n,\n \n'#FFFFCC'\n),\n\n          col.columns \n=\n \nc\n(\nrep\n(\n'#EFEFF0'\n,\n \n4\n),\n\n                          \nrep\n(\n'none'\n,\n \nncol\n(\nmx\n)\n \n-\n \n4\n)),\n\n          align \n=\n \n'rrrr|r'\n,\n\n          cgroup \n=\n cgroup\n,\n\n          n.cgroup \n=\n n.cgroup\n,\n\n          rgroup \n=\n \nc\n(\n'1st&nbsp;period'\n,\n \n'2nd&nbsp;period'\n,\n \n'3rd&nbsp;period'\n),\n\n          n.rgroup \n=\n \nrep\n(\n5\n,\n \n3\n),\n\n          tfoot \n=\n txtMergeLines\n(\n'&Delta;<sub>int</sub> correspnds to the change since start'\n,\n\n                                \n'&Delta;<sub>std</sub> corresponds to the change compared to national average'\n),\n\n          cspan.rgroup \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage age in Sweden counties over a period of 15 years. The Norbotten\ncounty is typically known for having a negative migration pattern\ncompared to Stockholm, while Uppsala has a proportionally large\npopulation of students.\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\nYear\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\n1st\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n\n0.8\n\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n\n0.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n\n0.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n\n0.8\n\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n\n1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n\n1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n\n1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n2nd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n\n1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n\n1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n\n1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n\n-1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n\n1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n\n1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n\n-1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n\n1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n\n1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n\n-1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\n3rd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n\n2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n\n2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n\n2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n\n2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n\n2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n\n2.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n\n2.5\n\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n\n-1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nlibrary\n(\nhtmlTable\n)\n\n\n\n# given the data in the first row\n\nhtmlTable\n(\nout_mx\n,\n\n          caption \n=\n \n'Average age in Sweden counties over a period of\n\n\n                     15 years. The Norbotten county is typically known\n\n\n                     for having a negative migration pattern compared to\n\n\n                     Stockholm, while Uppsala has a proportionally large \n\n\n                     population of students.'\n,\n\n          pos.rowlabel \n=\n \n'bottom'\n,\n\n          rowlabel \n=\n \n'Year'\n,\n \n          col.rgroup \n=\n \nc\n(\n'none'\n,\n \n'#FFFFCC'\n),\n\n          col.columns \n=\n \nc\n(\nrep\n(\n'#EFEFF0'\n,\n \n4\n),\n \nrep\n(\n'none'\n,\n \nncol\n(\nmx\n)\n \n-\n \n4\n)),\n\n          align \n=\n \n'rrrr|r'\n,\n\n          cgroup \n=\n cgroup\n,\n\n          n.cgroup \n=\n n.cgroup\n,\n\n          rgroup \n=\n \nc\n(\n'1st&nbsp;period'\n,\n \n'2nd&nbsp;period'\n,\n \n'3rd&nbsp;period'\n),\n\n          n.rgroup \n=\n \nrep\n(\n5\n,\n \n3\n),\n\n          tfoot \n=\n txtMergeLines\n(\n'&Delta;<sub>int</sub> correspnds to the change since start'\n,\n\n                                \n'&Delta;<sub>std</sub> corresponds to the change compared to national average'\n),\n\n          cspan.rgroup \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage age in Sweden counties over a period of 15 years. The Norbotten\ncounty is typically known for having a negative migration pattern\ncompared to Stockholm, while Uppsala has a proportionally large\npopulation of students.\n\n\n\n\n\n\n\n\n\n\n\n\nSweden\n\n\n\n\n\u00a0\n\n\n\n\nNorrbotten county\n\n\n\n\n\u00a0\n\n\n\n\nStockholm county\n\n\n\n\n\u00a0\n\n\n\n\nUppsala county\n\n\n\n\n\n\n\n\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\u00a0\n\n\n\n\nMen\n\n\n\n\n\u00a0\n\n\n\n\nWomen\n\n\n\n\n\n\n\n\nYear\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\u00a0\n\n\n\n\nAge\n\n\n\n\n\u0394\nint\n\n\n\n\n\n\u0394\nstd\n\n\n\n\n\n\n\n\n\n\n\n\n\n1st\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a01999\n\n\n\n\n38.9\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n0.0\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.0\n\n\n\n\n\n0.8\n\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.0\n\n\n\n\n\n0.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.3\n\n\n\n\n0.0\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.2\n\n\n\n\n0.0\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n39.3\n\n\n\n\n0.0\n\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02000\n\n\n\n\n39.0\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n41.6\n\n\n\n\n0.1\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.3\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.3\n\n\n\n\n\n0.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.4\n\n\n\n\n0.1\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n39.4\n\n\n\n\n0.1\n\n\n\n\n\n-2.2\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02001\n\n\n\n\n39.2\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n0.2\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.5\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n42.5\n\n\n\n\n0.6\n\n\n\n\n\n0.8\n\n\n\n\n\n\u00a0\n\n\n\n\n37.5\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.4\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n39.6\n\n\n\n\n0.3\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02002\n\n\n\n\n39.3\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n41.8\n\n\n\n\n0.3\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n0.8\n\n\n\n\n\n1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n42.8\n\n\n\n\n0.9\n\n\n\n\n\n1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n37.6\n\n\n\n\n0.3\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.6\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.6\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n39.7\n\n\n\n\n0.4\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02003\n\n\n\n\n39.4\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n0.4\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.0\n\n\n\n\n\n1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n43.0\n\n\n\n\n1.1\n\n\n\n\n\n1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.7\n\n\n\n\n0.4\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.8\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n39.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\n\n\n\n2nd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02004\n\n\n\n\n39.6\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.2\n\n\n\n\n\n1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n43.1\n\n\n\n\n1.2\n\n\n\n\n\n1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.1\n\n\n\n\n0.9\n\n\n\n\n\n-1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n0.7\n\n\n\n\n\n-2.0\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02005\n\n\n\n\n39.7\n\n\n\n\n0.8\n\n\n\n\n\u00a0\n\n\n\n\n42.0\n\n\n\n\n0.5\n\n\n\n\n\u00a0\n\n\n\n\n41.1\n\n\n\n\n1.4\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n43.4\n\n\n\n\n1.5\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n40.3\n\n\n\n\n0.2\n\n\n\n\n\n-1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n38.3\n\n\n\n\n1.1\n\n\n\n\n\n-1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.8\n\n\n\n\n\n-1.9\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02006\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.3\n\n\n\n\n1.6\n\n\n\n\n\n1.5\n\n\n\n\n\n\u00a0\n\n\n\n\n43.5\n\n\n\n\n1.6\n\n\n\n\n\n1.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n40.2\n\n\n\n\n0.1\n\n\n\n\n\n-1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n38.5\n\n\n\n\n1.3\n\n\n\n\n\n-1.3\n\n\n\n\n\n\u00a0\n\n\n\n\n40.4\n\n\n\n\n1.1\n\n\n\n\n\n-1.7\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02007\n\n\n\n\n39.8\n\n\n\n\n0.9\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.5\n\n\n\n\n1.8\n\n\n\n\n\n1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n43.8\n\n\n\n\n1.9\n\n\n\n\n\n1.7\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n38.6\n\n\n\n\n1.4\n\n\n\n\n\n-1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02008\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.7\n\n\n\n\n2.0\n\n\n\n\n\n1.8\n\n\n\n\n\n\u00a0\n\n\n\n\n44.0\n\n\n\n\n2.1\n\n\n\n\n\n1.9\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.1\n\n\n\n\n0.0\n\n\n\n\n\n-2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n38.7\n\n\n\n\n1.5\n\n\n\n\n\n-1.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.5\n\n\n\n\n1.2\n\n\n\n\n\n-1.6\n\n\n\n\n\n\n\n\n\n3rd\u00a0period\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02009\n\n\n\n\n39.9\n\n\n\n\n1.0\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n41.9\n\n\n\n\n2.2\n\n\n\n\n\n2.0\n\n\n\n\n\n\u00a0\n\n\n\n\n44.2\n\n\n\n\n2.3\n\n\n\n\n\n2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n38.8\n\n\n\n\n1.6\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02010\n\n\n\n\n40.0\n\n\n\n\n1.1\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n0.6\n\n\n\n\n\u00a0\n\n\n\n\n42.1\n\n\n\n\n2.4\n\n\n\n\n\n2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n44.4\n\n\n\n\n2.5\n\n\n\n\n\n2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n37.8\n\n\n\n\n0.5\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n40.0\n\n\n\n\n-0.1\n\n\n\n\n\n-2.1\n\n\n\n\n\n\u00a0\n\n\n\n\n38.9\n\n\n\n\n1.7\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.6\n\n\n\n\n1.3\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02011\n\n\n\n\n40.1\n\n\n\n\n1.2\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.3\n\n\n\n\n2.6\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.5\n\n\n\n\n2.6\n\n\n\n\n\n2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.0\n\n\n\n\n1.8\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.7\n\n\n\n\n1.4\n\n\n\n\n\n-1.5\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02012\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.6\n\n\n\n\n2.7\n\n\n\n\n\n2.4\n\n\n\n\n\n\u00a0\n\n\n\n\n37.9\n\n\n\n\n0.6\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.1\n\n\n\n\n1.9\n\n\n\n\n\n-1.1\n\n\n\n\n\n\u00a0\n\n\n\n\n40.8\n\n\n\n\n1.5\n\n\n\n\n\n-1.4\n\n\n\n\n\n\n\n\n\n\u00a0\u00a02013\n\n\n\n\n40.2\n\n\n\n\n1.3\n\n\n\n\n\u00a0\n\n\n\n\n42.2\n\n\n\n\n0.7\n\n\n\n\n\u00a0\n\n\n\n\n42.4\n\n\n\n\n2.7\n\n\n\n\n\n2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n44.7\n\n\n\n\n2.8\n\n\n\n\n\n2.5\n\n\n\n\n\n\u00a0\n\n\n\n\n38.0\n\n\n\n\n0.7\n\n\n\n\n\n-2.2\n\n\n\n\n\n\u00a0\n\n\n\n\n39.9\n\n\n\n\n-0.2\n\n\n\n\n\n-2.3\n\n\n\n\n\n\u00a0\n\n\n\n\n39.2\n\n\n\n\n2.0\n\n\n\n\n\n-1.0\n\n\n\n\n\n\u00a0\n\n\n\n\n40.9\n\n\n\n\n1.6\n\n\n\n\n\n-1.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u0394\nint\n correspnds to the change since start\n\n\u0394\nstd\n corresponds to the change compared to national average\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe \nztable\n package\n\u00b6\n\n\nThe package can also export to \n\\LaTeX\n\\LaTeX\n.\n\n\nExample 1\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nlibrary\n(\nztable\n)\n\n\n\noptions\n(\nztable.type\n=\n'html'\n)\n\n\n\n# given the data in the first row\n\nzt \n<-\n ztable\n(\nout_mx\n,\n \n             caption \n=\n \n'Average age in Sweden counties over a period of\n\n\n             15 years. The Norbotten county is typically known\n\n\n             for having a negative migration pattern compared to\n\n\n             Stockholm, while Uppsala has a proportionally large \n\n\n             population of students.'\n,\n\n             zebra.type \n=\n \n1\n,\n\n             zebra \n=\n \n'peach'\n,\n\n             align\n=\npaste\n(\nrep\n(\n'r'\n,\n \nncol\n(\nout_mx\n)\n \n+\n \n1\n),\n collapse \n=\n \n''\n))\n\n\n# zt <- addcgroup(zt,\n\n\n#                 cgroup = cgroup,\n\n\n#                 n.cgroup = n.cgroup)\n\n\n# Causes an error:\n\n\n# Error in if (result <= length(vlines)) { : \n\nzt \n<-\n addrgroup\n(\nzt\n,\n \n                rgroup \n=\n \nc\n(\n'1st&nbsp;period'\n,\n \n'2nd&nbsp;period'\n,\n \n'3rd&nbsp;period'\n),\n\n                n.rgroup \n=\n \nrep\n(\n5\n,\n \n3\n))\n\n\n\nprint\n(\nzt\n)",
            "title": "Tables"
        },
        {
            "location": "/Tables/#example-1",
            "text": "1\n2\n3\n4\n5\n6\n7 |   Right   |   Left   |   Default   |   Center   |  | ------: |:- ---- | --------- |:- -----: |  |     12    |    12    |      12     |      12    |  |    123    |    123   |     123     |     123    |  |      1    |      1   |       1     |       1    |  :   Demonstration   of   pipe   table   syntax .     Demonstration of pipe table syntax.    Right  Left  Default  Center      12  12  12  12    123  123  123  123    1  1  1  1",
            "title": "Example 1"
        },
        {
            "location": "/Tables/#example-2",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 : Sample grid table.\n\n+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| Bananas       | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - tasty            |\n+---------------+---------------+--------------------+    Sample grid table.         Fruit  Price  Advantages      Bananas  $1.34   built-in wrapper  bright color     Oranges  $2.10   cures scurvy  tasty",
            "title": "Example 2"
        },
        {
            "location": "/Tables/#the-xtable-package",
            "text": "The output is in HTML.",
            "title": "The xtable package"
        },
        {
            "location": "/Tables/#example-1_1",
            "text": "1\n2\n3\n4\n5\n6\n7 library ( xtable )  # given the data in the first row  print ( xtable ( output , \n             caption  =   'A test table' ,  \n             align  =   c ( 'l' ,   'c' ,   'r' )), \n      type  =   'html' )        A test table           1st header     2nd header     \n      1st row     Content A     Content B    \n      2nd row     Content C     Content D",
            "title": "Example 1"
        },
        {
            "location": "/Tables/#the-knitrkable-function",
            "text": "The output is in Markdown.",
            "title": "The knitr::kable function"
        },
        {
            "location": "/Tables/#example-1_2",
            "text": "1\n2\n3\n4\n5\n6 library ( knitr )  # given the data in the first row \nkable ( output ,  \n      caption  =   'A test table' ,  \n      align  =   c ( 'c' ,   'r' ))     A test table     1st header  2nd header      1st row  Content A  Content B    2nd row  Content C  Content D     We can also write  knitr::kable()  without calling  library(knitr) .",
            "title": "Example 1"
        },
        {
            "location": "/Tables/#the-htmltable-package",
            "text": "htmlTable  on GitHub.",
            "title": "The htmlTable package"
        },
        {
            "location": "/Tables/#example-1_3",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 output  <-  \n   matrix ( paste ( 'Content' ,   LETTERS [ 1 : 16 ]),  \n         ncol  =   4 ,  byrow  =   TRUE )  library ( htmlTable ) \n\nhtmlTable ( output , \n          header  =   paste ( c ( '1st' ,   '2nd' ,   '3rd' ,   '4th' ),   'header' ), \n          rnames  =   paste ( c ( '1st' ,   '2nd' ,   '3rd' ,   '4th' ),   'row' ), \n          rgroup  =   c ( 'Group A' ,   'Group B' ), \n          n.rgroup  =   c ( 2 , 2 ), \n          cgroup  =   c ( 'Cgroup 1' ,   'Cgroup 2&dagger;' ), \n          n.cgroup  =   c ( 2 , 2 ),  \n          caption  =   'Basic table with both column spanners (groups) and row groups' , \n          tfoot  =   '&dagger; A table footer commment' )        \nBasic table with both column spanners (groups) and row groups      \nCgroup 1  \n\u00a0  \nCgroup 2\u2020      \n1st header  \n2nd header  \n\u00a0  \n3rd header  \n4th header      \nGroup A    \n\u00a0\u00a01st row  \nContent A  \nContent B  \n\u00a0  \nContent C  \nContent D    \n\u00a0\u00a02nd row  \nContent E  \nContent F  \n\u00a0  \nContent G  \nContent H    \nGroup B    \n\u00a0\u00a03rd row  \nContent I  \nContent J  \n\u00a0  \nContent K  \nContent L    \n\u00a0\u00a04th row  \nContent M  \nContent N  \n\u00a0  \nContent O  \nContent P      \n\u2020 A table footer commment",
            "title": "Example 1"
        },
        {
            "location": "/Tables/#example-2_1",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 library ( htmlTable )  # given the data in the first row \nhtmlTable ( txtRound ( mx ,   1 ),  \n          cgroup  =  cgroup , \n          n.cgroup  =  n.cgroup , \n          rgroup  =   c ( 'First period' ,   'Second period' ,   'Third period' ), \n          n.rgroup  =   rep ( 5 ,   3 ), \n          tfoot  =  txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , \n                                 '&Delta;<sub>std</sub> corresponds to the change compared to national average' ))          \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen      \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \nFirst period    \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0  \n0.8  \n\u00a0  \n41.9  \n0.0  \n0.4  \n\u00a0  \n37.3  \n0.0  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.4  \n\u00a0  \n37.2  \n0.0  \n-1.7  \n\u00a0  \n39.3  \n0.0  \n-2.2    \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3  \n1.0  \n\u00a0  \n42.2  \n0.3  \n0.6  \n\u00a0  \n37.4  \n0.1  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.5  \n\u00a0  \n37.5  \n0.3  \n-1.5  \n\u00a0  \n39.4  \n0.1  \n-2.2    \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5  \n1.0  \n\u00a0  \n42.5  \n0.6  \n0.8  \n\u00a0  \n37.5  \n0.2  \n-1.7  \n\u00a0  \n40.1  \n0.0  \n-1.6  \n\u00a0  \n37.6  \n0.4  \n-1.6  \n\u00a0  \n39.6  \n0.3  \n-2.1    \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8  \n1.2  \n\u00a0  \n42.8  \n0.9  \n1.0  \n\u00a0  \n37.6  \n0.3  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.6  \n\u00a0  \n37.8  \n0.6  \n-1.5  \n\u00a0  \n39.7  \n0.4  \n-2.1    \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0  \n1.3  \n\u00a0  \n43.0  \n1.1  \n1.1  \n\u00a0  \n37.7  \n0.4  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.7  \n\u00a0  \n38.0  \n0.8  \n-1.4  \n\u00a0  \n39.8  \n0.5  \n-2.1    \nSecond period    \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2  \n1.3  \n\u00a0  \n43.1  \n1.2  \n1.1  \n\u00a0  \n37.8  \n0.5  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.1  \n0.9  \n-1.5  \n\u00a0  \n40.0  \n0.7  \n-2.0    \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4  \n1.4  \n\u00a0  \n43.4  \n1.5  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.3  \n1.1  \n-1.4  \n\u00a0  \n40.1  \n0.8  \n-1.9    \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6  \n1.5  \n\u00a0  \n43.5  \n1.6  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.9  \n\u00a0  \n40.2  \n0.1  \n-1.9  \n\u00a0  \n38.5  \n1.3  \n-1.3  \n\u00a0  \n40.4  \n1.1  \n-1.7    \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8  \n1.7  \n\u00a0  \n43.8  \n1.9  \n1.7  \n\u00a0  \n37.8  \n0.5  \n-2.0  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.6  \n1.4  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0  \n1.8  \n\u00a0  \n44.0  \n2.1  \n1.9  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.7  \n1.5  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \nThird period    \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2  \n2.0  \n\u00a0  \n44.2  \n2.3  \n2.1  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.8  \n1.6  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4  \n2.1  \n\u00a0  \n44.4  \n2.5  \n2.3  \n\u00a0  \n37.8  \n0.5  \n-2.2  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.9  \n1.7  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6  \n2.2  \n\u00a0  \n44.5  \n2.6  \n2.3  \n\u00a0  \n37.9  \n0.6  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.0  \n1.8  \n-1.1  \n\u00a0  \n40.7  \n1.4  \n-1.5    \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.6  \n2.7  \n2.4  \n\u00a0  \n37.9  \n0.6  \n-2.3  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.1  \n1.9  \n-1.1  \n\u00a0  \n40.8  \n1.5  \n-1.4    \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.7  \n2.8  \n2.5  \n\u00a0  \n38.0  \n0.7  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.2  \n2.0  \n-1.0  \n\u00a0  \n40.9  \n1.6  \n-1.3      \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average",
            "title": "Example 2"
        },
        {
            "location": "/Tables/#example-3",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 library ( htmlTable )  # given the data in the first row \nhtmlTable ( txtRound ( mx ,   1 ),  \n          align  =   'rrrr|r' , \n          cgroup  =  cgroup , \n          n.cgroup  =  n.cgroup , \n          rgroup  =   c ( 'First period' ,   'Second period' ,   'Third period' ), \n          n.rgroup  =   rep ( 5 ,   3 ), \n          tfoot  =  txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , \n                                 '&Delta;<sub>std</sub> corresponds to the change compared to national average' ))          \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen      \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \nFirst period    \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0  \n0.8  \n\u00a0  \n41.9  \n0.0  \n0.4  \n\u00a0  \n37.3  \n0.0  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.4  \n\u00a0  \n37.2  \n0.0  \n-1.7  \n\u00a0  \n39.3  \n0.0  \n-2.2    \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3  \n1.0  \n\u00a0  \n42.2  \n0.3  \n0.6  \n\u00a0  \n37.4  \n0.1  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.5  \n\u00a0  \n37.5  \n0.3  \n-1.5  \n\u00a0  \n39.4  \n0.1  \n-2.2    \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5  \n1.0  \n\u00a0  \n42.5  \n0.6  \n0.8  \n\u00a0  \n37.5  \n0.2  \n-1.7  \n\u00a0  \n40.1  \n0.0  \n-1.6  \n\u00a0  \n37.6  \n0.4  \n-1.6  \n\u00a0  \n39.6  \n0.3  \n-2.1    \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8  \n1.2  \n\u00a0  \n42.8  \n0.9  \n1.0  \n\u00a0  \n37.6  \n0.3  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.6  \n\u00a0  \n37.8  \n0.6  \n-1.5  \n\u00a0  \n39.7  \n0.4  \n-2.1    \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0  \n1.3  \n\u00a0  \n43.0  \n1.1  \n1.1  \n\u00a0  \n37.7  \n0.4  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.7  \n\u00a0  \n38.0  \n0.8  \n-1.4  \n\u00a0  \n39.8  \n0.5  \n-2.1    \nSecond period    \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2  \n1.3  \n\u00a0  \n43.1  \n1.2  \n1.1  \n\u00a0  \n37.8  \n0.5  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.1  \n0.9  \n-1.5  \n\u00a0  \n40.0  \n0.7  \n-2.0    \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4  \n1.4  \n\u00a0  \n43.4  \n1.5  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.3  \n1.1  \n-1.4  \n\u00a0  \n40.1  \n0.8  \n-1.9    \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6  \n1.5  \n\u00a0  \n43.5  \n1.6  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.9  \n\u00a0  \n40.2  \n0.1  \n-1.9  \n\u00a0  \n38.5  \n1.3  \n-1.3  \n\u00a0  \n40.4  \n1.1  \n-1.7    \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8  \n1.7  \n\u00a0  \n43.8  \n1.9  \n1.7  \n\u00a0  \n37.8  \n0.5  \n-2.0  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.6  \n1.4  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0  \n1.8  \n\u00a0  \n44.0  \n2.1  \n1.9  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.7  \n1.5  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \nThird period    \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2  \n2.0  \n\u00a0  \n44.2  \n2.3  \n2.1  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.8  \n1.6  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4  \n2.1  \n\u00a0  \n44.4  \n2.5  \n2.3  \n\u00a0  \n37.8  \n0.5  \n-2.2  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.9  \n1.7  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6  \n2.2  \n\u00a0  \n44.5  \n2.6  \n2.3  \n\u00a0  \n37.9  \n0.6  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.0  \n1.8  \n-1.1  \n\u00a0  \n40.7  \n1.4  \n-1.5    \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.6  \n2.7  \n2.4  \n\u00a0  \n37.9  \n0.6  \n-2.3  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.1  \n1.9  \n-1.1  \n\u00a0  \n40.8  \n1.5  \n-1.4    \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.7  \n2.8  \n2.5  \n\u00a0  \n38.0  \n0.7  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.2  \n2.0  \n-1.0  \n\u00a0  \n40.9  \n1.6  \n-1.3      \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average",
            "title": "Example 3"
        },
        {
            "location": "/Tables/#example-4",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 library ( htmlTable )  # given the data in the first row \nhtmlTable ( txtRound ( mx ,   1 ),  \n          col.columns  =   c ( rep ( '#E6E6F0' ,   4 ), \n                           rep ( 'none' ,   ncol ( mx )   -   4 )), \n          align  =   'rrrr|r' , \n          cgroup  =  cgroup , \n          n.cgroup  =  n.cgroup , \n          rgroup  =   c ( 'First period' ,   'Second period' ,   'Third period' ), \n          n.rgroup  =   rep ( 5 ,   3 ), \n                    tfoot  =  txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , \n                                           '&Delta;<sub>std</sub> corresponds to the change compared to national average' ))          \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen      \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \nFirst period    \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0  \n0.8  \n\u00a0  \n41.9  \n0.0  \n0.4  \n\u00a0  \n37.3  \n0.0  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.4  \n\u00a0  \n37.2  \n0.0  \n-1.7  \n\u00a0  \n39.3  \n0.0  \n-2.2    \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3  \n1.0  \n\u00a0  \n42.2  \n0.3  \n0.6  \n\u00a0  \n37.4  \n0.1  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.5  \n\u00a0  \n37.5  \n0.3  \n-1.5  \n\u00a0  \n39.4  \n0.1  \n-2.2    \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5  \n1.0  \n\u00a0  \n42.5  \n0.6  \n0.8  \n\u00a0  \n37.5  \n0.2  \n-1.7  \n\u00a0  \n40.1  \n0.0  \n-1.6  \n\u00a0  \n37.6  \n0.4  \n-1.6  \n\u00a0  \n39.6  \n0.3  \n-2.1    \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8  \n1.2  \n\u00a0  \n42.8  \n0.9  \n1.0  \n\u00a0  \n37.6  \n0.3  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.6  \n\u00a0  \n37.8  \n0.6  \n-1.5  \n\u00a0  \n39.7  \n0.4  \n-2.1    \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0  \n1.3  \n\u00a0  \n43.0  \n1.1  \n1.1  \n\u00a0  \n37.7  \n0.4  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.7  \n\u00a0  \n38.0  \n0.8  \n-1.4  \n\u00a0  \n39.8  \n0.5  \n-2.1    \nSecond period    \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2  \n1.3  \n\u00a0  \n43.1  \n1.2  \n1.1  \n\u00a0  \n37.8  \n0.5  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.1  \n0.9  \n-1.5  \n\u00a0  \n40.0  \n0.7  \n-2.0    \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4  \n1.4  \n\u00a0  \n43.4  \n1.5  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.3  \n1.1  \n-1.4  \n\u00a0  \n40.1  \n0.8  \n-1.9    \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6  \n1.5  \n\u00a0  \n43.5  \n1.6  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.9  \n\u00a0  \n40.2  \n0.1  \n-1.9  \n\u00a0  \n38.5  \n1.3  \n-1.3  \n\u00a0  \n40.4  \n1.1  \n-1.7    \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8  \n1.7  \n\u00a0  \n43.8  \n1.9  \n1.7  \n\u00a0  \n37.8  \n0.5  \n-2.0  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.6  \n1.4  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0  \n1.8  \n\u00a0  \n44.0  \n2.1  \n1.9  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.7  \n1.5  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \nThird period    \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2  \n2.0  \n\u00a0  \n44.2  \n2.3  \n2.1  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.8  \n1.6  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4  \n2.1  \n\u00a0  \n44.4  \n2.5  \n2.3  \n\u00a0  \n37.8  \n0.5  \n-2.2  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.9  \n1.7  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6  \n2.2  \n\u00a0  \n44.5  \n2.6  \n2.3  \n\u00a0  \n37.9  \n0.6  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.0  \n1.8  \n-1.1  \n\u00a0  \n40.7  \n1.4  \n-1.5    \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.6  \n2.7  \n2.4  \n\u00a0  \n37.9  \n0.6  \n-2.3  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.1  \n1.9  \n-1.1  \n\u00a0  \n40.8  \n1.5  \n-1.4    \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.7  \n2.8  \n2.5  \n\u00a0  \n38.0  \n0.7  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.2  \n2.0  \n-1.0  \n\u00a0  \n40.9  \n1.6  \n-1.3      \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average",
            "title": "Example 4"
        },
        {
            "location": "/Tables/#example-5",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 library ( htmlTable )  # given the data in the first row \nhtmlTable ( txtRound ( mx ,   1 ), \n          col.rgroup  =   c ( 'none' ,   '#FFFFCC' ), \n          col.columns  =   c ( rep ( '#EFEFF0' ,   4 ), \n                           rep ( 'none' ,   ncol ( mx )   -   4 )), \n          align  =   'rrrr|r' , \n          cgroup  =  cgroup , \n          n.cgroup  =  n.cgroup , \n           # I use the &nbsp; - the no breaking space as I don't want to have a \n           # row break in the row group. This adds a little space in the table \n           # when used together with the cspan.rgroup=1. \n          rgroup  =   c ( '1st&nbsp;period' ,   '2nd&nbsp;period' ,   '3rd&nbsp;period' ), \n          n.rgroup  =   rep ( 5 ,   3 ), \n          tfoot  =  txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , \n                                 '&Delta;<sub>std</sub> corresponds to the change compared to national average' ), \n          cspan.rgroup  =   1 )          \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen      \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \n1st\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0  \n0.8  \n\u00a0  \n41.9  \n0.0  \n0.4  \n\u00a0  \n37.3  \n0.0  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.4  \n\u00a0  \n37.2  \n0.0  \n-1.7  \n\u00a0  \n39.3  \n0.0  \n-2.2    \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3  \n1.0  \n\u00a0  \n42.2  \n0.3  \n0.6  \n\u00a0  \n37.4  \n0.1  \n-1.6  \n\u00a0  \n40.1  \n0.0  \n-1.5  \n\u00a0  \n37.5  \n0.3  \n-1.5  \n\u00a0  \n39.4  \n0.1  \n-2.2    \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5  \n1.0  \n\u00a0  \n42.5  \n0.6  \n0.8  \n\u00a0  \n37.5  \n0.2  \n-1.7  \n\u00a0  \n40.1  \n0.0  \n-1.6  \n\u00a0  \n37.6  \n0.4  \n-1.6  \n\u00a0  \n39.6  \n0.3  \n-2.1    \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8  \n1.2  \n\u00a0  \n42.8  \n0.9  \n1.0  \n\u00a0  \n37.6  \n0.3  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.6  \n\u00a0  \n37.8  \n0.6  \n-1.5  \n\u00a0  \n39.7  \n0.4  \n-2.1    \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0  \n1.3  \n\u00a0  \n43.0  \n1.1  \n1.1  \n\u00a0  \n37.7  \n0.4  \n-1.7  \n\u00a0  \n40.2  \n0.1  \n-1.7  \n\u00a0  \n38.0  \n0.8  \n-1.4  \n\u00a0  \n39.8  \n0.5  \n-2.1    \n2nd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2  \n1.3  \n\u00a0  \n43.1  \n1.2  \n1.1  \n\u00a0  \n37.8  \n0.5  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.1  \n0.9  \n-1.5  \n\u00a0  \n40.0  \n0.7  \n-2.0    \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4  \n1.4  \n\u00a0  \n43.4  \n1.5  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.8  \n\u00a0  \n40.3  \n0.2  \n-1.7  \n\u00a0  \n38.3  \n1.1  \n-1.4  \n\u00a0  \n40.1  \n0.8  \n-1.9    \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6  \n1.5  \n\u00a0  \n43.5  \n1.6  \n1.4  \n\u00a0  \n37.9  \n0.6  \n-1.9  \n\u00a0  \n40.2  \n0.1  \n-1.9  \n\u00a0  \n38.5  \n1.3  \n-1.3  \n\u00a0  \n40.4  \n1.1  \n-1.7    \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8  \n1.7  \n\u00a0  \n43.8  \n1.9  \n1.7  \n\u00a0  \n37.8  \n0.5  \n-2.0  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.6  \n1.4  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0  \n1.8  \n\u00a0  \n44.0  \n2.1  \n1.9  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.1  \n0.0  \n-2.0  \n\u00a0  \n38.7  \n1.5  \n-1.2  \n\u00a0  \n40.5  \n1.2  \n-1.6    \n3rd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2  \n2.0  \n\u00a0  \n44.2  \n2.3  \n2.1  \n\u00a0  \n37.8  \n0.5  \n-2.1  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.8  \n1.6  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4  \n2.1  \n\u00a0  \n44.4  \n2.5  \n2.3  \n\u00a0  \n37.8  \n0.5  \n-2.2  \n\u00a0  \n40.0  \n-0.1  \n-2.1  \n\u00a0  \n38.9  \n1.7  \n-1.1  \n\u00a0  \n40.6  \n1.3  \n-1.5    \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6  \n2.2  \n\u00a0  \n44.5  \n2.6  \n2.3  \n\u00a0  \n37.9  \n0.6  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.0  \n1.8  \n-1.1  \n\u00a0  \n40.7  \n1.4  \n-1.5    \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.6  \n2.7  \n2.4  \n\u00a0  \n37.9  \n0.6  \n-2.3  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.1  \n1.9  \n-1.1  \n\u00a0  \n40.8  \n1.5  \n-1.4    \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7  \n2.2  \n\u00a0  \n44.7  \n2.8  \n2.5  \n\u00a0  \n38.0  \n0.7  \n-2.2  \n\u00a0  \n39.9  \n-0.2  \n-2.3  \n\u00a0  \n39.2  \n2.0  \n-1.0  \n\u00a0  \n40.9  \n1.6  \n-1.3      \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average",
            "title": "Example 5"
        },
        {
            "location": "/Tables/#example-6",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 library ( htmlTable )  # given the data in the first row \nhtmlTable ( out_mx , \n          caption  =   'Average age in Sweden counties over a period of                       15 years. The Norbotten county is typically known                       for having a negative migration pattern compared to                       Stockholm, while Uppsala has a proportionally large                        population of students.' , \n          pos.rowlabel  =   'bottom' , \n          rowlabel = 'Year' ,  \n          col.rgroup  =   c ( 'none' ,   '#FFFFCC' ), \n          col.columns  =   c ( rep ( '#EFEFF0' ,   4 ), \n                           rep ( 'none' ,   ncol ( mx )   -   4 )), \n          align  =   'rrrr|r' , \n          cgroup  =  cgroup , \n          n.cgroup  =  n.cgroup , \n          rgroup  =   c ( '1st&nbsp;period' ,   '2nd&nbsp;period' ,   '3rd&nbsp;period' ), \n          n.rgroup  =   rep ( 5 ,   3 ), \n          tfoot  =  txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , \n                                 '&Delta;<sub>std</sub> corresponds to the change compared to national average' ), \n          cspan.rgroup  =   1 )        \nAverage age in Sweden counties over a period of 15 years. The Norbotten\ncounty is typically known for having a negative migration pattern\ncompared to Stockholm, while Uppsala has a proportionally large\npopulation of students.      \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen    \nYear  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \n1st\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0   0.8   \n\u00a0  \n41.9  \n0.0   0.4   \n\u00a0  \n37.3  \n0.0   -1.6   \n\u00a0  \n40.1  \n0.0   -1.4   \n\u00a0  \n37.2  \n0.0   -1.7   \n\u00a0  \n39.3  \n0.0   -2.2     \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3   1.0   \n\u00a0  \n42.2  \n0.3   0.6   \n\u00a0  \n37.4  \n0.1   -1.6   \n\u00a0  \n40.1  \n0.0   -1.5   \n\u00a0  \n37.5  \n0.3   -1.5   \n\u00a0  \n39.4  \n0.1   -2.2     \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5   1.0   \n\u00a0  \n42.5  \n0.6   0.8   \n\u00a0  \n37.5  \n0.2   -1.7   \n\u00a0  \n40.1  \n0.0   -1.6   \n\u00a0  \n37.6  \n0.4   -1.6   \n\u00a0  \n39.6  \n0.3   -2.1     \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8   1.2   \n\u00a0  \n42.8  \n0.9   1.0   \n\u00a0  \n37.6  \n0.3   -1.7   \n\u00a0  \n40.2  \n0.1   -1.6   \n\u00a0  \n37.8  \n0.6   -1.5   \n\u00a0  \n39.7  \n0.4   -2.1     \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0   1.3   \n\u00a0  \n43.0  \n1.1   1.1   \n\u00a0  \n37.7  \n0.4   -1.7   \n\u00a0  \n40.2  \n0.1   -1.7   \n\u00a0  \n38.0  \n0.8   -1.4   \n\u00a0  \n39.8  \n0.5   -2.1     \n2nd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2   1.3   \n\u00a0  \n43.1  \n1.2   1.1   \n\u00a0  \n37.8  \n0.5   -1.8   \n\u00a0  \n40.3  \n0.2   -1.7   \n\u00a0  \n38.1  \n0.9   -1.5   \n\u00a0  \n40.0  \n0.7   -2.0     \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4   1.4   \n\u00a0  \n43.4  \n1.5   1.4   \n\u00a0  \n37.9  \n0.6   -1.8   \n\u00a0  \n40.3  \n0.2   -1.7   \n\u00a0  \n38.3  \n1.1   -1.4   \n\u00a0  \n40.1  \n0.8   -1.9     \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6   1.5   \n\u00a0  \n43.5  \n1.6   1.4   \n\u00a0  \n37.9  \n0.6   -1.9   \n\u00a0  \n40.2  \n0.1   -1.9   \n\u00a0  \n38.5  \n1.3   -1.3   \n\u00a0  \n40.4  \n1.1   -1.7     \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8   1.7   \n\u00a0  \n43.8  \n1.9   1.7   \n\u00a0  \n37.8  \n0.5   -2.0   \n\u00a0  \n40.1  \n0.0   -2.0   \n\u00a0  \n38.6  \n1.4   -1.2   \n\u00a0  \n40.5  \n1.2   -1.6     \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0   1.8   \n\u00a0  \n44.0  \n2.1   1.9   \n\u00a0  \n37.8  \n0.5   -2.1   \n\u00a0  \n40.1  \n0.0   -2.0   \n\u00a0  \n38.7  \n1.5   -1.2   \n\u00a0  \n40.5  \n1.2   -1.6     \n3rd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2   2.0   \n\u00a0  \n44.2  \n2.3   2.1   \n\u00a0  \n37.8  \n0.5   -2.1   \n\u00a0  \n40.0  \n-0.1   -2.1   \n\u00a0  \n38.8  \n1.6   -1.1   \n\u00a0  \n40.6  \n1.3   -1.5     \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4   2.1   \n\u00a0  \n44.4  \n2.5   2.3   \n\u00a0  \n37.8  \n0.5   -2.2   \n\u00a0  \n40.0  \n-0.1   -2.1   \n\u00a0  \n38.9  \n1.7   -1.1   \n\u00a0  \n40.6  \n1.3   -1.5     \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6   2.2   \n\u00a0  \n44.5  \n2.6   2.3   \n\u00a0  \n37.9  \n0.6   -2.2   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.0  \n1.8   -1.1   \n\u00a0  \n40.7  \n1.4   -1.5     \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7   2.2   \n\u00a0  \n44.6  \n2.7   2.4   \n\u00a0  \n37.9  \n0.6   -2.3   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.1  \n1.9   -1.1   \n\u00a0  \n40.8  \n1.5   -1.4     \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7   2.2   \n\u00a0  \n44.7  \n2.8   2.5   \n\u00a0  \n38.0  \n0.7   -2.2   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.2  \n2.0   -1.0   \n\u00a0  \n40.9  \n1.6   -1.3       \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average",
            "title": "Example 6"
        },
        {
            "location": "/Tables/#example-7",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 library ( htmlTable )  # given the data in the first row \nhtmlTable ( out_mx , \n          caption  =   'Average age in Sweden counties over a period of                       15 years. The Norbotten county is typically known                       for having a negative migration pattern compared to                       Stockholm, while Uppsala has a proportionally large                        population of students.' , \n          pos.rowlabel  =   'bottom' , \n          rowlabel  =   'Year' ,  \n          col.rgroup  =   c ( 'none' ,   '#FFFFCC' ), \n          col.columns  =   c ( rep ( '#EFEFF0' ,   4 ),   rep ( 'none' ,   ncol ( mx )   -   4 )), \n          align  =   'rrrr|r' , \n          cgroup  =  cgroup , \n          n.cgroup  =  n.cgroup , \n          rgroup  =   c ( '1st&nbsp;period' ,   '2nd&nbsp;period' ,   '3rd&nbsp;period' ), \n          n.rgroup  =   rep ( 5 ,   3 ), \n          tfoot  =  txtMergeLines ( '&Delta;<sub>int</sub> correspnds to the change since start' , \n                                 '&Delta;<sub>std</sub> corresponds to the change compared to national average' ), \n          cspan.rgroup  =   1 )        \nAverage age in Sweden counties over a period of 15 years. The Norbotten\ncounty is typically known for having a negative migration pattern\ncompared to Stockholm, while Uppsala has a proportionally large\npopulation of students.      \nSweden  \n\u00a0  \nNorrbotten county  \n\u00a0  \nStockholm county  \n\u00a0  \nUppsala county      \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen  \n\u00a0  \nMen  \n\u00a0  \nWomen    \nYear  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std   \n\u00a0  \nAge  \n\u0394 int   \n\u0394 std       \n1st\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a01999  \n38.9  \n0.0  \n\u00a0  \n41.5  \n0.0  \n\u00a0  \n39.7  \n0.0   0.8   \n\u00a0  \n41.9  \n0.0   0.4   \n\u00a0  \n37.3  \n0.0   -1.6   \n\u00a0  \n40.1  \n0.0   -1.4   \n\u00a0  \n37.2  \n0.0   -1.7   \n\u00a0  \n39.3  \n0.0   -2.2     \n\u00a0\u00a02000  \n39.0  \n0.1  \n\u00a0  \n41.6  \n0.1  \n\u00a0  \n40.0  \n0.3   1.0   \n\u00a0  \n42.2  \n0.3   0.6   \n\u00a0  \n37.4  \n0.1   -1.6   \n\u00a0  \n40.1  \n0.0   -1.5   \n\u00a0  \n37.5  \n0.3   -1.5   \n\u00a0  \n39.4  \n0.1   -2.2     \n\u00a0\u00a02001  \n39.2  \n0.3  \n\u00a0  \n41.7  \n0.2  \n\u00a0  \n40.2  \n0.5   1.0   \n\u00a0  \n42.5  \n0.6   0.8   \n\u00a0  \n37.5  \n0.2   -1.7   \n\u00a0  \n40.1  \n0.0   -1.6   \n\u00a0  \n37.6  \n0.4   -1.6   \n\u00a0  \n39.6  \n0.3   -2.1     \n\u00a0\u00a02002  \n39.3  \n0.4  \n\u00a0  \n41.8  \n0.3  \n\u00a0  \n40.5  \n0.8   1.2   \n\u00a0  \n42.8  \n0.9   1.0   \n\u00a0  \n37.6  \n0.3   -1.7   \n\u00a0  \n40.2  \n0.1   -1.6   \n\u00a0  \n37.8  \n0.6   -1.5   \n\u00a0  \n39.7  \n0.4   -2.1     \n\u00a0\u00a02003  \n39.4  \n0.5  \n\u00a0  \n41.9  \n0.4  \n\u00a0  \n40.7  \n1.0   1.3   \n\u00a0  \n43.0  \n1.1   1.1   \n\u00a0  \n37.7  \n0.4   -1.7   \n\u00a0  \n40.2  \n0.1   -1.7   \n\u00a0  \n38.0  \n0.8   -1.4   \n\u00a0  \n39.8  \n0.5   -2.1     \n2nd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02004  \n39.6  \n0.7  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n40.9  \n1.2   1.3   \n\u00a0  \n43.1  \n1.2   1.1   \n\u00a0  \n37.8  \n0.5   -1.8   \n\u00a0  \n40.3  \n0.2   -1.7   \n\u00a0  \n38.1  \n0.9   -1.5   \n\u00a0  \n40.0  \n0.7   -2.0     \n\u00a0\u00a02005  \n39.7  \n0.8  \n\u00a0  \n42.0  \n0.5  \n\u00a0  \n41.1  \n1.4   1.4   \n\u00a0  \n43.4  \n1.5   1.4   \n\u00a0  \n37.9  \n0.6   -1.8   \n\u00a0  \n40.3  \n0.2   -1.7   \n\u00a0  \n38.3  \n1.1   -1.4   \n\u00a0  \n40.1  \n0.8   -1.9     \n\u00a0\u00a02006  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.3  \n1.6   1.5   \n\u00a0  \n43.5  \n1.6   1.4   \n\u00a0  \n37.9  \n0.6   -1.9   \n\u00a0  \n40.2  \n0.1   -1.9   \n\u00a0  \n38.5  \n1.3   -1.3   \n\u00a0  \n40.4  \n1.1   -1.7     \n\u00a0\u00a02007  \n39.8  \n0.9  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.5  \n1.8   1.7   \n\u00a0  \n43.8  \n1.9   1.7   \n\u00a0  \n37.8  \n0.5   -2.0   \n\u00a0  \n40.1  \n0.0   -2.0   \n\u00a0  \n38.6  \n1.4   -1.2   \n\u00a0  \n40.5  \n1.2   -1.6     \n\u00a0\u00a02008  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.7  \n2.0   1.8   \n\u00a0  \n44.0  \n2.1   1.9   \n\u00a0  \n37.8  \n0.5   -2.1   \n\u00a0  \n40.1  \n0.0   -2.0   \n\u00a0  \n38.7  \n1.5   -1.2   \n\u00a0  \n40.5  \n1.2   -1.6     \n3rd\u00a0period      \n\u00a0      \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0        \n\u00a0          \n\u00a0\u00a02009  \n39.9  \n1.0  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n41.9  \n2.2   2.0   \n\u00a0  \n44.2  \n2.3   2.1   \n\u00a0  \n37.8  \n0.5   -2.1   \n\u00a0  \n40.0  \n-0.1   -2.1   \n\u00a0  \n38.8  \n1.6   -1.1   \n\u00a0  \n40.6  \n1.3   -1.5     \n\u00a0\u00a02010  \n40.0  \n1.1  \n\u00a0  \n42.1  \n0.6  \n\u00a0  \n42.1  \n2.4   2.1   \n\u00a0  \n44.4  \n2.5   2.3   \n\u00a0  \n37.8  \n0.5   -2.2   \n\u00a0  \n40.0  \n-0.1   -2.1   \n\u00a0  \n38.9  \n1.7   -1.1   \n\u00a0  \n40.6  \n1.3   -1.5     \n\u00a0\u00a02011  \n40.1  \n1.2  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.3  \n2.6   2.2   \n\u00a0  \n44.5  \n2.6   2.3   \n\u00a0  \n37.9  \n0.6   -2.2   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.0  \n1.8   -1.1   \n\u00a0  \n40.7  \n1.4   -1.5     \n\u00a0\u00a02012  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7   2.2   \n\u00a0  \n44.6  \n2.7   2.4   \n\u00a0  \n37.9  \n0.6   -2.3   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.1  \n1.9   -1.1   \n\u00a0  \n40.8  \n1.5   -1.4     \n\u00a0\u00a02013  \n40.2  \n1.3  \n\u00a0  \n42.2  \n0.7  \n\u00a0  \n42.4  \n2.7   2.2   \n\u00a0  \n44.7  \n2.8   2.5   \n\u00a0  \n38.0  \n0.7   -2.2   \n\u00a0  \n39.9  \n-0.2   -2.3   \n\u00a0  \n39.2  \n2.0   -1.0   \n\u00a0  \n40.9  \n1.6   -1.3       \n\u0394 int  correspnds to the change since start \n\u0394 std  corresponds to the change compared to national average",
            "title": "Example 7"
        },
        {
            "location": "/Tables/#the-ztable-package",
            "text": "The package can also export to  \\LaTeX \\LaTeX .",
            "title": "The ztable package"
        },
        {
            "location": "/Tables/#example-1_4",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 library ( ztable )  options ( ztable.type = 'html' )  # given the data in the first row \nzt  <-  ztable ( out_mx ,  \n             caption  =   'Average age in Sweden counties over a period of               15 years. The Norbotten county is typically known               for having a negative migration pattern compared to               Stockholm, while Uppsala has a proportionally large                population of students.' , \n             zebra.type  =   1 , \n             zebra  =   'peach' , \n             align = paste ( rep ( 'r' ,   ncol ( out_mx )   +   1 ),  collapse  =   '' ))  # zt <- addcgroup(zt,  #                 cgroup = cgroup,  #                 n.cgroup = n.cgroup)  # Causes an error:  # Error in if (result <= length(vlines)) { :  \nzt  <-  addrgroup ( zt ,  \n                rgroup  =   c ( '1st&nbsp;period' ,   '2nd&nbsp;period' ,   '3rd&nbsp;period' ), \n                n.rgroup  =   rep ( 5 ,   3 ))  print ( zt )",
            "title": "Example 1"
        },
        {
            "location": "/embedding/",
            "text": "Foreword\n\n\nNotes, snippets, and results.\n\n\n\n\nEmbedding HTML outputs in general\n\u00b6\n\n\n\n\nWe can embed just about any HTML snippet with \n<iframe>\n or \n<embed>\n. \n\n\nEmbedding static visualizations from packages such as \nggplot2\n, \nggmap\n or \nrworldmap\n, to name a few, can be done with image outputs (.png, .pdf) or HTML outputs. \n\n\nFor htmlwidgets, we can work with HTML outputs, as we demonstrate further down, or with the Shiny server. In both cases, the final results are interactive.\n\n\nSome interactive visualization packages, like \nggvis\n, require the Shiny server to be fully interactive.\n\n\nThe HTML document heading:\n\n\n\n\n1\n2\n3\n4\n5\n---\noutput:\n  html_document:\n    code_folding: hide\n---\n\n\n\n\n\n\nIf the figures are generated with \nfig.width=5, fig.height=5\n; they should fit in a 520x520px box. In some case, we might need more room\u2026\n\n\nEmbedding outputs from\u2026\n\u00b6\n\n\n\u2026the \nleaflet\n package\n\u00b6\n\n\n1\n<iframe\n \nseamless\n \nsrc=\n\"../leaflet_frag.html\"\n \nwidth=\n520px\n \nheight=\n520px\n \n></iframe>\n\n\n\n\n\n\n\n\n\n\n1\n<embed\n \nseamless\n \nsrc=\n\"../leaflet_frag.html\"\n \nwidth=\n520px\n \nheight=\n520px\n \n></embed>\n\n\n\n\n\n\n\n\n\n\u2026the \ndygraphs\n package\n\u00b6\n\n\n1\n<embed\n \nseamless\n \nsrc=\n\"../dygraphs_frag.html\"\n \nwidth=\n520px\n \nheight=\n520px\n \n></embed>\n\n\n\n\n\n\n\n\n\n\u2026the \nplotly\n package\n\u00b6\n\n\n1\n<embed\n \nseamless\n \nsrc=\n\"../plotly_frag.html\"\n \nwidth=\n520px\n \nheight=\n520px\n \n></embed>\n\n\n\n\n\n\n\n\n\n\u2026the \nrbokeh\n package\n\u00b6\n\n\n1\n<embed\n \nseamless\n \nsrc=\n\"../rbokeh_frag.html\"\n \nwidth=\n540px\n \nheight=\n540px\n \n></embed>\n\n\n\n\n\n\n\n\n\n\u2026the \nhighcharters\n package\n\u00b6\n\n\nHighcharts (www.highcharts.com) is a Highsoft software product which is not free for commercial and Governmental use.\n\n\n1\n<embed\n \nseamless\n \nsrc=\n\"../highcharters_frag.html\"\n \nwidth=\n600px\n \nheight=\n600px\n \n></embed>\n\n\n\n\n\n\n\n\n\n\u2026the \ndatatables\n package\n\u00b6\n\n\n1\n<embed\n \nseamless\n \nsrc=\n\"../datatable_frag.html\"\n \nwidth=\n700px\n \nheight=\n600px\n \n></embed>",
            "title": "Embedding HTML into HTML"
        },
        {
            "location": "/embedding/#embedding-outputs-from",
            "text": "",
            "title": "Embedding outputs from..."
        },
        {
            "location": "/embedding/#the-leaflet-package",
            "text": "1 <iframe   seamless   src= \"../leaflet_frag.html\"   width= 520px   height= 520px   ></iframe>     1 <embed   seamless   src= \"../leaflet_frag.html\"   width= 520px   height= 520px   ></embed>",
            "title": "...the leaflet package"
        },
        {
            "location": "/embedding/#the-dygraphs-package",
            "text": "1 <embed   seamless   src= \"../dygraphs_frag.html\"   width= 520px   height= 520px   ></embed>",
            "title": "...the dygraphs package"
        },
        {
            "location": "/embedding/#the-plotly-package",
            "text": "1 <embed   seamless   src= \"../plotly_frag.html\"   width= 520px   height= 520px   ></embed>",
            "title": "...the plotly package"
        },
        {
            "location": "/embedding/#the-rbokeh-package",
            "text": "1 <embed   seamless   src= \"../rbokeh_frag.html\"   width= 540px   height= 540px   ></embed>",
            "title": "...the rbokeh package"
        },
        {
            "location": "/embedding/#the-highcharters-package",
            "text": "Highcharts (www.highcharts.com) is a Highsoft software product which is not free for commercial and Governmental use.  1 <embed   seamless   src= \"../highcharters_frag.html\"   width= 600px   height= 600px   ></embed>",
            "title": "...the highcharters package"
        },
        {
            "location": "/embedding/#the-datatables-package",
            "text": "1 <embed   seamless   src= \"../datatable_frag.html\"   width= 700px   height= 600px   ></embed>",
            "title": "...the datatables package"
        },
        {
            "location": "/embedding2/",
            "text": "Foreword\n\n\nNotes & snippets.\n\n\n\n\nStand-alone .Rdm document\n\u00b6\n\n\nIt is one document with a header; the settings determine the output (HTML, Word, PDF) style and appearance:\n\n\n1\n2\n3\n4\n---\ntitle: \"Document only\"\noutput: html_document\n---\n\n\n\n\n\n\nEmbedding .Rmd sub-documents into a .Rmd document\n\u00b6\n\n\nIt is also possible to create sub-documents to be casted in the main document.\n\n\nThe sub-documents have to headers, no settings. They are not even \nhtml_fragment\n. They simply contain Markdown text strings and code chunks.\n\n\nThe main document is like a stand-alone document. Its settings are applied to itself and its \u2018children\u2019.\n\n\nWithin the main document, we call sub-documents with this code chunk (path and filename):\n\n\n1\n2\n```{r, child=\"sub1.Rmd\"}\n```\n\n\n\n\n\n\nWe can add extra options: \neval=FALSE\n, \necho=FALSE\n, \ninclude=FALSE\n, etc. to bypass the main settings.\n\n\nWe can compile sub-documents to test them.\n\n\nWe compile the main document to wrap up the complete document (\u2018parent + children\u2019).",
            "title": "Embedding Rmd into Rmd"
        },
        {
            "location": "/embedding2/#embedding-rmd-sub-documents-into-a-rmd-document",
            "text": "It is also possible to create sub-documents to be casted in the main document.  The sub-documents have to headers, no settings. They are not even  html_fragment . They simply contain Markdown text strings and code chunks.  The main document is like a stand-alone document. Its settings are applied to itself and its \u2018children\u2019.  Within the main document, we call sub-documents with this code chunk (path and filename):  1\n2 ```{r, child=\"sub1.Rmd\"}\n```   We can add extra options:  eval=FALSE ,  echo=FALSE ,  include=FALSE , etc. to bypass the main settings.  We can compile sub-documents to test them.  We compile the main document to wrap up the complete document (\u2018parent + children\u2019).",
            "title": "Embedding .Rmd sub-documents into a .Rmd document"
        },
        {
            "location": "/Frequentist_Bayesian/",
            "text": "Foreword\n\n\n\n\nNotes.\n\n\n\n\n\n\nSo what does Bayesian statistics mean for A/B testing?\n\n\nFirst, let\u2019s summarize Bayesian and Frequentist approaches, and what the\n\ndifference between them is.\n\n\nFrequentist\n\u00b6\n\n\nUsing a Frequentist method means making predictions on underlying truths of the experiment using only data from the current experiment. We learn frequentist statistics in entry-level statistics courses.\n\n\nA t-test, where we ask, \u201cIs this variation different from the control?\u201d is a basic building block of this approach. The focus is on the observations alone; no other data is used and no judgment. The procedure is objective and based solely on the test data and the assumed model.\n\n\nBayesian\n\u00b6\n\n\nIn Bayesian statistics, past knowledge of similar experiments is encoded into a statistical device known as a prior, and this prior is combined with current experiment data to make a conclusion on the test at hand. So, the biggest distinction is that Bayesian probability specifies that there is some prior probability.\n\n\nA fundamental aspect of Bayesian inference is updating your beliefs in light of new evidence. Essentially, you start out with a prior belief and then update it in light of new evidence.\n\n\nFor example, restaurant owners know that if by 5 p.m. there are 50 reservations, then they can predict there will be around 250 covers for the night. This is a prior and can be updated with new sets of data.\n\n\nConclusion\n\u00b6\n\n\nIn the Bayesian approach, the parameters that we are trying to estimate, are treated as random variables having some known prior distribution. In the Frequentist approach, they are a fixed but unknown; there is no probability associated with it. Random variables are governed by their parameters (mean, variance, etc), and distributions (Gaussian, Poisson,\n\nbinomial, etc). The prior is just the prior belief about these\n\nparameters.\n\n\nIn this way, we can think of the Bayesian approach to treating\n\nprobabilities as degrees of belief, rather than as frequencies generated by some unknown process.\n\n\nIn summary, the difference is that in the Bayesian view, a probability is assigned to a hypothesis. In the Frequentist view, a hypothesis is tested without being assigned a probability.\n\n\n\n\n\n\n\n\n\n\nFrequentist\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\nDefinition of probability\n\n\nLong-run expected frequency in repeated (actual or hypothetical) experiments (law of large numbers).\n\n\nRelative degree of belief in the state of the world.\n\n\n\n\n\n\nPoint estimate\n\n\nMaximum likelihood estimate.\n\n\nMean, mode, or median of the posterior probability distribution.\n\n\n\n\n\n\nConfidence intervals for parameters\n\n\nBased on the likelihood ratio test; i.e., the expected probability distribution of the maximum likelihood estimate over many experiments.\n\n\nCredible intervals based on the posterior probability distribution.\n\n\n\n\n\n\nConfidence intervals for non-parameters\n\n\nBased on likelihood profile/ratio test, or by resampling from the sampling distribution of the parameter.\n\n\nCalculated directly from the distribution of parameters.\n\n\n\n\n\n\nModel selection\n\n\nDiscard terms that are not significantly different from a nested (null) model at a previously set confidence interval level.\n\n\nRetain terms in models, on the argument that processes are not absent simply because they are not statistically significant.\n\n\n\n\n\n\nDifficulties\n\n\nConfidence intervals are confusing (range that will contain the true value in a proportion alpha or repeated experiments); rejection of model terms of non-significance.\n\n\nSubjectivity; need to specify priors.",
            "title": "Frequentist vs. Bayesian"
        },
        {
            "location": "/Frequentist_Bayesian/#bayesian",
            "text": "In Bayesian statistics, past knowledge of similar experiments is encoded into a statistical device known as a prior, and this prior is combined with current experiment data to make a conclusion on the test at hand. So, the biggest distinction is that Bayesian probability specifies that there is some prior probability.  A fundamental aspect of Bayesian inference is updating your beliefs in light of new evidence. Essentially, you start out with a prior belief and then update it in light of new evidence.  For example, restaurant owners know that if by 5 p.m. there are 50 reservations, then they can predict there will be around 250 covers for the night. This is a prior and can be updated with new sets of data.",
            "title": "Bayesian"
        },
        {
            "location": "/Frequentist_Bayesian/#conclusion",
            "text": "In the Bayesian approach, the parameters that we are trying to estimate, are treated as random variables having some known prior distribution. In the Frequentist approach, they are a fixed but unknown; there is no probability associated with it. Random variables are governed by their parameters (mean, variance, etc), and distributions (Gaussian, Poisson, \nbinomial, etc). The prior is just the prior belief about these \nparameters.  In this way, we can think of the Bayesian approach to treating \nprobabilities as degrees of belief, rather than as frequencies generated by some unknown process.  In summary, the difference is that in the Bayesian view, a probability is assigned to a hypothesis. In the Frequentist view, a hypothesis is tested without being assigned a probability.      Frequentist  Bayesian      Definition of probability  Long-run expected frequency in repeated (actual or hypothetical) experiments (law of large numbers).  Relative degree of belief in the state of the world.    Point estimate  Maximum likelihood estimate.  Mean, mode, or median of the posterior probability distribution.    Confidence intervals for parameters  Based on the likelihood ratio test; i.e., the expected probability distribution of the maximum likelihood estimate over many experiments.  Credible intervals based on the posterior probability distribution.    Confidence intervals for non-parameters  Based on likelihood profile/ratio test, or by resampling from the sampling distribution of the parameter.  Calculated directly from the distribution of parameters.    Model selection  Discard terms that are not significantly different from a nested (null) model at a previously set confidence interval level.  Retain terms in models, on the argument that processes are not absent simply because they are not statistically significant.    Difficulties  Confidence intervals are confusing (range that will contain the true value in a proportion alpha or repeated experiments); rejection of model terms of non-significance.  Subjectivity; need to specify priors.",
            "title": "Conclusion"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nVariables\n\u00b6\n\n\nNominal variables in R\n\n\n1\n2\n3\n4\n5\n# Create a numeric vector with the identifiers of the participants of your survey\n\nparticipants_1 \n<-\n \nc\n(\n2\n,\n \n3\n,\n \n5\n,\n \n7\n,\n \n11\n,\n \n13\n,\n \n17\n)\n\n\n\n# Check what type of values R thinks the vector consists of\n\n\nclass\n(\nparticipants_1\n)\n\n\n\n\n\n\n\n1\n## [1] \"numeric\"\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Transform the numeric vector to a factor vector\n\nparticipants_2 \n<-\n \nfactor\n(\nparticipants_1\n)\n\n\n\n# Check what type of values R thinks the vector consists of now\n\n\nclass\n(\nparticipants_2\n)\n\n\n\n\n\n\n\n1\n## [1] \"factor\"\n\n\n\n\n\n\nOrdinal variables in R\n\n\n1\n2\n3\n# Create a vector of temperature observations\n\ntemperature_vector \n<-\n \nc\n(\n'High'\n,\n \n'Low'\n,\n \n'High'\n,\n \n'Low'\n,\n \n'Medium'\n)\n\ntemperature_vector\n\n\n\n\n\n\n1\n## [1] \"High\"   \"Low\"    \"High\"   \"Low\"    \"Medium\"\n\n\n\n\n\n\n1\n2\n3\n# Specify that they are ordinal variables with the given levels\n\nfactor_temperature_vector \n<-\n \nfactor\n(\ntemperature_vector\n,\n order \n=\n \nTRUE\n,\n levels \n=\n \nc\n(\n'Low'\n,\n \n'Medium'\n,\n \n'High'\n))\n\nfactor_temperature_vector\n\n\n\n\n\n\n1\n2\n## [1] High   Low    High   Low    Medium\n## Levels: Low < Medium < High\n\n\n\n\n\n\nInterval and Ratio variables in R\n\n\n1\n2\n3\n4\n5\n6\n7\n# Assign to the variable 'longitudes' a vector with the longitudes\n\n\n# This is an interval variable.\n\nlongitudes \n<-\n \nc\n(\n10\n,\n \n20\n,\n \n30\n,\n \n40\n)\n\n\n\n# Assign the times it takes for an athlete to run 100 meters to the variable 'chronos'\n\n\n# This is a ratio variable.\n\nchronos \n<-\n \nc\n(\n10.60\n,\n \n10.12\n,\n \n9.58\n,\n \n11.1\n)\n\n\n\n\n\n\n\nHistograms and Distributions\n\u00b6\n\n\nCreating histograms in R\n\n\n1\n2\n# Print the data set in the console\n\n\nhead\n(\nimpact\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n##   subject condition verbal_memory_baseline visual_memory_baseline\n## 1       1   control                     95                     88\n## 2       2   control                     90                     82\n## 3       3   control                     87                     77\n## 4       4   control                     84                     72\n## 5       5   control                     92                     77\n## 6       6   control                     89                     79\n##   visual.motor_speed_baseline reaction_time_baseline\n## 1                       35.29                   0.42\n## 2                       31.47                   0.63\n## 3                       30.87                   0.56\n## 4                       41.87                   0.66\n## 5                       33.28                   0.56\n## 6                       40.73                   0.81\n##   impulse_control_baseline total_symptom_baseline verbal_memory_retest\n## 1                       11                      0                   97\n## 2                        7                      0                   86\n## 3                        8                      0                   90\n## 4                        7                      0                   85\n## 5                        7                      1                   87\n## 6                        6                      0                   91\n##   visual_memory_retest visual.motor_speed_retest reaction_time_retest\n## 1                   86                     35.61                 0.65\n## 2                   80                     37.01                 0.49\n## 3                   79                     20.15                 0.75\n## 4                   70                     33.26                 0.19\n## 5                   77                     28.34                 0.59\n## 6                   85                     33.47                 0.48\n##   impulse_control_retest total_symptom_retest\n## 1                     10                    0\n## 2                      7                    0\n## 3                      9                    0\n## 4                      8                    0\n## 5                      8                    1\n## 6                      5                    0\n\n\n\n\n\n\n1\n2\n3\n# Use the describe() function to see some summary information per variable\n\n\n#describe(impact)\n\n\nsummary\n(\nimpact\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n##     subject       condition         verbal_memory_baseline\n##  Min.   : 1.00   Length:40          Min.   :75.00         \n##  1st Qu.:10.75   Class :character   1st Qu.:85.00         \n##  Median :20.50   Mode  :character   Median :91.00         \n##  Mean   :20.50                      Mean   :89.75         \n##  3rd Qu.:30.25                      3rd Qu.:95.00         \n##  Max.   :40.00                      Max.   :98.00         \n##  visual_memory_baseline visual.motor_speed_baseline reaction_time_baseline\n##  Min.   :59.00          Min.   :26.29               Min.   :0.4200        \n##  1st Qu.:68.75          1st Qu.:31.59               1st Qu.:0.5675        \n##  Median :75.00          Median :33.50               Median :0.6500        \n##  Mean   :74.88          Mean   :34.03               Mean   :0.6670        \n##  3rd Qu.:81.25          3rd Qu.:36.44               3rd Qu.:0.7325        \n##  Max.   :91.00          Max.   :41.87               Max.   :1.2000        \n##  impulse_control_baseline total_symptom_baseline verbal_memory_retest\n##  Min.   : 2.000           Min.   :0.00           Min.   :59          \n##  1st Qu.: 7.000           1st Qu.:0.00           1st Qu.:74          \n##  Median : 8.500           Median :0.00           Median :85          \n##  Mean   : 8.275           Mean   :0.05           Mean   :82          \n##  3rd Qu.:10.000           3rd Qu.:0.00           3rd Qu.:91          \n##  Max.   :12.000           Max.   :1.00           Max.   :97          \n##  visual_memory_retest visual.motor_speed_retest reaction_time_retest\n##  Min.   :54.00        Min.   :20.15             Min.   :0.1900      \n##  1st Qu.:66.75        1st Qu.:30.33             1st Qu.:0.5575      \n##  Median :72.00        Median :35.15             Median :0.6500      \n##  Mean   :71.90        Mean   :35.83             Mean   :0.6730      \n##  3rd Qu.:79.00        3rd Qu.:39.41             3rd Qu.:0.7325      \n##  Max.   :86.00        Max.   :60.77             Max.   :1.3000      \n##  impulse_control_retest total_symptom_retest\n##  Min.   : 1.00          Min.   : 0.00       \n##  1st Qu.: 5.00          1st Qu.: 0.00       \n##  Median : 7.00          Median : 7.00       \n##  Mean   : 6.75          Mean   :13.88       \n##  3rd Qu.: 9.00          3rd Qu.:27.00       \n##  Max.   :12.00          Max.   :43.00\n\n\n\n\n\n\n1\n2\n3\n# Select the variable 'verbal_memory_baseline' from the 'impact' data.frame and assign it to the variable 'verbal_baseline'\n\nverbal_baseline \n<-\n impact\n$\nverbal_memory_baseline\nverbal_baseline\n\n\n\n\n\n\n1\n2\n##  [1] 95 90 87 84 92 89 78 97 93 90 89 97 79 86 85 85 98 95 96 92 79 85 97\n## [24] 89 75 75 84 93 88 97 93 96 84 89 95 95 97 95 92 95\n\n\n\n\n\n\n1\n2\n# Plot a histogram of the verbal_baseline variable that you have just created\n\nhist\n(\nverbal_baseline\n,\n main \n=\n \n'Distribution of verbal memory baseline scores'\n,\n xlab \n=\n \n'score'\n,\n ylab \n=\n \n'frequency'\n)\n\n\n\n\n\n\n\n\n\nLet us go wine tasting (red wine)\n\n\n1\n2\n3\n4\n5\n# Read in the data set and assign to the object\n\nred_wine_data \n<-\n readWorksheetFromFile\n(\n'A Hands-on Introduction to Statistics with R.xls'\n,\n sheet \n=\n \n'red_wine_data'\n,\n header \n=\n \nTRUE\n,\n startCol \n=\n \n1\n,\n startRow \n=\n \n1\n)\n\n\n\n# This will print the data set in the console\n\n\nhead\n(\nred_wine_data\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   subject condition Ratings\n## 1       1 Australia      77\n## 2       2 Australia      82\n## 3       3 Australia      75\n## 4       4 Australia      92\n## 5       5 Australia      83\n## 6       6 Australia      75\n\n\n\n\n\n\n1\n2\n3\n# Print basic statistical properties of the red_wine_data data.frame. Use the describe() function\n\n\n#describe(red_wine_data)\n\n\nsummary\n(\nred_wine_data\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##     subject       condition            Ratings     \n##  Min.   :  1.0   Length:400         Min.   :39.00  \n##  1st Qu.:100.8   Class :character   1st Qu.:67.00  \n##  Median :200.5   Mode  :character   Median :74.00  \n##  Mean   :200.5                      Mean   :73.94  \n##  3rd Qu.:300.2                      3rd Qu.:81.00  \n##  Max.   :400.0                      Max.   :98.00\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n# Split the data.frame in subsets for each country and assign these subsets to the variables below\n\nred_usa \n<-\n \nsubset\n(\nred_wine_data\n,\n red_wine_data\n$\ncondition \n==\n \n'USA'\n)\n\nred_france \n<-\n \nsubset\n(\nred_wine_data\n,\n red_wine_data\n$\ncondition \n==\n \n'France'\n)\n\nred_australia \n<-\n \nsubset\n(\nred_wine_data\n,\n red_wine_data\n$\ncondition \n==\n \n'Australia'\n)\n\nred_argentina \n<-\n \nsubset\n(\nred_wine_data\n,\n red_wine_data\n$\ncondition \n==\n \n'Argentina'\n)\n\n\n\n# Select only the Ratings variable for each of these subsets and assign them to the variables below\n\nred_ratings_usa \n<-\n red_usa\n$\nRatings\nred_ratings_france \n<-\n red_france\n$\nRatings\nred_ratings_australia \n<-\n red_australia\n$\nRatings\nred_ratings_argentina \n<-\n red_argentina\n$\nRatings\n\n\n## Create a 2 by 2 matrix of histograms\n\n\n# Organize the histograms so that they are structured in a 2 by 2 matrix.\n\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n2\n))\n\n\n\n# Plot four histograms, one for each subject\n\nhist\n(\nred_ratings_usa\n)\n\nhist\n(\nred_ratings_france\n)\n\nhist\n(\nred_ratings_australia\n)\n\nhist\n(\nred_ratings_argentina\n)\n\n\n\n\n\n\n\n\n\nLet us go wine tasting (white wine)\n\n\n1\n2\n3\n4\n5\n# Read in the data set and assign to the object\n\nwhite_wine_data \n<-\n readWorksheetFromFile\n(\n'A Hands-on Introduction to Statistics with R.xls'\n,\n sheet \n=\n \n'white_wine_data'\n,\n header \n=\n \nTRUE\n,\n startCol \n=\n \n1\n,\n startRow \n=\n \n1\n)\n\n\n\n# This will print the data set in the console\n\n\nhead\n(\nwhite_wine_data\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   condition Ratings\n## 1 Australia    85.6\n## 2 Australia    85.6\n## 3 Australia    85.6\n## 4 Australia    85.6\n## 5 Australia    85.6\n## 6 Australia    85.6\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Assign the scores for each country to a variable\n\nwhite_ratings_france \n<-\n \nsubset\n(\nwhite_wine_data\n,\n white_wine_data\n$\ncondition \n==\n \n'France'\n)\n$\nRatings\nwhite_ratings_argentina \n<-\n \nsubset\n(\nwhite_wine_data\n,\n white_wine_data\n$\ncondition \n==\n \n'Argentina'\n)\n$\nRatings\nwhite_ratings_australia \n<-\n \nsubset\n(\nwhite_wine_data\n,\n white_wine_data\n$\ncondition \n==\n \n'Australia'\n)\n$\nRatings\nwhite_ratings_usa \n<-\n \nsubset\n(\nwhite_wine_data\n,\n white_wine_data\n$\ncondition \n==\n \n'USA'\n)\n$\nRatings\n\n\n# Plot a histogram for each of the countries\n\n\n# Organize the histograms so that they are structured in a 2 by 2 matrix.\n\npar\n(\nmfrow \n=\n \nc\n(\n2\n,\n2\n))\n\n\nhist\n(\nwhite_ratings_usa\n,\n main \n=\n \n'USA white ratings'\n,\n xlab \n=\n \n'score'\n)\n\nhist\n(\nwhite_ratings_australia\n,\n main \n=\n \n'Australia white ratings'\n,\n xlab \n=\n \n'score'\n)\n\nhist\n(\nwhite_ratings_argentina\n,\n main \n=\n \n'Argentina white ratings'\n,\n xlab \n=\n \n'score'\n)\n\nhist\n(\nwhite_ratings_france\n,\n main \n=\n \n'France white ratings'\n,\n xlab \n=\n \n'score'\n)\n\n\n\n\n\n\n\n\n\nScales of Measurement\n\u00b6\n\n\nConverting a distribution to Z-scale\n\n\n1\n2\n3\n4\n# Read in the data set and assign to the object\n\nratings_australia \n<-\n readWorksheetFromFile\n(\n'A Hands-on Introduction to Statistics with R.xls'\n,\n sheet \n=\n \n'ratings_australia'\n,\n header \n=\n \nTRUE\n,\n startCol \n=\n \n1\n,\n startRow \n=\n \n1\n)\n\n\nratings_australia \n<-\n \nas.vector\n(\nratings_australia\n$\nratings_australia\n)\n\n\n\n\n\n\n\n1\n2\n# Print the ratings for the Australian red wine\n\nratings_australia\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##   [1] 77 82 75 92 83 75 84 86 85 79 92 84 77 65 89 81 81 88 87 85 87 86 82\n##  [24] 67 85 81 80 71 78 84 91 80 84 81 71 78 78 81 89 86 80 79 86 85 76 76\n##  [47] 84 86 80 87 84 77 83 73 91 95 78 74 85 80 98 81 86 81 76 82 68 91 82\n##  [70] 96 84 76 85 74 72 83 78 81 82 77 77 80 89 70 85 83 88 79 84 83 77 89\n##  [93] 89 86 92 85 72 77 72 78\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Convert these ratings to Z-scores. Use the `scale()` function\n\nz_scores_australia \n<-\n \nscale\n(\nratings_australia\n)\n\n\n\n# Plot both the original data and the scaled data in histograms next to each other\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n2\n))\n\n\n\n# Plot the histogram for the original scores\n\nhist\n(\nratings_australia\n)\n\n\n\n# Plot the histogram for the Z-scores\n\nhist\n(\nz_scores_australia\n)\n\n\n\n\n\n\n\n\n\nMeasures of Central Tendency\n\u00b6\n\n\nThe mean of a Fibonacci sequence\n\n\n1\n2\n3\n4\n5\n6\n# create a vector that contains the Fibonacci elements\n\nfibonacci \n<-\n \nc\n(\n0\n,\n \n1\n,\n \n1\n,\n \n2\n,\n \n3\n,\n \n5\n,\n \n8\n,\n \n13\n)\n \n\n\n# calculate the mean manually. Use the sum() and the length() functions\n\nmean \n<-\n \nsum\n(\nfibonacci\n)\n/\nlength\n(\nfibonacci\n)\n\n\nmean\n\n\n\n\n\n\n\n1\n## [1] 4.125\n\n\n\n\n\n\n1\n2\n3\n# calculate the mean the easy way\n\nmean_check \n<-\n \nmean\n(\nfibonacci\n)\n\nmean_check\n\n\n\n\n\n\n1\n## [1] 4.125\n\n\n\n\n\n\nSetting up histograms\n\n\n1\n2\n3\n4\n# Read in the data set and assign to the object\n\nwine_data \n<-\n readWorksheetFromFile\n(\n'A Hands-on Introduction to Statistics with R.xls'\n,\n sheet \n=\n \n'wine_data'\n,\n header \n=\n \nTRUE\n,\n startCol \n=\n \n1\n,\n startRow \n=\n \n1\n)\n\n\n\nhead\n(\nwine_data\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   condition Ratings\n## 1       Red      77\n## 2       Red      82\n## 3       Red      75\n## 4       Red      92\n## 5       Red      83\n## 6       Red      75\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# create the two subsets\n\nred_wine \n<-\n \nsubset\n(\nwine_data\n,\n wine_data\n$\ncondition \n==\n \n'Red'\n)\n\nwhite_wine \n<-\n \nsubset\n(\nwine_data\n,\n wine_data\n$\ncondition \n==\n \n'White'\n)\n\n\n\n# Plot the histograms of the ratings of both subsets\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n2\n))\n\nhist\n(\nred_wine\n$\nRatings\n,\n main \n=\n \n'Shiraz'\n,\n xlab \n=\n \n'Ratings'\n)\n\nhist\n(\nwhite_wine\n$\nRatings\n,\n main \n=\n \n'Pinot Grigio'\n,\n xlab \n=\n \n'Ratings'\n)\n\n\n\n\n\n\n\n\n\nRobustness to outliers\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# create the outlier and add it to the dataset\n\noutlier \n<-\n \ndata.frame\n(\ncondition \n=\n \n'Red'\n,\n Ratings \n=\n \n0\n)\n\n\nred_wine_extreme \n<-\n \nrbind\n(\nred_wine\n,\n outlier\n)\n\n\n\n# calculate the difference in means and display it afterwards\n\ndiff_means \n<-\n \nmean\n(\nred_wine\n$\nRatings\n)\n \n-\n \nmean\n(\nred_wine_extreme\n$\nRatings\n)\n\n\ndiff_means\n\n\n\n\n\n\n1\n## [1] 0.8093069\n\n\n\n\n\n\n1\n2\n3\n4\n# calculate the difference in medians and display it afterwards\n\ndiff_medians \n<-\n median\n(\nred_wine\n$\nRatings\n)\n \n-\n median\n(\nred_wine_extreme\n$\nRatings\n)\n\n\ndiff_medians\n\n\n\n\n\n\n1\n## [1] 0\n\n\n\n\n\n\nMeasures of Variability\n\u00b6\n\n\nMichael Jordan\u2019s first NBA season - Global overview\n\n\n1\n2\n3\n4\n# Read in the data set and assign to the object\n\ndata_jordan \n<-\n readWorksheetFromFile\n(\n'A Hands-on Introduction to Statistics with R.xls'\n,\n sheet \n=\n \n'data_jordan'\n,\n header \n=\n \nTRUE\n,\n startCol \n=\n \n1\n,\n startRow \n=\n \n1\n)\n\n\n\nhead\n(\ndata_jordan\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   game points\n## 1    1     16\n## 2    2     21\n## 3    3     37\n## 4    4     25\n## 5    5     17\n## 6    6     25\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Make a scatterplot of the data on which a horizontal line with height equal to the mean is drawn.\n\nmean_jordan \n<-\n \nmean\n(\ndata_jordan\n$\npoints\n)\n\nplot\n(\ndata_jordan\n$\ngame\n,\n data_jordan\n$\npoints\n,\nmain \n=\n \n'1st NBA season of Michael Jordan'\n)\n\n\nabline\n(\nh \n=\n mean_jordan\n)\n\n\n\n\n\n\n\n\n\nMichael Jordan\u2019s first NBA season - Calculate the variance manually\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Calculate the differences with respect to the mean \n\ndiff \n<-\n data_jordan\n$\npoints \n-\n \nmean\n(\ndata_jordan\n$\npoints\n)\n\n\n\n# Calculate the squared differences\n\nsquared_diff \n<-\n \ndiff\n^\n2\n\n\n\n# Combine all pieces of the puzzle in order to acquire the variance\n\nvariance \n<-\n \nsum\n(\nsquared_diff\n)\n/\n(\nlength\n(\ndata_jordan\n$\npoints\n)\n \n-\n \n1\n)\n\nvariance\n\n\n\n\n\n\n1\n## [1] 66.73427\n\n\n\n\n\n\n1\n2\n# Compare your result to the correct solution. You can find the correct solution by calculating it with the `var()` function.\n\nvar\n(\ndata_jordan\n$\npoints\n)\n\n\n\n\n\n\n\n1\n## [1] 66.73427",
            "title": "Statistics with R, Course One, Introduction"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/#histograms-and-distributions",
            "text": "Creating histograms in R  1\n2 # Print the data set in the console  head ( impact )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 ##   subject condition verbal_memory_baseline visual_memory_baseline\n## 1       1   control                     95                     88\n## 2       2   control                     90                     82\n## 3       3   control                     87                     77\n## 4       4   control                     84                     72\n## 5       5   control                     92                     77\n## 6       6   control                     89                     79\n##   visual.motor_speed_baseline reaction_time_baseline\n## 1                       35.29                   0.42\n## 2                       31.47                   0.63\n## 3                       30.87                   0.56\n## 4                       41.87                   0.66\n## 5                       33.28                   0.56\n## 6                       40.73                   0.81\n##   impulse_control_baseline total_symptom_baseline verbal_memory_retest\n## 1                       11                      0                   97\n## 2                        7                      0                   86\n## 3                        8                      0                   90\n## 4                        7                      0                   85\n## 5                        7                      1                   87\n## 6                        6                      0                   91\n##   visual_memory_retest visual.motor_speed_retest reaction_time_retest\n## 1                   86                     35.61                 0.65\n## 2                   80                     37.01                 0.49\n## 3                   79                     20.15                 0.75\n## 4                   70                     33.26                 0.19\n## 5                   77                     28.34                 0.59\n## 6                   85                     33.47                 0.48\n##   impulse_control_retest total_symptom_retest\n## 1                     10                    0\n## 2                      7                    0\n## 3                      9                    0\n## 4                      8                    0\n## 5                      8                    1\n## 6                      5                    0   1\n2\n3 # Use the describe() function to see some summary information per variable  #describe(impact)  summary ( impact )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 ##     subject       condition         verbal_memory_baseline\n##  Min.   : 1.00   Length:40          Min.   :75.00         \n##  1st Qu.:10.75   Class :character   1st Qu.:85.00         \n##  Median :20.50   Mode  :character   Median :91.00         \n##  Mean   :20.50                      Mean   :89.75         \n##  3rd Qu.:30.25                      3rd Qu.:95.00         \n##  Max.   :40.00                      Max.   :98.00         \n##  visual_memory_baseline visual.motor_speed_baseline reaction_time_baseline\n##  Min.   :59.00          Min.   :26.29               Min.   :0.4200        \n##  1st Qu.:68.75          1st Qu.:31.59               1st Qu.:0.5675        \n##  Median :75.00          Median :33.50               Median :0.6500        \n##  Mean   :74.88          Mean   :34.03               Mean   :0.6670        \n##  3rd Qu.:81.25          3rd Qu.:36.44               3rd Qu.:0.7325        \n##  Max.   :91.00          Max.   :41.87               Max.   :1.2000        \n##  impulse_control_baseline total_symptom_baseline verbal_memory_retest\n##  Min.   : 2.000           Min.   :0.00           Min.   :59          \n##  1st Qu.: 7.000           1st Qu.:0.00           1st Qu.:74          \n##  Median : 8.500           Median :0.00           Median :85          \n##  Mean   : 8.275           Mean   :0.05           Mean   :82          \n##  3rd Qu.:10.000           3rd Qu.:0.00           3rd Qu.:91          \n##  Max.   :12.000           Max.   :1.00           Max.   :97          \n##  visual_memory_retest visual.motor_speed_retest reaction_time_retest\n##  Min.   :54.00        Min.   :20.15             Min.   :0.1900      \n##  1st Qu.:66.75        1st Qu.:30.33             1st Qu.:0.5575      \n##  Median :72.00        Median :35.15             Median :0.6500      \n##  Mean   :71.90        Mean   :35.83             Mean   :0.6730      \n##  3rd Qu.:79.00        3rd Qu.:39.41             3rd Qu.:0.7325      \n##  Max.   :86.00        Max.   :60.77             Max.   :1.3000      \n##  impulse_control_retest total_symptom_retest\n##  Min.   : 1.00          Min.   : 0.00       \n##  1st Qu.: 5.00          1st Qu.: 0.00       \n##  Median : 7.00          Median : 7.00       \n##  Mean   : 6.75          Mean   :13.88       \n##  3rd Qu.: 9.00          3rd Qu.:27.00       \n##  Max.   :12.00          Max.   :43.00   1\n2\n3 # Select the variable 'verbal_memory_baseline' from the 'impact' data.frame and assign it to the variable 'verbal_baseline' \nverbal_baseline  <-  impact $ verbal_memory_baseline\nverbal_baseline   1\n2 ##  [1] 95 90 87 84 92 89 78 97 93 90 89 97 79 86 85 85 98 95 96 92 79 85 97\n## [24] 89 75 75 84 93 88 97 93 96 84 89 95 95 97 95 92 95   1\n2 # Plot a histogram of the verbal_baseline variable that you have just created \nhist ( verbal_baseline ,  main  =   'Distribution of verbal memory baseline scores' ,  xlab  =   'score' ,  ylab  =   'frequency' )     Let us go wine tasting (red wine)  1\n2\n3\n4\n5 # Read in the data set and assign to the object \nred_wine_data  <-  readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' ,  sheet  =   'red_wine_data' ,  header  =   TRUE ,  startCol  =   1 ,  startRow  =   1 )  # This will print the data set in the console  head ( red_wine_data )    1\n2\n3\n4\n5\n6\n7 ##   subject condition Ratings\n## 1       1 Australia      77\n## 2       2 Australia      82\n## 3       3 Australia      75\n## 4       4 Australia      92\n## 5       5 Australia      83\n## 6       6 Australia      75   1\n2\n3 # Print basic statistical properties of the red_wine_data data.frame. Use the describe() function  #describe(red_wine_data)  summary ( red_wine_data )    1\n2\n3\n4\n5\n6\n7 ##     subject       condition            Ratings     \n##  Min.   :  1.0   Length:400         Min.   :39.00  \n##  1st Qu.:100.8   Class :character   1st Qu.:67.00  \n##  Median :200.5   Mode  :character   Median :74.00  \n##  Mean   :200.5                      Mean   :73.94  \n##  3rd Qu.:300.2                      3rd Qu.:81.00  \n##  Max.   :400.0                      Max.   :98.00    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 # Split the data.frame in subsets for each country and assign these subsets to the variables below \nred_usa  <-   subset ( red_wine_data ,  red_wine_data $ condition  ==   'USA' ) \nred_france  <-   subset ( red_wine_data ,  red_wine_data $ condition  ==   'France' ) \nred_australia  <-   subset ( red_wine_data ,  red_wine_data $ condition  ==   'Australia' ) \nred_argentina  <-   subset ( red_wine_data ,  red_wine_data $ condition  ==   'Argentina' )  # Select only the Ratings variable for each of these subsets and assign them to the variables below \nred_ratings_usa  <-  red_usa $ Ratings\nred_ratings_france  <-  red_france $ Ratings\nred_ratings_australia  <-  red_australia $ Ratings\nred_ratings_argentina  <-  red_argentina $ Ratings ## Create a 2 by 2 matrix of histograms  # Organize the histograms so that they are structured in a 2 by 2 matrix. \npar ( mfrow  =   c ( 2 , 2 ))  # Plot four histograms, one for each subject \nhist ( red_ratings_usa ) \nhist ( red_ratings_france ) \nhist ( red_ratings_australia ) \nhist ( red_ratings_argentina )     Let us go wine tasting (white wine)  1\n2\n3\n4\n5 # Read in the data set and assign to the object \nwhite_wine_data  <-  readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' ,  sheet  =   'white_wine_data' ,  header  =   TRUE ,  startCol  =   1 ,  startRow  =   1 )  # This will print the data set in the console  head ( white_wine_data )    1\n2\n3\n4\n5\n6\n7 ##   condition Ratings\n## 1 Australia    85.6\n## 2 Australia    85.6\n## 3 Australia    85.6\n## 4 Australia    85.6\n## 5 Australia    85.6\n## 6 Australia    85.6    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Assign the scores for each country to a variable \nwhite_ratings_france  <-   subset ( white_wine_data ,  white_wine_data $ condition  ==   'France' ) $ Ratings\nwhite_ratings_argentina  <-   subset ( white_wine_data ,  white_wine_data $ condition  ==   'Argentina' ) $ Ratings\nwhite_ratings_australia  <-   subset ( white_wine_data ,  white_wine_data $ condition  ==   'Australia' ) $ Ratings\nwhite_ratings_usa  <-   subset ( white_wine_data ,  white_wine_data $ condition  ==   'USA' ) $ Ratings # Plot a histogram for each of the countries  # Organize the histograms so that they are structured in a 2 by 2 matrix. \npar ( mfrow  =   c ( 2 , 2 )) \n\nhist ( white_ratings_usa ,  main  =   'USA white ratings' ,  xlab  =   'score' ) \nhist ( white_ratings_australia ,  main  =   'Australia white ratings' ,  xlab  =   'score' ) \nhist ( white_ratings_argentina ,  main  =   'Argentina white ratings' ,  xlab  =   'score' ) \nhist ( white_ratings_france ,  main  =   'France white ratings' ,  xlab  =   'score' )",
            "title": "Histograms and Distributions"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/#scales-of-measurement",
            "text": "Converting a distribution to Z-scale  1\n2\n3\n4 # Read in the data set and assign to the object \nratings_australia  <-  readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' ,  sheet  =   'ratings_australia' ,  header  =   TRUE ,  startCol  =   1 ,  startRow  =   1 ) \n\nratings_australia  <-   as.vector ( ratings_australia $ ratings_australia )    1\n2 # Print the ratings for the Australian red wine \nratings_australia   1\n2\n3\n4\n5 ##   [1] 77 82 75 92 83 75 84 86 85 79 92 84 77 65 89 81 81 88 87 85 87 86 82\n##  [24] 67 85 81 80 71 78 84 91 80 84 81 71 78 78 81 89 86 80 79 86 85 76 76\n##  [47] 84 86 80 87 84 77 83 73 91 95 78 74 85 80 98 81 86 81 76 82 68 91 82\n##  [70] 96 84 76 85 74 72 83 78 81 82 77 77 80 89 70 85 83 88 79 84 83 77 89\n##  [93] 89 86 92 85 72 77 72 78    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Convert these ratings to Z-scores. Use the `scale()` function \nz_scores_australia  <-   scale ( ratings_australia )  # Plot both the original data and the scaled data in histograms next to each other \npar ( mfrow  =   c ( 1 , 2 ))  # Plot the histogram for the original scores \nhist ( ratings_australia )  # Plot the histogram for the Z-scores \nhist ( z_scores_australia )",
            "title": "Scales of Measurement"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/#measures-of-central-tendency",
            "text": "The mean of a Fibonacci sequence  1\n2\n3\n4\n5\n6 # create a vector that contains the Fibonacci elements \nfibonacci  <-   c ( 0 ,   1 ,   1 ,   2 ,   3 ,   5 ,   8 ,   13 )   # calculate the mean manually. Use the sum() and the length() functions \nmean  <-   sum ( fibonacci ) / length ( fibonacci )  mean    1 ## [1] 4.125   1\n2\n3 # calculate the mean the easy way \nmean_check  <-   mean ( fibonacci ) \nmean_check   1 ## [1] 4.125   Setting up histograms  1\n2\n3\n4 # Read in the data set and assign to the object \nwine_data  <-  readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' ,  sheet  =   'wine_data' ,  header  =   TRUE ,  startCol  =   1 ,  startRow  =   1 )  head ( wine_data )    1\n2\n3\n4\n5\n6\n7 ##   condition Ratings\n## 1       Red      77\n## 2       Red      82\n## 3       Red      75\n## 4       Red      92\n## 5       Red      83\n## 6       Red      75   1\n2\n3\n4\n5\n6\n7\n8 # create the two subsets \nred_wine  <-   subset ( wine_data ,  wine_data $ condition  ==   'Red' ) \nwhite_wine  <-   subset ( wine_data ,  wine_data $ condition  ==   'White' )  # Plot the histograms of the ratings of both subsets \npar ( mfrow  =   c ( 1 , 2 )) \nhist ( red_wine $ Ratings ,  main  =   'Shiraz' ,  xlab  =   'Ratings' ) \nhist ( white_wine $ Ratings ,  main  =   'Pinot Grigio' ,  xlab  =   'Ratings' )     Robustness to outliers  1\n2\n3\n4\n5\n6\n7\n8\n9 # create the outlier and add it to the dataset \noutlier  <-   data.frame ( condition  =   'Red' ,  Ratings  =   0 ) \n\nred_wine_extreme  <-   rbind ( red_wine ,  outlier )  # calculate the difference in means and display it afterwards \ndiff_means  <-   mean ( red_wine $ Ratings )   -   mean ( red_wine_extreme $ Ratings ) \n\ndiff_means   1 ## [1] 0.8093069   1\n2\n3\n4 # calculate the difference in medians and display it afterwards \ndiff_medians  <-  median ( red_wine $ Ratings )   -  median ( red_wine_extreme $ Ratings ) \n\ndiff_medians   1 ## [1] 0",
            "title": "Measures of Central Tendency"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_One,_Introduction/#measures-of-variability",
            "text": "Michael Jordan\u2019s first NBA season - Global overview  1\n2\n3\n4 # Read in the data set and assign to the object \ndata_jordan  <-  readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' ,  sheet  =   'data_jordan' ,  header  =   TRUE ,  startCol  =   1 ,  startRow  =   1 )  head ( data_jordan )    1\n2\n3\n4\n5\n6\n7 ##   game points\n## 1    1     16\n## 2    2     21\n## 3    3     37\n## 4    4     25\n## 5    5     17\n## 6    6     25   1\n2\n3\n4\n5 # Make a scatterplot of the data on which a horizontal line with height equal to the mean is drawn. \nmean_jordan  <-   mean ( data_jordan $ points ) \nplot ( data_jordan $ game ,  data_jordan $ points , main  =   '1st NBA season of Michael Jordan' ) \n\nabline ( h  =  mean_jordan )     Michael Jordan\u2019s first NBA season - Calculate the variance manually  1\n2\n3\n4\n5\n6\n7\n8\n9 # Calculate the differences with respect to the mean  \ndiff  <-  data_jordan $ points  -   mean ( data_jordan $ points )  # Calculate the squared differences \nsquared_diff  <-   diff ^ 2  # Combine all pieces of the puzzle in order to acquire the variance \nvariance  <-   sum ( squared_diff ) / ( length ( data_jordan $ points )   -   1 ) \nvariance   1 ## [1] 66.73427   1\n2 # Compare your result to the correct solution. You can find the correct solution by calculating it with the `var()` function. \nvar ( data_jordan $ points )    1 ## [1] 66.73427",
            "title": "Measures of Variability"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Two,_Student_s_T-test/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nIntroduction to t-tests\n\u00b6\n\n\n\n\nTest p-values for significance with z-tests, t-tests.\n\n\nSingle sample t-test: group of people from a particular geographic region perform on a well-known test of intelligence. In particular, you are interested in finding out whether or not this group scores significantly higher than the overall population on an IQ test. This is a form of Null Hypothesis Significance Testing (NHST), where the null hypothesis is that there\u2019s no difference between this group and the overall population.\n\n\nDependent t-test: single group of voters to rate their likelihood of voting for the candidate before the speech and again after the speech; understand if voters from a particular neighborhood are likely to vote differently when compared to the overall\n\n    voting population.\n\n\nIndependent t-test: significant difference in preferences between these two groups; compare liberals and convervatives.\n\n\nt-distribution, observed value, expected value, standard error.\n\n\n\n\nGenerate density plots of different t-distributions\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n# Generate a vector of 100 values between -4 and 4\n\nx \n<-\n \nseq\n(\n-4\n,\n \n4\n,\n length \n=\n \n100\n)\n\n\n\n# Simulate the t-distribution\n\ny_1 \n<-\n dt\n(\nx\n,\n \n4\n)\n\ny_2 \n<-\n dt\n(\nx\n,\n \n6\n)\n\ny_3 \n<-\n dt\n(\nx\n,\n \n8\n)\n\ny_4 \n<-\n dt\n(\nx\n,\n \n10\n)\n\ny_5 \n<-\n dt\n(\nx\n,\n \n12\n)\n\n\n\n# Plot the t-distributions\n\nplot\n(\nx\n,\n y_1\n,\n type \n=\n \n'l'\n,\n lwd \n=\n \n2\n,\n xlab \n=\n \n'T value'\n,\n ylab \n=\n \n'Density'\n,\n main \n=\n \n'Comparison of t-distributions'\n)\n\n\nlines\n(\nx\n,\n y_2\n,\n col \n=\n \n'red'\n)\n\n\n#lines(x, y_3, col = 'orange')\n\n\n#lines(x, y_4, col = 'green')\n\n\n#lines(x, y_5, col = 'blue')\n\n\n\n# Add a legend\n\nlegend\n(\n'topright'\n,\n \nc\n(\n'df = 4'\n,\n \n'df = 6'\n,\n \n'df = 8'\n,\n \n'df = 10'\n,\n \n'df = 12'\n),\n title \n=\n \n'T distributions'\n,\n col \n=\n \nc\n(\n'black'\n,\n \n'red'\n,\n \n'orange'\n,\n \n'green'\n,\n \n'blue'\n),\n lty \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\nThe working memory dataset\n\n\nConduct a dependent (or paired) t-test on the \u201cworking memory\u201d dataset. This dataset consists of the intelligence scores for subjects before and after training, as well as for a control group. Our goal is to assess whether intelligence training results in significantly different intelligence scores for the individuals.\n\n\nThe observations of individuals before and after training are two samples from the same group at different points in time, which calls for a dependent t-test. This will test whether or not the difference in mean intelligence scores before and after training are significant.\n\n\n1\n2\n# Print the data set in the console\n\n\nhead\n(\nwm\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   cond pre post gain train\n## 1  t08   8    9    1     1\n## 2  t08   8   10    2     1\n## 3  t08   8    8    0     1\n## 4  t08   8    7   -1     1\n## 5  t08   9   11    2     1\n## 6  t08   9   10    1     1\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\nlibrary\n(\nHmisc\n)\n\n\n\n# Create a subset for the data that contains information on those subject who trained\n\nwm_t \n<-\n \nsubset\n(\nwm\n,\n wm\n$\ntrain \n==\n \n1\n)\n\n\n \n# Summary statistics \n\ndescribe\n(\nwm_t\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n## wm_t \n## \n##  5  Variables      80  Observations\n## ---------------------------------------------------------------------------\n## cond \n##        n  missing distinct \n##       80        0        4 \n##                               \n## Value       t08  t12  t17  t19\n## Frequency    20   20   20   20\n## Proportion 0.25 0.25 0.25 0.25\n## ---------------------------------------------------------------------------\n## pre \n##        n  missing distinct     Info     Mean      Gmd \n##       80        0        5    0.955    10.03    1.551 \n##                                         \n## Value          8     9    10    11    12\n## Frequency     12    20    19    12    17\n## Proportion 0.150 0.250 0.238 0.150 0.212\n## ---------------------------------------------------------------------------\n## post \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       80        0       13    0.984    13.51     2.87     9.95    10.00 \n##      .25      .50      .75      .90      .95 \n##    12.00    14.00    15.00    17.00    18.00 \n##                                                                       \n## Value          7     8     9    10    11    12    13    14    15    16\n## Frequency      1     1     2     5     8    11    11    15     8     9\n## Proportion 0.012 0.012 0.025 0.062 0.100 0.138 0.138 0.188 0.100 0.112\n##                             \n## Value         17    18    19\n## Frequency      4     2     3\n## Proportion 0.050 0.025 0.038\n## ---------------------------------------------------------------------------\n## gain \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       80        0       10    0.975    3.487    2.415      0.0      1.0 \n##      .25      .50      .75      .90      .95 \n##      2.0      3.0      5.0      6.1      7.0 \n##                                                                       \n## Value         -1     0     1     2     3     4     5     6     7     9\n## Frequency      2     3     7    18    12    16     6     8     6     2\n## Proportion 0.025 0.038 0.088 0.225 0.150 0.200 0.075 0.100 0.075 0.025\n## ---------------------------------------------------------------------------\n## train \n##        n  missing distinct     Info     Mean      Gmd \n##       80        0        1        0        1        0 \n##              \n## Value       1\n## Frequency  80\n## Proportion  1\n## ---------------------------------------------------------------------------\n\n\n\n\n\n\n1\n2\n# Create a boxplot with pre- and post-training groups \n\nboxplot\n(\nwm_t\n$\npre\n,\n wm_t\n$\npost\n,\n main \n=\n \n\"Boxplot\"\n,\n xlab \n=\n \n\"Pre and Post Training\"\n,\n ylab \n=\n \n\"Intelligence Score\"\n,\n col \n=\n \nc\n(\n\"red\"\n,\n \n\"green\"\n))\n\n\n\n\n\n\n\n\n\nPerforming dependent t-tests manually in R (1)\n\n\nConducting a dependent t-test, also known as a paired t-test, requires the following steps:\n\n\n\n\nDefine null and alternative hypotheses\n\n\nDecide significance level \n\u03b1\n\n\nCompute observed t-value\n\n\nFind critical value\n\n\nCompare observed value to critical value\n\n\n\n\nWe\u2019re performing a Null Hypothesis Significance Test (NHST), so our null hypothesis is that there\u2019s no effect (i.e. training has no impact on intelligence scores). The alternative hypothesis is that training results in signficantly different  intelligence scores. We\u2019ll use a significance level of 0.05, which is very common in statistics.\n\n\nCompute the observed t-value.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# Define the sample size\n\nn \n<-\n \ndim\n(\nwm_t\n)[\n1\n]\n\n\n\n# Calculate the degrees of freedom\n\ndf \n<-\n n \n-\n \n1\n\n\n \n# Find the critical t-value\n\nt_crit \n<-\n \nabs\n(\nqt\n(\n0.025\n,\n df\n))\n\n\n\n# Calculate the mean of the difference in scores. The differences are already in the dataset under the column 'gain'.\n\nmean_diff \n<-\n \nsum\n(\nwm_t\n$\ngain\n)\n/\nn\n\n\n# Calculate the standard deviation\n\nxdt \n<-\n \nsum\n(\nwm_t\n$\ngain\n^\n2\n)\n\nxdt2 \n<-\n xdt\n/\nn\nsd_diff2 \n<-\n \nsqrt\n((\nxdt \n-\n xdt2\n)\n/\n(\nn \n-\n \n1\n))\n\nsd_diff \n<-\n \nsqrt\n((\nsum\n(\nwm_t\n$\ngain\n^\n2\n)\n \n-\n \n((\nsum\n(\nwm_t\n$\ngain\n))\n^\n2\n/\nn\n))\n/\n(\nn \n-\n \n1\n))\n\nsd_diff2\n\n\n\n\n\n\n1\n## [1] 4.091149\n\n\n\n\n\n\n1\nsd_diff\n\n\n\n\n\n\n1\n## [1] 2.152383\n\n\n\n\n\n\nPerforming dependent t-tests manually in R (2)\n\n\nNow that we\u2019ve determined our null and alternative hypotheses, decided on a significance level, and computed our observed t-value, all that remains is to calculate the critical value for this test and compare it to our observed t-value. This will tell us whether we have sufficient evidence to reject our null hypothesis. We\u2019ll even go one step further and compute an effect size with Cohen\u2019s d!\n\n\nThe critical value is the point on the relevant t-distribution that determines whether the value we observed is extreme enough to warrant rejecting the null hypothesis. Recall that a t-distribution is defined by its degrees of freedom, which in turn is equal to the sample size minus 1. In this example, we have 80 subjects so the relevant t-distribution has 79 degrees of freedom.\n\n\nWe\u2019re performing a two-tailed t-test in this situation since we care about detecting a significant effect in either the positive or negative direction. In other words, we want to know if training significantly increases or decreases intelligence, however, given that our observed t-value is positive (14.49) the right-hand is the only relevant value here.\n\n\nFurthermore, since our desired significance level (i.e. alpha) is 0.05, our critical value is the point on our t-distribution at which 0.025 (0.05 / 2) of its total area of 1 is to the right and thus 0.975 (1 - 0.025) of its total area is to the left.\n\n\nThis point is called the 0.975 quantile and is computed for a\n\nt-distrbution.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# The variables from the previous exercise are still preloaded, type ls() in the console to see them\n\nn \n<-\n \ndim\n(\nwm_t\n)[\n1\n]\n\ndf \n<-\n n \n-\n \n1\n\nt_crit \n<-\n \nabs\n(\nqt\n(\n0.025\n,\n df\n))\n\nmean_diff \n<-\n \nsum\n(\nwm_t\n$\ngain\n)\n/\nn\nsd_diff \n<-\n \nsqrt\n((\nsum\n(\nwm_t\n$\ngain\n^\n2\n)\n \n-\n \n((\nsum\n(\nwm_t\n$\ngain\n))\n^\n2\n/\nn\n))\n/\n(\nn \n-\n \n1\n))\n\n\n\n# Calculate the t-value for this test\n\nt_value \n<-\n mean_diff\n/\n(\nsd_diff\n/\nsqrt\n(\nn\n))\n\n\n\n# Check whether or not the mean difference is statistically significant\n\nt_value\n\n\n\n\n\n\n1\n## [1] 14.49238\n\n\n\n\n\n\n1\nt_crit\n\n\n\n\n\n\n1\n## [1] 1.99045\n\n\n\n\n\n\n1\n2\n3\n4\n# Calculate the confidence interval\n\nconf_upper \n<-\n mean_diff \n+\n t_crit \n*\n \n(\nsd_diff\n/\nsqrt\n(\nn\n))\n\nconf_lower \n<-\n mean_diff \n-\n t_crit \n*\n \n(\nsd_diff\n/\nsqrt\n(\nn\n))\n\nconf_upper\n\n\n\n\n\n\n1\n## [1] 3.966489\n\n\n\n\n\n\n1\nconf_lower\n\n\n\n\n\n\n1\n## [1] 3.008511\n\n\n\n\n\n\n1\n2\n3\n# Calculate Cohen's d\n\ncohens_d \n<-\n mean_diff\n/\nsd_diff\ncohens_d\n\n\n\n\n\n\n1\n## [1] 1.620297\n\n\n\n\n\n\nLetting R do all the dirty work: Dependent t-tests\n\n\nThe \nCohensD\n function (not showns).\n\n\nCohen\u2019s d Determine that the difference pre- and post-training is statistically significant; and the effect size, meaning the effect of training on intelligence gains particularly strong or not.\n\n\nCohen\u2019s d is unbiased by sample size. Cohen\u2019s d provides a standardized difference between two means. Cohen\u2019s d is calculated by subtracting one group mean from the other, then dividing by the pooled standard deviation.\n\n\n1\n2\n# Conduct a paired t-test using the t.test function\n\nt.test\n(\nwm_t\n$\npost\n,\n wm_t\n$\npre\n,\n paired \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  Paired t-test\n## \n## data:  wm_t$post and wm_t$pre\n## t = 14.492, df = 79, p-value < 2.2e-16\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  3.008511 3.966489\n## sample estimates:\n## mean of the differences \n##                  3.4875\n\n\n\n\n\n\n1\n2\n3\n4\n# Calculate Cohen's d\n\n\nlibrary\n(\nlsr\n)\n\n\ncohensD\n(\nwm_t\n$\npost\n,\n wm_t\n$\npre\n,\n method \n=\n \n'paired'\n)\n\n\n\n\n\n\n\n1\n## [1] 1.620297\n\n\n\n\n\n\nIndependent t-tests\n\u00b6\n\n\nAn independent t-test is appropriate when you want to compare the the means for two independent groups.\n\n\nPreliminary statistics\n\n\nFor independent t-tests you will revisit the working memory dataset from the previous chapter. In this dataset, subjects were randomly assigned to four different training groups that trained for 8, 12, 17 and 19 days.\n\n\n1\n2\n3\n4\n5\n# Read in the data set and assign to the object\n\nwm_t \n<-\n readWorksheetFromFile\n(\n'A Hands-on Introduction to Statistics with R.xls'\n,\n sheet \n=\n \n'wm_t'\n,\n header \n=\n \nTRUE\n,\n startCol \n=\n \n1\n,\n startRow \n=\n \n1\n)\n\n\n\n# This will print the data set in the console\n\n\nhead\n(\nwm_t\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   cond pre post gain train\n## 1  t08   8    9    1     1\n## 2  t08   8   10    2     1\n## 3  t08   8    8    0     1\n## 4  t08   8    7   -1     1\n## 5  t08   9   11    2     1\n## 6  t08   9   10    1     1\n\n\n\n\n\n\nAdd statistical functions.\n\n\n1\n2\n# Load the psych(ology) package\n\n\nlibrary\n(\npsych\n)\n\n\n\n\n\n\n\nLevene\u2019s test for homogeneity of variance.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Create subsets for each training time\n\nwm_t08 \n<-\n \nsubset\n(\nwm_t\n,\n cond \n==\n \n't08'\n)\n\nwm_t12 \n<-\n \nsubset\n(\nwm_t\n,\n cond \n==\n \n't12'\n)\n\nwm_t17 \n<-\n \nsubset\n(\nwm_t\n,\n cond \n==\n \n't17'\n)\n\nwm_t19 \n<-\n \nsubset\n(\nwm_t\n,\n cond \n==\n \n't19'\n)\n\n\n\n# Summary statistics of the change in training scores before and after exercise\n\ndescribe\n(\nwm_t08\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n##       vars  n  mean   sd median trimmed  mad min  max range  skew kurtosis\n## cond*    1 20   NaN   NA     NA     NaN   NA Inf -Inf  -Inf    NA       NA\n## pre      2 20 10.05 1.50   10.0   10.06 1.48   8   12     4  0.01    -1.53\n## post     3 20 11.40 2.14   11.5   11.50 2.22   7   15     8 -0.25    -0.84\n## gain     4 20  1.35 1.23    1.0    1.44 1.48  -1    3     4 -0.32    -0.82\n## train    5 20  1.00 0.00    1.0    1.00 0.00   1    1     0   NaN      NaN\n##         se\n## cond*   NA\n## pre   0.34\n## post  0.48\n## gain  0.27\n## train 0.00\n\n\n\n\n\n\n1\ndescribe\n(\nwm_t12\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n##       vars  n mean   sd median trimmed  mad min  max range skew kurtosis\n## cond*    1 20  NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA\n## pre      2 20  9.9 1.45     10    9.88 1.48   8   12     4 0.16    -1.43\n## post     3 20 12.5 1.88     12   12.38 2.22  10   17     7 0.48    -0.54\n## gain     4 20  2.6 1.27      2    2.50 0.00   0    5     5 0.44    -0.54\n## train    5 20  1.0 0.00      1    1.00 0.00   1    1     0  NaN      NaN\n##         se\n## cond*   NA\n## pre   0.32\n## post  0.42\n## gain  0.28\n## train 0.00\n\n\n\n\n\n\n1\ndescribe\n(\nwm_t17\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n##       vars  n mean   sd median trimmed  mad min  max range skew kurtosis\n## cond*    1 20  NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA\n## pre      2 20 10.0 1.34     10   10.00 1.48   8   12     4 0.25    -1.34\n## post     3 20 14.4 1.85     14   14.25 1.48  12   19     7 0.63    -0.27\n## gain     4 20  4.4 1.39      4    4.25 1.48   3    7     4 0.64    -1.12\n## train    5 20  1.0 0.00      1    1.00 0.00   1    1     0  NaN      NaN\n##         se\n## cond*   NA\n## pre   0.30\n## post  0.41\n## gain  0.31\n## train 0.00\n\n\n\n\n\n\n1\ndescribe\n(\nwm_t19\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n##       vars  n  mean   sd median trimmed  mad min  max range skew kurtosis\n## cond*    1 20   NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA\n## pre      2 20 10.15 1.27   10.0   10.19 1.48   8   12     4 0.03    -1.10\n## post     3 20 15.75 1.86   16.0   15.69 1.48  13   19     6 0.16    -1.03\n## gain     4 20  5.60 1.73    5.5    5.50 2.22   3    9     6 0.36    -0.76\n## train    5 20  1.00 0.00    1.0    1.00 0.00   1    1     0  NaN      NaN\n##         se\n## cond*   NA\n## pre   0.28\n## post  0.42\n## gain  0.39\n## train 0.00\n\n\n\n\n\n\n1\n2\n# Create a boxplot of the different training times\n\nggplot\n(\nwm_t\n,\n aes\n(\nx \n=\n cond\n,\n y \n=\n gain\n,\n fill \n=\n cond\n))\n \n+\n geom_boxplot\n()\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# Levene's test\n\n\nlibrary\n(\ncar\n)\n\n\nleveneTest\n(\nwm_t\n$\ngain \n~\n wm_t\n$\ncond\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  3  1.3134 0.2763\n##       76\n\n\n\n\n\n\nConducting an independent t-test manually (1)\n\n\nPerform an independent t-test the same way we did for the dependent t-test in the previous chapter. Continuing with the working memory example, our null hypothesis is that the difference in intelligence score gain between the group that trained for 8 days and the group that trained for 19 days is equal to zero. If our observed t-value is sufficiently large, we can reject the null in favor of the alternative hypothesis, which would imply a significant difference in intelligence gain between the two training groups.\n\n\nCalculation of the observed t-value for an independent t-test is similar to the dependent t-test, but involves slightly  different formulas.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Calculate mean difference by subtracting the gain for t08 by the gain for t19\n\nmean_t08 \n<-\n \nmean\n(\nwm_t08\n$\ngain\n)\n\nmean_t19 \n<-\n \nmean\n(\nwm_t19\n$\ngain\n)\n\nmean_diff \n<-\n \n(\nmean_t19 \n-\n mean_t08\n)\n\n\n\n# Calculate degrees of freedom\n\nn_t08 \n<-\n \ndim\n(\nwm_t08\n)[\n1\n]\n\nn_t19 \n<-\n \ndim\n(\nwm_t19\n)[\n1\n]\n\ndf \n<-\n n_t08 \n+\n n_t19 \n-\n \n2\n\n\n\n# Calculate the pooled standard error\n\nvar_t08 \n<-\n \n(\nsum\n((\nwm_t08\n$\ngain \n-\n mean_t08\n)\n^\n2\n))\n/\n(\nn_t08 \n-\n \n1\n)\n\nvar_t19 \n<-\n \n(\nsum\n((\nwm_t19\n$\ngain \n-\n mean_t19\n)\n^\n2\n))\n/\n(\nn_t19 \n-\n \n1\n)\n\nse_pooled \n<-\n \nsqrt\n((\nvar_t08\n/\nn_t08\n)\n \n+\n \n(\nvar_t19\n/\nn_t19\n))\n\n\n\n\n\n\n\nConducting an independent t-test manually (2)\n\n\nCompute the observed t-value. Then we will determine the p-value using the relevant t-distribution. If you recall, in the last chapter we calculated the critical value. The p-value is simply an alternative approach to hypothesis testing and determining the significance of your results. It\u2019s good to practice both! Finally, we will finish by calculating effect size via Cohen\u2019s d.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# All variables from the previous exercises are preloaded in your workspace\n\n\n# Type ls() to see them\n\n\n# Calculate the t-value\n\nt_value \n<-\n mean_diff\n/\nse_pooled\n\n\n# Calculate p-value\n\n\n#two-tail test, 0.05/2 = 0.025\n\np_value \n<-\n \n2\n*\n(\n1\n-\npt\n(\nt_value\n,\ndf \n=\n df\n))\n\n\n\n# Calculate Cohen's d\n\nsd_t08 \n<-\n sd\n(\nwm_t08\n$\ngain\n)\n\nsd_t19 \n<-\n sd\n(\nwm_t19\n$\ngain\n)\n\npooled_sd \n<-\n \n(\nsd_t08 \n+\n sd_t19\n)\n \n/\n \n2\n\ncohens_d \n<-\n mean_diff\n/\npooled_sd\n\n\n\n\n\n\nLetting R do all the dirty work: Independent t-tests\n\n\n1\n2\n# Conduct an independent t-test \n\nt.test\n(\nwm_t19\n$\ngain\n,\n wm_t08\n$\ngain\n,\nvar.equal \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  Two Sample t-test\n## \n## data:  wm_t19$gain and wm_t08$gain\n## t = 8.9677, df = 38, p-value = 6.443e-11\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  3.290588 5.209412\n## sample estimates:\n## mean of x mean of y \n##      5.60      1.35\n\n\n\n\n\n\n1\n2\n# Calculate Cohen's d\n\ncohensD\n(\nwm_t19\n$\ngain\n,\n wm_t08\n$\ngain\n,\n method \n=\n \n'pooled'\n)\n\n\n\n\n\n\n\n1\n## [1] 2.835822",
            "title": "Statistics with R, Course Two, Student's t-test"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Two,_Student_s_T-test/#independent-t-tests",
            "text": "An independent t-test is appropriate when you want to compare the the means for two independent groups.  Preliminary statistics  For independent t-tests you will revisit the working memory dataset from the previous chapter. In this dataset, subjects were randomly assigned to four different training groups that trained for 8, 12, 17 and 19 days.  1\n2\n3\n4\n5 # Read in the data set and assign to the object \nwm_t  <-  readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' ,  sheet  =   'wm_t' ,  header  =   TRUE ,  startCol  =   1 ,  startRow  =   1 )  # This will print the data set in the console  head ( wm_t )    1\n2\n3\n4\n5\n6\n7 ##   cond pre post gain train\n## 1  t08   8    9    1     1\n## 2  t08   8   10    2     1\n## 3  t08   8    8    0     1\n## 4  t08   8    7   -1     1\n## 5  t08   9   11    2     1\n## 6  t08   9   10    1     1   Add statistical functions.  1\n2 # Load the psych(ology) package  library ( psych )    Levene\u2019s test for homogeneity of variance.  1\n2\n3\n4\n5\n6\n7\n8 # Create subsets for each training time \nwm_t08  <-   subset ( wm_t ,  cond  ==   't08' ) \nwm_t12  <-   subset ( wm_t ,  cond  ==   't12' ) \nwm_t17  <-   subset ( wm_t ,  cond  ==   't17' ) \nwm_t19  <-   subset ( wm_t ,  cond  ==   't19' )  # Summary statistics of the change in training scores before and after exercise \ndescribe ( wm_t08 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ##       vars  n  mean   sd median trimmed  mad min  max range  skew kurtosis\n## cond*    1 20   NaN   NA     NA     NaN   NA Inf -Inf  -Inf    NA       NA\n## pre      2 20 10.05 1.50   10.0   10.06 1.48   8   12     4  0.01    -1.53\n## post     3 20 11.40 2.14   11.5   11.50 2.22   7   15     8 -0.25    -0.84\n## gain     4 20  1.35 1.23    1.0    1.44 1.48  -1    3     4 -0.32    -0.82\n## train    5 20  1.00 0.00    1.0    1.00 0.00   1    1     0   NaN      NaN\n##         se\n## cond*   NA\n## pre   0.34\n## post  0.48\n## gain  0.27\n## train 0.00   1 describe ( wm_t12 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ##       vars  n mean   sd median trimmed  mad min  max range skew kurtosis\n## cond*    1 20  NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA\n## pre      2 20  9.9 1.45     10    9.88 1.48   8   12     4 0.16    -1.43\n## post     3 20 12.5 1.88     12   12.38 2.22  10   17     7 0.48    -0.54\n## gain     4 20  2.6 1.27      2    2.50 0.00   0    5     5 0.44    -0.54\n## train    5 20  1.0 0.00      1    1.00 0.00   1    1     0  NaN      NaN\n##         se\n## cond*   NA\n## pre   0.32\n## post  0.42\n## gain  0.28\n## train 0.00   1 describe ( wm_t17 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ##       vars  n mean   sd median trimmed  mad min  max range skew kurtosis\n## cond*    1 20  NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA\n## pre      2 20 10.0 1.34     10   10.00 1.48   8   12     4 0.25    -1.34\n## post     3 20 14.4 1.85     14   14.25 1.48  12   19     7 0.63    -0.27\n## gain     4 20  4.4 1.39      4    4.25 1.48   3    7     4 0.64    -1.12\n## train    5 20  1.0 0.00      1    1.00 0.00   1    1     0  NaN      NaN\n##         se\n## cond*   NA\n## pre   0.30\n## post  0.41\n## gain  0.31\n## train 0.00   1 describe ( wm_t19 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ##       vars  n  mean   sd median trimmed  mad min  max range skew kurtosis\n## cond*    1 20   NaN   NA     NA     NaN   NA Inf -Inf  -Inf   NA       NA\n## pre      2 20 10.15 1.27   10.0   10.19 1.48   8   12     4 0.03    -1.10\n## post     3 20 15.75 1.86   16.0   15.69 1.48  13   19     6 0.16    -1.03\n## gain     4 20  5.60 1.73    5.5    5.50 2.22   3    9     6 0.36    -0.76\n## train    5 20  1.00 0.00    1.0    1.00 0.00   1    1     0  NaN      NaN\n##         se\n## cond*   NA\n## pre   0.28\n## post  0.42\n## gain  0.39\n## train 0.00   1\n2 # Create a boxplot of the different training times \nggplot ( wm_t ,  aes ( x  =  cond ,  y  =  gain ,  fill  =  cond ))   +  geom_boxplot ()     1\n2\n3\n4 # Levene's test  library ( car ) \n\nleveneTest ( wm_t $ gain  ~  wm_t $ cond )    1\n2\n3\n4 ## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  3  1.3134 0.2763\n##       76   Conducting an independent t-test manually (1)  Perform an independent t-test the same way we did for the dependent t-test in the previous chapter. Continuing with the working memory example, our null hypothesis is that the difference in intelligence score gain between the group that trained for 8 days and the group that trained for 19 days is equal to zero. If our observed t-value is sufficiently large, we can reject the null in favor of the alternative hypothesis, which would imply a significant difference in intelligence gain between the two training groups.  Calculation of the observed t-value for an independent t-test is similar to the dependent t-test, but involves slightly  different formulas.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Calculate mean difference by subtracting the gain for t08 by the gain for t19 \nmean_t08  <-   mean ( wm_t08 $ gain ) \nmean_t19  <-   mean ( wm_t19 $ gain ) \nmean_diff  <-   ( mean_t19  -  mean_t08 )  # Calculate degrees of freedom \nn_t08  <-   dim ( wm_t08 )[ 1 ] \nn_t19  <-   dim ( wm_t19 )[ 1 ] \ndf  <-  n_t08  +  n_t19  -   2  # Calculate the pooled standard error \nvar_t08  <-   ( sum (( wm_t08 $ gain  -  mean_t08 ) ^ 2 )) / ( n_t08  -   1 ) \nvar_t19  <-   ( sum (( wm_t19 $ gain  -  mean_t19 ) ^ 2 )) / ( n_t19  -   1 ) \nse_pooled  <-   sqrt (( var_t08 / n_t08 )   +   ( var_t19 / n_t19 ))    Conducting an independent t-test manually (2)  Compute the observed t-value. Then we will determine the p-value using the relevant t-distribution. If you recall, in the last chapter we calculated the critical value. The p-value is simply an alternative approach to hypothesis testing and determining the significance of your results. It\u2019s good to practice both! Finally, we will finish by calculating effect size via Cohen\u2019s d.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # All variables from the previous exercises are preloaded in your workspace  # Type ls() to see them  # Calculate the t-value \nt_value  <-  mean_diff / se_pooled # Calculate p-value  #two-tail test, 0.05/2 = 0.025 \np_value  <-   2 * ( 1 - pt ( t_value , df  =  df ))  # Calculate Cohen's d \nsd_t08  <-  sd ( wm_t08 $ gain ) \nsd_t19  <-  sd ( wm_t19 $ gain ) \npooled_sd  <-   ( sd_t08  +  sd_t19 )   /   2 \ncohens_d  <-  mean_diff / pooled_sd   Letting R do all the dirty work: Independent t-tests  1\n2 # Conduct an independent t-test  \nt.test ( wm_t19 $ gain ,  wm_t08 $ gain , var.equal  =   TRUE )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## \n##  Two Sample t-test\n## \n## data:  wm_t19$gain and wm_t08$gain\n## t = 8.9677, df = 38, p-value = 6.443e-11\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  3.290588 5.209412\n## sample estimates:\n## mean of x mean of y \n##      5.60      1.35   1\n2 # Calculate Cohen's d \ncohensD ( wm_t19 $ gain ,  wm_t08 $ gain ,  method  =   'pooled' )    1 ## [1] 2.835822",
            "title": "Independent t-tests"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nAn introduction to ANOVA\n\u00b6\n\n\nWorking memory experiment\n\n\nWe\u2019ll use data from the working memory experiment, which investigates the relationship between the number of training days and a change in IQ. There are four independent groups, each of which trained for a different period of time: 8, 12, 17, or 19 days. The independent variable is the number of training days and the dependent variable is the IQ gain.\n\n\n1\n2\n# Print the data set in the console\n\n\nhead\n(\nwm\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   subject condition   iq\n## 1       1    8 days 12.4\n## 2       2    8 days 11.8\n## 3       3    8 days 14.6\n## 4       4    8 days  7.7\n## 5       5    8 days 15.7\n## 6       6    8 days 11.6\n\n\n\n\n\n\n1\n2\n3\n4\nlibrary\n(\npsych\n)\n\n\n\n# Summary statistics by all groups (8 sessions, 12 sessions, 17 sessions, 19 sessions)\n\ndescribeBy\n(\nwm\n,\n wm\n$\ncondition\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n## \n##  Descriptive statistics by group \n## group: 12 days\n##            vars  n mean   sd median trimmed  mad  min  max range skew\n## subject       1 20 30.5 5.92  30.50   30.50 7.41 21.0 40.0  19.0 0.00\n## condition*    2 20  NaN   NA     NA     NaN   NA  Inf -Inf  -Inf   NA\n## iq            3 20 11.7 2.58  11.65   11.69 2.89  6.9 16.1   9.2 0.05\n##            kurtosis   se\n## subject       -1.38 1.32\n## condition*       NA   NA\n## iq            -1.06 0.58\n## -------------------------------------------------------- \n## group: 17 days\n##            vars  n mean   sd median trimmed  mad  min  max range skew\n## subject       1 20 50.5 5.92   50.5    50.5 7.41 41.0 60.0  19.0 0.00\n## condition*    2 20  NaN   NA     NA     NaN   NA  Inf -Inf  -Inf   NA\n## iq            3 20 13.9 2.26   13.6    13.9 2.00  9.8 18.1   8.3 0.11\n##            kurtosis   se\n## subject       -1.38 1.32\n## condition*       NA   NA\n## iq            -0.85 0.50\n## -------------------------------------------------------- \n## group: 19 days\n##            vars  n  mean   sd median trimmed  mad  min  max range  skew\n## subject       1 20 70.50 5.92   70.5   70.50 7.41 61.0 80.0  19.0  0.00\n## condition*    2 20   NaN   NA     NA     NaN   NA  Inf -Inf  -Inf    NA\n## iq            3 20 14.75 2.50   15.3   14.71 2.15 10.4 19.2   8.8 -0.09\n##            kurtosis   se\n## subject       -1.38 1.32\n## condition*       NA   NA\n## iq            -0.99 0.56\n## -------------------------------------------------------- \n## group: 8 days\n##            vars  n  mean   sd median trimmed  mad min  max range  skew\n## subject       1 20 10.50 5.92   10.5   10.50 7.41 1.0 20.0  19.0  0.00\n## condition*    2 20   NaN   NA     NA     NaN   NA Inf -Inf  -Inf    NA\n## iq            3 20 10.91 2.63   11.3   10.97 2.67 5.4 15.7  10.3 -0.21\n##            kurtosis   se\n## subject       -1.38 1.32\n## condition*       NA   NA\n## iq            -0.70 0.59\n\n\n\n\n\n\n1\n2\n# Boxplot IQ versus cond\n\nboxplot\n(\nwm\n$\niq \n~\n wm\n$\ncondition\n,\n main\n=\n\"Boxplot\"\n,\n xlab\n=\n\"Group (cond)\"\n,\n ylab\n=\n\"IQ\"\n)\n\n\n\n\n\n\n\n\n\nNotice that the IQ increases as the amount of training sessions increases.\n\n\nt-test vs ANOVA\n\n\nANOVA is used when more than two group means are compared, whereas a t-test can only compare two group means.\n\n\nGenerate density plot of the F-distribution\n\n\nThe test statistic associated with ANOVA is the F-test (or F-ratio). Recall that when carrying out a t-test, you computed an observed t-value, then compared that with a critical value derived from the relevant t-distribution. That t-distribution came from a family of t-distributions, each of which was defined entirely by its degrees of freedom.\n\n\nANOVA uses the same principle, but instead an observed F-value is computed and compared to the relevant F-distribution. That F-distribution comes from a family of F-distributions, each of which is defined by two numbers (i.e. degrees of freedom).\n\n\nF-distribution has a different shape than the t-distribution.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n# Create the vector x\n\nx \n<-\n \nseq\n(\nfrom \n=\n \n0\n,\n to \n=\n \n2\n,\n length \n=\n \n100\n)\n\n\n\n# Simulate the F-distributions\n\ny_1 \n<-\n df\n(\nx\n,\n \n1\n,\n \n1\n)\n\ny_2 \n<-\n df\n(\nx\n,\n \n3\n,\n \n1\n)\n\ny_3 \n<-\n df\n(\nx\n,\n \n6\n,\n \n1\n)\n\ny_4 \n<-\n df\n(\nx\n,\n \n3\n,\n \n3\n)\n\ny_5 \n<-\n df\n(\nx\n,\n \n6\n,\n \n3\n)\n\ny_6 \n<-\n df\n(\nx\n,\n \n3\n,\n \n6\n)\n\ny_7 \n<-\n df\n(\nx\n,\n \n6\n,\n \n6\n)\n\n\n\n# Plot the F-distributions\n\nplot\n(\nx\n,\n y_1\n,\n col \n=\n \n1\n,\n \n'l'\n)\n\nlines\n(\nx\n,\ny_2\n,\n col \n=\n \n2\n,\n \n'l'\n)\n\nlines\n(\nx\n,\ny_3\n,\n col \n=\n \n3\n,\n \n'l'\n)\n\nlines\n(\nx\n,\ny_4\n,\n col \n=\n \n4\n,\n \n'l'\n)\n\nlines\n(\nx\n,\ny_5\n,\n col \n=\n \n5\n,\n \n'l'\n)\n\nlines\n(\nx\n,\ny_6\n,\n col \n=\n \n6\n,\n \n'l'\n)\n\nlines\n(\nx\n,\ny_7\n,\n col \n=\n \n7\n,\n \n'l'\n)\n\n\n\n# Add the legend in the top right corner and with the title 'F distributions'\n\nlegend\n(\n'topright'\n,\n title \n=\n \n'F distributions'\n,\n \nc\n(\n'df = (1,1)'\n,\n \n'df = (3,1)'\n,\n \n'df = (6,1)'\n,\n \n'df = (3,3)'\n,\n \n'df = (6,3)'\n,\n \n'df = (3,6)'\n,\n \n'df = (6,6)'\n),\n col \n=\n \nc\n(\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n),\n lty \n=\n \n1\n)\n\n\n\n\n\n\n\n\n\nThe F-distribution cannot take negative values, because it is a ratio of variances and variances are always non-negative numbers. The distribution represents the ratio between the variance between groups and the variance within groups.\n\n\nBetween group sum of squares\n\n\nTo calculate the F-value, you need to calculate the ratio between the variance between groups and the variance within groups. Furthermore, to calculate the variance (i.e. mean of squares), you first have to calculate the sum of squares.\n\n\nNow, remember that the working memory experiment investigates the relationship between the change in IQ and the number of training sessions. Calculate the between group sum of squares for the data from this experiment.\n\n\n1\n2\n3\n4\n5\n6\n# Define number of subjects in each group\n\nn \n<-\n \n20\n\n\n\n# Calculate group means\n\nY_j \n<-\n \nas.numeric\n(\ntapply\n(\nwm\n$\niq\n,\n wm\n$\ncondition\n,\n \nmean\n))\n\nY_j\n\n\n\n\n\n\n1\n## [1] 11.700 13.905 14.750 10.910\n\n\n\n\n\n\n1\n2\n3\n# Calculate the grand mean\n\nY_T \n<-\n \nmean\n(\nwm\n$\niq\n)\n\nY_T\n\n\n\n\n\n\n1\n## [1] 12.81625\n\n\n\n\n\n\n1\n2\n3\n# Calculate the sum of squares\n\nSS_A \n<-\n \nsum\n((\nY_j \n-\n Y_T\n)\n^\n2\n)\n \n*\n n\nSS_A\n\n\n\n\n\n\n1\n## [1] 196.0914\n\n\n\n\n\n\nWithin groups sum of squares\n\n\nTo calculate the F-value, you also need the variance within groups. Similar to the last exercise, we\u2019ll start by computing the within groups sum of squares.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n# Create four subsets of the four groups, containing the IQ results\n\n\n\n# Make the subset for the group cond = '8 days'\n\nY_i1 \n<-\n \nsubset\n(\nwm\n$\niq\n,\n wm\n$\ncondition \n==\n \n'8 days'\n)\n\n\n\n# Make the subset for the group cond = '12 days'\n\nY_i2 \n<-\n \nsubset\n(\nwm\n$\niq\n,\n wm\n$\ncondition \n==\n \n'12 days'\n)\n\n\n\n# Make the subset for the group cond = '17 days'\n\nY_i3 \n<-\n \nsubset\n(\nwm\n$\niq\n,\n wm\n$\ncondition \n==\n \n'17 days'\n)\n\n\n\n# Make the subset for the group cond = '19 days'\n\nY_i4 \n<-\n \nsubset\n(\nwm\n$\niq\n,\n wm\n$\ncondition \n==\n \n'19 days'\n)\n\n\n\n# subtract the individual values by their group means\n\n\n# You have already calculated the group means in the previous exercise so use this result, the vector that contains these group means was called Y_j\n\nS_1 \n<-\n Y_i1 \n-\n Y_j\n[\n1\n]\n\nS_2 \n<-\n Y_i2 \n-\n Y_j\n[\n2\n]\n\n\n\n# Do it without the vector Y_j, so calculate the group means again.\n\nS_3 \n<-\n Y_i3 \n-\n \nmean\n(\nY_i3\n)\n\nS_4 \n<-\n Y_i4 \n-\n \nmean\n(\nY_i4\n)\n\n\n\n#Put everything back in one vector\n\nS_T \n<-\n \nc\n(\nS_1\n,\nS_2\n,\nS_3\n,\nS_4\n)\n\n\n\n#Calculate the sum of squares by using the vector S_T\n\nSS_SA \n<-\n \nsum\n(\nS_T\n^\n2\n)\n\n\n\n\n\n\n\nCalculating the F-ratio\n\n\nCalculate the F-ratio.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n# Number of groups\n\na \n<-\n \n4\n\n\n\n# Number of subject in each group\n\nn \n<-\n \n20\n\n\n\n# Define the degrees of freedom\n\ndf_A \n<-\n a \n-\n \n1\n\ndf_SA \n<-\n a\n*\n(\nn \n-\n \n1\n)\n\n\n\n# Calculate the mean squares (variances) by using the sum of squares SS_A and SS_SA\n\nMS_A \n<-\n SS_A\n/\ndf_A\nMS_SA \n<-\n SS_SA\n/\ndf_SA\n\n\n# Calculate the F-ratio\n\n\nF\n \n<-\n MS_A\n/\nMS_SA\n\n\n\n\n\n\nA faster way: ANOVA in R\n\n\nNormally, we do not have to do all calculations.\n\n\n1\n2\n3\n# Apply the aov function\n\nanova.wm \n<-\n aov\n(\nwm\n$\niq \n~\n wm\n$\ncondition\n)\n\nanova.wm\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n## Call:\n##    aov(formula = wm$iq ~ wm$condition)\n## \n## Terms:\n##                 wm$condition Residuals\n## Sum of Squares      196.0914  473.4175\n## Deg. of Freedom            3        76\n## \n## Residual standard error: 2.495832\n## Estimated effects may be unbalanced\n\n\n\n\n\n\n1\n2\n# Look at the summary table of the result\n\n\nsummary\n(\nanova.wm\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##              Df Sum Sq Mean Sq F value   Pr(>F)    \n## wm$condition  3  196.1   65.36   10.49 7.47e-06 ***\n## Residuals    76  473.4    6.23                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nF-value is significant.\n\n\nLevene\u2019s test\n\n\nThe assumptions of ANOVA are relatively simple. Similar to an independent t-test, we have a continuous dependent variable, which we assume to be normally distributed. Furthermore, we assume homogeneity of variance, which can be tested with Levene\u2019s test.\n\n\n1\n2\n3\n4\nlibrary\n(\ncar\n)\n\n\n\n# Levene's test\n\nleveneTest\n(\nwm\n$\niq \n~\n wm\n$\ncondition\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  3  0.1405 0.9355\n##       76\n\n\n\n\n\n\n1\n2\n# Levene's test with the change for the default, namely center = mean\n\nleveneTest\n(\nwm\n$\niq \n~\n wm\n$\ncondition\n,\n center \n=\n \nmean\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n## Levene's Test for Homogeneity of Variance (center = mean)\n##       Df F value Pr(>F)\n## group  3  0.1598  0.923\n##       76\n\n\n\n\n\n\nThe assumption of homogeneity of variance hold: the within group variance equivalent for all groups.\n\n\nPost-hoc analysis\n\u00b6\n\n\nPost-hoc tests help finding out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error.\n\n\nWhat does it mean to inflate the type I error?\n\n\nSuppose the post-hoc test involves performing three pairwise comparisons, each with the probability of a type I error set at 5%. The probability of making at least one type I error: if you assume independence of these three events, the maximum familywise error rate is then equal to: 1 - (0.95 x 0.95 x 0.95) = 14.26 %.\n\n\nIn other words, the probability of having at least one false alarm (i.e. type I error) is 14.26%.\n\n\nWhat is the maximum familywise error rate for the working memory experiment, assuming that you do all possible pairwise comparisons with a type I error of 5%? 26.49%.\n\n\nNull Hypothesis Significance Testing (NHST) is a statistical method used to test whether or not you are able to reject or retain the null hypothesis. This type of test can confront you with a type I error. This happens when the test rejects the null hypothesis, while it is actually true in reality. Furthermore, the test can also deliver a type II error. This is the failure to reject a null hypothesis when it is false. All hypothesis tests have a probability of making type I and II errors.\n\n\nSensitivity and specificity are two concepts that statisticians use to measure the performance of a statistical test. The sensitivity of a test is its true positive rate:\n\n\n\n\nsensitivity = \\frac{number~of~true~positives}{number~ of~true~positives + number~of~false~negatives}\n\n\nsensitivity = \\frac{number~of~true~positives}{number~ of~true~positives + number~of~false~negatives}\n\n\n\n\nThe specificity of a test is its true negative rate:\n\n\n\n\nspecificity = \\frac{number~of~true~negatives}{number~ of~true~negatives + number~of~false~positives}\n\n\nspecificity = \\frac{number~of~true~negatives}{number~ of~true~negatives + number~of~false~positives}\n\n\n\n\nCalculate both the sensitivity and specificity of the test based on numbers displayed in the NHST table?\n\n\n\n\nThe sensitivity is 0.89 and the specificity is 0.85.\n\n\nCalculate and interpret the results of Tukey\n\n\nIn a situation were you do multiple pairwise comparisons, the probability of type I errors in the process inflates substantially. Therefore, it is better to build in adjustments to take this into account. This is what Tukey tests and other post-hoc procedures do. They adjust the p-value to prevent inflation of the type I error rate. Use Tukey\u2019s procedure.\n\n\n1\n2\n3\n4\n5\n# Read in the data set and assign to the object\n\nwm \n<-\n readWorksheetFromFile\n(\n'A Hands-on Introduction to Statistics with R.xls'\n,\n sheet \n=\n \n'wm'\n,\n header \n=\n \nTRUE\n,\n startCol \n=\n \n1\n,\n startRow \n=\n \n1\n)\n\n\n\n# This will print the data set in the console\n\n\nhead\n(\nwm\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   cond pre post gain train\n## 1  t08   8    9    1     1\n## 2  t08   8   10    2     1\n## 3  t08   8    8    0     1\n## 4  t08   8    7   -1     1\n## 5  t08   9   11    2     1\n## 6  t08   9   10    1     1\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Revision: Analysis of variance\n\nanova_wm \n<-\n aov\n(\nwm\n$\ngain \n~\n wm\n$\ncond\n)\n\n\n\n# Summary Analysis of Variance\n\n\nsummary\n(\nanova_wm\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##              Df Sum Sq Mean Sq F value Pr(>F)    \n## wm$cond       4  274.0   68.51   34.57 <2e-16 ***\n## Residuals   115  227.9    1.98                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\n2\n# Post-hoc (Tukey)\n\nTukeyHSD\n(\nanova_wm\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = wm$gain ~ wm$cond)\n## \n## $`wm$cond`\n##               diff         lwr       upr     p adj\n## t08-control -0.625 -1.69355785 0.4435578 0.4871216\n## t12-control  0.625 -0.44355785 1.6935578 0.4871216\n## t17-control  2.425  1.35644215 3.4935578 0.0000001\n## t19-control  3.625  2.55644215 4.6935578 0.0000000\n## t12-t08      1.250  0.01613568 2.4838643 0.0454650\n## t17-t08      3.050  1.81613568 4.2838643 0.0000000\n## t19-t08      4.250  3.01613568 5.4838643 0.0000000\n## t17-t12      1.800  0.56613568 3.0338643 0.0008953\n## t19-t12      3.000  1.76613568 4.2338643 0.0000000\n## t19-t17      1.200 -0.03386432 2.4338643 0.0607853\n\n\n\n\n\n\n1\n2\n3\n# Plot confidence intervals\n\n\n#plot(c(TukeyHSD(anova_wm)$lwr,TukeyHSD(anova_wm)$upr))\n\nplot\n(\nTukeyHSD\n(\nanova_wm\n))\n\n\n\n\n\n\n\n\n\nBonferroni adjusted p-values\n\n\nJust like Tukey\u2019s procedure, the Bonferroni correction is a method that is used to counteract the problem of inflated type I errors while engaging in multiple pairwise comparisons between subgroups. Bonferroni is generally known as the most conservative method to control the familywise error rate.\n\n\nBonferroni is based on the idea that if you test \nN\n dependent or independent hypotheses, one way of maintaining the familywise error rate is to test each individual hypothesis at a statistical significance level that is deflated by a factor of \n\\frac{1}{n}\n\\frac{1}{n}\n. So, for a significance level for the whole family of tests of \n\u03b1\n, the Bonferroni correction would be to test each of the individual tests at a significance level of \n\\frac{\\alpha}{n}\n\\frac{\\alpha}{n}\n.\n\n\nThe Bonferroni correction is controversial. It is a strict measure to limit false positives and generates conservative p-value. Alternative: increase the sample size, compute the false discovery rate (the expected percent of false predictions in the set of predictions. For example if the algorithm returns 100 results with a false discovery rate of .3 then we should expect 70 of them to be correct.), and the Holm-Bonferroni method.\n\n\n1\n2\n3\n4\n# Use `p.adjust` \n\nbonferroni_ex \n<-\n p.adjust\n(\n.005\n,\n method\n=\n'bonferroni'\n,\n n \n=\n \n8\n)\n\n\nbonferroni_ex\n\n\n\n\n\n\n1\n## [1] 0.04\n\n\n\n\n\n\n1\n2\n# Pairwise T-test\n\npairwise.t.test\n(\nwm\n$\ngain\n,\nwm\n$\ncond\n,\n p.adjust \n=\n \n'bonferroni'\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n## \n##  Pairwise comparisons using t tests with pooled SD \n## \n## data:  wm$gain and wm$cond \n## \n##     control t08     t12     t17    \n## t08 1.00000 -       -       -      \n## t12 1.00000 0.05862 -       -      \n## t17 5.9e-08 3.8e-09 0.00096 -      \n## t19 6.4e-15 2.9e-15 6.7e-09 0.08084\n## \n## P value adjustment method: bonferroni\n\n\n\n\n\n\nBetween groups factorial ANOVA\n\u00b6\n\n\nData exploration with a barplot\n\n\nWe\u2019ll use in this chapter is a randomized controlled experiment investigating the effects of talking on a cell phone while driving. The dependent variable in the experiment is the number of driving errors that subjects made in a driving simulator. There are two independent variables:\n\n\n\n\nConversation difficulty: None, Low, High\n\n\nDriving difficulty: Easy, Difficult\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Read in the data set and assign to the object\n\nab \n<-\n readWorksheetFromFile\n(\n'A Hands-on Introduction to Statistics with R.xls'\n,\n sheet \n=\n \n'ab'\n,\n header \n=\n \nTRUE\n,\n startCol \n=\n \n1\n,\n startRow \n=\n \n1\n)\n\n\nab\n$\nsubject \n<-\n \nas.integer\n(\nab\n$\nsubject\n)\n\nab\n$\nconversation \n<-\n \nas.factor\n(\nab\n$\nconversation\n)\n\nab\n$\ndriving \n<-\n \nas.factor\n(\nab\n$\ndriving\n)\n\nab\n$\nerror \n<-\n \nas.integer\n(\nab\n$\nerror\n)\n\n\n\n# This will print the data set in the console\n\n\nhead\n(\nab\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   subject conversation driving errors error\n## 1       1         None    Easy     20    20\n## 2       2         None    Easy     19    19\n## 3       3         None    Easy     31    31\n## 4       4         None    Easy     27    27\n## 5       5         None    Easy     31    31\n## 6       6         None    Easy     17    17\n\n\n\n\n\n\nEach of the subjects was randomly assigned to one of six conditions formed by combining different values of the independent variables.\n\n\n\n\nsubject\n: unique identifier for each subject conversation: level of conversation difficulty.\n\n\ndriving\n : level of driving difficulty in the simulator.\n\n\nerrors\n: number of driving errors made.\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Use the tapply function to create your groups\n\nab_groups \n<-\n \ntapply\n(\nab\n$\nerrors\n,\n \nlist\n(\nab\n$\ndriving\n,\n ab\n$\nconversation\n),\n \nsum\n)\n\n\n\n# Make the required barplot\n\nbarplot\n(\nab_groups\n,\n beside \n=\n \nTRUE\n,\n col \n=\n \nc\n(\n'orange'\n,\n'blue'\n),\n main \n=\n \n'Driving Errors'\n,\n xlab \n=\n \n'Conversation Demands'\n,\n ylab \n=\n \n'Errors'\n)\n\n\n\n# Add the legend\n\nlegend\n(\n'topright'\n,\n \nc\n(\n'Difficult'\n,\n'Easy'\n),\n title \n=\n \n'Driving'\n,\n fill \n=\n \nc\n(\n'orange'\n,\n'blue'\n))\n\n\n\n\n\n\n\n\n\nThe driving errors made during different driving conditions are influenced by the level of conversation demand. In other words, the driving conditions have a different effect on the number of errors made, depending on the level of conversation demand.\n\n\nThe homogeneity of variance assumption\n\n\nBefore we do factorial ANOVA, we need to test the homogeneity of the variance assumption. When studying one-way ANOVA, we tested this assumption with the \nleveneTest\n function.\n\n\nWe now have two independent variables instead of just one.\n\n\n1\n2\n# Test the homogeneity of variance assumption\n\nleveneTest\n(\nab\n$\nerrors \n~\n ab\n$\nconversation \n*\n ab\n$\ndriving\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n## Levene's Test for Homogeneity of Variance (center = median)\n##        Df F value Pr(>F)\n## group   5  0.5206 0.7602\n##       114\n\n\n\n\n\n\nThe homogeneity of variance assumption holds.\n\n\nBy performing a \nleveneTest\n, we can check whether or not the homogeneity of variance assumption holds for a given dataset. The assumption must hold for the results of an ANOVA analysis to be valid.\n\n\nRecall from the first chapter that ANOVA makes use of F-statistics, or F-ratios, in which two types of degrees of freedom are involved.\n\n\n1\ndim\n(\nab\n)\n\n\n\n\n\n\n\n1\n## [1] 120   5\n\n\n\n\n\n\n1\nstr\n(\nab\n$\nconversation\n)\n\n\n\n\n\n\n\n1\n##  Factor w/ 3 levels \"High demand\",..: 3 3 3 3 3 3 3 3 3 3 ...\n\n\n\n\n\n\n1\nstr\n(\nab\n$\ndriving\n)\n\n\n\n\n\n\n\n1\n##  Factor w/ 2 levels \"Difficult\",\"Easy\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\n\n\n\n\nThere are 120 subjets and 2 (Easy, Difficult) * 3 (High Demand, Low Demand, None) = 6 groups.\n\n\nThe factorial ANOVA\n\n\n1\n2\n3\n4\n5\n# Factorial ANOVA\n\nab_model \n<-\n aov\n(\nab\n$\nerrors \n~\n ab\n$\nconversation \n*\n ab\n$\ndriving\n)\n\n\n\n# Get the summary table\n\n\nsummary\n(\nab_model\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##                             Df Sum Sq Mean Sq F value   Pr(>F)    \n## ab$conversation              2   4416    2208   36.14 6.98e-13 ***\n## ab$driving                   1   5782    5782   94.64  < 2e-16 ***\n## ab$conversation:ab$driving   2   1639     820   13.41 5.86e-06 ***\n## Residuals                  114   6965      61                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nBased on the summary table of the factorial ANOVA, the main effect for driving difficulty, the main effect for conversation difficulty and the interaction effect are all significant.\n\n\nThe interaction effect\n\n\nNow it\u2019s time to explore the interaction effect. You will do this with the help of a simple effects analysis.\n\n\nWhy a simple effects analysis? Well, remember what we had to do when you had a significant main effect in a one-way ANOVA? There, you just had to perform some post-hoc tests to see from which level of the categorical variable the main effect was coming. With an interaction effect, it is quite similar. Conduct a simple effects analysis of the variable \nconversation\n on the outcome variable \nerrors\n at each level of \ndriving\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Create the two subsets\n\nab_1 \n<-\n \nsubset\n(\nab\n,\n ab\n$\ndriving \n==\n \n'Easy'\n)\n  \nab_2 \n<-\n \nsubset\n(\nab\n,\n ab\n$\ndriving \n==\n \n'Difficult'\n)\n\n\n\n# Perform the one-way ANOVA for both subsets\n\naov_ab_1 \n<-\n aov\n(\nab_1\n$\nerrors \n~\n ab_1\n$\nconversation\n)\n\naov_ab_2 \n<-\n aov\n(\nab_2\n$\nerrors \n~\n ab_2\n$\nconversation\n)\n\n\n\n# Get the summary tables for both aov_ab_1 and aov_ab_2\n\n\nsummary\n(\naov_ab_1\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##                   Df Sum Sq Mean Sq F value Pr(>F)  \n## ab_1$conversation  2  504.7   252.3   4.928 0.0106 *\n## Residuals         57 2918.5    51.2                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\nsummary\n(\naov_ab_2\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##                   Df Sum Sq Mean Sq F value   Pr(>F)    \n## ab_2$conversation  2   5551    2776   39.09 2.05e-11 ***\n## Residuals         57   4047      71                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nThere a significant simple effect for the easy driving condition based on the summary table of \naov_ab_1\n.\n\n\nThere a significant simple effect for the difficult driving condition based on the summary table of \naov_ab_2\n.\n\n\nThe effect sizes\n\n\nThe definition of an interaction effect states that the effect of one variable changes across levels of the other variable. For example, we might expect the effect of conversation to be greater when driving conditions are difficult than when they are relatively easy.\n\n\nUnfortunately, it is not quite that simple. In order to really understand the different effect sizes, you should make use of the \netaSquared\n function.\n\n\n1\n2\n3\n4\n5\n6\nlibrary\n(\nlsr\n)\n\n\n\n# Calculate the etaSquared for the easy driving case\n\n\n#easy is ab_1\n\n\n#difficult is ab_2\n\netaSquared\n(\naov_ab_1\n,\n anova \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##                     eta.sq eta.sq.part      SS df        MS        F\n## ab_1$conversation 0.147433    0.147433  504.70  2 252.35000 4.928458\n## Residuals         0.852567          NA 2918.55 57  51.20263       NA\n##                            p\n## ab_1$conversation 0.01061116\n## Residuals                 NA\n\n\n\n\n\n\n1\n2\n# Calculate the etaSquared for the difficult driving case\n\netaSquared\n(\naov_ab_2\n,\n anova \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##                      eta.sq eta.sq.part       SS df         MS        F\n## ab_2$conversation 0.5783571   0.5783571 5551.033  2 2775.51667 39.09275\n## Residuals         0.4216429          NA 4046.900 57   70.99825       NA\n##                              p\n## ab_2$conversation 2.046097e-11\n## Residuals                   NA\n\n\n\n\n\n\nBased on the output of the \netaSquared\n function for the easy driving condition, the percentage of variance explained by the conversation variable is 14.7%; the percentage of variance explained by the conversation variable is 57.8%.\n\n\nPairwise comparisons\n\n\nFinally, let us look at pairwise comparisons for the simple effects. You can do this with the Tukey post-hoc test.\n\n\n1\n2\n# Tukey for easy driving\n\nTukeyHSD\n(\naov_ab_1\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = ab_1$errors ~ ab_1$conversation)\n## \n## $`ab_1$conversation`\n##                         diff        lwr        upr     p adj\n## Low demand-High demand -6.05 -11.495243 -0.6047574 0.0260458\n## None-High demand       -6.25 -11.695243 -0.8047574 0.0207614\n## None-Low demand        -0.20  -5.645243  5.2452426 0.9957026\n\n\n\n\n\n\n1\n2\n# Tukey for difficult driving\n\nTukeyHSD\n(\naov_ab_2\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = ab_2$errors ~ ab_2$conversation)\n## \n## $`ab_2$conversation`\n##                          diff       lwr        upr     p adj\n## Low demand-High demand  -9.75 -16.16202  -3.337979 0.0015849\n## None-High demand       -23.45 -29.86202 -17.037979 0.0000000\n## None-Low demand        -13.70 -20.11202  -7.287979 0.0000103\n\n\n\n\n\n\nFor \u2018Easy Driving\u2019, two mean differences in terms of number of errors are significant (below 0.05).\n\n\nFor \u2018Difficult Driving\u2019, three mean differences in terms of number of errors are significant (below 0.05).",
            "title": "Statistics with R, Course Three, Analysis of Variance"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/#post-hoc-analysis",
            "text": "Post-hoc tests help finding out which groups differ significantly from one other and which do not. More formally, post-hoc tests allow for multiple pairwise comparisons without inflating the type I error.  What does it mean to inflate the type I error?  Suppose the post-hoc test involves performing three pairwise comparisons, each with the probability of a type I error set at 5%. The probability of making at least one type I error: if you assume independence of these three events, the maximum familywise error rate is then equal to: 1 - (0.95 x 0.95 x 0.95) = 14.26 %.  In other words, the probability of having at least one false alarm (i.e. type I error) is 14.26%.  What is the maximum familywise error rate for the working memory experiment, assuming that you do all possible pairwise comparisons with a type I error of 5%? 26.49%.  Null Hypothesis Significance Testing (NHST) is a statistical method used to test whether or not you are able to reject or retain the null hypothesis. This type of test can confront you with a type I error. This happens when the test rejects the null hypothesis, while it is actually true in reality. Furthermore, the test can also deliver a type II error. This is the failure to reject a null hypothesis when it is false. All hypothesis tests have a probability of making type I and II errors.  Sensitivity and specificity are two concepts that statisticians use to measure the performance of a statistical test. The sensitivity of a test is its true positive rate:   sensitivity = \\frac{number~of~true~positives}{number~ of~true~positives + number~of~false~negatives}  sensitivity = \\frac{number~of~true~positives}{number~ of~true~positives + number~of~false~negatives}   The specificity of a test is its true negative rate:   specificity = \\frac{number~of~true~negatives}{number~ of~true~negatives + number~of~false~positives}  specificity = \\frac{number~of~true~negatives}{number~ of~true~negatives + number~of~false~positives}   Calculate both the sensitivity and specificity of the test based on numbers displayed in the NHST table?   The sensitivity is 0.89 and the specificity is 0.85.  Calculate and interpret the results of Tukey  In a situation were you do multiple pairwise comparisons, the probability of type I errors in the process inflates substantially. Therefore, it is better to build in adjustments to take this into account. This is what Tukey tests and other post-hoc procedures do. They adjust the p-value to prevent inflation of the type I error rate. Use Tukey\u2019s procedure.  1\n2\n3\n4\n5 # Read in the data set and assign to the object \nwm  <-  readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' ,  sheet  =   'wm' ,  header  =   TRUE ,  startCol  =   1 ,  startRow  =   1 )  # This will print the data set in the console  head ( wm )    1\n2\n3\n4\n5\n6\n7 ##   cond pre post gain train\n## 1  t08   8    9    1     1\n## 2  t08   8   10    2     1\n## 3  t08   8    8    0     1\n## 4  t08   8    7   -1     1\n## 5  t08   9   11    2     1\n## 6  t08   9   10    1     1   1\n2\n3\n4\n5 # Revision: Analysis of variance \nanova_wm  <-  aov ( wm $ gain  ~  wm $ cond )  # Summary Analysis of Variance  summary ( anova_wm )    1\n2\n3\n4\n5 ##              Df Sum Sq Mean Sq F value Pr(>F)    \n## wm$cond       4  274.0   68.51   34.57 <2e-16 ***\n## Residuals   115  227.9    1.98                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1\n2 # Post-hoc (Tukey) \nTukeyHSD ( anova_wm )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 ##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = wm$gain ~ wm$cond)\n## \n## $`wm$cond`\n##               diff         lwr       upr     p adj\n## t08-control -0.625 -1.69355785 0.4435578 0.4871216\n## t12-control  0.625 -0.44355785 1.6935578 0.4871216\n## t17-control  2.425  1.35644215 3.4935578 0.0000001\n## t19-control  3.625  2.55644215 4.6935578 0.0000000\n## t12-t08      1.250  0.01613568 2.4838643 0.0454650\n## t17-t08      3.050  1.81613568 4.2838643 0.0000000\n## t19-t08      4.250  3.01613568 5.4838643 0.0000000\n## t17-t12      1.800  0.56613568 3.0338643 0.0008953\n## t19-t12      3.000  1.76613568 4.2338643 0.0000000\n## t19-t17      1.200 -0.03386432 2.4338643 0.0607853   1\n2\n3 # Plot confidence intervals  #plot(c(TukeyHSD(anova_wm)$lwr,TukeyHSD(anova_wm)$upr)) \nplot ( TukeyHSD ( anova_wm ))     Bonferroni adjusted p-values  Just like Tukey\u2019s procedure, the Bonferroni correction is a method that is used to counteract the problem of inflated type I errors while engaging in multiple pairwise comparisons between subgroups. Bonferroni is generally known as the most conservative method to control the familywise error rate.  Bonferroni is based on the idea that if you test  N  dependent or independent hypotheses, one way of maintaining the familywise error rate is to test each individual hypothesis at a statistical significance level that is deflated by a factor of  \\frac{1}{n} \\frac{1}{n} . So, for a significance level for the whole family of tests of  \u03b1 , the Bonferroni correction would be to test each of the individual tests at a significance level of  \\frac{\\alpha}{n} \\frac{\\alpha}{n} .  The Bonferroni correction is controversial. It is a strict measure to limit false positives and generates conservative p-value. Alternative: increase the sample size, compute the false discovery rate (the expected percent of false predictions in the set of predictions. For example if the algorithm returns 100 results with a false discovery rate of .3 then we should expect 70 of them to be correct.), and the Holm-Bonferroni method.  1\n2\n3\n4 # Use `p.adjust`  \nbonferroni_ex  <-  p.adjust ( .005 ,  method = 'bonferroni' ,  n  =   8 ) \n\nbonferroni_ex   1 ## [1] 0.04   1\n2 # Pairwise T-test \npairwise.t.test ( wm $ gain , wm $ cond ,  p.adjust  =   'bonferroni' )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ## \n##  Pairwise comparisons using t tests with pooled SD \n## \n## data:  wm$gain and wm$cond \n## \n##     control t08     t12     t17    \n## t08 1.00000 -       -       -      \n## t12 1.00000 0.05862 -       -      \n## t17 5.9e-08 3.8e-09 0.00096 -      \n## t19 6.4e-15 2.9e-15 6.7e-09 0.08084\n## \n## P value adjustment method: bonferroni",
            "title": "Post-hoc analysis"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Three,_Analysis_of_Variance/#between-groups-factorial-anova",
            "text": "Data exploration with a barplot  We\u2019ll use in this chapter is a randomized controlled experiment investigating the effects of talking on a cell phone while driving. The dependent variable in the experiment is the number of driving errors that subjects made in a driving simulator. There are two independent variables:   Conversation difficulty: None, Low, High  Driving difficulty: Easy, Difficult     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Read in the data set and assign to the object \nab  <-  readWorksheetFromFile ( 'A Hands-on Introduction to Statistics with R.xls' ,  sheet  =   'ab' ,  header  =   TRUE ,  startCol  =   1 ,  startRow  =   1 ) \n\nab $ subject  <-   as.integer ( ab $ subject ) \nab $ conversation  <-   as.factor ( ab $ conversation ) \nab $ driving  <-   as.factor ( ab $ driving ) \nab $ error  <-   as.integer ( ab $ error )  # This will print the data set in the console  head ( ab )    1\n2\n3\n4\n5\n6\n7 ##   subject conversation driving errors error\n## 1       1         None    Easy     20    20\n## 2       2         None    Easy     19    19\n## 3       3         None    Easy     31    31\n## 4       4         None    Easy     27    27\n## 5       5         None    Easy     31    31\n## 6       6         None    Easy     17    17   Each of the subjects was randomly assigned to one of six conditions formed by combining different values of the independent variables.   subject : unique identifier for each subject conversation: level of conversation difficulty.  driving  : level of driving difficulty in the simulator.  errors : number of driving errors made.    1\n2\n3\n4\n5\n6\n7\n8 # Use the tapply function to create your groups \nab_groups  <-   tapply ( ab $ errors ,   list ( ab $ driving ,  ab $ conversation ),   sum )  # Make the required barplot \nbarplot ( ab_groups ,  beside  =   TRUE ,  col  =   c ( 'orange' , 'blue' ),  main  =   'Driving Errors' ,  xlab  =   'Conversation Demands' ,  ylab  =   'Errors' )  # Add the legend \nlegend ( 'topright' ,   c ( 'Difficult' , 'Easy' ),  title  =   'Driving' ,  fill  =   c ( 'orange' , 'blue' ))     The driving errors made during different driving conditions are influenced by the level of conversation demand. In other words, the driving conditions have a different effect on the number of errors made, depending on the level of conversation demand.  The homogeneity of variance assumption  Before we do factorial ANOVA, we need to test the homogeneity of the variance assumption. When studying one-way ANOVA, we tested this assumption with the  leveneTest  function.  We now have two independent variables instead of just one.  1\n2 # Test the homogeneity of variance assumption \nleveneTest ( ab $ errors  ~  ab $ conversation  *  ab $ driving )    1\n2\n3\n4 ## Levene's Test for Homogeneity of Variance (center = median)\n##        Df F value Pr(>F)\n## group   5  0.5206 0.7602\n##       114   The homogeneity of variance assumption holds.  By performing a  leveneTest , we can check whether or not the homogeneity of variance assumption holds for a given dataset. The assumption must hold for the results of an ANOVA analysis to be valid.  Recall from the first chapter that ANOVA makes use of F-statistics, or F-ratios, in which two types of degrees of freedom are involved.  1 dim ( ab )    1 ## [1] 120   5   1 str ( ab $ conversation )    1 ##  Factor w/ 3 levels \"High demand\",..: 3 3 3 3 3 3 3 3 3 3 ...   1 str ( ab $ driving )    1 ##  Factor w/ 2 levels \"Difficult\",\"Easy\": 2 2 2 2 2 2 2 2 2 2 ...   There are 120 subjets and 2 (Easy, Difficult) * 3 (High Demand, Low Demand, None) = 6 groups.  The factorial ANOVA  1\n2\n3\n4\n5 # Factorial ANOVA \nab_model  <-  aov ( ab $ errors  ~  ab $ conversation  *  ab $ driving )  # Get the summary table  summary ( ab_model )    1\n2\n3\n4\n5\n6\n7 ##                             Df Sum Sq Mean Sq F value   Pr(>F)    \n## ab$conversation              2   4416    2208   36.14 6.98e-13 ***\n## ab$driving                   1   5782    5782   94.64  < 2e-16 ***\n## ab$conversation:ab$driving   2   1639     820   13.41 5.86e-06 ***\n## Residuals                  114   6965      61                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   Based on the summary table of the factorial ANOVA, the main effect for driving difficulty, the main effect for conversation difficulty and the interaction effect are all significant.  The interaction effect  Now it\u2019s time to explore the interaction effect. You will do this with the help of a simple effects analysis.  Why a simple effects analysis? Well, remember what we had to do when you had a significant main effect in a one-way ANOVA? There, you just had to perform some post-hoc tests to see from which level of the categorical variable the main effect was coming. With an interaction effect, it is quite similar. Conduct a simple effects analysis of the variable  conversation  on the outcome variable  errors  at each level of  driving .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Create the two subsets \nab_1  <-   subset ( ab ,  ab $ driving  ==   'Easy' )   \nab_2  <-   subset ( ab ,  ab $ driving  ==   'Difficult' )  # Perform the one-way ANOVA for both subsets \naov_ab_1  <-  aov ( ab_1 $ errors  ~  ab_1 $ conversation ) \naov_ab_2  <-  aov ( ab_2 $ errors  ~  ab_2 $ conversation )  # Get the summary tables for both aov_ab_1 and aov_ab_2  summary ( aov_ab_1 )    1\n2\n3\n4\n5 ##                   Df Sum Sq Mean Sq F value Pr(>F)  \n## ab_1$conversation  2  504.7   252.3   4.928 0.0106 *\n## Residuals         57 2918.5    51.2                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1 summary ( aov_ab_2 )    1\n2\n3\n4\n5 ##                   Df Sum Sq Mean Sq F value   Pr(>F)    \n## ab_2$conversation  2   5551    2776   39.09 2.05e-11 ***\n## Residuals         57   4047      71                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   There a significant simple effect for the easy driving condition based on the summary table of  aov_ab_1 .  There a significant simple effect for the difficult driving condition based on the summary table of  aov_ab_2 .  The effect sizes  The definition of an interaction effect states that the effect of one variable changes across levels of the other variable. For example, we might expect the effect of conversation to be greater when driving conditions are difficult than when they are relatively easy.  Unfortunately, it is not quite that simple. In order to really understand the different effect sizes, you should make use of the  etaSquared  function.  1\n2\n3\n4\n5\n6 library ( lsr )  # Calculate the etaSquared for the easy driving case  #easy is ab_1  #difficult is ab_2 \netaSquared ( aov_ab_1 ,  anova  =   TRUE )    1\n2\n3\n4\n5\n6 ##                     eta.sq eta.sq.part      SS df        MS        F\n## ab_1$conversation 0.147433    0.147433  504.70  2 252.35000 4.928458\n## Residuals         0.852567          NA 2918.55 57  51.20263       NA\n##                            p\n## ab_1$conversation 0.01061116\n## Residuals                 NA   1\n2 # Calculate the etaSquared for the difficult driving case \netaSquared ( aov_ab_2 ,  anova  =   TRUE )    1\n2\n3\n4\n5\n6 ##                      eta.sq eta.sq.part       SS df         MS        F\n## ab_2$conversation 0.5783571   0.5783571 5551.033  2 2775.51667 39.09275\n## Residuals         0.4216429          NA 4046.900 57   70.99825       NA\n##                              p\n## ab_2$conversation 2.046097e-11\n## Residuals                   NA   Based on the output of the  etaSquared  function for the easy driving condition, the percentage of variance explained by the conversation variable is 14.7%; the percentage of variance explained by the conversation variable is 57.8%.  Pairwise comparisons  Finally, let us look at pairwise comparisons for the simple effects. You can do this with the Tukey post-hoc test.  1\n2 # Tukey for easy driving \nTukeyHSD ( aov_ab_1 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = ab_1$errors ~ ab_1$conversation)\n## \n## $`ab_1$conversation`\n##                         diff        lwr        upr     p adj\n## Low demand-High demand -6.05 -11.495243 -0.6047574 0.0260458\n## None-High demand       -6.25 -11.695243 -0.8047574 0.0207614\n## None-Low demand        -0.20  -5.645243  5.2452426 0.9957026   1\n2 # Tukey for difficult driving \nTukeyHSD ( aov_ab_2 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = ab_2$errors ~ ab_2$conversation)\n## \n## $`ab_2$conversation`\n##                          diff       lwr        upr     p adj\n## Low demand-High demand  -9.75 -16.16202  -3.337979 0.0015849\n## None-High demand       -23.45 -29.86202 -17.037979 0.0000000\n## None-Low demand        -13.70 -20.11202  -7.287979 0.0000103   For \u2018Easy Driving\u2019, two mean differences in terms of number of errors are significant (below 0.05).  For \u2018Difficult Driving\u2019, three mean differences in terms of number of errors are significant (below 0.05).",
            "title": "Between groups factorial ANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Four,_Repeated_Measures_Anova/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nAn introduction to repeated measures\n\u00b6\n\n\nThe independent t-test is analogous to between-groups ANOVA and the paired-sample t-test is analogous to repeated measures ANOVA.\n\n\nIn a between-groups design, each subject is exposed to two or more treatments or conditions over time. In a within-subjects design, each subject is allocated to exactly one treatment or condition.\n\n\nBetween:\n\n\n\n\nExperiment 1: You want to test the effect of alcohol on test scores of students. There are three conditions: the student consumed no alcohol, two glasses of beer, or five glasses of beer. Alcohol tolerance and time spent studying should also be considered somehow.\n\n\nExperiment 2: You want to investigate the effects of certain fertilizers on plant growth. Assume you have two different\n\n    fertilizers, A and B. Consider three conditions: you give the plant no fertilizer, fertilizer A, or fertilizer B. You measure the height of the plant after a specific period of time to see whether the fertilizers had an effect.\n\n\n\n\nUse a within-subjects design for for Experiment 1 and between-groups design for Experiment 2.\n\n\nIs it always either manipulation between-groups or manipulation within-groups, or are there experiments where you could use either approach? In some cases, either approach is possible.\n\n\nExplore the working memory data\n\n\n1\n2\n# Print the data set in the console\n\nstr\n(\nwm\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n## 'data.frame':    80 obs. of  3 variables:\n##  $ subject  : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ condition: Factor w/ 4 levels \"12 days\",\"17 days\",..: 4 4 4 4 4 4 4 4 4 4 ...\n##  $ iq       : num  12.4 11.8 14.6 7.7 15.7 11.6 7 8.4 10.7 10.6 ...\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nlibrary\n(\npsych\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\n\n# Define the variable subject as a categorical variable\n\nwm\n$\nsubject \n<-\n \nfactor\n(\nwm\n$\nsubject\n)\n\n\n\n# Summary statistics by all groups (8 sessions, 12 sessions, 17 sessions, 19 sessions)\n\ndescribeBy\n(\nwm\n,\n wm\n$\ncondition\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n## \n##  Descriptive statistics by group \n## group: 12 days\n##            vars  n mean   sd median trimmed  mad min  max range skew\n## subject*      1 20 10.5 5.92  10.50   10.50 7.41 1.0 20.0  19.0 0.00\n## condition*    2 20  1.0 0.00   1.00    1.00 0.00 1.0  1.0   0.0  NaN\n## iq            3 20 11.7 2.58  11.65   11.69 2.89 6.9 16.1   9.2 0.05\n##            kurtosis   se\n## subject*      -1.38 1.32\n## condition*      NaN 0.00\n## iq            -1.06 0.58\n## -------------------------------------------------------- \n## group: 17 days\n##            vars  n mean   sd median trimmed  mad min  max range skew\n## subject*      1 20 10.5 5.92   10.5    10.5 7.41 1.0 20.0  19.0 0.00\n## condition*    2 20  2.0 0.00    2.0     2.0 0.00 2.0  2.0   0.0  NaN\n## iq            3 20 13.9 2.26   13.6    13.9 2.00 9.8 18.1   8.3 0.11\n##            kurtosis   se\n## subject*      -1.38 1.32\n## condition*      NaN 0.00\n## iq            -0.85 0.50\n## -------------------------------------------------------- \n## group: 19 days\n##            vars  n  mean   sd median trimmed  mad  min  max range  skew\n## subject*      1 20 10.50 5.92   10.5   10.50 7.41  1.0 20.0  19.0  0.00\n## condition*    2 20  3.00 0.00    3.0    3.00 0.00  3.0  3.0   0.0   NaN\n## iq            3 20 14.75 2.50   15.3   14.71 2.15 10.4 19.2   8.8 -0.09\n##            kurtosis   se\n## subject*      -1.38 1.32\n## condition*      NaN 0.00\n## iq            -0.99 0.56\n## -------------------------------------------------------- \n## group: 8 days\n##            vars  n  mean   sd median trimmed  mad min  max range  skew\n## subject*      1 20 10.50 5.92   10.5   10.50 7.41 1.0 20.0  19.0  0.00\n## condition*    2 20  4.00 0.00    4.0    4.00 0.00 4.0  4.0   0.0   NaN\n## iq            3 20 10.91 2.63   11.3   10.97 2.67 5.4 15.7  10.3 -0.21\n##            kurtosis   se\n## subject*      -1.38 1.32\n## condition*      NaN 0.00\n## iq            -0.70 0.59\n\n\n\n\n\n\n1\n2\n# Boxplot IQ versus condition\n\nboxplot\n(\nwm\n$\niq \n~\n wm\n$\ncondition\n,\n main \n=\n \n'Boxplot'\n,\n xlab \n=\n \n'Training sessions'\n,\n ylab \n=\n \n'IQ'\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# Illustration data, each line represents the development of each subject by number of trainings\n\nggplot\n(\ndata \n=\n wm\n,\n aes\n(\nx \n=\n wm\n$\ncondition\n,\n y \n=\n wm\n$\niq\n,\n group \n=\n wm\n$\nsubject\n,\n colour \n=\n wm\n$\nsubject\n))\n \n+\n geom_line\n()\n \n+\n geom_point\n()\n\n\n\n\n\n\n\n\n\nReduced cost\n\n\nThe cost advantage of using manipulation within groups verses manipulation between groups for the working memory experiment is you need 60 subjects fewer.\n\n\nStatistically more powerful\n\n\nRepeated measures analysis accounts for individual differences across the experiment. This reduces the error term, which  increases statistical power.\n\n\nCounterbalancing\n\n\nSuppose you have three levels of an independent variable A (i.e. A1, A2, A3) and a blocked design. You want to use full counterbalancing to take into account order effects.\n\n\nWhat are all the possible orders that you need to use? In other words, what are the order conditions?\n\n\n(A1, A2, A3), (A1, A3, A2) , (A2, A1, A3), (A2, A3, A1) , (A3, A2, A1),\n\n(A3, A1, A2)\n\n\nNumber of order conditions?\n\n\nAssume the number of levels of the independent variable goes up and you want to completely counterbalance. What will happen to the number of order conditions you\u2019ll need?\n\n\nThe number becomes really large. An independent variable with n levels will have \nn!=n*(n???1)*(n???2)*...\n order conditions.\n\n\nLatin Squares design\n\n\nAs you hopefully realized in the previous exercise, completely counterbalancing is not always a practical solution for taking into account order effects. This is because the number of different orders required gets really large as the number of possible conditions increases.\n\n\nThe most common workaround to this problem is the Latin Squares design, in which you do not completely counterbalance, but instead put each condition at every position (at least) once.\n\n\nWhich of the following examples has been constructed according to the Latin Squares design?\n\n\n(A1, A2, A3), (A2, A3, A1), (A3, A1, A2)\n\n\nMore on Latin Squares\n\n\nThe number of order conditions is always equal to the number of levels of your independent variable.\n\n\nWhy is missing data a problem?\n\n\nIn a between-groups design, it is okay to have a slightly different number of subjects in each group, so if one subject drops out in one of the conditions then that group has just one less subject. Now you want to look at how subjects change over time and the different scores between two or more conditions for each subject.\n\n\nUnderstanding sphericity\n\n\nThe variances of the differences between all possible pairs of groups (i.e. levels of the independent variable) are equal.\n\n\nMauchly\u2019s test\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Define iq as a data frame where each column represents a condition\n\niq \n<-\n \ncbind\n(\nwm\n$\niq\n[\nwm\n$\ncondition \n==\n \n'8 days'\n],\n wm\n$\niq\n[\nwm\n$\ncondition \n==\n \n'12 days'\n],\n wm\n$\niq\n[\nwm\n$\ncondition \n==\n \n'17 days'\n],\n wm\n$\niq\n[\nwm\n$\ncondition \n==\n \n'19 days'\n])\n\n\n\n# Make an mlm object\n\nmlm \n<-\n lm\n(\niq \n~\n \n1\n)\n\n\n\n# Mauchly's test\n\nmauchly.test\n(\nmlm\n,\n x \n=\n \n~\n \n1\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Mauchly's test of sphericity\n## \n## data:  SSD matrix from lm(formula = iq ~ 1)\n## W = 0.81725, p-value = 0.9407\n\n\n\n\n\n\nBased on the results, the sphericity assumption holds.\n\n\nPros of repeated measures\n\n\nLess cost and statistically more powerful\n\n\nCons of repeated measures\n\n\nOrder effects, counterbalancing, missing data, and an extra assumption\n\n\nRepeated measures ANOVA\n\u00b6\n\n\nThe systematic between groups variance\n\n\nTo understand everything a bit better, we will calculate the F-ratio for a repeated measures design by ourself in the next exercises.\n\n\nFirst, we will need the systematic between-groups variance. This is the same as in the between-groups design\u2013the variance due to grouping by condition.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n# Define number of subjects for each condition\n\nn \n<-\n \n20\n\n\n\n# Calculate group means\n\ny_j \n<-\n \ntapply\n(\nwm\n$\niq\n,\n wm\n$\ncondition\n,\n \nmean\n)\n\n\n\n# Calculate the grand mean\n\ny_t \n<-\n \nmean\n(\ny_j\n)\n\n\n\n# Calculate the sum of squares\n\nss_cond \n<-\n \nsum\n((\ny_j \n-\n y_t\n)\n^\n2\n)\n*\nn\n\n\n# Define the degrees of freedom for conditions\n\ndf \n<-\n \n(\n4\n \n-\n \n1\n)\n\n\n\n# Calculate the mean squares (variance)\n\nms_cond \n<-\n ss_cond\n/\ndf\n\n\n\n\n\n\nThe subject variance\n\n\nWe will also need the error term of the repeated measures design. This can be calculated in a few steps.\n\n\n\n\nFirst calculate the systematic variance due to subjects.\n\n\nBelow, we will calculate the unsystematic variance, like we did with the between-groups design.\n\n\nIf we subtract these two results, we will get the error term of the repeated measures design.\n\n\n\n\nThe systematic (stable) subject variance will be taken out of the error term, so the error term is reduced in comparison with the between-groups design.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n# Define number of conditions for each subject\n\nn \n<-\n \n4\n\n\n\n# Calculate subject means\n\ny_j \n<-\n \ntapply\n(\nwm\n$\niq\n,\n wm\n$\nsubject\n,\n \nmean\n)\n\n\n\n# Calculate the grand mean\n\ny_t \n<-\n \nmean\n(\ny_j\n)\n\n\n\n# Calculate the sum of squares\n\nss_subjects \n<-\n \nsum\n((\ny_j \n-\n y_t\n)\n^\n2\n)\n*\nn\n\n\n# Define the degrees of freedom for subjects\n\ndf \n<-\n \n(\n20\n \n-\n \n1\n)\n\n\n\n# Calculate the mean squares (variance)\n\nms_subjects \n<-\n ss_subjects\n/\ndf\n\n\n\n\n\n\nThe unsystematic within groups variance\n\n\nTo calculate the error term of the repeated measures design, we need the unsystematic within-groups variance: the unsystematic variance or the error term of the between-groups design.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n# Create four subsets of the four groups, containing the IQ results\n\n    \n# Make the subset for the group condition = \"8 days\"\n\ny_i1 \n<-\n \nsubset\n(\nwm\n$\niq\n,\n wm\n$\ncondition \n==\n \n\"8 days\"\n)\n\n    \n# Make the subset for the group condition = \"12 days\"\n\ny_i2 \n<-\n \nsubset\n(\nwm\n$\niq\n,\n wm\n$\ncondition \n==\n \n\"12 days\"\n)\n\n    \n# Make the subset for the group condition = \"17 days\"\n\ny_i3 \n<-\n \nsubset\n(\nwm\n$\niq\n,\n wm\n$\ncondition \n==\n \n\"17 days\"\n)\n\n    \n# Make the subset for the group condition = \"19 days\"\n\ny_i4 \n<-\n \nsubset\n(\nwm\n$\niq\n,\n wm\n$\ncondition \n==\n \n\"19 days\"\n)\n\n\n\n# Subtract the individual values by their group means\n\ns_1 \n<-\n y_i1 \n-\n \nmean\n(\ny_i1\n)\n\ns_2 \n<-\n y_i2 \n-\n \nmean\n(\ny_i2\n)\n\ns_3 \n<-\n y_i3 \n-\n \nmean\n(\ny_i3\n)\n\ns_4 \n<-\n y_i4 \n-\n \nmean\n(\ny_i4\n)\n\n\n\n# Put everything back into one vector\n\ns_t \n<-\n \nc\n(\ns_1\n,\n s_2\n,\n s_3\n,\n s_4\n)\n\n\n\n# Calculate the within sum of squares by using the vector s_t\n\nss_sa \n<-\n \nsum\n(\ns_t\n^\n2\n)\n\n\n\n# Define the degrees of freedom\n\ndf \n<-\n \n4\n*\n(\n20-1\n)\n\n\n\n# Calculate the mean squares (variances)\n\nms_sa \n<-\n ss_sa\n/\ndf\n\n\n\n\n\n\nThe unsystematic variance for the repeated measures design\n\n\nNow we can easily calculate the unsystematic variance for the repeated measures design, also called the error term.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# ss_sa = ss_subjects + ss_rm\n\nss_rm \n<-\n ss_sa \n-\n ss_subjects\n\n\n# Define the degrees of freedom\n\ndf \n<-\n \n(\n20\n \n-\n \n1\n)\n*\n(\n4\n \n-\n \n1\n)\n\n\n\n# Calculate the mean squares (variances)\n\nms_rm \n<-\n ss_rm\n/\ndf\n\n\n\n\n\n\nNow we\u2019ve calculated the error term of the repeated measures design, which is clearly smaller than the error term of the between-groups design because you reduced this one. To complete you can now calculate the F-ratio and the corresponding p-value.\n\n\nF-ratio and p-value\n\n\nTo do the ANOVA analysis we actually need the F-ratio and the corresponding p-value.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Calculate the F-ratio\n\nf_rat \n<-\n ms_cond\n/\nms_rm\n\n\n# Define the degrees of freedom of the F-distribution\n\n\n# df1 for freedom of conditions (ss_cond)\n\n\n# df2 for (freedom of) conditions and subjects (sa_sa)\n\ndf1 \n<-\n \n(\n4\n \n-\n \n1\n)\n\ndf2 \n<-\n \n(\n4\n \n-\n \n1\n)\n*\n(\n20\n \n-\n \n1\n)\n\n\n\n# Calculate the p-value\n\np \n<-\n \n1\n \n-\n pf\n(\nf_rat\n,\n df1\n,\n df2\n)\n\n\n\n\n\n\n\nError term in a repeated measures design?\n\n\nThe inconsistent individual differences across conditions, the effect of subjects that differ across conditions. So it is an interaction between subjects and condition.\n\n\nAnova in R\n\n\nANOVA in R is usually done with the \naov\n function.\n\n\n1\n2\n3\n4\n5\n# anova model\n\nmodel \n<-\n aov\n(\nwm\n$\niq \n~\n wm\n$\ncondition \n+\n Error\n(\nwm\n$\nsubject \n/\n wm\n$\ncondition\n))\n\n\n\n# summary model\n\n\nsummary\n(\nmodel\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n## Error: wm$subject\n##           Df Sum Sq Mean Sq F value Pr(>F)\n## Residuals 19  175.6   9.242               \n## \n## Error: wm$subject:wm$condition\n##              Df Sum Sq Mean Sq F value   Pr(>F)    \n## wm$condition  3  196.1   65.36   12.51 2.16e-06 ***\n## Residuals    57  297.8    5.22                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nThe F-ratio is significant. Therefore, the number of training days does affect the IQ scores.\n\n\nEffect size\n\n\nCalculate eta-squared, which helps you estimate effect size.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Define the total sum of squares\n\n\n\n# ss_cond (syst. between groups or of the effect), \n\n\n# ss_sa (unsyst. within groups) =\n\n\n# ss_rm (unsyst. for repeated measures design) + # ss_subjects (ok)\n\nss_total \n<-\n ss_cond \n+\n ss_rm\n\n\n# Calculate the effect size\n\neta_sq \n<-\n ss_cond \n/\n ss_total\n\n\n\n\n\n\nPost-hoc test one\n\n\nWe will now look at a few different procedures for the post-hoc test. Let\u2019s start with the Holm procedure.\n\n\n1\n2\n# Post-hoc test: default procedure\n\n\nwith\n(\nwm\n,\n pairwise.t.test\n(\niq\n,\n condition\n,\n paired \n=\n \nT\n))\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  Pairwise comparisons using paired t tests \n## \n## data:  iq and condition \n## \n##         12 days 17 days 19 days\n## 17 days 0.01955 -       -      \n## 19 days 0.00270 0.40038 -      \n## 8 days  0.40038 0.00244 0.00054\n## \n## P value adjustment method: holm\n\n\n\n\n\n\nWe get a table with some values as the output for the post-hoc test and a line saying that we have used the Holm procedure. What are the values in the table of the output? p-values.\n\n\nRecall that the hypotheses are tested at a 5% significance level.\n\n\nWe can conclude that all pairwise comparisons are significant, except for the comparison between 19 and 17 and the comparison between 12 and 8.\n\n\nPost-hoc test: Bonferroni\n\n\nNow we\u2019ll take a look at the most conservative procedure, Bonferroni. This procedure will apply the most extreme adjustments to the p-values.\n\n\n1\n2\n# Post-hoc test: Bonferroni procedure\n\n\nwith\n(\nwm\n,\n pairwise.t.test\n(\niq\n,\n condition\n,\n paired \n=\n \nTRUE\n,\n p.adjust.method \n=\n \n'bonferroni'\n))\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  Pairwise comparisons using paired t tests \n## \n## data:  iq and condition \n## \n##         12 days 17 days 19 days\n## 17 days 0.03910 -       -      \n## 19 days 0.00405 1.00000 -      \n## 8 days  1.00000 0.00293 0.00054\n## \n## P value adjustment method: bonferroni\n\n\n\n\n\n\nNotice the change in p-values in comparison with the previous procedure.\n\n\nPaired t-test\n\n\nAssume that you do not know how to perform an analysis of variance (ANOVA), we may do a number of paired t-tests instead.\n\n\nHave a look at just one paired t-test and take, for example, the comparison between 12 days and 17 days.\n\n\n1\n2\n3\n4\n# Define two subsets containing the IQ scores for the condition group '12 days' and '17 days'\n\ncond_12days \n<-\n \nsubset\n(\nwm\n,\n condition \n==\n \n'12 days'\n)\n$\niq\ncond_17days \n<-\n \nsubset\n(\nwm\n,\n condition \n==\n \n'17 days'\n)\n$\niq\ncond_12days\n\n\n\n\n\n\n1\n2\n##  [1] 12.5 11.6  8.9  8.3 10.9 13.4 12.3  8.7  9.7  9.7  6.9 10.5 11.6 13.8\n## [15] 15.6 11.7 16.1 11.7 15.0 15.1\n\n\n\n\n\n\n1\n2\n# t-test\n\nt.test\n(\ncond_12days\n,\n cond_17days\n,\n paired \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  Paired t-test\n## \n## data:  cond_12days and cond_17days\n## t = -3.0549, df = 19, p-value = 0.006517\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -3.7157116 -0.6942884\n## sample estimates:\n## mean of the differences \n##                  -2.205\n\n\n\n\n\n\nIt is clear that we can apply different procedures for post-hoc tests. These procedures differ with respect to how they handle inflation of the possibility of a type I error and will therefore give us different p-values. However, these p-values will always be in a certain range.\n\n\nWhat is the (smallest) range of p-values for the comparison between 12 days and 17 days? Look at the results from applying the Bonferroni procedure as well as the paired t-test.\n\n\np-values:\n\n\n\n\nBonferroni 12 days-17 days: 0.03910\n\n\nPaired t-test (cond_12days, cond_17days): 0.006517\n\n\n\n\nTherefore: [0.0065, 0.0391].",
            "title": "Statistics with R, Course Four, Repeated Measures ANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Four,_Repeated_Measures_Anova/#repeated-measures-anova",
            "text": "The systematic between groups variance  To understand everything a bit better, we will calculate the F-ratio for a repeated measures design by ourself in the next exercises.  First, we will need the systematic between-groups variance. This is the same as in the between-groups design\u2013the variance due to grouping by condition.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 # Define number of subjects for each condition \nn  <-   20  # Calculate group means \ny_j  <-   tapply ( wm $ iq ,  wm $ condition ,   mean )  # Calculate the grand mean \ny_t  <-   mean ( y_j )  # Calculate the sum of squares \nss_cond  <-   sum (( y_j  -  y_t ) ^ 2 ) * n # Define the degrees of freedom for conditions \ndf  <-   ( 4   -   1 )  # Calculate the mean squares (variance) \nms_cond  <-  ss_cond / df   The subject variance  We will also need the error term of the repeated measures design. This can be calculated in a few steps.   First calculate the systematic variance due to subjects.  Below, we will calculate the unsystematic variance, like we did with the between-groups design.  If we subtract these two results, we will get the error term of the repeated measures design.   The systematic (stable) subject variance will be taken out of the error term, so the error term is reduced in comparison with the between-groups design.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 # Define number of conditions for each subject \nn  <-   4  # Calculate subject means \ny_j  <-   tapply ( wm $ iq ,  wm $ subject ,   mean )  # Calculate the grand mean \ny_t  <-   mean ( y_j )  # Calculate the sum of squares \nss_subjects  <-   sum (( y_j  -  y_t ) ^ 2 ) * n # Define the degrees of freedom for subjects \ndf  <-   ( 20   -   1 )  # Calculate the mean squares (variance) \nms_subjects  <-  ss_subjects / df   The unsystematic within groups variance  To calculate the error term of the repeated measures design, we need the unsystematic within-groups variance: the unsystematic variance or the error term of the between-groups design.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27 # Create four subsets of the four groups, containing the IQ results \n     # Make the subset for the group condition = \"8 days\" \ny_i1  <-   subset ( wm $ iq ,  wm $ condition  ==   \"8 days\" ) \n     # Make the subset for the group condition = \"12 days\" \ny_i2  <-   subset ( wm $ iq ,  wm $ condition  ==   \"12 days\" ) \n     # Make the subset for the group condition = \"17 days\" \ny_i3  <-   subset ( wm $ iq ,  wm $ condition  ==   \"17 days\" ) \n     # Make the subset for the group condition = \"19 days\" \ny_i4  <-   subset ( wm $ iq ,  wm $ condition  ==   \"19 days\" )  # Subtract the individual values by their group means \ns_1  <-  y_i1  -   mean ( y_i1 ) \ns_2  <-  y_i2  -   mean ( y_i2 ) \ns_3  <-  y_i3  -   mean ( y_i3 ) \ns_4  <-  y_i4  -   mean ( y_i4 )  # Put everything back into one vector \ns_t  <-   c ( s_1 ,  s_2 ,  s_3 ,  s_4 )  # Calculate the within sum of squares by using the vector s_t \nss_sa  <-   sum ( s_t ^ 2 )  # Define the degrees of freedom \ndf  <-   4 * ( 20-1 )  # Calculate the mean squares (variances) \nms_sa  <-  ss_sa / df   The unsystematic variance for the repeated measures design  Now we can easily calculate the unsystematic variance for the repeated measures design, also called the error term.  1\n2\n3\n4\n5\n6\n7\n8 # ss_sa = ss_subjects + ss_rm \nss_rm  <-  ss_sa  -  ss_subjects # Define the degrees of freedom \ndf  <-   ( 20   -   1 ) * ( 4   -   1 )  # Calculate the mean squares (variances) \nms_rm  <-  ss_rm / df   Now we\u2019ve calculated the error term of the repeated measures design, which is clearly smaller than the error term of the between-groups design because you reduced this one. To complete you can now calculate the F-ratio and the corresponding p-value.  F-ratio and p-value  To do the ANOVA analysis we actually need the F-ratio and the corresponding p-value.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Calculate the F-ratio \nf_rat  <-  ms_cond / ms_rm # Define the degrees of freedom of the F-distribution  # df1 for freedom of conditions (ss_cond)  # df2 for (freedom of) conditions and subjects (sa_sa) \ndf1  <-   ( 4   -   1 ) \ndf2  <-   ( 4   -   1 ) * ( 20   -   1 )  # Calculate the p-value \np  <-   1   -  pf ( f_rat ,  df1 ,  df2 )    Error term in a repeated measures design?  The inconsistent individual differences across conditions, the effect of subjects that differ across conditions. So it is an interaction between subjects and condition.  Anova in R  ANOVA in R is usually done with the  aov  function.  1\n2\n3\n4\n5 # anova model \nmodel  <-  aov ( wm $ iq  ~  wm $ condition  +  Error ( wm $ subject  /  wm $ condition ))  # summary model  summary ( model )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## \n## Error: wm$subject\n##           Df Sum Sq Mean Sq F value Pr(>F)\n## Residuals 19  175.6   9.242               \n## \n## Error: wm$subject:wm$condition\n##              Df Sum Sq Mean Sq F value   Pr(>F)    \n## wm$condition  3  196.1   65.36   12.51 2.16e-06 ***\n## Residuals    57  297.8    5.22                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   The F-ratio is significant. Therefore, the number of training days does affect the IQ scores.  Effect size  Calculate eta-squared, which helps you estimate effect size.  1\n2\n3\n4\n5\n6\n7\n8\n9 # Define the total sum of squares  # ss_cond (syst. between groups or of the effect),   # ss_sa (unsyst. within groups) =  # ss_rm (unsyst. for repeated measures design) + # ss_subjects (ok) \nss_total  <-  ss_cond  +  ss_rm # Calculate the effect size \neta_sq  <-  ss_cond  /  ss_total   Post-hoc test one  We will now look at a few different procedures for the post-hoc test. Let\u2019s start with the Holm procedure.  1\n2 # Post-hoc test: default procedure  with ( wm ,  pairwise.t.test ( iq ,  condition ,  paired  =   T ))     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## \n##  Pairwise comparisons using paired t tests \n## \n## data:  iq and condition \n## \n##         12 days 17 days 19 days\n## 17 days 0.01955 -       -      \n## 19 days 0.00270 0.40038 -      \n## 8 days  0.40038 0.00244 0.00054\n## \n## P value adjustment method: holm   We get a table with some values as the output for the post-hoc test and a line saying that we have used the Holm procedure. What are the values in the table of the output? p-values.  Recall that the hypotheses are tested at a 5% significance level.  We can conclude that all pairwise comparisons are significant, except for the comparison between 19 and 17 and the comparison between 12 and 8.  Post-hoc test: Bonferroni  Now we\u2019ll take a look at the most conservative procedure, Bonferroni. This procedure will apply the most extreme adjustments to the p-values.  1\n2 # Post-hoc test: Bonferroni procedure  with ( wm ,  pairwise.t.test ( iq ,  condition ,  paired  =   TRUE ,  p.adjust.method  =   'bonferroni' ))     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## \n##  Pairwise comparisons using paired t tests \n## \n## data:  iq and condition \n## \n##         12 days 17 days 19 days\n## 17 days 0.03910 -       -      \n## 19 days 0.00405 1.00000 -      \n## 8 days  1.00000 0.00293 0.00054\n## \n## P value adjustment method: bonferroni   Notice the change in p-values in comparison with the previous procedure.  Paired t-test  Assume that you do not know how to perform an analysis of variance (ANOVA), we may do a number of paired t-tests instead.  Have a look at just one paired t-test and take, for example, the comparison between 12 days and 17 days.  1\n2\n3\n4 # Define two subsets containing the IQ scores for the condition group '12 days' and '17 days' \ncond_12days  <-   subset ( wm ,  condition  ==   '12 days' ) $ iq\ncond_17days  <-   subset ( wm ,  condition  ==   '17 days' ) $ iq\ncond_12days   1\n2 ##  [1] 12.5 11.6  8.9  8.3 10.9 13.4 12.3  8.7  9.7  9.7  6.9 10.5 11.6 13.8\n## [15] 15.6 11.7 16.1 11.7 15.0 15.1   1\n2 # t-test \nt.test ( cond_12days ,  cond_17days ,  paired  =   TRUE )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## \n##  Paired t-test\n## \n## data:  cond_12days and cond_17days\n## t = -3.0549, df = 19, p-value = 0.006517\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -3.7157116 -0.6942884\n## sample estimates:\n## mean of the differences \n##                  -2.205   It is clear that we can apply different procedures for post-hoc tests. These procedures differ with respect to how they handle inflation of the possibility of a type I error and will therefore give us different p-values. However, these p-values will always be in a certain range.  What is the (smallest) range of p-values for the comparison between 12 days and 17 days? Look at the results from applying the Bonferroni procedure as well as the paired t-test.  p-values:   Bonferroni 12 days-17 days: 0.03910  Paired t-test (cond_12days, cond_17days): 0.006517   Therefore: [0.0065, 0.0391].",
            "title": "Repeated measures ANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Five,_Correlation_and_Regression/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets and results.\n\n\n\n\n\n\nAn introduction to Correlation\n\u00b6\n\n\nManual computation of correlation coefficients - Part 1\n\n\n1\n2\n# Print the data set in the console\n\nstr\n(\nPE\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## 'data.frame':    200 obs. of  4 variables:\n##  $ pid        : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ age        : num  60 40 29 47 48 42 55 43 39 51 ...\n##  $ activeyears: num  10 9 2 10 9 6 8 19 9 14 ...\n##  $ endurance  : num  18 36 51 18 23 30 8 40 28 15 ...\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Take a quick peek at both vectors\n\nA \n<-\n PE\n$\nactiveyears\nB \n<-\n PE\n$\nendurance\n\n\n# Save the differences of each vector element with the mean in a new variable\n\ndiff_A \n<-\n A \n-\n \nmean\n(\nA\n)\n\ndiff_B \n<-\n B \n-\n \nmean\n(\nB\n)\n\n\n\n# Do the summation of the elements of the vectors and divide by N-1 in order to acquire the covariance between the two vectors\n\ncov \n<-\n \nsum\n(\ndiff_A\n*\ndiff_B\n)\n \n/\n \n(\nlength\n(\nA\n)\n \n-\n \n1\n)\n\ncov\n\n\n\n\n\n\n1\n## [1] 16.59045\n\n\n\n\n\n\nManual computation of correlation coefficients - Part 2\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Square the differences that were found in the previous step\n\nsq_diff_A \n<-\n diff_A\n^\n2\n\nsq_diff_B \n<-\n diff_B\n^\n2\n\n\n\n# Take the sum of the elements, divide them by N-1 and consequently take the square root to acquire the sample standard deviations\n\nsd_A \n<-\n \nsqrt\n(\nsum\n(\nsq_diff_A\n)\n/\n(\nlength\n(\nA\n)\n \n-\n \n1\n))\n\nsd_B \n<-\n \nsqrt\n(\nsum\n(\nsq_diff_B\n)\n/\n(\nlength\n(\nB\n)\n \n-\n \n1\n))\n\nsd_A\n\n\n\n\n\n\n1\n## [1] 4.687134\n\n\n\n\n\n\n1\nsd_B\n\n\n\n\n\n\n1\n## [1] 10.83963\n\n\n\n\n\n\nManual computation of correlation coefficients - part 3\n\n\n1\n2\n3\n# Combine all the pieces of the puzzle\n\ncorrelation \n<-\n cov\n/\n(\nsd_A\n*\nsd_B\n)\n\ncorrelation\n\n\n\n\n\n\n1\n## [1] 0.3265402\n\n\n\n\n\n\n1\n2\n# Check the validity of your result with the cor() command\n\ncor\n(\nA\n,\nB\n)\n\n\n\n\n\n\n\n1\n## [1] 0.3265402\n\n\n\n\n\n\nCreating scatterplots\n\n\n1\n2\n3\n4\nlibrary\n(\npsych\n)\n\n\n\n# Summary statistics\n\ndescribe\n(\nPE\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n##             vars   n   mean    sd median trimmed   mad min max range skew\n## pid            1 200 101.81 58.85  101.5  101.71 74.87   1 204   203 0.01\n## age            2 200  49.41 10.48   48.0   49.46 10.38  20  82    62 0.06\n## activeyears    3 200  10.68  4.69   11.0   10.57  4.45   0  26    26 0.30\n## endurance      4 200  26.50 10.84   27.0   26.22 10.38   3  55    52 0.22\n##             kurtosis   se\n## pid            -1.21 4.16\n## age            -0.14 0.74\n## activeyears     0.46 0.33\n## endurance      -0.44 0.77\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# Scatter plots\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n3\n))\n\n\nplot\n(\nPE\n$\nage \n~\n PE\n$\nactiveyears\n)\n\nplot\n(\nPE\n$\nendurance \n~\n PE\n$\nactiveyears\n)\n\nplot\n(\nPE\n$\nendurance \n~\n PE\n$\nage\n)\n\n\n\n\n\n\n\n\n\n1\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n1\n))\n\n\n\n\n\n\n\nCorrelation matrix\n\n\n1\n2\n# Correlation Analysis \n\n\nround\n(\ncor\n(\nPE\n[\n2\n:\n4\n],\n use \n=\n \n'pairwise.complete.obs'\n,\n method \n=\n \n'pearson'\n),\n \n2\n)\n  \n\n\n\n\n\n\n1\n2\n3\n4\n##               age activeyears endurance\n## age          1.00        0.33     -0.08\n## activeyears  0.33        1.00      0.33\n## endurance   -0.08        0.33      1.00\n\n\n\n\n\n\n1\n2\n3\n# Do some correlation tests. If the null hypothesis of no correlation can be rejected on a significance level of 5%, then the relationship between variables is  significantly different from zero at the 95% confidence level\n\n\ncor.test\n(\nPE\n$\nage\n,\n PE\n$\nactiveyears\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  Pearson's product-moment correlation\n## \n## data:  PE$age and PE$activeyears\n## t = 4.9022, df = 198, p-value = 1.969e-06\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.1993491 0.4473145\n## sample estimates:\n##       cor \n## 0.3289909\n\n\n\n\n\n\n1\ncor.test\n(\nPE\n$\nage\n,\n PE\n$\nendurance\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  Pearson's product-moment correlation\n## \n## data:  PE$age and PE$endurance\n## t = -1.1981, df = 198, p-value = 0.2323\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.22097811  0.05454491\n## sample estimates:\n##         cor \n## -0.08483813\n\n\n\n\n\n\n1\ncor.test\n(\nPE\n$\nendurance\n,\n PE\n$\nactiveyears\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  Pearson's product-moment correlation\n## \n## data:  PE$endurance and PE$activeyears\n## t = 4.8613, df = 198, p-value = 2.37e-06\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.1967110 0.4451154\n## sample estimates:\n##       cor \n## 0.3265402\n\n\n\n\n\n\nCAUTION:\n\n\n\n\nThe magnitude of a correlation depends upon many factors, including sampling.\n\n\nThe magnitude of a correlation is also influenced by measurement of X & Y.\n\n\nThe correlation coefficient is a sample statistic, just like the mean.\n\n\n\n\nAn introduction to Linear Regression Models\n\u00b6\n\n\nImpact experiment\n\n\n1\n2\n3\n4\n5\n6\n7\n# Create a correlation matrix for the dataset (9-14 are the '2' variables only)\n\ncorrelations \n<-\n cor\n(\nPE\n[\n9\n:\n14\n,])\n\n\n\n# Create the scatterplot matrix for the dataset\n\n\nlibrary\n(\ncorrplot\n)\n\n\ncorrplot\n(\ncorrelations\n)\n\n\n\n\n\n\n\n\n\nManual computation of a simple linear regression\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n# Calculate the required means, standard deviations and correlation coefficient\n\nmean_ay \n<-\n \nmean\n(\nPE\n$\nactiveyears\n)\n\nmean_e \n<-\n \nmean\n(\nPE\n$\nendurance\n)\n\nsd_ay \n<-\n sd\n(\nPE\n$\nactiveyears\n)\n\nsd_e \n<-\n sd\n(\nPE\n$\nendurance\n)\n\nr \n<-\n cor\n(\nPE\n$\nactiveyears\n,\n PE\n$\nendurance\n)\n\n\n\n# Calculate the slope\n\nB_1 \n<-\n r \n*\n \n(\nsd_e\n)\n/\n(\nsd_ay\n)\n\n\n\n# Calculate the intercept\n\nB_0 \n<-\n mean_e \n-\n B_1 \n*\n mean_ay\n\n\n# Plot of ic2 against sym2\n\nplot\n(\nPE\n$\nactiveyear\n,\n PE\n$\nendurance\n,\n main \n=\n \n'Scatterplot'\n,\n ylab \n=\n \n'Endurance'\n,\n xlab \n=\n \n'Active Years'\n)\n\n\n\n# Add the regression line\n\nabline\n(\nB_0\n,\n B_1\n,\n col \n=\n \n'red'\n)\n\n\n\n\n\n\n\n\n\nExecuting a simple linear regression using R\n\n\n1\n2\n3\n4\n5\n# Construct the regression model\n\nmodel_1 \n<-\n lm\n(\nPE\n$\nendurance \n~\n PE\n$\nactiveyear\n)\n\n\n\n# Look at the results of the regression by using the summary function\n\n\nsummary\n(\nmodel_1\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## \n## Call:\n## lm(formula = PE$endurance ~ PE$activeyear)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -20.5006  -7.8066   0.5304   5.7649  31.0511 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    18.4386     1.8104  10.185  < 2e-16 ***\n## PE$activeyear   0.7552     0.1553   4.861 2.37e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.27 on 198 degrees of freedom\n## Multiple R-squared:  0.1066, Adjusted R-squared:  0.1021 \n## F-statistic: 23.63 on 1 and 198 DF,  p-value: 2.37e-06\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Extract the predicted values\n\npredicted \n<-\n fitted\n(\nmodel_1\n)\n\n\n\n# Create a scatter plot of Impulse Control against Symptom Score\n\nplot\n(\nPE\n$\nendurance \n~\n PE\n$\nactiveyear\n,\n main \n=\n \n'Scatterplot'\n,\n ylab \n=\n \n'Endurance'\n,\n xlab \n=\n \n'Active Years'\n)\n\n\n\n# Add a regression line\n\nabline\n(\nmodel_1\n,\n col \n=\n \n'red'\n)\n\nabline\n(\nlm\n(\npredicted \n~\n PE\n$\nactiveyears\n),\n col \n=\n \n'green'\n,\n lty \n=\n \n2\n)\n\n\n\n\n\n\n\n\n\nExecuting a multiple regression in R\n\n\n1\n2\n3\n4\n5\n# Multiple Regression\n\nmodel_2 \n<-\n lm\n(\nPE\n$\nendurance \n~\n PE\n$\nactiveyear \n+\n PE\n$\nage\n)\n\n\n\n# Examine the results of the regression\n\n\nsummary\n(\nmodel_2\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n## \n## Call:\n## lm(formula = PE$endurance ~ PE$activeyear + PE$age)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -21.4598  -7.7398   0.6984   5.5364  27.9230 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    27.7035     3.4779   7.966 1.29e-13 ***\n## PE$activeyear   0.9192     0.1610   5.708 4.16e-08 ***\n## PE$age         -0.2229     0.0720  -3.096  0.00225 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.06 on 197 degrees of freedom\n## Multiple R-squared:  0.1481, Adjusted R-squared:  0.1394 \n## F-statistic: 17.12 on 2 and 197 DF,  p-value: 1.394e-07\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# Extract the predicted values\n\npredicted \n<-\n fitted\n(\nmodel_2\n)\n\n\n\n# Plotting predicted scores against observed scores\n\nplot\n(\npredicted \n~\n PE\n$\nactiveyears\n,\n main \n=\n \n'Scatterplot'\n,\n ylab \n=\n \n'Endurance'\n,\n xlab \n=\n \n'Active Years'\n)\n\nabline\n(\nlm\n(\npredicted \n~\n PE\n$\nactiveyears\n),\n col \n=\n \n'green'\n)\n\n\n\n\n\n\n\n\n\nLinear Regression Models continued\n\u00b6\n\n\nCalculating the sum of squared residuals\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# Create a linear regression with `ic2` and `vismem2` as regressors\n\nmodel_1 \n<-\n lm\n(\nPE\n$\nendurance \n~\n PE\n$\nactiveyear \n+\n PE\n$\nage\n)\n\n\n\n# Extract the predicted values\n\npredicted_1 \n<-\n fitted\n(\nmodel_1\n)\n\n\n\n# Calculate the squared deviation of the predicted values from the observed values \n\ndeviation_1 \n<-\n \n(\npredicted_1 \n-\n PE\n$\nendurance\n)\n^\n2\n\n\n\n# Sum the squared deviations\n\nSSR_1 \n<-\n \nsum\n(\ndeviation_1\n)\n\nSSR_1\n\n\n\n\n\n\n1\n## [1] 19919.55\n\n\n\n\n\n\nStandardized linear regression\n\n\n1\n2\n3\n4\n5\n# Create a standardized simple linear regression\n\nmodel_1_z \n<-\n lm\n(\nscale\n(\nPE\n$\nendurance\n)\n \n~\n \nscale\n(\nPE\n$\nactiveyear\n))\n\n\n\n# Look at the output of this regression model\n\n\nsummary\n(\nmodel_1_z\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n## \n## Call:\n## lm(formula = scale(PE$endurance) ~ scale(PE$activeyear))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.89126 -0.72019  0.04893  0.53184  2.86459 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)          4.871e-17  6.700e-02   0.000        1    \n## scale(PE$activeyear) 3.265e-01  6.717e-02   4.861 2.37e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9476 on 198 degrees of freedom\n## Multiple R-squared:  0.1066, Adjusted R-squared:  0.1021 \n## F-statistic: 23.63 on 1 and 198 DF,  p-value: 2.37e-06\n\n\n\n\n\n\n1\n2\n3\n# Extract the R-Squared value for this regression\n\nr_square_1 \n<-\n \nsummary\n(\nmodel_1_z\n)\n$\nr.squared\nr_square_1\n\n\n\n\n\n\n1\n## [1] 0.1066285\n\n\n\n\n\n\n1\n2\n3\n# Calculate the correlation coefficient\n\ncorr_coef_1 \n<-\n \nsqrt\n(\nr_square_1\n)\n\ncorr_coef_1\n\n\n\n\n\n\n1\n## [1] 0.3265402\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Create a standardized multiple linear regression\n\nmodel_2_z \n<-\n lm\n(\nscale\n(\nPE\n$\nendurance\n)\n \n~\n \nscale\n(\nPE\n$\nactiveyear\n)\n \n+\n \nscale\n(\nPE\n$\nage\n))\n\n\n\n# Look at the output of this regression model\n\n\nsummary\n(\nmodel_2_z\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n## \n## Call:\n## lm(formula = scale(PE$endurance) ~ scale(PE$activeyear) + scale(PE$age))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.97975 -0.71403  0.06443  0.51076  2.57601 \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)           5.590e-17  6.560e-02   0.000  1.00000    \n## scale(PE$activeyear)  3.975e-01  6.964e-02   5.708 4.16e-08 ***\n## scale(PE$age)        -2.156e-01  6.964e-02  -3.096  0.00225 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9277 on 197 degrees of freedom\n## Multiple R-squared:  0.1481, Adjusted R-squared:  0.1394 \n## F-statistic: 17.12 on 2 and 197 DF,  p-value: 1.394e-07\n\n\n\n\n\n\n1\n2\n3\n# Extract the R-Squared value for this regression\n\nr_square_2 \n<-\n \nsummary\n(\nmodel_2_z\n)\n$\nr.squared\nr_square_2\n\n\n\n\n\n\n1\n## [1] 0.1480817\n\n\n\n\n\n\n1\n2\n3\n# Calculate the correlation coefficient\n\ncorr_coef_2 \n<-\n \nsqrt\n(\nr_square_2\n)\n\ncorr_coef_2\n\n\n\n\n\n\n1\n## [1] 0.3848139\n\n\n\n\n\n\nAssumptions of linear regression:\n\n\n\n\nNormal distribution for Y.\n\n\nLinear relationship between X and Y.\n\n\nHomoscedasticity.\n\n\nReliability of X and Y.\n\n\nValidity of X and Y.\n\n\nRandom and representative sampling.\n\n\n\n\nCheck it out with Anscombe\u2019s quartet plots.\n\n\nPlotting residuals\n\n\n1\n2\n3\n4\n5\n# Extract the residuals from the model\n\nresidual \n<-\n resid\n(\nmodel_2\n)\n\n\n\n# Draw a histogram of the residuals\n\nhist\n(\nresidual\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# Extract the predicted symptom scores from the model\n\npredicted \n<-\n fitted\n(\nmodel_2\n)\n\n\n\n# Plot the residuals against the predicted symptom scores\n\nplot\n(\nresidual \n~\n predicted\n,\n main \n=\n \n'Scatterplot'\n,\n ylab \n=\n \n'Model 2 Residuals'\n,\n xlab \n=\n \n'Model 2 Predicted Scores'\n)\n\nabline\n(\nlm\n(\nresidual \n~\n predicted\n),\n col \n=\n \n'red'\n)",
            "title": "Statistics with R, Course Five, Correlation and Regression"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Five,_Correlation_and_Regression/#an-introduction-to-linear-regression-models",
            "text": "Impact experiment  1\n2\n3\n4\n5\n6\n7 # Create a correlation matrix for the dataset (9-14 are the '2' variables only) \ncorrelations  <-  cor ( PE [ 9 : 14 ,])  # Create the scatterplot matrix for the dataset  library ( corrplot ) \n\ncorrplot ( correlations )     Manual computation of a simple linear regression   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 # Calculate the required means, standard deviations and correlation coefficient \nmean_ay  <-   mean ( PE $ activeyears ) \nmean_e  <-   mean ( PE $ endurance ) \nsd_ay  <-  sd ( PE $ activeyears ) \nsd_e  <-  sd ( PE $ endurance ) \nr  <-  cor ( PE $ activeyears ,  PE $ endurance )  # Calculate the slope \nB_1  <-  r  *   ( sd_e ) / ( sd_ay )  # Calculate the intercept \nB_0  <-  mean_e  -  B_1  *  mean_ay # Plot of ic2 against sym2 \nplot ( PE $ activeyear ,  PE $ endurance ,  main  =   'Scatterplot' ,  ylab  =   'Endurance' ,  xlab  =   'Active Years' )  # Add the regression line \nabline ( B_0 ,  B_1 ,  col  =   'red' )     Executing a simple linear regression using R  1\n2\n3\n4\n5 # Construct the regression model \nmodel_1  <-  lm ( PE $ endurance  ~  PE $ activeyear )  # Look at the results of the regression by using the summary function  summary ( model_1 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## \n## Call:\n## lm(formula = PE$endurance ~ PE$activeyear)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -20.5006  -7.8066   0.5304   5.7649  31.0511 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    18.4386     1.8104  10.185  < 2e-16 ***\n## PE$activeyear   0.7552     0.1553   4.861 2.37e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.27 on 198 degrees of freedom\n## Multiple R-squared:  0.1066, Adjusted R-squared:  0.1021 \n## F-statistic: 23.63 on 1 and 198 DF,  p-value: 2.37e-06   1\n2\n3\n4\n5\n6\n7\n8\n9 # Extract the predicted values \npredicted  <-  fitted ( model_1 )  # Create a scatter plot of Impulse Control against Symptom Score \nplot ( PE $ endurance  ~  PE $ activeyear ,  main  =   'Scatterplot' ,  ylab  =   'Endurance' ,  xlab  =   'Active Years' )  # Add a regression line \nabline ( model_1 ,  col  =   'red' ) \nabline ( lm ( predicted  ~  PE $ activeyears ),  col  =   'green' ,  lty  =   2 )     Executing a multiple regression in R  1\n2\n3\n4\n5 # Multiple Regression \nmodel_2  <-  lm ( PE $ endurance  ~  PE $ activeyear  +  PE $ age )  # Examine the results of the regression  summary ( model_2 )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 ## \n## Call:\n## lm(formula = PE$endurance ~ PE$activeyear + PE$age)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -21.4598  -7.7398   0.6984   5.5364  27.9230 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    27.7035     3.4779   7.966 1.29e-13 ***\n## PE$activeyear   0.9192     0.1610   5.708 4.16e-08 ***\n## PE$age         -0.2229     0.0720  -3.096  0.00225 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.06 on 197 degrees of freedom\n## Multiple R-squared:  0.1481, Adjusted R-squared:  0.1394 \n## F-statistic: 17.12 on 2 and 197 DF,  p-value: 1.394e-07   1\n2\n3\n4\n5\n6 # Extract the predicted values \npredicted  <-  fitted ( model_2 )  # Plotting predicted scores against observed scores \nplot ( predicted  ~  PE $ activeyears ,  main  =   'Scatterplot' ,  ylab  =   'Endurance' ,  xlab  =   'Active Years' ) \nabline ( lm ( predicted  ~  PE $ activeyears ),  col  =   'green' )",
            "title": "An introduction to Linear Regression Models"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Five,_Correlation_and_Regression/#linear-regression-models-continued",
            "text": "Calculating the sum of squared residuals   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # Create a linear regression with `ic2` and `vismem2` as regressors \nmodel_1  <-  lm ( PE $ endurance  ~  PE $ activeyear  +  PE $ age )  # Extract the predicted values \npredicted_1  <-  fitted ( model_1 )  # Calculate the squared deviation of the predicted values from the observed values  \ndeviation_1  <-   ( predicted_1  -  PE $ endurance ) ^ 2  # Sum the squared deviations \nSSR_1  <-   sum ( deviation_1 ) \nSSR_1   1 ## [1] 19919.55   Standardized linear regression  1\n2\n3\n4\n5 # Create a standardized simple linear regression \nmodel_1_z  <-  lm ( scale ( PE $ endurance )   ~   scale ( PE $ activeyear ))  # Look at the output of this regression model  summary ( model_1_z )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 ## \n## Call:\n## lm(formula = scale(PE$endurance) ~ scale(PE$activeyear))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.89126 -0.72019  0.04893  0.53184  2.86459 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)          4.871e-17  6.700e-02   0.000        1    \n## scale(PE$activeyear) 3.265e-01  6.717e-02   4.861 2.37e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9476 on 198 degrees of freedom\n## Multiple R-squared:  0.1066, Adjusted R-squared:  0.1021 \n## F-statistic: 23.63 on 1 and 198 DF,  p-value: 2.37e-06   1\n2\n3 # Extract the R-Squared value for this regression \nr_square_1  <-   summary ( model_1_z ) $ r.squared\nr_square_1   1 ## [1] 0.1066285   1\n2\n3 # Calculate the correlation coefficient \ncorr_coef_1  <-   sqrt ( r_square_1 ) \ncorr_coef_1   1 ## [1] 0.3265402   1\n2\n3\n4\n5 # Create a standardized multiple linear regression \nmodel_2_z  <-  lm ( scale ( PE $ endurance )   ~   scale ( PE $ activeyear )   +   scale ( PE $ age ))  # Look at the output of this regression model  summary ( model_2_z )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 ## \n## Call:\n## lm(formula = scale(PE$endurance) ~ scale(PE$activeyear) + scale(PE$age))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.97975 -0.71403  0.06443  0.51076  2.57601 \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)           5.590e-17  6.560e-02   0.000  1.00000    \n## scale(PE$activeyear)  3.975e-01  6.964e-02   5.708 4.16e-08 ***\n## scale(PE$age)        -2.156e-01  6.964e-02  -3.096  0.00225 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9277 on 197 degrees of freedom\n## Multiple R-squared:  0.1481, Adjusted R-squared:  0.1394 \n## F-statistic: 17.12 on 2 and 197 DF,  p-value: 1.394e-07   1\n2\n3 # Extract the R-Squared value for this regression \nr_square_2  <-   summary ( model_2_z ) $ r.squared\nr_square_2   1 ## [1] 0.1480817   1\n2\n3 # Calculate the correlation coefficient \ncorr_coef_2  <-   sqrt ( r_square_2 ) \ncorr_coef_2   1 ## [1] 0.3848139   Assumptions of linear regression:   Normal distribution for Y.  Linear relationship between X and Y.  Homoscedasticity.  Reliability of X and Y.  Validity of X and Y.  Random and representative sampling.   Check it out with Anscombe\u2019s quartet plots.  Plotting residuals  1\n2\n3\n4\n5 # Extract the residuals from the model \nresidual  <-  resid ( model_2 )  # Draw a histogram of the residuals \nhist ( residual )     1\n2\n3\n4\n5\n6 # Extract the predicted symptom scores from the model \npredicted  <-  fitted ( model_2 )  # Plot the residuals against the predicted symptom scores \nplot ( residual  ~  predicted ,  main  =   'Scatterplot' ,  ylab  =   'Model 2 Residuals' ,  xlab  =   'Model 2 Predicted Scores' ) \nabline ( lm ( residual  ~  predicted ),  col  =   'red' )",
            "title": "Linear Regression Models continued"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Six,_Multiple_Regression/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets only.\n\n\n\n\n\n\nA gentle introduction to the principles of multiple regression\n\u00b6\n\n\nMultiple regression: visualization of the relationships\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n# Perform the two single regressions and save them in a variable\n\n\n#model_years <- lm(salary ~ years, data = fs)\n\n\n#model_pubs <-  lm(salary ~ pubs, data = fs)\n\nmodel_years \n<-\n lm\n(\nfs\n$\nsalary \n~\n fs\n$\nyears\n)\n\nmodel_pubs \n<-\n lm\n(\nfs\n$\nsalary \n~\n fs\n$\npubs\n)\n\n\n\n# Plot both enhanced scatter plots in one plot matrix of 1 by 2\n\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n \n2\n))\n\n\n\n#plot(fs$years, fs$salary, main = 'plot_years', xlab = 'years', ylab = 'salary')\n\nplot\n(\nfs\n$\nsalary \n~\n fs\n$\nyears\n,\n main \n=\n \n'plot_years'\n,\n xlab \n=\n \n'years'\n,\n ylab \n=\n \n'salary'\n)\n\nabline\n(\nmodel_years\n)\n\n\n\n#plot(fs$pubs, fs$salary, main = 'plot_pubs', xlab = 'pubs', ylab = 'salary')\n\nplot\n(\nfs\n$\nsalary \n~\n fs\n$\npubs\n,\n main \n=\n \n'plot_pubs'\n,\n xlab \n=\n \n'pubs'\n,\n ylab \n=\n \n'salary'\n)\n\nabline\n(\nmodel_pubs\n)\n\n\n\n\n\n\n\nMultiple regression: model selection\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n# Do a single regression of salary onto years of experience and check the output\n\nmodel_1 \n<-\n lm\n(\nfs\n$\nsalary \n~\n fs\n$\nyears\n)\n\n\nsummary\n(\nmodel_1\n)\n\n\n\n# Do a multiple regression of salary onto years of experience and numbers of publications and check the output\n\nmodel_2 \n<-\n lm\n(\nfs\n$\nsalary \n~\n fs\n$\nyears \n+\n fs\n$\npubs\n)\n\n\nsummary\n(\nmodel_2\n)\n\n\n\n# Save the R squared of both models in preliminary variables\n\npreliminary_model_1 \n<-\n \nsummary\n(\nmodel_1\n)\n$\nr.squared\npreliminary_model_2 \n<-\n \nsummary\n(\nmodel_2\n)\n$\nr.squared\n\n\n# Round them off while you save them in new variables\n\nr_squared \n<-\n \nc\n()\n\nr_squared\n[\n1\n]\n \n<-\n \nround\n(\npreliminary_model_1\n,\n \n3\n)\n\nr_squared\n[\n2\n]\n \n<-\n \nround\n(\npreliminary_model_2\n,\n \n3\n)\n\n\n\n# Print out the vector to see both R squared coefficients\n\nr_squared\n\n\n\n\n\n\nMultiple regression: beware of redundancy\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Do multiple regression and check the regression output\n\nmodel_3 \n<-\n lm\n(\nfs\n$\nsalary \n~\n fs\n$\nyears \n+\n fs\n$\npubs \n+\n fs\n$\nage\n)\n\n\nsummary\n(\nmodel_3\n)\n\n\n\n# Round off the R squared coefficients and save the result in the vector (in one step!)\n\nr_squared\n[\n3\n]\n \n<-\n \nround\n(\nsummary\n(\nmodel_3\n)\n$\nr.squared\n,\n3\n)\n\n\n\n# Print out the vector in order to display all R squared coefficients simultaneously\n\nr_squared\n\n\n\n\n\n\nIntuition behind estimation of multiple regression coefficients\n\u00b6\n\n\nDefinition of matrices\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Construction of 3 by 8 matrix r that contains the numbers 1 up to 24\n\nr \n<-\n \nmatrix\n(\nseq\n(\n1\n,\n24\n),\n \n3\n)\n\n\n\n# Construction of 3 by 8 matrix s that contains the numbers 21 up to 44\n\ns \n<-\n \nmatrix\n(\nseq\n(\n21\n,\n44\n),\n \n3\n)\n\n\n\n# Take the transpose t of matrix r\n\nt \n<-\n \nt\n(\nr\n)\n\n\n\n\n\n\n\nAddition, subtraction and multiplication of matrices\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Compute the sum of matrices r and s\n\noperation_1 \n<-\n r \n+\n s\n\n\n# Compute the difference between matrices r and s\n\noperation_2 \n<-\n r \n-\n s\n\n\n# Multiply matrices t and s\n\noperation_3 \n<-\n t \n%*%\n s\n\n\n\n\n\n\nRow vector of sums\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# The raw dataframe `X` is already loaded in.\n\nX\n\n\n# Construction of 1 by 10 matrix I of which the elements are all 1\n\nI \n<-\n \nmatrix\n(\nrep\n(\n1\n,\n10\n),\n \n1\n,\n \n10\n)\n\n\n\n# Compute the row vector of sums\n\nt_mat \n<-\n I \n%*%\n X\n\n\n\n\n\n\nRow vector of means and matrix of means\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n# The data matrix `X` and the row vector of sums (`T`) are saved and can be used.\n\n\n# Number of observations\n\nn \n=\n \n10\n\n\n\n# Compute the row vector of means\n\n\n# you summed up the row, you divide by the nrow to compute the average\n\nM \n<-\n t_mat \n/\n \n10\n\n\n\n# Construction of 10 by 1 matrix J of which the elements are all 1\n\nJ \n<-\n \nmatrix\n(\nrep\n(\n1\n,\n10\n),\n \n10\n,\n \n1\n)\n\n\n\n# Compute the matrix of means \n\nMM \n<-\n J \n%*%\n M\n\n\n\n\n\n\nMatrix of deviation scores\n\n\n1\n2\n3\n4\n# The previously generated matrices X, M and MM do not need to be constructed again but are saved and can be used.\n\n\n\n# Matrix of deviation scores D \n\nD \n<-\n X \n-\n MM\n\n\n\n\n\n\nSum of squares and sum of cross products matrix\n\n\n1\n2\n3\n4\n# The previously generated matrices X, M, MM and D do not need to be constructed again but are saved and can be used.\n\n\n\n# Sum of squares and sum of cross products matrix\n\nS \n<-\n \nt\n(\nD\n)\n \n%*%\n D\n\n\n\n\n\n\nCalculating the correlation matrix\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# The previously generated matrices X, M, MM, D and S do not need to be constructed again but are saved and can be used.\n\nn \n=\n \n10\n\n\n\n# Construct the variance-covariance matrix \n\nC \n<-\n  S \n*\n \n1\n/\nn\n\n\n# Generate the standard deviations matrix \n\nSD \n<-\n \ndiag\n(\nx \n=\n \ndiag\n(\nC\n)\n^\n(\n1\n/\n2\n),\n nrow \n=\n \n3\n,\n ncol \n=\n \n3\n)\n\n\n\n# Compute the correlation matrix\n\nR \n<-\n \nsolve\n(\nSD\n)\n \n%*%\n C \n%*%\n \nsolve\n(\nSD\n)\n\n\n\n\n\n\n\nDummy coding\n\u00b6\n\n\nStarting off\n\n\n1\n2\n# Summary statistics\n\ndescribeBy\n(\nfs\n,\n fs\n$\ndept\n)\n\n\n\n\n\n\n\nA system to code categorical predictors in a regression analysis\n\n\nSuppose we have a categorical vector of observations. The vector counts 4 distinct groups. Here is how to assign the dummy variables:\n\n\n\n\n\n\n\n\nProfID\n\n\nGroup\n\n\nPubs\n\n\nD1\n\n\nD2\n\n\nD3\n\n\n\n\n\n\n\n\n\n\nNU\n\n\nCognitive\n\n\n83\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\nZH\n\n\nClinical\n\n\n74\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\n\n\nMK\n\n\nDevelopmental\n\n\n80\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\nRH\n\n\nSocial\n\n\n68\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n\n\n\n\n\nSummary statistics:\n\n\n\n\n\n\n\n\nGroup\n\n\nM\n\n\nSD\n\n\nN\n\n\n\n\n\n\n\n\n\n\nCognitive\n\n\n93.31\n\n\n29.48\n\n\n13\n\n\n\n\n\n\nClinical\n\n\n60.67\n\n\n11.12\n\n\n8\n\n\n\n\n\n\nDevelopmental\n\n\n103.5\n\n\n23.64\n\n\n6\n\n\n\n\n\n\nSocial\n\n\n70.13\n\n\n21.82\n\n\n9\n\n\n\n\n\n\n\n\n\nModel:\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\n(\nD\n1\n)+\n\u03b2\n2\n(\nD\n2\n)+\n\u03b2\n3\n(\nD\n3\n)+\n\u03f5\n\n\nCoefficient:\n\n\n\n\n\n\n\n\n\n\nB\n\n\nSE\n\n\nB\n\n\nt\n\n\np\n\n\n\n\n\n\n\n\n\n\n\n\n93.31\n\n\n6.5\n\n\n0\n\n\n14.37\n\n\n<.001\n\n\n\n\n\n\nD1 (Clinical)\n\n\n-32.64\n\n\n10.16\n\n\n-0.51\n\n\n-3.21\n\n\n0.003\n\n\n\n\n\n\nD2 (Devel)\n\n\n10.19\n\n\n11.56\n\n\n0.14\n\n\n0.88\n\n\n0.384\n\n\n\n\n\n\nD3 (Social)\n\n\n-23.18\n\n\n10.52\n\n\n-0.35\n\n\n-2.2\n\n\n0.035\n\n\n\n\n\n\n\n\n\nCreating dummy variables (1)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# Create the dummy variables\n\ndept_code \n<-\n dummy.code\n(\nfs\n$\ndept\n)\n\ndept_code\n\n\n# Merge the dataset in an extended dataframe\n\nextended_fs \n<-\n \ncbind\n(\nfs\n,\n dept_code\n)\n\n\n\n# Look at the extended dataframe\n\nextended_fs\n\n\n# Provide summary statistics\n\n\nsummary\n(\nextended_fs\n)\n\n\n\n\n\n\n\nCreating dummy variables (2)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n# Regress salary against years and publications\n\nmodel \n<-\n lm\n(\nfs\n$\nsalary \n~\n fs\n$\nyears \n+\n fs\n$\npubs\n)\n\n\n\n# Apply the summary function to get summarized results for model\n\n\nsummary\n(\nmodel\n)\n\n\n\n# Compute the confidence intervals for model\n\nconfint\n(\nmodel\n)\n \n\n\n# Create dummies for the categorical variable fs$dept by using the C() function\n\ndept_code \n<-\n C\n(\nfs\n$\ndept\n,\n treatment\n)\n\n\n\n# Regress salary against years, publications and department \n\nmodel_dummy \n<-\n lm\n(\nfs\n$\nsalary \n~\n fs\n$\nyears \n+\n fs\n$\npubs \n+\n dept_code\n)\n\n\n\n# Apply the summary function to get summarized results for model_dummy\n\n\nsummary\n(\nmodel_dummy\n)\n\n\n\n# Compute the confidence intervals for model_dummy\n\nconfint\n(\nmodel_dummy\n)\n\n\n\n\n\n\n\nModel selection: ANOVA\n\n\n1\n2\n# Compare model 4 with model3\n\nanova\n(\nmodel\n,\n model_dummy\n)\n\n\n\n\n\n\n\nDiscrepancy between actual and predicted means\n\n\n1\n2\n# Actual means of fs$salary\n\n\ntapply\n(\nfs\n$\nsalary\n,\n fs\n$\ndept\n,\n \nmean\n)\n\n\n\n\n\n\n\nUnweighted effects coding\n\n\nConsult the PDF for \u2018Unweighted Effects Coding\u2019.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Number of levels\n\nfs\n$\ndept\n\n\n# Factorize the categorical variable fs$dept and name the factorized variable dept.f\n\ndept.f \n<-\n \nfactor\n(\nfs\n$\ndept\n)\n\n\n\n# Assign the 3 levels generated in step 2 to dept.f\n\ncontrasts\n(\ndept.f\n)\n \n<-\n contr.sum\n(\n3\n)\n\n\n\n# Regress salary against dept.f\n\nmodel_unweighted \n<-\n lm\n(\nfs\n$\nsalary \n~\n dept.f\n)\n\n\n\n# Apply the summary() function\n\n\nsummary\n(\nmodel_unweighted\n)\n\n\n\n\n\n\n\nWeighted effects coding\n\n\nConsult \u2018Weighted Effects Coding\u2019.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# Factorize the categorical variable fs$dept and name the factorized variable dept.g\n\ndept.g \n<-\n \nfactor\n(\nfs\n$\ndept\n)\n\n\n\n# Assign the weights matrix to dept.g \n\ncontrasts\n(\ndept.g\n)\n \n<-\n weights\n\n\n# Regress salary against dept.f and apply the summary() function\n\nmodel_weighted \n<-\n lm\n(\nfs\n$\nsalary \n~\n dept.g\n)\n\n\n\n# Apply the summary() function\n\n\nsummary\n(\nmodel_weighted\n)",
            "title": "Statistics with R, Course Six, Multiple Regression"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Six,_Multiple_Regression/#intuition-behind-estimation-of-multiple-regression-coefficients",
            "text": "Definition of matrices  1\n2\n3\n4\n5\n6\n7\n8 # Construction of 3 by 8 matrix r that contains the numbers 1 up to 24 \nr  <-   matrix ( seq ( 1 , 24 ),   3 )  # Construction of 3 by 8 matrix s that contains the numbers 21 up to 44 \ns  <-   matrix ( seq ( 21 , 44 ),   3 )  # Take the transpose t of matrix r \nt  <-   t ( r )    Addition, subtraction and multiplication of matrices  1\n2\n3\n4\n5\n6\n7\n8 # Compute the sum of matrices r and s \noperation_1  <-  r  +  s # Compute the difference between matrices r and s \noperation_2  <-  r  -  s # Multiply matrices t and s \noperation_3  <-  t  %*%  s   Row vector of sums  1\n2\n3\n4\n5\n6\n7\n8 # The raw dataframe `X` is already loaded in. \nX # Construction of 1 by 10 matrix I of which the elements are all 1 \nI  <-   matrix ( rep ( 1 , 10 ),   1 ,   10 )  # Compute the row vector of sums \nt_mat  <-  I  %*%  X   Row vector of means and matrix of means   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 # The data matrix `X` and the row vector of sums (`T`) are saved and can be used.  # Number of observations \nn  =   10  # Compute the row vector of means  # you summed up the row, you divide by the nrow to compute the average \nM  <-  t_mat  /   10  # Construction of 10 by 1 matrix J of which the elements are all 1 \nJ  <-   matrix ( rep ( 1 , 10 ),   10 ,   1 )  # Compute the matrix of means  \nMM  <-  J  %*%  M   Matrix of deviation scores  1\n2\n3\n4 # The previously generated matrices X, M and MM do not need to be constructed again but are saved and can be used.  # Matrix of deviation scores D  \nD  <-  X  -  MM   Sum of squares and sum of cross products matrix  1\n2\n3\n4 # The previously generated matrices X, M, MM and D do not need to be constructed again but are saved and can be used.  # Sum of squares and sum of cross products matrix \nS  <-   t ( D )   %*%  D   Calculating the correlation matrix   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # The previously generated matrices X, M, MM, D and S do not need to be constructed again but are saved and can be used. \nn  =   10  # Construct the variance-covariance matrix  \nC  <-   S  *   1 / n # Generate the standard deviations matrix  \nSD  <-   diag ( x  =   diag ( C ) ^ ( 1 / 2 ),  nrow  =   3 ,  ncol  =   3 )  # Compute the correlation matrix \nR  <-   solve ( SD )   %*%  C  %*%   solve ( SD )",
            "title": "Intuition behind estimation of multiple regression coefficients"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Six,_Multiple_Regression/#dummy-coding",
            "text": "Starting off  1\n2 # Summary statistics \ndescribeBy ( fs ,  fs $ dept )    A system to code categorical predictors in a regression analysis  Suppose we have a categorical vector of observations. The vector counts 4 distinct groups. Here is how to assign the dummy variables:     ProfID  Group  Pubs  D1  D2  D3      NU  Cognitive  83  0  0  0    ZH  Clinical  74  1  0  0    MK  Developmental  80  0  1  0    RH  Social  68  0  0  1     Summary statistics:     Group  M  SD  N      Cognitive  93.31  29.48  13    Clinical  60.67  11.12  8    Developmental  103.5  23.64  6    Social  70.13  21.82  9     Model:  Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 ( D 1 )+ \u03b2 2 ( D 2 )+ \u03b2 3 ( D 3 )+ \u03f5  Coefficient:      B  SE  B  t  p       93.31  6.5  0  14.37  <.001    D1 (Clinical)  -32.64  10.16  -0.51  -3.21  0.003    D2 (Devel)  10.19  11.56  0.14  0.88  0.384    D3 (Social)  -23.18  10.52  -0.35  -2.2  0.035     Creating dummy variables (1)   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # Create the dummy variables \ndept_code  <-  dummy.code ( fs $ dept ) \ndept_code # Merge the dataset in an extended dataframe \nextended_fs  <-   cbind ( fs ,  dept_code )  # Look at the extended dataframe \nextended_fs # Provide summary statistics  summary ( extended_fs )    Creating dummy variables (2)   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 # Regress salary against years and publications \nmodel  <-  lm ( fs $ salary  ~  fs $ years  +  fs $ pubs )  # Apply the summary function to get summarized results for model  summary ( model )  # Compute the confidence intervals for model \nconfint ( model )   # Create dummies for the categorical variable fs$dept by using the C() function \ndept_code  <-  C ( fs $ dept ,  treatment )  # Regress salary against years, publications and department  \nmodel_dummy  <-  lm ( fs $ salary  ~  fs $ years  +  fs $ pubs  +  dept_code )  # Apply the summary function to get summarized results for model_dummy  summary ( model_dummy )  # Compute the confidence intervals for model_dummy \nconfint ( model_dummy )    Model selection: ANOVA  1\n2 # Compare model 4 with model3 \nanova ( model ,  model_dummy )    Discrepancy between actual and predicted means  1\n2 # Actual means of fs$salary  tapply ( fs $ salary ,  fs $ dept ,   mean )    Unweighted effects coding  Consult the PDF for \u2018Unweighted Effects Coding\u2019.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Number of levels \nfs $ dept # Factorize the categorical variable fs$dept and name the factorized variable dept.f \ndept.f  <-   factor ( fs $ dept )  # Assign the 3 levels generated in step 2 to dept.f \ncontrasts ( dept.f )   <-  contr.sum ( 3 )  # Regress salary against dept.f \nmodel_unweighted  <-  lm ( fs $ salary  ~  dept.f )  # Apply the summary() function  summary ( model_unweighted )    Weighted effects coding  Consult \u2018Weighted Effects Coding\u2019.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # Factorize the categorical variable fs$dept and name the factorized variable dept.g \ndept.g  <-   factor ( fs $ dept )  # Assign the weights matrix to dept.g  \ncontrasts ( dept.g )   <-  weights # Regress salary against dept.f and apply the summary() function \nmodel_weighted  <-  lm ( fs $ salary  ~  dept.g )  # Apply the summary() function  summary ( model_weighted )",
            "title": "Dummy coding"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Seven,_Moderation_and_Mediation/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nSnippets only.\n\n\n\n\n\n\nAn introduction to moderation\n\u00b6\n\n\nData exploration\n\n\n1\n2\n3\n4\n5\n6\n7\nlibrary\n(\npsych\n)\n\n\n\n# Summary statistics\n\ndescribeBy\n(\nmod\n,\n mod\n$\ncondition\n)\n\n\n\n# Create a boxplot of the data\n\nboxplot\n(\nmod\n$\niq \n~\n mod\n$\ncondition\n,\n main \n=\n \n'Boxplot'\n,\n ylab \n=\n \n'IQ'\n,\n xlab \n=\n \n'Group condition'\n)\n\n\n\n\n\n\n\nCalculate correlations\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n# Create subsets of the three groups\n\n\n\n# Make the subset for the group condition = 'control'\n\nmod_control \n<-\n \nsubset\n(\nmod\n,\n condition \n==\n \n'control'\n)\n\n\n\n# Make the subset for the group condition = 'threat1'\n\nmod_threat1 \n<-\n \nsubset\n(\nmod\n,\n condition \n==\n \n'threat1'\n)\n\n\n\n# Make the subset for the group condition = 'threat2'\n\nmod_threat2 \n<-\n \nsubset\n(\nmod\n,\n condition \n==\n \n'threat2'\n)\n\n\n\n# Calculate the correlations\n\ncor\n(\nmod_control\n$\niq\n,\n mod_control\n$\nwm\n,\n method \n=\n \n'pearson'\n)\n\n\ncor\n(\nmod_threat1\n$\niq\n,\n mod_threat1\n$\nwm\n,\n method \n=\n \n'pearson'\n)\n\n\ncor\n(\nmod_threat2\n$\niq\n,\n mod_threat2\n$\nwm\n,\n method \n=\n \n'pearson'\n)\n\n\n\n\n\n\n\nModel with and without moderation\n\n\nA moderator variable (Z) will enhance a regression model if the relationship between X and Y varies as a function of Z.\n\n\nExperimental research\n\n\n\n\nThe manipulation of an X causes change in a Y.\n\n\nA moderator variable (Z) implies that the effect of the X on the Y is NOT consistent across the distribution of Z.\n\n\n\n\nCorrelational research\n\n\n\n\nAssume a correlation between X and Y.\n\n\nA moderator variable (Z) implies that the correlation between X and Y is NOT consistent across the distribution of Z.\n\n\n\n\nIf both X and Z are continuous:\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nX\n\u2005+\u2005\n\u03b2\n2\nZ\n\u2005+\u2005\n\u03b2\n3\n(\nX\n\u2005*\u2005\nZ\n)+\n\u03f5\n\n\nIf X is categorical and Z is continuous:\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\n(\nD\n1\n)+\n\u03b2\n2\n(\nD\n2\n)+\n\u03b2\n3\nZ\n\u2005+\u2005\n\u03b2\n4\n(\nD\n1\n\u2005*\u2005\nZ\n)+\n\u03b2\n5\n(\nD\n2\n\u2005*\u2005\nZ\n)+\n\u03f5\n\n\nConsult the PDF for performing tests.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# Model without moderation (tests for 'first-order effects')\n\nmodel_1 \n<-\n lm\n(\nmod\n$\niq \n~\n mod\n$\nwm \n+\n mod\n$\nd1 \n+\n mod\n$\nd2\n)\n\n\n \n# Make a summary of model_1\n\n\nsummary\n(\nmodel_1\n)\n\n\n\n# Create new predictor variables\n\nwm_d1 \n<-\n mod\n$\nwm \n*\n mod\n$\nd1\nwm_d2 \n<-\n mod\n$\nwm \n*\n mod\n$\nd2\n\n\n# Model with moderation\n\nmodel_2 \n<-\n lm\n(\nmod\n$\niq \n~\n mod\n$\nwm \n+\n mod\n$\nd1 \n+\n mod\n$\nd2 \n+\n wm_d1 \n+\n wm_d2\n)\n\n\n\n# Make a summary of model_2\n\n\nsummary\n(\nmodel_2\n)\n\n\n\n\n\n\n\nModel comparison\n\n\n1\n2\n# Compare model_1 and model_2\n\nanova\n(\nmodel_1\n,\n model_2\n)\n\n\n\n\n\n\n\nScatterplot\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Choose colors to represent the points by group\n\ncolor \n<-\n \nc\n(\n'red'\n,\n \n'green'\n,\n \n'blue'\n)\n\n\n\n# Illustration of the first-order effects of working memory on IQ\n\nggplot\n(\nmod\n,\n aes\n(\nx \n=\n wm\n,\n y \n=\n iq\n))\n \n+\n geom_smooth\n(\nmethod \n=\n \n'lm'\n,\n color \n=\n \n'black'\n)\n \n+\n \n  geom_point\n(\naes\n(\ncolor \n=\n condition\n))\n\n\n\n# Illustration of the moderation effect of working memory on IQ\n\nggplot\n(\nmod\n,\n aes\n(\nx \n=\n wm\n,\n y \n=\n iq\n))\n \n+\n geom_smooth\n(\naes\n(\ngroup \n=\n condition\n),\n method \n=\n \n'lm'\n,\n se \n=\n \nT\n,\n color \n=\n \n'black'\n,\n fullrange \n=\n \nT\n)\n \n+\n geom_point\n(\naes\n(\ncolor \n=\n condition\n))\n\n\n\n\n\n\n\nCentering data\n\n\n1\n2\n3\n4\n5\n# Define wm_center\n\nwm_center \n<-\n mod\n$\nwm \n-\n \nmean\n(\nmod\n$\nwm\n)\n\n\n\n# Compare with the variable wm.centered\n\n\nall.equal\n(\nwm_center\n,\n mod\n$\nwm.centered\n)\n\n\n\n\n\n\n\nAn introduction to centering predictors\n\u00b6\n\n\nCentering versus no centering\n\n\nTo center means to put in deviation form: \nX\nC\n\u2004=\u2004\nX\n\u2005\u2212\u2005\nM\n. Convert raw scores to deviation scores.\n\n\nTwo reason:\n\n\n\n\nConceptual: regression constant will be more meaningful.\n\n\nStatistical: avoid multicolinearity.\n\n\n\n\nConceptual.\n\n\n\n\nThe intercept, \n\u03b2\n0\n, is the predicted score on Y (child\u2019s verbal ability) when all predictors (X = mother\u2019s vocabulary, Z = child\u2019s age) are zero.\n\n\nIf X = zero or Z = zero is meaningless, or impossible, then \n\u03b2\n0\n will be difficult to interpret.\n\n\nIn contrast, if X = zero and Z = zero, are the average then \n\u03b2\n0\n is easy to interpret.\n\n\nThe regression coefficient \n\u03b2\n1\n is the slope for X assuming an average score on Z.\n\n\nNo moderation effect implies that \n\u03b2\n1\n is consistent across the entire distribution of Z.\n\n\nIn contrast, a moderation effect implies that \n\u03b2\n1\n is NOT consistent across the entire distribution of Z.\n\n\nWhere in the distribution of Z is \n\u03b2\n1\n most representative of the relationship between X & Y?\n\n\n\n\nStatistical\n\n\n\n\nThe predictors, X and Z, can become highly correlated with the product, (X*Z).\n\n\nMulticolinearity: when two predictor variables in a GLM are so highly correlated that they are essentially redundant and it becomes difficult to estimate \n\u03b2\n values associated with each predictor.\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Model without moderation and with centered data\n\nmodel_1_centered \n<-\n lm\n(\nmod\n$\niq \n~\n mod\n$\nwm.centered \n+\n mod\n$\nd1 \n+\n mod\n$\nd2\n)\n\n\n\n# Make a summary of model_1_centered\n\n\nsummary\n(\nmodel_1_centered\n)\n\n\n\n\n\n\n\nCentering versus no centering with moderation\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Create new predictor variables\n\nwm_d1_centered \n<-\n mod\n$\nwm.centered \n*\n mod\n$\nd1\nwm_d2_centered \n<-\n mod\n$\nwm.centered \n*\n mod\n$\nd2\n\n\n# Define model_2_centered\n\nmodel_2_centered \n<-\n lm\n(\nmod\n$\niq \n~\n mod\n$\nwm.centered \n+\n mod\n$\nd1 \n+\n mod\n$\nd2 \n+\n wm_d1_centered \n+\n wm_d2_centered\n)\n\n\n\n# Make a summary of model_2_centered\n\n\nsummary\n(\nmodel_2_centered\n)\n\n\n\n\n\n\n\nModel comparison\n\n\n1\n2\n3\n4\n5\n# Compare model_1_centered and model_2_centered\n\nanova\n(\nmodel_1_centered\n,\n model_2_centered\n)\n\n\n\n# Compare model_1 and model_2\n\nanova\n(\nmodel_1\n,\n model_2\n)\n\n\n\n\n\n\n\nSome correlations\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# Calculate the correlations between working memory capacity and the product terms\n\n cor_wmd1 \n<-\n cor\n(\nmod\n$\nwm\n,\n wm_d1\n)\n\n cor_wmd2 \n<-\n cor\n(\nmod\n$\nwm\n,\n wm_d2\n)\n\n cor_wmd1_centered \n<-\n cor\n(\nmod\n$\nwm.centered\n,\n wm_d1_centered\n)\n\n cor_wmd2_centered \n<-\n cor\n(\nmod\n$\nwm.centered\n,\n wm_d2_centered\n)\n\n\n\n# Calculate the correlations between the dummy variables and the product terms\n\n cor_d1d1\n<-\n cor\n(\nmod\n$\nd1\n,\n wm_d1\n)\n\n cor_d2d2 \n<-\n cor\n(\nmod\n$\nd2\n,\n wm_d2\n)\n\n cor_d1d1_centered \n<-\n cor\n(\nmod\n$\nd1\n,\n wm_d1_centered\n)\n\n cor_d2d2_centered \n<-\n cor\n(\nmod\n$\nd2\n,\n wm_d2_centered\n)\n\n\n\n# correlations\n\n \nrbind\n(\nc\n(\ncor_wmd1\n,\n cor_wmd2\n),\n \nc\n(\ncor_wmd1_centered\n,\n cor_wmd2_centered\n))\n\n \nrbind\n(\nc\n(\ncor_d1d1\n,\n cor_d2d2\n),\n \nc\n(\ncor_d1d1_centered\n,\n cor_d2d2_centered\n))\n\n\n\n\n\n\n\nAn introduction to mediation\n\u00b6\n\n\nModel with and without mediation\n\n\n\n\nX: Experimental manipulation (Stereotype threat).\n\n\nY: Behavioral outcome (IQ score).\n\n\nM: Mediator (Mechanism = Working memory capacity).\n\n\n\n\nA mediation analysis is typically conducted to better understand an observed effect of a correlation between X and Y. Why, and how, does stereotype threat influence IQ test performance?\n\n\nIf X and Y are correlated then we can use regression to predict Y from X\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nX\n\u2005+\u2005\n\u03f5\n\n\nIf X and Y are correlated BECAUSE of the mediator M, then (X \u00e0 M \u00e0 Y):\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nM\n\u2005+\u2005\n\u03f5\n\n\nM\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nX\n\u2005+\u2005\n\u03f5\n\n\nIf X and Y are correlated BECAUSE of the mediator M, and:\n\n\nY\n\u2004=\u2004\n\u03b2\n0\n\u2005+\u2005\n\u03b2\n1\nM\n\u2005+\u2005\n\u03b2\n2\nX\n\u2005+\u2005\n\u03f5\n\n\nA mediator variable (M) accounts for some (partial) or all of the relationship between X and Y.\n\n\nCAUTION:\n\n\n\n\nCorrelation does not imply causation!\n\n\nIn other words, there is a BIG difference between statistical mediation and true causal mediation.\n\n\n\n\nData exploration\n\n\n1\n2\n3\n4\n5\n# Summary statistics\n\ndescribeBy\n(\nmed\n,\n med\n$\ncondition\n)\n\n\n\n# Create a boxplot of the data\n\nboxplot\n(\nmed\n$\niq \n~\n med\n$\ncondition\n,\n main \n=\n \n'Boxplot'\n,\n xlab \n=\n \n'Group condition'\n,\n ylab \n=\n \n'IQ'\n)\n\n\n\n\n\n\n\nRun 3 regression models on the data\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n# Run the three regression models\n\n\n# outcome ~ predictor\n\nmodel_yx \n<-\n lm\n(\nmed\n$\niq \n~\n med\n$\ncondition\n)\n\n\n\n# mediator ~ predictor\n\nmodel_mx \n<-\n lm\n(\nmed\n$\nwm \n~\n med\n$\ncondition\n)\n\n\n\n# outcome ~ predictor + mediator\n\nmodel_yxm \n<-\n lm\n(\nmed\n$\niq \n~\n med\n$\ncondition \n+\n med\n$\nwm\n)\n\n\n\n# Make a summary of the three models\n\n\nsummary\n(\nmodel_yx\n)\n\n\nsummary\n(\nmodel_mx\n)\n\n\nsummary\n(\nmodel_yxm\n)\n\n\n\n\n\n\n\nSobel test\n\n\n1\n2\n3\n4\n5\n# Compare the previous results to the output of the sobel function\n\n\n\n# sobel(pred,med,out)\n\nmodel_all \n<-\n sobel\n(\nmed\n$\ncondition\n,\n med\n$\nwm\n,\n med\n$\niq\n)\n\nmodel_all\n\n\n\n\n\n\nThe Sobel test is a method of testing the significance of a mediation\n\neffect. Consult the PDF.",
            "title": "Statistics with R, Course Seven, Moderation and Mediation"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Seven,_Moderation_and_Mediation/#an-introduction-to-centering-predictors",
            "text": "Centering versus no centering  To center means to put in deviation form:  X C \u2004=\u2004 X \u2005\u2212\u2005 M . Convert raw scores to deviation scores.  Two reason:   Conceptual: regression constant will be more meaningful.  Statistical: avoid multicolinearity.   Conceptual.   The intercept,  \u03b2 0 , is the predicted score on Y (child\u2019s verbal ability) when all predictors (X = mother\u2019s vocabulary, Z = child\u2019s age) are zero.  If X = zero or Z = zero is meaningless, or impossible, then  \u03b2 0  will be difficult to interpret.  In contrast, if X = zero and Z = zero, are the average then  \u03b2 0  is easy to interpret.  The regression coefficient  \u03b2 1  is the slope for X assuming an average score on Z.  No moderation effect implies that  \u03b2 1  is consistent across the entire distribution of Z.  In contrast, a moderation effect implies that  \u03b2 1  is NOT consistent across the entire distribution of Z.  Where in the distribution of Z is  \u03b2 1  most representative of the relationship between X & Y?   Statistical   The predictors, X and Z, can become highly correlated with the product, (X*Z).  Multicolinearity: when two predictor variables in a GLM are so highly correlated that they are essentially redundant and it becomes difficult to estimate  \u03b2  values associated with each predictor.    1\n2\n3\n4\n5 # Model without moderation and with centered data \nmodel_1_centered  <-  lm ( mod $ iq  ~  mod $ wm.centered  +  mod $ d1  +  mod $ d2 )  # Make a summary of model_1_centered  summary ( model_1_centered )    Centering versus no centering with moderation  1\n2\n3\n4\n5\n6\n7\n8\n9 # Create new predictor variables \nwm_d1_centered  <-  mod $ wm.centered  *  mod $ d1\nwm_d2_centered  <-  mod $ wm.centered  *  mod $ d2 # Define model_2_centered \nmodel_2_centered  <-  lm ( mod $ iq  ~  mod $ wm.centered  +  mod $ d1  +  mod $ d2  +  wm_d1_centered  +  wm_d2_centered )  # Make a summary of model_2_centered  summary ( model_2_centered )    Model comparison  1\n2\n3\n4\n5 # Compare model_1_centered and model_2_centered \nanova ( model_1_centered ,  model_2_centered )  # Compare model_1 and model_2 \nanova ( model_1 ,  model_2 )    Some correlations   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 # Calculate the correlations between working memory capacity and the product terms \n cor_wmd1  <-  cor ( mod $ wm ,  wm_d1 ) \n cor_wmd2  <-  cor ( mod $ wm ,  wm_d2 ) \n cor_wmd1_centered  <-  cor ( mod $ wm.centered ,  wm_d1_centered ) \n cor_wmd2_centered  <-  cor ( mod $ wm.centered ,  wm_d2_centered )  # Calculate the correlations between the dummy variables and the product terms \n cor_d1d1 <-  cor ( mod $ d1 ,  wm_d1 ) \n cor_d2d2  <-  cor ( mod $ d2 ,  wm_d2 ) \n cor_d1d1_centered  <-  cor ( mod $ d1 ,  wm_d1_centered ) \n cor_d2d2_centered  <-  cor ( mod $ d2 ,  wm_d2_centered )  # correlations \n  rbind ( c ( cor_wmd1 ,  cor_wmd2 ),   c ( cor_wmd1_centered ,  cor_wmd2_centered )) \n  rbind ( c ( cor_d1d1 ,  cor_d2d2 ),   c ( cor_d1d1_centered ,  cor_d2d2_centered ))",
            "title": "An introduction to centering predictors"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Course_Seven,_Moderation_and_Mediation/#an-introduction-to-mediation",
            "text": "Model with and without mediation   X: Experimental manipulation (Stereotype threat).  Y: Behavioral outcome (IQ score).  M: Mediator (Mechanism = Working memory capacity).   A mediation analysis is typically conducted to better understand an observed effect of a correlation between X and Y. Why, and how, does stereotype threat influence IQ test performance?  If X and Y are correlated then we can use regression to predict Y from X  Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 X \u2005+\u2005 \u03f5  If X and Y are correlated BECAUSE of the mediator M, then (X \u00e0 M \u00e0 Y):  Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 M \u2005+\u2005 \u03f5  M \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 X \u2005+\u2005 \u03f5  If X and Y are correlated BECAUSE of the mediator M, and:  Y \u2004=\u2004 \u03b2 0 \u2005+\u2005 \u03b2 1 M \u2005+\u2005 \u03b2 2 X \u2005+\u2005 \u03f5  A mediator variable (M) accounts for some (partial) or all of the relationship between X and Y.  CAUTION:   Correlation does not imply causation!  In other words, there is a BIG difference between statistical mediation and true causal mediation.   Data exploration  1\n2\n3\n4\n5 # Summary statistics \ndescribeBy ( med ,  med $ condition )  # Create a boxplot of the data \nboxplot ( med $ iq  ~  med $ condition ,  main  =   'Boxplot' ,  xlab  =   'Group condition' ,  ylab  =   'IQ' )    Run 3 regression models on the data   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 # Run the three regression models  # outcome ~ predictor \nmodel_yx  <-  lm ( med $ iq  ~  med $ condition )  # mediator ~ predictor \nmodel_mx  <-  lm ( med $ wm  ~  med $ condition )  # outcome ~ predictor + mediator \nmodel_yxm  <-  lm ( med $ iq  ~  med $ condition  +  med $ wm )  # Make a summary of the three models  summary ( model_yx )  summary ( model_mx )  summary ( model_yxm )    Sobel test  1\n2\n3\n4\n5 # Compare the previous results to the output of the sobel function  # sobel(pred,med,out) \nmodel_all  <-  sobel ( med $ condition ,  med $ wm ,  med $ iq ) \nmodel_all   The Sobel test is a method of testing the significance of a mediation \neffect. Consult the PDF.",
            "title": "An introduction to mediation"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/",
            "text": "Foreword\n\n\n\n\nOutput options: the \u2018tango\u2019 syntax and the \u2018readable\u2019 theme.\n\n\nCodes and snippets.\n\n\n\n\n\n\nDescriptive Statistics\n\u00b6\n\n\nExtract basic, exploratory statistics from datasets with \napply\n, \nsummary\n, \nfivenum\n, \ndescribe\n, and \nstat.desc\n.\n\n\n1\n2\n# dataset\n\n\nhead\n(\nlongley\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##      GNP.deflator     GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234.289      235.6        159.0    107.608 1947   60.323\n## 1948         88.5 259.426      232.5        145.6    108.632 1948   61.122\n## 1949         88.2 258.054      368.2        161.6    109.773 1949   60.171\n\n\n\n\n\n\n1\n2\n3\n# apply a function\n\n\n# excluding missing values\n\n\nsapply\n(\nlongley\n,\n \nmean\n,\n na.rm \n=\n \nTRUE\n)\n \n\n\n\n\n\n\n1\n2\n3\n4\n## GNP.deflator          GNP   Unemployed Armed.Forces   Population \n##     101.6813     387.6984     319.3313     260.6687     117.4240 \n##         Year     Employed \n##    1954.5000      65.3170\n\n\n\n\n\n\n1\n2\n# mean, median, 25th and 75th quartiles, min, max\n\n\nsummary\n(\nlongley\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n##   GNP.deflator         GNP          Unemployed     Armed.Forces  \n##  Min.   : 83.00   Min.   :234.3   Min.   :187.0   Min.   :145.6  \n##  1st Qu.: 94.53   1st Qu.:317.9   1st Qu.:234.8   1st Qu.:229.8  \n##  Median :100.60   Median :381.4   Median :314.4   Median :271.8  \n##  Mean   :101.68   Mean   :387.7   Mean   :319.3   Mean   :260.7  \n##  3rd Qu.:111.25   3rd Qu.:454.1   3rd Qu.:384.2   3rd Qu.:306.1  \n##  Max.   :116.90   Max.   :554.9   Max.   :480.6   Max.   :359.4  \n##    Population         Year         Employed    \n##  Min.   :107.6   Min.   :1947   Min.   :60.17  \n##  1st Qu.:111.8   1st Qu.:1951   1st Qu.:62.71  \n##  Median :116.8   Median :1954   Median :65.50  \n##  Mean   :117.4   Mean   :1954   Mean   :65.32  \n##  3rd Qu.:122.3   3rd Qu.:1958   3rd Qu.:68.29  \n##  Max.   :130.1   Max.   :1962   Max.   :70.55\n\n\n\n\n\n\n1\n2\n# Tukey min, lower-hinge, median, upper-hinge, max\n\nfivenum\n(\nlongley\n$\nGNP\n)\n\n\n\n\n\n\n\n1\n## [1] 234.289 306.787 381.427 463.625 554.894\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# n, nmiss, unique, mean, 5, 10, 25, 50, 75, 90, 95th percentiles\n\n\n# 5 lowest and 5 highest scores\n\n\nlibrary\n(\nHmisc\n)\n\n\ndescribe\n(\nlongley\n)\n\n\n\n\n\n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n## longley \n## \n##  7  Variables      16  Observations\n## ---------------------------------------------------------------------------\n## GNP.deflator \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1    101.7    12.74    86.90    88.35 \n##      .25      .50      .75      .90      .95 \n##    94.53   100.60   111.25   114.95   116.00 \n##                                                                       \n## Value       83.0  88.2  88.5  89.5  96.2  98.1  99.0 100.0 101.2 104.6\n## Frequency      1     1     1     1     1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062\n##                                               \n## Value      108.4 110.8 112.6 114.2 115.7 116.9\n## Frequency      1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062\n## ---------------------------------------------------------------------------\n## GNP \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1    387.7    117.8    252.1    258.7 \n##      .25      .50      .75      .90      .95 \n##    317.9    381.4    454.1    510.4    527.4 \n##                                                                           \n## Value      234.289 258.054 259.426 284.599 328.975 346.999 363.112 365.385\n## Frequency        1       1       1       1       1       1       1       1\n## Proportion   0.062   0.062   0.062   0.062   0.062   0.062   0.062   0.062\n##                                                                           \n## Value      397.469 419.180 442.769 444.546 482.704 502.601 518.173 554.894\n## Frequency        1       1       1       1       1       1       1       1\n## Proportion   0.062   0.062   0.062   0.062   0.062   0.062   0.062   0.062\n## ---------------------------------------------------------------------------\n## Unemployed \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1    319.3    110.1    191.6    201.6 \n##      .25      .50      .75      .90      .95 \n##    234.8    314.4    384.2    434.4    471.2 \n##                                                                       \n## Value      187.0 193.2 209.9 232.5 235.6 282.2 290.4 293.6 335.1 357.8\n## Frequency      1     1     1     1     1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062\n##                                               \n## Value      368.2 381.3 393.1 400.7 468.1 480.6\n## Frequency      1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062\n## ---------------------------------------------------------------------------\n## Armed.Forces \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1    260.7    79.85    155.7    160.3 \n##      .25      .50      .75      .90      .95 \n##    229.8    271.8    306.1    344.9    355.9 \n##                                                                       \n## Value      145.6 159.0 161.6 165.0 251.4 255.2 257.2 263.7 279.8 282.7\n## Frequency      1     1     1     1     1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062\n##                                               \n## Value      285.7 304.8 309.9 335.0 354.7 359.4\n## Frequency      1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062\n## ---------------------------------------------------------------------------\n## Population \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1    117.4    8.229    108.4    109.2 \n##      .25      .50      .75      .90      .95 \n##    111.8    116.8    122.3    126.6    128.4 \n##                                                                           \n## Value      107.608 108.632 109.773 110.929 112.075 113.270 115.094 116.219\n## Frequency        1       1       1       1       1       1       1       1\n## Proportion   0.062   0.062   0.062   0.062   0.062   0.062   0.062   0.062\n##                                                                           \n## Value      117.388 118.734 120.445 121.950 123.366 125.368 127.852 130.081\n## Frequency        1       1       1       1       1       1       1       1\n## Proportion   0.062   0.062   0.062   0.062   0.062   0.062   0.062   0.062\n## ---------------------------------------------------------------------------\n## Year \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1     1954    5.667     1948     1948 \n##      .25      .50      .75      .90      .95 \n##     1951     1954     1958     1960     1961 \n##                                                                       \n## Value       1947  1948  1949  1950  1951  1952  1953  1954  1955  1956\n## Frequency      1     1     1     1     1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062 0.062\n##                                               \n## Value       1957  1958  1959  1960  1961  1962\n## Frequency      1     1     1     1     1     1\n## Proportion 0.062 0.062 0.062 0.062 0.062 0.062\n## ---------------------------------------------------------------------------\n## Employed \n##        n  missing distinct     Info     Mean      Gmd      .05      .10 \n##       16        0       16        1    65.32    4.153    60.28    60.72 \n##      .25      .50      .75      .90      .95 \n##    62.71    65.50    68.29    69.45    69.81 \n##                                                                          \n## Value      60.171 60.323 61.122 61.187 63.221 63.639 63.761 64.989 66.019\n## Frequency       1      1      1      1      1      1      1      1      1\n## Proportion  0.062  0.062  0.062  0.062  0.062  0.062  0.062  0.062  0.062\n##                                                            \n## Value      66.513 67.857 68.169 68.655 69.331 69.564 70.551\n## Frequency       1      1      1      1      1      1      1\n## Proportion  0.062  0.062  0.062  0.062  0.062  0.062  0.062\n## ---------------------------------------------------------------------------\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# nbr.val, nbr.null, nbr.na, min max, range, sum,\n\n\n# median, mean, SE.mean, CI.mean, var, std.dev, coef.var\n\n\nlibrary\n(\npastecs\n)\n\n\nstat.desc\n(\nlongley\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n##              GNP.deflator          GNP   Unemployed Armed.Forces\n## nbr.val        16.0000000   16.0000000   16.0000000   16.0000000\n## nbr.null        0.0000000    0.0000000    0.0000000    0.0000000\n## nbr.na          0.0000000    0.0000000    0.0000000    0.0000000\n## min            83.0000000  234.2890000  187.0000000  145.6000000\n## max           116.9000000  554.8940000  480.6000000  359.4000000\n## range          33.9000000  320.6050000  293.6000000  213.8000000\n## sum          1626.9000000 6203.1750000 5109.3000000 4170.7000000\n## median        100.6000000  381.4270000  314.3500000  271.7500000\n## mean          101.6812500  387.6984375  319.3312500  260.6687500\n## SE.mean         2.6978884   24.8487344   23.3616062   17.3979901\n## CI.mean.0.95    5.7504129   52.9638237   49.7940849   37.0829381\n## var           116.4576250 9879.3536593 8732.2342917 4843.0409583\n## std.dev        10.7915534   99.3949378   93.4464247   69.5919604\n## coef.var        0.1061312    0.2563718    0.2926316    0.2669747\n##                Population         Year     Employed\n## nbr.val      1.600000e+01 1.600000e+01 1.600000e+01\n## nbr.null     0.000000e+00 0.000000e+00 0.000000e+00\n## nbr.na       0.000000e+00 0.000000e+00 0.000000e+00\n## min          1.076080e+02 1.947000e+03 6.017100e+01\n## max          1.300810e+02 1.962000e+03 7.055100e+01\n## range        2.247300e+01 1.500000e+01 1.038000e+01\n## sum          1.878784e+03 3.127200e+04 1.045072e+03\n## median       1.168035e+02 1.954500e+03 6.550400e+01\n## mean         1.174240e+02 1.954500e+03 6.531700e+01\n## SE.mean      1.739025e+00 1.190238e+00 8.779921e-01\n## CI.mean.0.95 3.706645e+00 2.536932e+00 1.871396e+00\n## var          4.838735e+01 2.266667e+01 1.233392e+01\n## std.dev      6.956102e+00 4.760952e+00 3.511968e+00\n## coef.var     5.923918e-02 2.435893e-03 5.376806e-02\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# item name, item number, nvalid, mean, sd,\n\n\n# median, mad, min, max, skew, kurtosis, se\n\n\nlibrary\n(\npsych\n)\n\n\ndescribe\n(\nlongley\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n##              vars  n    mean    sd  median trimmed    mad     min     max\n## GNP.deflator    1 16  101.68 10.79  100.60  101.93  15.79   83.00  116.90\n## GNP             2 16  387.70 99.39  381.43  386.71 118.57  234.29  554.89\n## Unemployed      3 16  319.33 93.45  314.35  317.26 116.75  187.00  480.60\n## Armed.Forces    4 16  260.67 69.59  271.75  261.84  52.78  145.60  359.40\n## Population      5 16  117.42  6.96  116.80  117.22   8.17  107.61  130.08\n## Year            6 16 1954.50  4.76 1954.50 1954.50   5.93 1947.00 1962.00\n## Employed        7 16   65.32  3.51   65.50   65.31   4.31   60.17   70.55\n##               range  skew kurtosis    se\n## GNP.deflator  33.90 -0.13    -1.40  2.70\n## GNP          320.61  0.02    -1.35 24.85\n## Unemployed   293.60  0.14    -1.30 23.36\n## Armed.Forces 213.80 -0.37    -1.20 17.40\n## Population    22.47  0.26    -1.27  1.74\n## Year          15.00  0.00    -1.43  1.19\n## Employed      10.38 -0.09    -1.55  0.88\n\n\n\n\n\n\n1\n2\n# with\n\n\nwith\n(\nlongley\n,\n median\n(\nGNP\n))\n\n\n\n\n\n\n\n1\n## [1] 381.427\n\n\n\n\n\n\n1\n2\n# vs.\n\nmedian\n(\nlongley\n$\nGNP\n)\n\n\n\n\n\n\n\n1\n## [1] 381.427\n\n\n\n\n\n\nGroup data\n\u00b6\n\n\n1\n2\n# dataset\n\n\nhead\n(\nmtcars\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##                mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n\n\n\n\n\n\n1\n2\n# variable grouped by one factor to show a function\n\n\nby\n(\nmtcars\n$\nmpg\n,\n mtcars\n$\ncyl\n,\n FUN \n=\n \nfunction\n(\nx\n)\n \n{\n \nc\n(\nm \n=\n \nmean\n(\nx\n),\n s \n=\n sd\n(\nx\n))\n \n})\n \n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## mtcars$cyl: 4\n##         m         s \n## 26.663636  4.509828 \n## -------------------------------------------------------- \n## mtcars$cyl: 6\n##         m         s \n## 19.742857  1.453567 \n## -------------------------------------------------------- \n## mtcars$cyl: 8\n##         m         s \n## 15.100000  2.560048\n\n\n\n\n\n\n1\n2\n3\n4\n# description statistics by group\n\n\nlibrary\n(\npsych\n)\n\n\ndescribeBy\n(\nmtcars\n,\n group \n=\n mtcars\n$\nam\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n## \n##  Descriptive statistics by group \n## group: 0\n##      vars  n   mean     sd median trimmed    mad    min    max  range\n## mpg     1 19  17.15   3.83  17.30   17.12   3.11  10.40  24.40  14.00\n## cyl     2 19   6.95   1.54   8.00    7.06   0.00   4.00   8.00   4.00\n## disp    3 19 290.38 110.17 275.80  289.71 124.83 120.10 472.00 351.90\n## hp      4 19 160.26  53.91 175.00  161.06  77.10  62.00 245.00 183.00\n## drat    5 19   3.29   0.39   3.15    3.28   0.22   2.76   3.92   1.16\n## wt      6 19   3.77   0.78   3.52    3.75   0.45   2.46   5.42   2.96\n## qsec    7 19  18.18   1.75  17.82   18.07   1.19  15.41  22.90   7.49\n## vs      8 19   0.37   0.50   0.00    0.35   0.00   0.00   1.00   1.00\n## am      9 19   0.00   0.00   0.00    0.00   0.00   0.00   0.00   0.00\n## gear   10 19   3.21   0.42   3.00    3.18   0.00   3.00   4.00   1.00\n## carb   11 19   2.74   1.15   3.00    2.76   1.48   1.00   4.00   3.00\n##       skew kurtosis    se\n## mpg   0.01    -0.80  0.88\n## cyl  -0.95    -0.74  0.35\n## disp  0.05    -1.26 25.28\n## hp   -0.01    -1.21 12.37\n## drat  0.50    -1.30  0.09\n## wt    0.98     0.14  0.18\n## qsec  0.85     0.55  0.40\n## vs    0.50    -1.84  0.11\n## am     NaN      NaN  0.00\n## gear  1.31    -0.29  0.10\n## carb -0.14    -1.57  0.26\n## -------------------------------------------------------- \n## group: 1\n##      vars  n   mean    sd median trimmed   mad   min    max  range  skew\n## mpg     1 13  24.39  6.17  22.80   24.38  6.67 15.00  33.90  18.90  0.05\n## cyl     2 13   5.08  1.55   4.00    4.91  0.00  4.00   8.00   4.00  0.87\n## disp    3 13 143.53 87.20 120.30  131.25 58.86 71.10 351.00 279.90  1.33\n## hp      4 13 126.85 84.06 109.00  114.73 63.75 52.00 335.00 283.00  1.36\n## drat    5 13   4.05  0.36   4.08    4.02  0.27  3.54   4.93   1.39  0.79\n## wt      6 13   2.41  0.62   2.32    2.39  0.68  1.51   3.57   2.06  0.21\n## qsec    7 13  17.36  1.79  17.02   17.39  2.34 14.50  19.90   5.40 -0.23\n## vs      8 13   0.54  0.52   1.00    0.55  0.00  0.00   1.00   1.00 -0.14\n## am      9 13   1.00  0.00   1.00    1.00  0.00  1.00   1.00   0.00   NaN\n## gear   10 13   4.38  0.51   4.00    4.36  0.00  4.00   5.00   1.00  0.42\n## carb   11 13   2.92  2.18   2.00    2.64  1.48  1.00   8.00   7.00  0.98\n##      kurtosis    se\n## mpg     -1.46  1.71\n## cyl     -0.90  0.43\n## disp     0.40 24.19\n## hp       0.56 23.31\n## drat     0.21  0.10\n## wt      -1.17  0.17\n## qsec    -1.42  0.50\n## vs      -2.13  0.14\n## am        NaN  0.00\n## gear    -1.96  0.14\n## carb    -0.21  0.60\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# description statistics by group \n\n\nlibrary\n(\ndoBy\n)\n\n\nsummaryBy\n(\nmpg \n+\n wt \n~\n cyl \n+\n vs\n,\n data \n=\n mtcars\n,\n \n  FUN \n=\n \nfunction\n(\nx\n)\n \n{\n \nc\n(\nm \n=\n \nmean\n(\nx\n),\n s \n=\n sd\n(\nx\n))\n \n}\n \n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##   cyl vs    mpg.m     mpg.s     wt.m      wt.s\n## 1   4  0 26.00000        NA 2.140000        NA\n## 2   4  1 26.73000 4.7481107 2.300300 0.5982073\n## 3   6  0 20.56667 0.7505553 2.755000 0.1281601\n## 4   6  1 19.12500 1.6317169 3.388750 0.1162164\n## 5   8  0 15.10000 2.5600481 3.999214 0.7594047\n\n\n\n\n\n\nFrequency Tables, CrossTables, and Independence\n\u00b6\n\n\nCreate frequency and contingency tables from categorical variables. Perform tests of independence, measures of association, and graphically display results.\n\n\n2D frequency tables\n\u00b6\n\n\nmytable \n<-\n \ntable\n(\nA\n,\n B\n)\n where A are rows, B are columns.\n\n\n1\n2\n3\n# dataset\n\nmytable \n<-\n \nmatrix\n(\nc\n(\n1\n,\n2\n,\n3\n,\n4\n),\n nrow \n=\n \n2\n)\n\nmytable\n\n\n\n\n\n\n1\n2\n3\n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\n\n\n\n\n\n\n1\n2\n# A frequencies (summed over columns = 1)\n\n\nmargin.table\n(\nmytable\n,\n \n1\n)\n\n\n\n\n\n\n\n1\n## [1] 4 6\n\n\n\n\n\n\n1\n2\n# B frequencies (summed over rows = 2)\n\n\nmargin.table\n(\nmytable\n,\n \n2\n)\n\n\n\n\n\n\n\n1\n## [1] 3 7\n\n\n\n\n\n\n1\n2\n3\n# A/(A + B)\n\n\n# cell percentages\n\n\nprop.table\n(\nmytable\n)\n\n\n\n\n\n\n\n1\n2\n3\n##      [,1] [,2]\n## [1,]  0.1  0.3\n## [2,]  0.2  0.4\n\n\n\n\n\n\n1\n2\n# row percentages\n\n\nprop.table\n(\nmytable\n,\n \n1\n)\n\n\n\n\n\n\n\n1\n2\n3\n##           [,1]      [,2]\n## [1,] 0.2500000 0.7500000\n## [2,] 0.3333333 0.6666667\n\n\n\n\n\n\n1\n2\n# column percentages \n\n\nprop.table\n(\nmytable\n,\n \n2\n)\n\n\n\n\n\n\n\n1\n2\n3\n##           [,1]      [,2]\n## [1,] 0.3333333 0.4285714\n## [2,] 0.6666667 0.5714286\n\n\n\n\n\n\n3D frequency tables\n\u00b6\n\n\n1\n2\n# dataset\n\n\nhead\n(\nCO2\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## Grouped Data: uptake ~ conc | Plant\n##   Plant   Type  Treatment conc uptake\n## 1   Qn1 Quebec nonchilled   95   16.0\n## 2   Qn1 Quebec nonchilled  175   30.4\n## 3   Qn1 Quebec nonchilled  250   34.8\n\n\n\n\n\n\n1\n2\n3\n4\nA \n<-\n \nas.numeric\n(\nCO2\n[,\n \n'Plant'\n])\n\nB \n<-\n CO2\n[,\n \n'conc'\n]\n\nC \n<-\n CO2\n[,\n \n'uptake'\n]\n\nmytable \n<-\n \ntable\n(\nA\n,\n B\n,\n C\n)\n\n\n\n\n\n\n\n1\n2\n# several arrays (3D)\n\n\ndim\n(\nmytable\n)\n \n# the printout is immense\n\n\n\n\n\n\n\n1\n## [1] 12  7 76\n\n\n\n\n\n\n1\n2\n# folded table (2D)\n\n\ndim\n(\nftable\n(\nmytable\n))\n \n# the printout is immense\n\n\n\n\n\n\n\n1\n## [1] 84 76\n\n\n\n\n\n\n1\n2\n3\n4\n# 3-Way frequency Table\n\nmytable \n<-\n xtabs\n(\n~\nA \n+\n B \n+\n C\n,\n data \n=\n mytable\n,\n na.action \n=\n na.omit\n)\n\n\n\ndim\n(\nftable\n(\nmytable\n))\n \n# the printout is immense\n\n\n\n\n\n\n\n1\n## [1] 84 76\n\n\n\n\n\n\nCrossTable\n\u00b6\n\n\n1\n2\nA \n<-\n \nas.numeric\n(\nCO2\n[\n1\n:\n8\n,\n \n'Plant'\n])\n\nA\n\n\n\n\n\n\n1\n## [1] 1 1 1 1 1 1 1 2\n\n\n\n\n\n\n1\n2\nB \n<-\n CO2\n[\n1\n:\n8\n,\n \n'conc'\n]\n\nB\n\n\n\n\n\n\n1\n## [1]   95  175  250  350  500  675 1000   95\n\n\n\n\n\n\n1\n2\n3\n4\n# 2-way cross tabulation\n\n\nlibrary\n(\ngmodels\n)\n\n\nCrossTable\n(\nA\n,\n B\n)\n \n# mydata$myrowvar x mydata$mycolvar\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n## \n##  \n##    Cell Contents\n## |-------------------------|\n## |                       N |\n## | Chi-square contribution |\n## |           N / Row Total |\n## |           N / Col Total |\n## |         N / Table Total |\n## |-------------------------|\n## \n##  \n## Total Observations in Table:  8 \n## \n##  \n##              | B \n##            A |        95 |       175 |       250 |       350 |       500 |       675 |      1000 | Row Total | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n##            1 |         1 |         1 |         1 |         1 |         1 |         1 |         1 |         7 | \n##              |     0.321 |     0.018 |     0.018 |     0.018 |     0.018 |     0.018 |     0.018 |           | \n##              |     0.143 |     0.143 |     0.143 |     0.143 |     0.143 |     0.143 |     0.143 |     0.875 | \n##              |     0.500 |     1.000 |     1.000 |     1.000 |     1.000 |     1.000 |     1.000 |           | \n##              |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |           | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n##            2 |         1 |         0 |         0 |         0 |         0 |         0 |         0 |         1 | \n##              |     2.250 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |           | \n##              |     1.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.125 | \n##              |     0.500 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |           | \n##              |     0.125 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |           | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n## Column Total |         2 |         1 |         1 |         1 |         1 |         1 |         1 |         8 | \n##              |     0.250 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |           | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n## \n##\n\n\n\n\n\n\nTests of independence\n\u00b6\n\n\nThere are more tests of independence in section 10, Resampling Statistics.\n\n\n1\n2\n# dataset, a contingency table\n\ncolors\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85\n\n\n\n\n\n\n1\n2\n3\n# chi-square test on 2-way tables\n\n\n# test independence of the row and column variables, p-value is calculated from the asymptotic chi-squared distribution of the test statistic\n\nchisq.test\n(\ncolors\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Pearson's Chi-squared test\n## \n## data:  colors\n## X-squared = 1240, df = 12, p-value < 2.2e-16\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# dataset, a contingency table in 2x2 matrix form\n\nTeaTasting \n<-\n\n\nmatrix\n(\nc\n(\n3\n,\n \n1\n,\n \n1\n,\n \n3\n),\n\n       nrow \n=\n \n2\n,\n\n       dimnames \n=\n \nlist\n(\nGuess \n=\n \nc\n(\n\"Milk\"\n,\n \n\"Tea\"\n),\n\n                       Truth \n=\n \nc\n(\n\"Milk\"\n,\n \n\"Tea\"\n)))\n\nTeaTasting\n\n\n\n\n\n\n1\n2\n3\n4\n##       Truth\n## Guess  Milk Tea\n##   Milk    3   1\n##   Tea     1   3\n\n\n\n\n\n\n1\n2\n# Fisher exact test\n\nfisher.test\n(\nTeaTasting\n,\n alternative \n=\n \n\"greater\"\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  Fisher's Exact Test for Count Data\n## \n## data:  TeaTasting\n## p-value = 0.2429\n## alternative hypothesis: true odds ratio is greater than 1\n## 95 percent confidence interval:\n##  0.3135693       Inf\n## sample estimates:\n## odds ratio \n##   6.408309\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n# 3D contingency table, where the last dimension refers to the strata\n\nRabbits \n<-\n\n\narray\n(\nc\n(\n0\n,\n \n0\n,\n \n6\n,\n \n5\n,\n\n        \n3\n,\n \n0\n,\n \n3\n,\n \n6\n,\n\n        \n6\n,\n \n2\n,\n \n0\n,\n \n4\n,\n\n        \n5\n,\n \n6\n,\n \n1\n,\n \n0\n,\n\n        \n2\n,\n \n5\n,\n \n0\n,\n \n0\n),\n\n      dim \n=\n \nc\n(\n2\n,\n \n2\n,\n \n5\n),\n\n      dimnames \n=\n \nlist\n(\n\n          Delay \n=\n \nc\n(\n\"None\"\n,\n \n\"1.5h\"\n),\n\n          Response \n=\n \nc\n(\n\"Cured\"\n,\n \n\"Died\"\n),\n\n          Penicillin.Level \n=\n \nc\n(\n\"1/8\"\n,\n \n\"1/4\"\n,\n \n\"1/2\"\n,\n \n\"1\"\n,\n \n\"4\"\n)))\n\nRabbits\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n## , , Penicillin.Level = 1/8\n## \n##       Response\n## Delay  Cured Died\n##   None     0    6\n##   1.5h     0    5\n## \n## , , Penicillin.Level = 1/4\n## \n##       Response\n## Delay  Cured Died\n##   None     3    3\n##   1.5h     0    6\n## \n## , , Penicillin.Level = 1/2\n## \n##       Response\n## Delay  Cured Died\n##   None     6    0\n##   1.5h     2    4\n## \n## , , Penicillin.Level = 1\n## \n##       Response\n## Delay  Cured Died\n##   None     5    1\n##   1.5h     6    0\n## \n## , , Penicillin.Level = 4\n## \n##       Response\n## Delay  Cured Died\n##   None     2    0\n##   1.5h     5    0\n\n\n\n\n\n\n1\n2\n# Mantel-Haenszel test / Cochran-Mantel-Haenszel chi-squared test, hypothesis that two nominal variables are conditionally independent in each stratum, assuming that there is no three-way interaction.\n\nmantelhaen.test\n(\nRabbits\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  Mantel-Haenszel chi-squared test with continuity correction\n## \n## data:  Rabbits\n## Mantel-Haenszel X-squared = 3.9286, df = 1, p-value = 0.04747\n## alternative hypothesis: true common odds ratio is not equal to 1\n## 95 percent confidence interval:\n##   1.026713 47.725133\n## sample estimates:\n## common odds ratio \n##                 7\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# dataset\n\n\n# 3-way contingency table based on variables A, B, and C\n\nA \n<-\n CO2\n[,\n \n'Plant'\n]\n\nB \n<-\n CO2\n[,\n \n'conc'\n]\n\nC \n<-\n CO2\n[,\n \n'uptake'\n]\n\nmytable \n<-\n xtabs\n(\n~\nA\n+\nB\n+\nC\n)\n \n# a 3D array\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# loglinear Models\n\n\n# mutual independence: A, B, and C are pairwise independent\n\n\nlibrary\n(\nMASS\n)\n\n\nloglm\n(\n~\nA \n+\n B \n+\n C\n,\n mytable\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n## Call:\n## loglm(formula = ~A + B + C, data = mytable)\n## \n## Statistics:\n##                        X^2   df  P(> X^2)\n## Likelihood Ratio  720.1035 6291 1.0000000\n## Pearson          6300.0000 6291 0.4656775\n\n\n\n\n\n\n1\n2\n# conditional independence: A is independent of B, given C\n\nloglm\n(\n~\nA \n+\n B \n+\n C \n+\n A \n*\n C \n+\n B \n*\n C\n,\n mytable\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n## Call:\n## loglm(formula = ~A + B + C + A * C + B * C, data = mytable)\n## \n## Statistics:\n##                       X^2   df P(> X^2)\n## Likelihood Ratio 12.13685 5016        1\n## Pearson               NaN 5016      NaN\n\n\n\n\n\n\n1\n2\n# no three-way interaction\n\nloglm\n(\n~\nA \n+\n B \n+\n C \n+\n A \n*\n B \n+\n A \n*\n C \n+\n B \n*\n C\n,\n mytable\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n## Call:\n## loglm(formula = ~A + B + C + A * B + A * C + B * C, data = mytable)\n## \n## Statistics:\n##                       X^2   df P(> X^2)\n## Likelihood Ratio 1.038376 4950        1\n## Pearson               NaN 4950      NaN\n\n\n\n\n\n\nMeasures of association\n\u00b6\n\n\nAssociation between two nominal variables, giving a value between 0 and +1 (inclusive). It is based on Pearson\u2019s chi-squared statistic.\n\n\n1\n2\n# Dataset\n\nstr\n(\nArthritis\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n## 'data.frame':    84 obs. of  5 variables:\n##  $ ID       : int  57 46 77 17 36 23 75 39 33 55 ...\n##  $ Treatment: Factor w/ 2 levels \"Placebo\",\"Treated\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ Sex      : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ Age      : int  27 29 30 32 46 58 59 59 63 63 ...\n##  $ Improved : Ord.factor w/ 3 levels \"None\"<\"Some\"<..: 2 1 1 3 3 3 1 3 1 1 ...\n\n\n\n\n\n\n1\n2\ntab \n<-\n xtabs\n(\n~\nImproved \n+\n Treatment\n,\n data \n=\n Arthritis\n)\n\ntab\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##         Treatment\n## Improved Placebo Treated\n##   None        29      13\n##   Some         7       7\n##   Marked       7      21\n\n\n\n\n\n\n1\nsummary\n(\nassocstats\n(\ntab\n))\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n## \n## Call: xtabs(formula = ~Improved + Treatment, data = Arthritis)\n## Number of cases in table: 84 \n## Number of factors: 2 \n## Test for independence of all factors:\n##  Chisq = 13.055, df = 2, p-value = 0.001463\n##                     X^2 df  P(> X^2)\n## Likelihood Ratio 13.530  2 0.0011536\n## Pearson          13.055  2 0.0014626\n## \n## Phi-Coefficient   : NA \n## Contingency Coeff.: 0.367 \n## Cramer's V        : 0.394\n\n\n\n\n\n\n1\n2\n3\n4\n# phi coefficient, contingency coefficient, and Cram\u00e9r's V for an 2D table\n\n\nlibrary\n(\nvcd\n)\n\n\nassocstats\n(\ntab\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##                     X^2 df  P(> X^2)\n## Likelihood Ratio 13.530  2 0.0011536\n## Pearson          13.055  2 0.0014626\n## \n## Phi-Coefficient   : NA \n## Contingency Coeff.: 0.367 \n## Cramer's V        : 0.394\n\n\n\n\n\n\n1\n2\n# Dataset\n\nTeaTasting\n\n\n\n\n\n\n1\n2\n3\n4\n##       Truth\n## Guess  Milk Tea\n##   Milk    3   1\n##   Tea     1   3\n\n\n\n\n\n\n1\n2\n3\n4\n# Cohen's kappa and weighted kappa for a confusion matrix\n\n\nlibrary\n(\nvcd\n)\n\n\n\nkappa\n(\nTeaTasting\n)\n\n\n\n\n\n\n\n1\n## [1] 2.333333\n\n\n\n\n\n\nCorrelations\n\u00b6\n\n\n1\n2\n# dataset\n\n\nhead\n(\nmtcars\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##                mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n\n\n\n\n\n\n1\n2\n3\n# correlations/covariances among numeric variables in\n\n\n# a data frame\n\ncor\n(\nmtcars\n,\n use \n=\n \n\"complete.obs\"\n,\n method \n=\n \n\"kendall\"\n)\n \n# method = \"pearson\", \"spearman\" or \"kendall\"\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n##             mpg        cyl       disp         hp        drat         wt\n## mpg   1.0000000 -0.7953134 -0.7681311 -0.7428125  0.46454879 -0.7278321\n## cyl  -0.7953134  1.0000000  0.8144263  0.7851865 -0.55131785  0.7282611\n## disp -0.7681311  0.8144263  1.0000000  0.6659987 -0.49898277  0.7433824\n## hp   -0.7428125  0.7851865  0.6659987  1.0000000 -0.38262689  0.6113081\n## drat  0.4645488 -0.5513178 -0.4989828 -0.3826269  1.00000000 -0.5471495\n## wt   -0.7278321  0.7282611  0.7433824  0.6113081 -0.54714953  1.0000000\n## qsec  0.3153652 -0.4489698 -0.3008155 -0.4729061  0.03272155 -0.1419881\n## vs    0.5896790 -0.7710007 -0.6033059 -0.6305926  0.37510111 -0.4884787\n## am    0.4690128 -0.4946212 -0.5202739 -0.3039956  0.57554849 -0.6138790\n## gear  0.4331509 -0.5125435 -0.4759795 -0.2794458  0.58392476 -0.5435956\n## carb -0.5043945  0.4654299  0.4137360  0.5959842 -0.09535193  0.3713741\n##             qsec         vs          am        gear        carb\n## mpg   0.31536522  0.5896790  0.46901280  0.43315089 -0.50439455\n## cyl  -0.44896982 -0.7710007 -0.49462115 -0.51254349  0.46542994\n## disp -0.30081549 -0.6033059 -0.52027392 -0.47597955  0.41373600\n## hp   -0.47290613 -0.6305926 -0.30399557 -0.27944584  0.59598416\n## drat  0.03272155  0.3751011  0.57554849  0.58392476 -0.09535193\n## wt   -0.14198812 -0.4884787 -0.61387896 -0.54359562  0.37137413\n## qsec  1.00000000  0.6575431 -0.16890405 -0.09126069 -0.50643945\n## vs    0.65754312  1.0000000  0.16834512  0.26974788 -0.57692729\n## am   -0.16890405  0.1683451  1.00000000  0.77078758 -0.05859929\n## gear -0.09126069  0.2697479  0.77078758  1.00000000  0.09801487\n## carb -0.50643945 -0.5769273 -0.05859929  0.09801487  1.00000000\n\n\n\n\n\n\n1\ncov\n(\nmtcars\n,\n use \n=\n \n\"complete.obs\"\n)\n \n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n##              mpg         cyl        disp          hp         drat\n## mpg    36.324103  -9.1723790  -633.09721 -320.732056   2.19506351\n## cyl    -9.172379   3.1895161   199.66028  101.931452  -0.66836694\n## disp -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915\n## hp   -320.732056 101.9314516  6721.15867 4700.866935 -16.45110887\n## drat    2.195064  -0.6683669   -47.06402  -16.451109   0.28588135\n## wt     -5.116685   1.3673710   107.68420   44.192661  -0.37272073\n## qsec    4.509149  -1.8868548   -96.05168  -86.770081   0.08714073\n## vs      2.017137  -0.7298387   -44.37762  -24.987903   0.11864919\n## am      1.803931  -0.4657258   -36.56401   -8.320565   0.19015121\n## gear    2.135685  -0.6491935   -50.80262   -6.358871   0.27598790\n## carb   -5.363105   1.5201613    79.06875   83.036290  -0.07840726\n##               wt         qsec           vs           am        gear\n## mpg   -5.1166847   4.50914919   2.01713710   1.80393145   2.1356855\n## cyl    1.3673710  -1.88685484  -0.72983871  -0.46572581  -0.6491935\n## disp 107.6842040 -96.05168145 -44.37762097 -36.56401210 -50.8026210\n## hp    44.1926613 -86.77008065 -24.98790323  -8.32056452  -6.3588710\n## drat  -0.3727207   0.08714073   0.11864919   0.19015121   0.2759879\n## wt     0.9573790  -0.30548161  -0.27366129  -0.33810484  -0.4210806\n## qsec  -0.3054816   3.19316613   0.67056452  -0.20495968  -0.2804032\n## vs    -0.2736613   0.67056452   0.25403226   0.04233871   0.0766129\n## am    -0.3381048  -0.20495968   0.04233871   0.24899194   0.2923387\n## gear  -0.4210806  -0.28040323   0.07661290   0.29233871   0.5443548\n## carb   0.6757903  -1.89411290  -0.46370968   0.04637097   0.3266129\n##             carb\n## mpg  -5.36310484\n## cyl   1.52016129\n## disp 79.06875000\n## hp   83.03629032\n## drat -0.07840726\n## wt    0.67579032\n## qsec -1.89411290\n## vs   -0.46370968\n## am    0.04637097\n## gear  0.32661290\n## carb  2.60887097\n\n\n\n\n\n\n1\n2\n3\n4\n# correlations with significance levels\n\n\nlibrary\n(\nHmisc\n)\n\n\nrcorr\n(\nas.matrix\n(\nmtcars\n),\n type \n=\n \n\"pearson\"\n)\n \n# type can be pearson or spearman\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n##        mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n## mpg   1.00 -0.85 -0.85 -0.78  0.68 -0.87  0.42  0.66  0.60  0.48 -0.55\n## cyl  -0.85  1.00  0.90  0.83 -0.70  0.78 -0.59 -0.81 -0.52 -0.49  0.53\n## disp -0.85  0.90  1.00  0.79 -0.71  0.89 -0.43 -0.71 -0.59 -0.56  0.39\n## hp   -0.78  0.83  0.79  1.00 -0.45  0.66 -0.71 -0.72 -0.24 -0.13  0.75\n## drat  0.68 -0.70 -0.71 -0.45  1.00 -0.71  0.09  0.44  0.71  0.70 -0.09\n## wt   -0.87  0.78  0.89  0.66 -0.71  1.00 -0.17 -0.55 -0.69 -0.58  0.43\n## qsec  0.42 -0.59 -0.43 -0.71  0.09 -0.17  1.00  0.74 -0.23 -0.21 -0.66\n## vs    0.66 -0.81 -0.71 -0.72  0.44 -0.55  0.74  1.00  0.17  0.21 -0.57\n## am    0.60 -0.52 -0.59 -0.24  0.71 -0.69 -0.23  0.17  1.00  0.79  0.06\n## gear  0.48 -0.49 -0.56 -0.13  0.70 -0.58 -0.21  0.21  0.79  1.00  0.27\n## carb -0.55  0.53  0.39  0.75 -0.09  0.43 -0.66 -0.57  0.06  0.27  1.00\n## \n## n= 32 \n## \n## \n## P\n##      mpg    cyl    disp   hp     drat   wt     qsec   vs     am     gear  \n## mpg         0.0000 0.0000 0.0000 0.0000 0.0000 0.0171 0.0000 0.0003 0.0054\n## cyl  0.0000        0.0000 0.0000 0.0000 0.0000 0.0004 0.0000 0.0022 0.0042\n## disp 0.0000 0.0000        0.0000 0.0000 0.0000 0.0131 0.0000 0.0004 0.0010\n## hp   0.0000 0.0000 0.0000        0.0100 0.0000 0.0000 0.0000 0.1798 0.4930\n## drat 0.0000 0.0000 0.0000 0.0100        0.0000 0.6196 0.0117 0.0000 0.0000\n## wt   0.0000 0.0000 0.0000 0.0000 0.0000        0.3389 0.0010 0.0000 0.0005\n## qsec 0.0171 0.0004 0.0131 0.0000 0.6196 0.3389        0.0000 0.2057 0.2425\n## vs   0.0000 0.0000 0.0000 0.0000 0.0117 0.0010 0.0000        0.3570 0.2579\n## am   0.0003 0.0022 0.0004 0.1798 0.0000 0.0000 0.2057 0.3570        0.0000\n## gear 0.0054 0.0042 0.0010 0.4930 0.0000 0.0005 0.2425 0.2579 0.0000       \n## carb 0.0011 0.0019 0.0253 0.0000 0.6212 0.0146 0.0000 0.0007 0.7545 0.1290\n##      carb  \n## mpg  0.0011\n## cyl  0.0019\n## disp 0.0253\n## hp   0.0000\n## drat 0.6212\n## wt   0.0146\n## qsec 0.0000\n## vs   0.0007\n## am   0.7545\n## gear 0.1290\n## carb\n\n\n\n\n\n\n1\n2\n3\n# dataset\n\nx \n<-\n mtcars\n[\n1\n:\n3\n]\n\ny \n<-\n mtcars\n[\n4\n:\n6\n]\n\n\n\n\n\n\n\n1\n2\n# correlation between two vectors\n\ncor\n(\nx\n,\n y\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##              hp       drat         wt\n## mpg  -0.7761684  0.6811719 -0.8676594\n## cyl   0.8324475 -0.6999381  0.7824958\n## disp  0.7909486 -0.7102139  0.8879799\n\n\n\n\n\n\nPolychoric correlation\n\n\nThe correlation between two theorised normally distributed continuous latent variables, from two observed ordinal variables.\n\n\n1\n2\n3\n# dataset\n\n\n# 2-way contingency table of counts\n\ncolors\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85\n\n\n\n\n\n\n1\n2\n3\n4\n# polychoric correlation\n\n\nlibrary\n(\npolycor\n)\n\n\npolychor\n(\ncolors\n)\n\n\n\n\n\n\n\n1\n## [1] 0.4743984\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# heterogeneous correlations in one matrix\n\n\n# pearson (numeric-numeric),\n\n\n# polyserial (numeric-ordinal),\n\n\n# and polychoric (ordinal-ordinal)\n\n\n# a data frame with ordered factors and numeric variables\n\nhetcor\n(\ncolors\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n## \n## Two-Step Estimates\n## \n## Correlations/Type of Correlation:\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## fair.hair           1  Pearson     Pearson   Pearson    Pearson\n## red.hair       0.8333        1     Pearson   Pearson    Pearson\n## medium.hair    0.2597   0.6467           1   Pearson    Pearson\n## dark.hair     -0.7091  -0.2251      0.1911         1    Pearson\n## black.hair     -0.781  -0.3875    -0.06329    0.9674          1\n## \n## Standard Errors:\n##             fair.hair red.hair medium.hair dark.hair\n## fair.hair                                           \n## red.hair      0.03628                               \n## medium.hair    0.3607   0.1383                      \n## dark.hair     0.09977   0.3733      0.3841          \n## black.hair    0.06019   0.3004      0.4092  0.001491\n## \n## n = 4 \n## \n## P-values for Tests of Bivariate Normality:\n##             fair.hair red.hair medium.hair dark.hair\n## fair.hair                                           \n## red.hair       0.8306                               \n## medium.hair    0.6939   0.8609                      \n## dark.hair      0.7465   0.6335      0.7161          \n## black.hair     0.6582   0.5927      0.6711    0.9873\n\n\n\n\n\n\nt-tests\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# independent 2-group t-test\n\nt.test\n(\ny \n~\n x\n)\n \n# where y is numeric and x is a binary factor\n\n\n\n# independent 2-group t-test\n\nt.test\n(\ny1\n,\n y2\n)\n \n# where y1 and y2 are numeric \n\n\n\n# paired t-test\n\nt.test\n(\ny1\n,\n y2\n,\n paired \n=\n \nTRUE\n)\n \n# where y1 & y2 are numeric \n\n\n\n# one sample t-test\n\nt.test\n(\ny\n,\n mu \n=\n \n3\n)\n \n# Ho: mu=3\n\n\n\n\n\n\n\n1\n2\n3\n# dataset\n\n\n# 2-way contingency table\n\ncolors\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85\n\n\n\n\n\n\n1\nmean\n(\nas.numeric\n(\ncolors\n[\n1\n,]))\n \n# row 1 average\n\n\n\n\n\n\n\n1\n## [1] 143.6\n\n\n\n\n\n\n1\n2\n# independent 2-group t-test\n\nt.test\n(\nas.numeric\n(\ncolors\n[\n1\n,]),\n \nas.numeric\n(\ncolors\n[\n2\n,]))\n \n# where y1 and y2 are numeric \n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  Welch Two Sample t-test\n## \n## data:  as.numeric(colors[1, ]) and as.numeric(colors[2, ])\n## t = -1.164, df = 5.5777, p-value = 0.2918\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -541.5725  196.7725\n## sample estimates:\n## mean of x mean of y \n##     143.6     316.0\n\n\n\n\n\n\n1\n2\n# one sample t-test\n\nt.test\n(\nas.numeric\n(\ncolors\n[\n1\n,]),\n mu \n=\n \n0\n)\n \n# Ho: mu=0\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  One Sample t-test\n## \n## data:  as.numeric(colors[1, ])\n## t = 2.348, df = 4, p-value = 0.07868\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  -26.2009 313.4009\n## sample estimates:\n## mean of x \n##     143.6\n\n\n\n\n\n\n1\nt.test\n(\nas.numeric\n(\ncolors\n[\n1\n,]),\n mu \n=\n \n0\n,\n alternative\n=\n\"greater\"\n)\n \n# Ho: mu=<0\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## \n##  One Sample t-test\n## \n## data:  as.numeric(colors[1, ])\n## t = 2.348, df = 4, p-value = 0.03934\n## alternative hypothesis: true mean is greater than 0\n## 95 percent confidence interval:\n##  13.22123      Inf\n## sample estimates:\n## mean of x \n##     143.6\n\n\n\n\n\n\n\n\nalternative=\"less\"\n or \nalternative=\"greater\"\n option to specify a one tailed test.\n\n\nvar.equal = TRUE\n option to specify equal variances and a pooled variance estimate.\n\n\n\n\nFor multivariate tests and ANOVA, see sections 8 and 9.\n\n\nNonparametric statistics\n\u00b6\n\n\nNonnormal distributions.\n\n\nBivariate tests\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# independent 2-group Mann-Whitney U test\n\nwilcox.test\n(\ny \n~\n A\n)\n \n# where y is numeric and A is A binary factor\n\n\n\n# independent 2-group Mann-Whitney U test\n\nwilcox.test\n(\ny\n,\n x\n)\n \n# where y and x are numeric\n\n\n\n# dependent 2-group Wilcoxon signed rank test\n\nwilcox.test\n(\ny1\n,\n y2\n,\n paired \n=\n \nTRUE\n)\n \n# where y1 and y2 are numeric\n\n\n\n\n\n\n\n1\n2\n# 2-way contingency table\n\ncolors\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85\n\n\n\n\n\n\n1\n2\n# independent 2-group Mann-Whitney U Test\n\nwilcox.test\n(\nas.numeric\n(\ncolors\n[\n1\n,]),\n \nas.numeric\n(\ncolors\n[\n2\n,]))\n \n# where y and x are numeric\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n## \n##  Wilcoxon rank sum test\n## \n## data:  as.numeric(colors[1, ]) and as.numeric(colors[2, ])\n## W = 8, p-value = 0.4206\n## alternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n\nalternative=\"less\"\n or \nalternative=\"greater\"\n option to specify a one\n\ntailed test.\n\n\nANOVA\n\u00b6\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Kruskal Wallis test one-Way ANOVA by ranks\n\nkruskal.test\n(\ny \n~\n A\n)\n \n# where y1 is numeric and A is a factor\n\n\n\n# randomized block design - Friedman test\n\nfriedman.test\n(\ny \n~\n A \n|\n B\n)\n \n# where y are the data values, A is a grouping factor and B is a blocking factor\n\n\n\n# Kruskal Wallis test one-way ANOVA by ranks\n\nkruskal.test\n(\ny\n,\n x\n)\n \n# where y and x are numeric\n\n\n\n\n\n\n\n1\n2\n3\n# dataset\n\n\n# 2-way contingency table\n\ncolors\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85\n\n\n\n\n\n\n1\n2\n# Kruskal Wallis test one-way ANOVA by ranks\n\nkruskal.test\n(\nas.numeric\n(\ncolors\n[\n1\n,]),\n \nas.numeric\n(\ncolors\n[\n2\n,]))\n \n# where y and x are numeric\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  as.numeric(colors[1, ]) and as.numeric(colors[2, ])\n## Kruskal-Wallis chi-squared = 4, df = 4, p-value = 0.406\n\n\n\n\n\n\nMultiple Regressions\n\u00b6\n\n\nFitting the Model\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n# multiple linear regression\n\nfit \n<-\n lm\n(\ny \n~\n x1 \n+\n x2 \n+\n x3\n,\n data \n=\n mydata\n)\n\n\nsummary\n(\nfit\n)\n \n# show results\n\n\n\n# useful functions\n\ncoefficients\n(\nfit\n)\n \n# model coefficients\n\nconfint\n(\nfit\n,\n level \n=\n \n0.95\n)\n \n# CIs for model parameters\n\nfitted\n(\nfit\n)\n \n# predicted values\n\nresiduals\n(\nfit\n)\n \n# residuals\n\nanova\n(\nfit\n)\n \n# ANOVA table\n\nvcov\n(\nfit\n)\n \n# covariance matrix for model parameters\n\ninfluence\n(\nfit\n)\n \n# regression diagnostics \n\n\n\n\n\n\n\n1\n2\n# dataset\n\nstr\n(\nlongley\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## 'data.frame':    16 obs. of  7 variables:\n##  $ GNP.deflator: num  83 88.5 88.2 89.5 96.2 ...\n##  $ GNP         : num  234 259 258 285 329 ...\n##  $ Unemployed  : num  236 232 368 335 210 ...\n##  $ Armed.Forces: num  159 146 162 165 310 ...\n##  $ Population  : num  108 109 110 111 112 ...\n##  $ Year        : int  1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ...\n##  $ Employed    : num  60.3 61.1 60.2 61.2 63.2 ...\n\n\n\n\n\n\n1\n2\n3\n# multiple linear regression example\n\nfit \n<-\n lm\n(\nArmed.Forces \n~\n GNP \n+\n Population\n,\n data \n=\n longley\n)\n\n\nsummary\n(\nfit\n)\n \n# show results\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n## \n## Call:\n## lm(formula = Armed.Forces ~ GNP + Population, data = longley)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -70.349 -33.569   5.076  16.409 104.037 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)   \n## (Intercept) 4123.922   1276.579   3.230  0.00657 **\n## GNP            3.365      0.986   3.413  0.00463 **\n## Population   -44.011     14.089  -3.124  0.00807 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 50.56 on 13 degrees of freedom\n## Multiple R-squared:  0.5426, Adjusted R-squared:  0.4723 \n## F-statistic: 7.712 on 2 and 13 DF,  p-value: 0.006191\n\n\n\n\n\n\n1\n2\n# other useful functions\n\ncoefficients\n(\nfit\n)\n \n# model coefficients\n\n\n\n\n\n\n\n1\n2\n## (Intercept)         GNP  Population \n## 4123.922484    3.365215  -44.010955\n\n\n\n\n\n\n1\nconfint\n(\nfit\n,\n level \n=\n \n0.95\n)\n \n# CIs for model parameters\n\n\n\n\n\n\n\n1\n2\n3\n4\n##                   2.5 %      97.5 %\n## (Intercept) 1366.042123 6881.802846\n## GNP            1.235097    5.495333\n## Population   -74.447969  -13.573942\n\n\n\n\n\n\n1\nfitted\n(\nfit\n)\n \n# predicted values\n\n\n\n\n\n\n\n1\n2\n3\n4\n##     1947     1948     1949     1950     1951     1952     1953     1954 \n## 176.4245 215.9487 161.1151 199.5681 298.4663 306.5279 288.1248 230.9633 \n##     1955     1956     1957     1958     1959     1960     1961     1962 \n## 295.1332 308.9566 313.0359 252.7794 318.8698 297.7176 240.7975 266.2711\n\n\n\n\n\n\n1\nresiduals\n(\nfit\n)\n \n# residuals\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##        1947        1948        1949        1950        1951        1952 \n## -17.4245114 -70.3487085   0.4848669 -34.5681071  11.4336564  52.8721086 \n##        1953        1954        1955        1956        1957        1958 \n##  66.5752438 104.0367029   9.6668098 -23.2566323 -33.2359499  10.9205505 \n##        1959        1960        1961        1962 \n## -63.6698197 -46.3175746  16.4025069  16.4288577\n\n\n\n\n\n\n1\nanova\n(\nfit\n)\n \n# ANOVA table\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## Analysis of Variance Table\n## \n## Response: Armed.Forces\n##            Df Sum Sq Mean Sq F value   Pr(>F)   \n## GNP         1  14479 14478.7  5.6649 0.033301 * \n## Population  1  24941 24940.8  9.7583 0.008068 **\n## Residuals  13  33226  2555.9                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\nvcov\n(\nfit\n)\n \n# covariance matrix for model parameters\n\n\n\n\n\n\n\n1\n2\n3\n4\n##             (Intercept)          GNP   Population\n## (Intercept) 1629652.882 1239.7478355 -17970.27388\n## GNP            1239.748    0.9721911    -13.76775\n## Population   -17970.274  -13.7677545    198.49444\n\n\n\n\n\n\n1\ninfluence\n(\nfit\n)\n \n# regression diagnostics \n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n## $hat\n##       1947       1948       1949       1950       1951       1952 \n## 0.27412252 0.17438942 0.31563199 0.16765687 0.21219668 0.21127212 \n##       1953       1954       1955       1956       1957       1958 \n## 0.11339116 0.08602104 0.10270245 0.12845683 0.13251347 0.11070418 \n##       1959       1960       1961       1962 \n## 0.15598638 0.15164367 0.32488437 0.33842686 \n## \n## $coefficients\n##      (Intercept)          GNP  Population\n## 1947  128.042531  0.131479474 -1.53731106\n## 1948   29.040635  0.121992502 -0.69544934\n## 1949   -6.396740 -0.005738655  0.07379997\n## 1950  177.778426  0.175668519 -2.11609661\n## 1951  133.333008  0.093997366 -1.43810938\n## 1952  638.680267  0.462229807 -6.92955762\n## 1953  422.107967  0.305133565 -4.56222462\n## 1954 -385.998493 -0.325674565  4.42308458\n## 1955   54.458118  0.042127997 -0.59713302\n## 1956 -163.371695 -0.131240587  1.81041089\n## 1957 -212.039316 -0.179084306  2.37664757\n## 1958  -51.395691 -0.033854337  0.55600613\n## 1959 -329.488802 -0.311550864  3.79446941\n## 1960    3.116881 -0.049904757  0.10916689\n## 1961 -242.199361 -0.158976865  2.60043035\n## 1962 -194.416431 -0.113799395  2.04462752\n## \n## $sigma\n##     1947     1948     1949     1950     1951     1952     1953     1954 \n## 52.28757 47.63741 52.61955 51.47046 52.48826 49.73420 48.50003 42.21357 \n##     1955     1956     1957     1958     1959     1960     1961     1962 \n## 52.53729 52.12610 51.60167 52.51353 48.66817 50.57779 52.30331 52.29577 \n## \n## $wt.res\n##        1947        1948        1949        1950        1951        1952 \n## -17.4245114 -70.3487085   0.4848669 -34.5681071  11.4336564  52.8721086 \n##        1953        1954        1955        1956        1957        1958 \n##  66.5752438 104.0367029   9.6668098 -23.2566323 -33.2359499  10.9205505 \n##        1959        1960        1961        1962 \n## -63.6698197 -46.3175746  16.4025069  16.4288577\n\n\n\n\n\n\nDiagnostic plots\n\u00b6\n\n\n1\n2\n3\n# diagnostic plots\n\nlayout\n(\nmatrix\n(\nc\n(\n1\n,\n2\n,\n3\n,\n4\n),\n2\n,\n2\n))\n \n# optional 4 graphs/page\n\nplot\n(\nfit\n)\n\n\n\n\n\n\n\n\n\n1\nlayout\n(\nmatrix\n(\nc\n(\n1\n,\n1\n,\n1\n,\n1\n),\n2\n,\n2\n))\n\n\n\n\n\n\n\nComparing two models with ANOVA\n\u00b6\n\n\n1\n2\n3\n4\n5\n# compare models\n\nfit1 \n<-\n lm\n(\ny \n~\n x1 \n+\n x2 \n+\n x3 \n+\n x4\n,\n data \n=\n mydata\n)\n\nfit2 \n<-\n lm\n(\ny \n~\n x1 \n+\n x2\n)\n\n\nanova\n(\nfit1\n,\n fit2\n)\n \n\n\n\n\n\n\n1\n2\n3\n4\n5\n# compare models\n\nfit1 \n<-\n lm\n(\nArmed.Forces \n~\n GNP \n+\n Population\n,\n data \n=\n longley\n)\n\nfit2 \n<-\n lm\n(\nArmed.Forces \n~\n GNP\n,\n data \n=\n longley\n)\n\n\nanova\n(\nfit1\n,\n fit2\n)\n \n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## Analysis of Variance Table\n## \n## Model 1: Armed.Forces ~ GNP + Population\n## Model 2: Armed.Forces ~ GNP\n##   Res.Df   RSS Df Sum of Sq      F   Pr(>F)   \n## 1     13 33226                                \n## 2     14 58167 -1    -24941 9.7583 0.008068 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nCross validation\n\u00b6\n\n\n1\n2\n3\n4\n5\n# dataset\n\n\nlibrary\n(\nDAAG\n)\n\n\ndata\n(\nhouseprices\n)\n\n\nhead\n(\nhouseprices\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##    area bedrooms sale.price\n## 9   694        4        192\n## 10  905        4        215\n## 11  802        4        215\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# k-fold cross-validation\n\n\nlibrary\n(\nDAAG\n)\n\n\n\n# case 1, # 3 fold cross-validation\n\nCVlm\n(\nhouseprices\n,\n form.lm \n=\n formula\n(\nsale.price \n~\n area\n),\n m \n=\n \n3\n,\n dots \n=\n \nFALSE\n,\n seed \n=\n \n29\n,\n plotit \n=\n \nc\n(\n\"Observed\"\n,\n\"Residual\"\n),\n main \n=\n \n\"Small symbols show cross-validation predicted values\"\n,\n legend.pos \n=\n \n\"topleft\"\n,\n printit \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## Analysis of Variance Table\n## \n## Response: sale.price\n##           Df Sum Sq Mean Sq F value Pr(>F)  \n## area       1  18566   18566       8  0.014 *\n## Residuals 13  30179    2321                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n## \n## fold 1 \n## Observations in test set: 5 \n##              11  20    21     22   23\n## area        802 696 771.0 1006.0 1191\n## cvpred      204 188 199.3  234.7  262\n## sale.price  215 255 260.0  293.0  375\n## CV residual  11  67  60.7   58.3  113\n## \n## Sum of squares = 24351    Mean square = 4870    n = 5 \n## \n## fold 2 \n## Observations in test set: 5 \n##              10   13    14      17     18\n## area        905  716 963.0 1018.00 887.00\n## cvpred      255  224 264.4  273.38 252.06\n## sale.price  215  113 185.0  276.00 260.00\n## CV residual -40 -112 -79.4    2.62   7.94\n## \n## Sum of squares = 20416    Mean square = 4083    n = 5 \n## \n## fold 3 \n## Observations in test set: 5 \n##                 9   12     15    16     19\n## area        694.0 1366 821.00 714.0 790.00\n## cvpred      183.2  388 221.94 189.3 212.49\n## sale.price  192.0  274 212.00 220.0 221.50\n## CV residual   8.8 -114  -9.94  30.7   9.01\n## \n## Sum of squares = 14241    Mean square = 2848    n = 5 \n## \n## Overall (Sum over all 5 folds) \n##   ms \n## 3934\n\n\n\n\n\n\n1\n2\n# dataset\n\n\nhead\n(\nlongley\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##      GNP.deflator GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234        236          159        108 1947     60.3\n## 1948         88.5 259        232          146        109 1948     61.1\n## 1949         88.2 258        368          162        110 1949     60.2\n\n\n\n\n\n\n1\n2\n# case 2, # 3 fold cross-validation\n\nCVlm\n(\nlongley\n,\n form.lm \n=\n formula\n(\nArmed.Forces \n~\n GNP \n+\n Population\n),\n m \n=\n \n3\n,\n dots \n=\n \nFALSE\n,\n seed \n=\n \n29\n,\n plotit \n=\n \nc\n(\n\"Observed\"\n,\n\"Residual\"\n),\n main \n=\n \n\"Small symbols show cross-validation predicted values\"\n,\n legend.pos \n=\n \n\"topleft\"\n,\n printit \n=\n \nTRUE\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## Analysis of Variance Table\n## \n## Response: Armed.Forces\n##            Df Sum Sq Mean Sq F value Pr(>F)   \n## GNP         1  14479   14479    5.66 0.0333 * \n## Population  1  24941   24941    9.76 0.0081 **\n## Residuals  13  33226    2556                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n## \n## fold 1 \n## Observations in test set: 5 \n##               1953 1954  1957  1960  1962\n## Predicted    288.1  231 313.0 297.7 266.3\n## cvpred       281.4  219 310.6 295.7 263.1\n## Armed.Forces 354.7  335 279.8 251.4 282.7\n## CV residual   73.3  116 -30.8 -44.3  19.6\n## \n## Sum of squares = 22004    Mean square = 4401    n = 5 \n## \n## fold 2 \n## Observations in test set: 6 \n##               1948   1949   1955   1958  1959  1961\n## Predicted    215.9 161.12 295.13 252.78 318.9 240.8\n## cvpred       231.9 171.41 306.33 255.07 324.5 234.9\n## Armed.Forces 145.6 161.60 304.80 263.70 255.2 257.2\n## CV residual  -86.3  -9.81  -1.53   8.63 -69.3  22.3\n## \n## Sum of squares = 12917    Mean square = 2153    n = 6 \n## \n## fold 3 \n## Observations in test set: 5 \n##               1947  1950  1951  1952   1956\n## Predicted    176.4 199.6 298.5 306.5 308.96\n## cvpred       192.8 211.9 282.3 288.9 295.17\n## Armed.Forces 159.0 165.0 309.9 359.4 285.70\n## CV residual  -33.8 -46.9  27.6  70.5  -9.47\n## \n## Sum of squares = 9161    Mean square = 1832    n = 5 \n## \n## Overall (Sum over all 5 folds) \n##   ms \n## 2755\n\n\n\n\n\n\nVariable selection \u2013 Heuristic methods\n\u00b6\n\n\n1\n2\n# dataset\n\n\nhead\n(\nlongley\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##      GNP.deflator GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234        236          159        108 1947     60.3\n## 1948         88.5 259        232          146        109 1948     61.1\n## 1949         88.2 258        368          162        110 1949     60.2\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# Stepwise Regression\n\n\nlibrary\n(\nMASS\n)\n\n\nfit \n<-\n lm\n(\nArmed.Forces \n~\n GNP \n+\n Population \n+\n Employed \n+\n Unemployed\n,\n data \n=\n longley\n)\n\nstep \n<-\n stepAIC\n(\nfit\n,\n direction \n=\n \n\"both\"\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## Start:  AIC=125\n## Armed.Forces ~ GNP + Population + Employed + Unemployed\n## \n##              Df Sum of Sq   RSS AIC\n## <none>                    21655 125\n## - Unemployed  1      4508 26163 126\n## - Population  1      5522 27177 127\n## - Employed    1     10208 31863 130\n## - GNP         1     15323 36978 132\n\n\n\n\n\n\n1\nstep\n$\nanova \n# display results\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n## Stepwise Model Path \n## Analysis of Deviance Table\n## \n## Initial Model:\n## Armed.Forces ~ GNP + Population + Employed + Unemployed\n## \n## Final Model:\n## Armed.Forces ~ GNP + Population + Employed + Unemployed\n## \n## \n##   Step Df Deviance Resid. Df Resid. Dev AIC\n## 1                         11      21655 125\n\n\n\n\n\n\nThe goal is to reduce the AIC. Adding variable does not improve the model. Let\u2019s opt for the most valuable variable.\n\n\n1\n2\nfit \n<-\n lm\n(\nArmed.Forces \n~\n Unemployed\n,\n data \n=\n longley\n)\n\nstep \n<-\n stepAIC\n(\nfit\n,\n direction \n=\n \n\"both\"\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n## Start:  AIC=138\n## Armed.Forces ~ Unemployed\n## \n##              Df Sum of Sq   RSS AIC\n## - Unemployed  1      2287 72646 137\n## <none>                    70359 138\n## \n## Step:  AIC=137\n## Armed.Forces ~ 1\n## \n##              Df Sum of Sq   RSS AIC\n## <none>                    72646 137\n## + Unemployed  1      2287 70359 138\n\n\n\n\n\n\n1\nstep\n$\nanova \n# display results\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n## Stepwise Model Path \n## Analysis of Deviance Table\n## \n## Initial Model:\n## Armed.Forces ~ Unemployed\n## \n## Final Model:\n## Armed.Forces ~ 1\n## \n## \n##           Step Df Deviance Resid. Df Resid. Dev AIC\n## 1                                 14      70359 138\n## 2 - Unemployed  1     2287        15      72646 137\n\n\n\n\n\n\nVariable selection \u2013 Graphical methods\n\u00b6\n\n\n1\n2\n# model\n\nfit \n<-\n lm\n(\nArmed.Forces \n~\n GNP \n+\n Population \n+\n Employed \n+\n Unemployed\n,\n data \n=\n longley\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n# all Subsets Regression\n\n\nlibrary\n(\nleaps\n)\n\n\nleaps \n<-\n regsubsets\n(\nArmed.Forces \n~\n GNP \n+\n Population \n+\n Employed \n+\n Unemployed\n,\n data \n=\n longley\n,\n nbest \n=\n \n10\n)\n\n\n\n# view results\n\n\nsummary\n(\nleaps\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n## Subset selection object\n## Call: regsubsets.formula(Armed.Forces ~ GNP + Population + Employed + \n##     Unemployed, data = longley, nbest = 10)\n## 4 Variables  (and intercept)\n##            Forced in Forced out\n## GNP            FALSE      FALSE\n## Population     FALSE      FALSE\n## Employed       FALSE      FALSE\n## Unemployed     FALSE      FALSE\n## 10 subsets of each size up to 4\n## Selection Algorithm: exhaustive\n##          GNP Population Employed Unemployed\n## 1  ( 1 ) \" \" \" \"        \"*\"      \" \"       \n## 1  ( 2 ) \"*\" \" \"        \" \"      \" \"       \n## 1  ( 3 ) \" \" \"*\"        \" \"      \" \"       \n## 1  ( 4 ) \" \" \" \"        \" \"      \"*\"       \n## 2  ( 1 ) \"*\" \"*\"        \" \"      \" \"       \n## 2  ( 2 ) \"*\" \" \"        \" \"      \"*\"       \n## 2  ( 3 ) \" \" \"*\"        \" \"      \"*\"       \n## 2  ( 4 ) \" \" \" \"        \"*\"      \"*\"       \n## 2  ( 5 ) \" \" \"*\"        \"*\"      \" \"       \n## 2  ( 6 ) \"*\" \" \"        \"*\"      \" \"       \n## 3  ( 1 ) \"*\" \"*\"        \"*\"      \" \"       \n## 3  ( 2 ) \"*\" \" \"        \"*\"      \"*\"       \n## 3  ( 3 ) \"*\" \"*\"        \" \"      \"*\"       \n## 3  ( 4 ) \" \" \"*\"        \"*\"      \"*\"       \n## 4  ( 1 ) \"*\" \"*\"        \"*\"      \"*\"\n\n\n\n\n\n\n1\n2\n3\n# plot a table of models showing variables in each model\n\n\n# models are ordered by the selection statistic\n\nplot\n(\nleaps\n,\n scale \n=\n \n\"r2\"\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# plot statistic by subset size\n\n\nlibrary\n(\ncar\n)\n\n\nsubsets\n(\nleaps\n,\n statistic \n=\n \n\"rsq\"\n,\n legend \n=\n \nFALSE\n)\n \n# available criteria are rsq, rss, adjr2, cp, bic\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##            Abbreviation\n## GNP                   G\n## Population            P\n## Employed              E\n## Unemployed            U\n\n\n\n\n\n\n1\nsubsets\n(\nleaps\n,\n statistic \n=\n \n\"bic\"\n,\n legend \n=\n \nFALSE\n)\n \n# available criteria are rsq, rss, adjr2, cp, bic\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##            Abbreviation\n## GNP                   G\n## Population            P\n## Employed              E\n## Unemployed            U\n\n\n\n\n\n\nVariable selection \u2013 Relative importance\n\u00b6\n\n\nModel: \nfit \n<-\n lm\n(\nArmed.Forces \n~\n GNP \n+\n Population \n+\n Employed \n+\n Unemployed\n,\n data \n=\n longley\n)\n.\n\n\nWarning: \neval=FALSE\n.\n\n\n1\n2\n3\n4\n# calculate the relative importance of each predictor\n\n\nlibrary\n(\nrelaimpo\n)\n\n\ncalc.relimp\n(\nfit\n,\n type \n=\n \nc\n(\n\"lmg\"\n,\n \n\"last\"\n,\n \n\"first\"\n,\n \n\"pratt\"\n),\n rela \n=\n \nTRUE\n)\n\n\n\n\n\n\n\nResults.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nResponse variable: Armed.Forces \nTotal response variance: 4843 \nAnalysis based on 16 observations\n\n4 Regressors: \nGNP Population Employed Unemployed \nProportion of variance explained by model: 70.2%\nMetrics are normalized to sum to 100% (rela=TRUE).\n\nRelative importance metrics:\n\n             lmg  last first  pratt\nGNP        0.328 0.431 0.348  4.566\nPopulation 0.241 0.155 0.232 -1.934\nEmployed   0.217 0.287 0.365 -1.780\nUnemployed 0.215 0.127 0.055  0.148\n\nAverage coefficients for different model sizes:\n\n               1X     2Xs     3Xs     4Xs\nGNP         0.313   1.301   3.641   5.026\nPopulation  3.646 -14.815 -24.637 -37.260\nEmployed    9.062  17.646 -34.249 -54.144\nUnemployed -0.132  -0.511  -0.584  -0.436\n\n\n\n\n\n\nBootstrapping\n\n\nModel: \nfit \n<-\n lm\n(\nArmed.Forces \n~\n GNP \n+\n Population \n+\n Employed \n+\n Unemployed\n,\n data \n=\n longley\n)\n.\n\n\nWarning: \neval=FALSE\n.\n\n\n1\n2\n3\n# bootstrap measures of relative importance (1000 samples)\n\nboot \n<-\n boot.relimp\n(\nfit\n,\n b \n=\n \n1000\n,\n type \n=\n \nc\n(\n\"lmg\"\n,\n \n\"last\"\n,\n \n\"first\"\n,\n \n\"pratt\"\n),\n rank \n=\n \nTRUE\n,\n diff \n=\n \nTRUE\n,\n rela \n=\n \nTRUE\n)\n\nbooteval.relimp\n(\nboot\n)\n \n# print result\n\n\n\n\n\n\n\nResults.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\nResponse variable: Armed.Forces \nTotal response variance: 4843 \nAnalysis based on 16 observations\n\n4 Regressors: \nGNP Population Employed Unemployed \nProportion of variance explained by model: 70.2%\nMetrics are normalized to sum to 100% (rela=TRUE).\n\nRelative importance metrics:\n\n             lmg  last first  pratt\nGNP        0.328 0.431 0.348  4.566\nPopulation 0.241 0.155 0.232 -1.934\nEmployed   0.217 0.287 0.365 -1.780\nUnemployed 0.215 0.127 0.055  0.148\n\nAverage coefficients for different model sizes:\n\n               1X     2Xs     3Xs     4Xs\nGNP         0.313   1.301   3.641   5.026\nPopulation  3.646 -14.815 -24.637 -37.260\nEmployed    9.062  17.646 -34.249 -54.144\nUnemployed -0.132  -0.511  -0.584  -0.436\n\n\n Confidence interval information ( 1000 bootstrap replicates, bty= perc ): \nRelative Contributions with confidence intervals:\n\n                                 Lower  Upper\n                 percentage 0.95 0.95    0.95   \nGNP.lmg           0.3280    ABC_  0.1488  0.4030\nPopulation.lmg    0.2410    _BCD  0.1572  0.3060\nEmployed.lmg      0.2170    ABCD  0.1107  0.3240\nUnemployed.lmg    0.2150    ABCD  0.0586  0.5550\n\nGNP.last          0.4310    ABCD  0.0101  0.5660\nPopulation.last   0.1550    _BCD  0.0003  0.3850\nEmployed.last     0.2870    ABCD  0.0476  0.6280\nUnemployed.last   0.1270    ABCD  0.0009  0.7810\n\nGNP.first         0.3480    ABCD  0.0151  0.3960\nPopulation.first  0.2320    _BCD  0.0070  0.3060\nEmployed.first    0.3650    ABCD  0.0159  0.4020\nUnemployed.first  0.0550    ABCD  0.0004  0.9280\n\nGNP.pratt         4.5660    ABCD -1.0160 10.5380\nPopulation.pratt -1.9340    ABCD -6.0800  2.2050\nEmployed.pratt   -1.7800    _BCD -5.0290  0.8510\nUnemployed.pratt  0.1480    ABC_ -0.3250  1.0910\n\nLetters indicate the ranks covered by bootstrap CIs. \n(Rank bootstrap confidence intervals always obtained by percentile method) \nCAUTION: Bootstrap confidence intervals can be somewhat liberal.\n\n\n Differences between Relative Contributions:\n\n                                            Lower   Upper\n                            difference 0.95 0.95    0.95   \nGNP-Population.lmg           0.0871         -0.0188  0.1521\nGNP-Employed.lmg             0.1106         -0.0479  0.1949\nGNP-Unemployed.lmg           0.1130         -0.4021  0.3281\nPopulation-Employed.lmg      0.0236         -0.1091  0.1124\nPopulation-Unemployed.lmg    0.0259         -0.3904  0.2417\nEmployed-Unemployed.lmg      0.0023         -0.4315  0.2295\n\nGNP-Population.last          0.2756         -0.1062  0.4502\nGNP-Employed.last            0.1438         -0.5564  0.4011\nGNP-Unemployed.last          0.3041         -0.7408  0.5502\nPopulation-Employed.last    -0.1318         -0.6191  0.2653\nPopulation-Unemployed.last   0.0285         -0.7382  0.3622\nEmployed-Unemployed.last     0.1603         -0.6784  0.4852\n\nGNP-Population.first         0.1161         -0.0590  0.1736\nGNP-Employed.first          -0.0172         -0.0893  0.0934\nGNP-Unemployed.first         0.2930         -0.9047  0.3766\nPopulation-Employed.first   -0.1333         -0.2134  0.0688\nPopulation-Unemployed.first  0.1769         -0.8967  0.2969\nEmployed-Unemployed.first    0.3102         -0.8937  0.3823\n\nGNP-Population.pratt         6.4995         -2.2069 15.7978\nGNP-Employed.pratt           6.3461         -1.3879 15.0093\nGNP-Unemployed.pratt         4.4180         -1.9134 10.6456\nPopulation-Employed.pratt   -0.1534         -4.1860  5.9691\nPopulation-Unemployed.pratt -2.0815         -6.1439  2.2348\nEmployed-Unemployed.pratt   -1.9281         -5.2057  0.1725\n\n* indicates that CI for difference does not include 0. \nCAUTION: Bootstrap confidence intervals can be somewhat liberal.\n\n\n\n\n\n\nWarning: \neval=FALSE\n.\n\n\n1\nplot\n(\nbooteval.relimp\n(\nboot\n,\n sort \n=\n \nTRUE\n))\n \n# to plot the results\n\n\n\n\n\n\n\nThe .png file.\n\n\n\n\n\n\n\n\nGoing further\n\u00b6\n\n\n\n\nThe \nnls\n package provides functions for nonlinear regression.\n\n\nPerform robust regression with the \nrlm\n function in the \nMASS\n package.\n\n\nThe \nrobust\n package provides a comprehensive library of robust methods, including regression.\n\n\nThe \nrobustbase\n package also provides basic robust statistics including model selection methods.\n\n\n\n\nRegression diagnostics\n\u00b6\n\n\n1\n2\n3\n# assume that we are fitting a multiple linear regression\n\nfit \n<-\n lm\n(\nmpg \n~\n disp \n+\n hp \n+\n wt \n+\n drat\n,\n data \n=\n mtcars\n)\n\nfit\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n## \n## Call:\n## lm(formula = mpg ~ disp + hp + wt + drat, data = mtcars)\n## \n## Coefficients:\n## (Intercept)         disp           hp           wt         drat  \n##    29.14874      0.00382     -0.03478     -3.47967      1.76805\n\n\n\n\n\n\nOutliers\n\u00b6\n\n\n1\n2\n3\n4\nlibrary\n(\ncar\n)\n\n\n\n# assessing outliers\n\noutlierTest\n(\nfit\n)\n \n# Bonferonni p-value for most extreme obs\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n## No Studentized residuals with Bonferonni p < 0.05\n## Largest |rstudent|:\n##                rstudent unadjusted p-value Bonferonni p\n## Toyota Corolla     2.52             0.0184        0.588\n\n\n\n\n\n\n1\nqqPlot\n(\nfit\n,\n main \n=\n \n\"QQ Plot\"\n)\n \n#qq plot for studentized resid\n\n\n\n\n\n\n\n\n\n1\nleveragePlots\n(\nfit\n)\n \n# leverage plots\n\n\n\n\n\n\n\n\n\nInfluential observations\n\u00b6\n\n\nModel: \nfit \n<-\n lm\n(\nmpg \n~\n disp \n+\n hp \n+\n wt \n+\n drat\n,\n data \n=\n mtcars\n)\n.\n\n\n1\n2\n3\n4\n5\n# influential observations\n\n\n# added variable plots\n\n\nlibrary\n(\ncar\n)\n\n\navPlots\n(\nfit\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n1\n))\n\n\n\n# Cook's D plot\n\n\n# identify D values > 4/(n-k-1)\n\ncutoff \n<-\n \n4\n/\n((\nnrow\n(\nmtcars\n)\n \n-\n \nlength\n(\nfit\n$\ncoefficients\n)\n \n-\n \n2\n))\n\nplot\n(\nfit\n,\n which \n=\n \n4\n,\n cook.levels \n=\n cutoff\n)\n\n\n\n\n\n\n\n\n\nWarning: \neval=FALSE\n; interactive function.\n\n\n1\n2\n# influence plot\n\ninfluencePlot\n(\nfit\n,\n id.method \n=\n \n\"identify\"\n,\n main \n=\n \n\"Influence Plot\"\n,\n sub \n=\n \n\"Circle size is proportial to Cook's Distance\"\n \n)\n\n\n\n\n\n\n\n\n\nNonnormality\n\u00b6\n\n\nModel: \nfit \n<-\n lm\n(\nmpg \n~\n disp \n+\n hp \n+\n wt \n+\n drat\n,\n data \n=\n mtcars\n)\n.\n\n\n1\n2\n3\n4\n5\n# normality of residuals\n\n\n# qq plot for studentized resid\n\n\nlibrary\n(\ncar\n)\n\n\nqqPlot\n(\nfit\n,\n main \n=\n \n\"QQ Plot\"\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# distribution of studentized residuals\n\n\nlibrary\n(\nMASS\n)\n\n\nsresid \n<-\n studres\n(\nfit\n)\n\nhist\n(\nsresid\n,\n freq \n=\n \nFALSE\n,\n main \n=\n \n\"Distribution of Studentized Residuals\"\n)\n\nxfit \n<-\n \nseq\n(\nmin\n(\nsresid\n),\n \nmax\n(\nsresid\n),\nlength \n=\n \n40\n)\n\nyfit \n<-\n dnorm\n(\nxfit\n)\n\nlines\n(\nxfit\n,\n yfit\n)\n \n\n\n\n\n\n\n\n\nHeteroscedasticity\n\u00b6\n\n\nNonconstant error variance.\n\n\nModel: \nfit \n<-\n lm\n(\nmpg \n~\n disp \n+\n hp \n+\n wt \n+\n drat\n,\n data \n=\n mtcars\n)\n.\n\n\n1\n2\n3\n# evaluate homoscedasticity\n\n\n# non-constant error variance test\n\nncvTest\n(\nfit\n)\n\n\n\n\n\n\n\n1\n2\n3\n## Non-constant Variance Score Test \n## Variance formula: ~ fitted.values \n## Chisquare = 1.43    Df = 1     p = 0.232\n\n\n\n\n\n\n1\n2\n# plot studentized residuals vs. fitted values\n\nspreadLevelPlot\n(\nfit\n)\n\n\n\n\n\n\n\n\n\n1\n2\n## \n## Suggested power transformation:  0.662\n\n\n\n\n\n\nMulticollinearity\n\u00b6\n\n\nModel: \nfit \n<-\n lm\n(\nmpg \n~\n disp \n+\n hp \n+\n wt \n+\n drat\n,\n data \n=\n mtcars\n)\n.\n\n\n1\n2\n# evaluatecCollinearity\n\nvif\n(\nfit\n)\n\n\n\n\n\n\n\n1\n2\n## disp   hp   wt drat \n## 8.21 2.89 5.10 2.28\n\n\n\n\n\n\n1\nsqrt\n(\nvif\n(\nfit\n))\n \n>\n \n2\n \n# benchmark = 1.96, rounded to 2\n\n\n\n\n\n\n\n1\n2\n##  disp    hp    wt  drat \n##  TRUE FALSE  TRUE FALSE\n\n\n\n\n\n\nNonlinearity\n\u00b6\n\n\nModel: \nfit \n<-\n lm\n(\nmpg \n~\n disp \n+\n hp \n+\n wt \n+\n drat\n,\n data \n=\n mtcars\n)\n.\n\n\n1\n2\n3\n# evaluate nonlinearity\n\n\n# component + residual plot\n\ncrPlots\n(\nfit\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# Ceres plots\n\nceresPlots\n(\nfit\n)\n\n\n\n\n\n\n\n\n\nAutocorrelation\n\u00b6\n\n\nSerial correlation or non-independence of errors.\n\n\nModel: \nfit \n<-\n lm\n(\nmpg \n~\n disp \n+\n hp \n+\n wt \n+\n drat\n,\n data \n=\n mtcars\n)\n.\n\n\n1\n2\n# test for autocorrelated errors\n\ndurbinWatsonTest\n(\nfit\n)\n\n\n\n\n\n\n\n1\n2\n3\n##  lag Autocorrelation D-W Statistic p-value\n##    1           0.101          1.74    0.29\n##  Alternative hypothesis: rho != 0\n\n\n\n\n\n\nGlobal diagnostic\n\u00b6\n\n\nModel: \nfit \n<-\n lm\n(\nmpg \n~\n disp \n+\n hp \n+\n wt \n+\n drat\n,\n data \n=\n mtcars\n)\n.\n\n\n1\n2\n3\n4\n5\n# global test of model assumptions\n\n\nlibrary\n(\ngvlma\n)\n\n\ngvmodel \n<-\n gvlma\n(\nfit\n)\n\n\nsummary\n(\ngvmodel\n)\n \n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n## \n## Call:\n## lm(formula = mpg ~ disp + hp + wt + drat, data = mtcars)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.508 -1.905 -0.506  0.982  5.688 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 29.14874    6.29359    4.63  8.2e-05 ***\n## disp         0.00382    0.01080    0.35   0.7268    \n## hp          -0.03478    0.01160   -3.00   0.0058 ** \n## wt          -3.47967    1.07837   -3.23   0.0033 ** \n## drat         1.76805    1.31978    1.34   0.1915    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.6 on 27 degrees of freedom\n## Multiple R-squared:  0.838,  Adjusted R-squared:  0.814 \n## F-statistic: 34.8 on 4 and 27 DF,  p-value: 2.7e-10\n## \n## \n## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\n## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\n## Level of Significance =  0.05 \n## \n## Call:\n##  gvlma(x = fit) \n## \n##                      Value p-value                   Decision\n## Global Stat        13.9382 0.00750 Assumptions NOT satisfied!\n## Skewness            4.3131 0.03782 Assumptions NOT satisfied!\n## Kurtosis            0.0138 0.90654    Assumptions acceptable.\n## Link Function       8.7166 0.00315 Assumptions NOT satisfied!\n## Heteroscedasticity  0.8947 0.34421    Assumptions acceptable.\n\n\n\n\n\n\nANOVA\n\u00b6\n\n\nAnalysis of variance (ANOVA) is an alternative to regressions among other applications.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n# lower case letters are numeric variables\n\n\n# upper case letters are factors\n\n\n\n# one-way ANOVA (completely randomized design)\n\nfit \n<-\n aov\n(\ny \n~\n A\n,\n data \n=\n mydataframe\n)\n\n\n\n# randomized block design (B is the blocking factor)\n\nfit \n<-\n aov\n(\ny \n~\n A \n+\n B\n,\n data \n=\n mydataframe\n)\n\n\n\n# two-way factorial design\n\nfit \n<-\n aov\n(\ny \n~\n A \n+\n B \n+\n A\n:\nB\n,\n data \n=\n mydataframe\n)\n\nfit \n<-\n aov\n(\ny \n~\n A\n*\nB\n,\n data \n=\n mydataframe\n)\n \n# same thing\n\n\n\n# analysis of covariance\n\nfit \n<-\n aov\n(\ny \n~\n A \n+\n x\n,\n data \n=\n mydataframe\n)\n \n\n\n\n\n\n\n1\n2\n# dataset\n\n\nhead\n(\nCO2\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## Grouped Data: uptake ~ conc | Plant\n##   Plant   Type  Treatment conc uptake\n## 1   Qn1 Quebec nonchilled   95   16.0\n## 2   Qn1 Quebec nonchilled  175   30.4\n## 3   Qn1 Quebec nonchilled  250   34.8\n\n\n\n\n\n\n1\n2\n3\n# one-way ANOVA (completely randomized design)\n\nfit \n<-\n aov\n(\nuptake \n~\n Plant\n,\n data \n=\n CO2\n)\n\nfit\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n## Call:\n##    aov(formula = uptake ~ Plant, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## Estimated effects are balanced\n\n\n\n\n\n\n1\nsummary\n(\nfit\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\n2\n3\n# randomized block design (B is the blocking factor)\n\nfit \n<-\n aov\n(\nuptake \n~\n Plant \n+\n Type\n,\n data \n=\n CO2\n)\n\nfit\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## Call:\n##    aov(formula = uptake ~ Plant + Type, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## 1 out of 13 effects not estimable\n## Estimated effects are balanced\n\n\n\n\n\n\n1\nsummary\n(\nfit\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\n2\n3\n# two-way factorial design\n\nfit \n<-\n aov\n(\nuptake \n~\n Plant \n+\n Type \n+\n Plant\n:\nType\n,\n data \n=\n CO2\n)\n\nfit\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## Call:\n##    aov(formula = uptake ~ Plant + Type + Plant:Type, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## 12 out of 24 effects not estimable\n## Estimated effects are balanced\n\n\n\n\n\n\n1\nsummary\n(\nfit\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\n2\nfit \n<-\n aov\n(\nuptake \n~\n Plant\n*\nType\n,\n data \n=\n CO2\n)\n \n# same thing\n\nfit\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n## Call:\n##    aov(formula = uptake ~ Plant * Type, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## 12 out of 24 effects not estimable\n## Estimated effects are balanced\n\n\n\n\n\n\n1\nsummary\n(\nfit\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\n2\n3\n# Analysis of Covariance\n\nfit \n<-\n aov\n(\nuptake \n~\n uptake \n+\n conc\n,\n data \n=\n CO2\n)\n\nfit\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n## Call:\n##    aov(formula = uptake ~ uptake + conc, data = CO2)\n## \n## Terms:\n##                 conc Residuals\n## Sum of Squares  2285      7422\n## Deg. of Freedom    1        82\n## \n## Residual standard error: 9.51\n## Estimated effects may be unbalanced\n\n\n\n\n\n\n1\nsummary\n(\nfit\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## conc         1   2285    2285    25.2 2.9e-06 ***\n## Residuals   82   7422      91                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nEvaluate model effects\n\u00b6\n\n\n\n\nType I sequential SS: A+B and B+A will produce different results!\n\n\nUse the \ndrop1\n to produce the familiar Type III results; compare each term with the full model.\n\n\n\n\n\n\n\n1\n2\n3\n# display Type I ANOVA table\n\nfit1 \n<-\n aov\n(\nuptake \n~\n Plant \n+\n Type\n,\n data \n=\n CO2\n)\n\n\nsummary\n(\nfit1\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\n2\n3\n# display Type I ANOVA table\n\nfit2 \n<-\n aov\n(\nuptake \n~\n Type \n+\n Plant\n,\n data \n=\n CO2\n)\n\n\nsummary\n(\nfit2\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Type         1   3366    3366   50.02 8.1e-10 ***\n## Plant       10   1497     150    2.22   0.026 *  \n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\n2\n# display type III SS and F tests\n\ndrop1\n(\nfit1\n,\n \n~\n \n.\n,\n test \n=\n \n\"F\"\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n## Single term deletions\n## \n## Model:\n## uptake ~ Plant + Type\n##        Df Sum of Sq  RSS AIC F value Pr(>F)  \n## <none>              4845 365                 \n## Plant  10      1497 6341 367    2.22  0.026 *\n## Type    0         0 4845 365                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\ndrop1\n(\nfit2\n,\n \n~\n \n.\n,\n test \n=\n \n\"F\"\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n## Single term deletions\n## \n## Model:\n## uptake ~ Type + Plant\n##        Df Sum of Sq  RSS AIC F value Pr(>F)  \n## <none>              4845 365                 \n## Type    0         0 4845 365                 \n## Plant  10      1497 6341 367    2.22  0.026 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nCompare nested models directly\n\u00b6\n\n\n1\n2\n3\n4\nfit1 \n<-\n aov\n(\nuptake \n~\n Plant \n+\n Type\n,\n data \n=\n CO2\n)\n\nfit2 \n<-\n aov\n(\nuptake \n~\n Plant\n,\n data \n=\n CO2\n)\n\n\nanova\n(\nfit1\n,\n fit2\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n## Analysis of Variance Table\n## \n## Model 1: uptake ~ Plant + Type\n## Model 2: uptake ~ Plant\n##   Res.Df  RSS Df Sum of Sq F Pr(>F)\n## 1     72 4845                      \n## 2     72 4845  0         0\n\n\n\n\n\n\n1\n2\n3\n4\nfit2 \n<-\n aov\n(\nuptake \n~\n Plant \n+\n Type\n,\n data \n=\n CO2\n)\n\nfit1 \n<-\n aov\n(\nuptake \n~\n Plant\n,\n data \n=\n CO2\n)\n\n\nanova\n(\nfit1\n,\n fit2\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n## Analysis of Variance Table\n## \n## Model 1: uptake ~ Plant\n## Model 2: uptake ~ Plant + Type\n##   Res.Df  RSS Df Sum of Sq F Pr(>F)\n## 1     72 4845                      \n## 2     72 4845  0         0\n\n\n\n\n\n\nMultiple comparisons\n\u00b6\n\n\n\n\nTukey HSD tests for post hoc comparisons on each factor in the model.\n\n\nFactors as an option.\n\n\nType I SS.\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# model\n\nfit \n<-\n aov\n(\nuptake \n~\n Plant \n+\n Type\n,\n data \n=\n CO2\n)\n\n\n\n# Tukey honestly significant differences\n\nTukeyHSD\n(\nfit\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = uptake ~ Plant + Type, data = CO2)\n## \n## $Plant\n##            diff    lwr     upr p adj\n## Qn2-Qn1   1.929 -12.88  16.739 1.000\n## Qn3-Qn1   4.386 -10.42  19.196 0.997\n## Qc1-Qn1  -3.257 -18.07  11.553 1.000\n## Qc3-Qn1  -0.643 -15.45  14.168 1.000\n## Qc2-Qn1  -0.529 -15.34  14.282 1.000\n## Mn3-Qn1  -9.114 -23.92   5.696 0.639\n## Mn2-Qn1  -5.886 -20.70   8.925 0.970\n## Mn1-Qn1  -6.829 -21.64   7.982 0.918\n## Mc2-Qn1 -21.086 -35.90  -6.275 0.000\n## Mc3-Qn1 -15.929 -30.74  -1.118 0.024\n## Mc1-Qn1 -15.229 -30.04  -0.418 0.038\n## Qn3-Qn2   2.457 -12.35  17.268 1.000\n## Qc1-Qn2  -5.186 -20.00   9.625 0.989\n## Qc3-Qn2  -2.571 -17.38  12.239 1.000\n## Qc2-Qn2  -2.457 -17.27  12.353 1.000\n## Mn3-Qn2 -11.043 -25.85   3.768 0.346\n## Mn2-Qn2  -7.814 -22.62   6.996 0.822\n## Mn1-Qn2  -8.757 -23.57   6.053 0.694\n## Mc2-Qn2 -23.014 -37.82  -8.204 0.000\n## Mc3-Qn2 -17.857 -32.67  -3.047 0.006\n## Mc1-Qn2 -17.157 -31.97  -2.347 0.010\n## Qc1-Qn3  -7.643 -22.45   7.168 0.842\n## Qc3-Qn3  -5.029 -19.84   9.782 0.991\n## Qc2-Qn3  -4.914 -19.72   9.896 0.993\n## Mn3-Qn3 -13.500 -28.31   1.311 0.108\n## Mn2-Qn3 -10.271 -25.08   4.539 0.457\n## Mn1-Qn3 -11.214 -26.02   3.596 0.323\n## Mc2-Qn3 -25.471 -40.28 -10.661 0.000\n## Mc3-Qn3 -20.314 -35.12  -5.504 0.001\n## Mc1-Qn3 -19.614 -34.42  -4.804 0.002\n## Qc3-Qc1   2.614 -12.20  17.425 1.000\n## Qc2-Qc1   2.729 -12.08  17.539 1.000\n## Mn3-Qc1  -5.857 -20.67   8.953 0.971\n## Mn2-Qc1  -2.629 -17.44  12.182 1.000\n## Mn1-Qc1  -3.571 -18.38  11.239 1.000\n## Mc2-Qc1 -17.829 -32.64  -3.018 0.006\n## Mc3-Qc1 -12.671 -27.48   2.139 0.167\n## Mc1-Qc1 -11.971 -26.78   2.839 0.233\n## Qc2-Qc3   0.114 -14.70  14.925 1.000\n## Mn3-Qc3  -8.471 -23.28   6.339 0.735\n## Mn2-Qc3  -5.243 -20.05   9.568 0.988\n## Mn1-Qc3  -6.186 -21.00   8.625 0.958\n## Mc2-Qc3 -20.443 -35.25  -5.632 0.001\n## Mc3-Qc3 -15.286 -30.10  -0.475 0.037\n## Mc1-Qc3 -14.586 -29.40   0.225 0.057\n## Mn3-Qc2  -8.586 -23.40   6.225 0.719\n## Mn2-Qc2  -5.357 -20.17   9.453 0.985\n## Mn1-Qc2  -6.300 -21.11   8.511 0.952\n## Mc2-Qc2 -20.557 -35.37  -5.747 0.001\n## Mc3-Qc2 -15.400 -30.21  -0.589 0.034\n## Mc1-Qc2 -14.700 -29.51   0.111 0.054\n## Mn2-Mn3   3.229 -11.58  18.039 1.000\n## Mn1-Mn3   2.286 -12.52  17.096 1.000\n## Mc2-Mn3 -11.971 -26.78   2.839 0.233\n## Mc3-Mn3  -6.814 -21.62   7.996 0.919\n## Mc1-Mn3  -6.114 -20.92   8.696 0.961\n## Mn1-Mn2  -0.943 -15.75  13.868 1.000\n## Mc2-Mn2 -15.200 -30.01  -0.389 0.039\n## Mc3-Mn2 -10.043 -24.85   4.768 0.493\n## Mc1-Mn2  -9.343 -24.15   5.468 0.603\n## Mc2-Mn1 -14.257 -29.07   0.553 0.070\n## Mc3-Mn1  -9.100 -23.91   5.711 0.641\n## Mc1-Mn1  -8.400 -23.21   6.411 0.746\n## Mc3-Mc2   5.157  -9.65  19.968 0.989\n## Mc1-Mc2   5.857  -8.95  20.668 0.971\n## Mc1-Mc3   0.700 -14.11  15.511 1.000\n\n\n\n\n\n\nVisualizing results\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# two-way interaction plot\n\n\nattach\n(\nmtcars\n)\n\n\ngear \n<-\n \nfactor\n(\ngear\n)\n\ncyl \n<-\n \nfactor\n(\ncyl\n)\n\n\n\n# two-way interactions\n\n\nlibrary\n(\ncar\n)\n\n\ninteraction.plot\n(\ncyl\n,\n gear\n,\n mpg\n,\n type \n=\n \n\"b\"\n,\n col \n=\n \nc\n(\n1\n:\n3\n),\n leg.bty \n=\n \n\"o\"\n,\n leg.bg \n=\n \n\"beige\"\n,\n lwd \n=\n \n2\n,\n pch \n=\n \nc\n(\n18\n,\n24\n,\n22\n),\n xlab \n=\n \n\"Number of Cylinders\"\n,\n ylab \n=\n \n\"Mean Miles Per Gallon\"\n,\n main \n=\n \n\"Interaction Plot\"\n)\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n# mean plots for single factors, and includes confidence intervals\n\n\nlibrary\n(\ngplots\n)\n\n\nplotmeans\n(\nmpg \n~\n cyl\n,\n xlab \n=\n \n\"Number of Cylinders\"\n,\n ylab \n=\n \n\"Miles Per Gallon\"\n,\n main \n=\n \n\"Mean Plot\\nwith 95% CI\"\n)\n\n\n\n\n\n\n\n\n\n1\ndetach\n(\nmtcars\n)\n\n\n\n\n\n\n\nMANOVA\n\u00b6\n\n\nMultivariate analysis of variance (MANOVA) With more than one dependent variable Y. We can run several ANOVA over different Y, or one MANOVA with one Y built with several variables.\n\n\n1\nhead\n(\nlongley\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##      GNP.deflator GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234        236          159        108 1947     60.3\n## 1948         88.5 259        232          146        109 1948     61.1\n## 1949         88.2 258        368          162        110 1949     60.2\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nattach\n(\nlongley\n)\n\n\n\n# 2x2 factorial MANOVA with 3 dependent variables, Y\n\nY \n<-\n \ncbind\n(\nUnemployed\n,\n Armed.Forces\n,\n Employed\n)\n \n# Y\n\nfit \n<-\n manova\n(\nY \n~\n Population\n*\nGNP\n)\n \n# Y ~ X\n\n\n\n# display type I SS\n\n\nsummary\n(\nfit\n,\n test \n=\n \n\"Pillai\"\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##                Df Pillai approx F num Df den Df  Pr(>F)    \n## Population      1  0.997     1074      3     10 7.7e-13 ***\n## GNP             1  0.888       26      3     10 4.6e-05 ***\n## Population:GNP  1  0.870       22      3     10 9.4e-05 ***\n## Residuals      12                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\n2\n3\n4\n# test = \"Wilks\", \"Hotelling-Lawley\", and \"Roy\"\n\n\n\n# display univariate statistics\n\nsummary.aov\n(\nfit\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n##  Response Unemployed :\n##                Df Sum Sq Mean Sq F value  Pr(>F)    \n## Population      1  61739   61739   45.92   2e-05 ***\n## GNP             1  42841   42841   31.87 0.00011 ***\n## Population:GNP  1  10270   10270    7.64 0.01715 *  \n## Residuals      12  16133    1344                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  Response Armed.Forces :\n##                Df Sum Sq Mean Sq F value Pr(>F)   \n## Population      1   9647    9647    4.25 0.0617 . \n## GNP             1  29772   29772   13.10 0.0035 **\n## Population:GNP  1   5958    5958    2.62 0.1314   \n## Residuals      12  27268    2272                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  Response Employed :\n##                Df Sum Sq Mean Sq F value  Pr(>F)    \n## Population      1  170.6   170.6  546.45 2.2e-11 ***\n## GNP             1   10.5    10.5   33.60 8.5e-05 ***\n## Population:GNP  1    0.1     0.1    0.41    0.54    \n## Residuals      12    3.7     0.3                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# display type III SS\n\nfit1 \n<-\n manova\n(\nY \n~\n Population\n*\nGNP\n)\n\nfit2 \n<-\n manova\n(\nY \n~\n GNP\n*\nPopulation\n)\n\n\n# type III GNP effect\n\n\nsummary\n(\nfit1\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##                Df Pillai approx F num Df den Df  Pr(>F)    \n## Population      1  0.997     1074      3     10 7.7e-13 ***\n## GNP             1  0.888       26      3     10 4.6e-05 ***\n## Population:GNP  1  0.870       22      3     10 9.4e-05 ***\n## Residuals      12                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n1\n2\n# type III Population effect\n\n\nsummary\n(\nfit2\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##                Df Pillai approx F num Df den Df  Pr(>F)    \n## GNP             1  0.997     1088      3     10 7.2e-13 ***\n## Population      1  0.786       12      3     10  0.0011 ** \n## GNP:Population  1  0.870       22      3     10 9.4e-05 ***\n## Residuals      12                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nMultiple comparisons\n\n\nTukeyHSD\n and \nplot\n do work with a MANOVA fit. Run each dependent variable separately to obtain them\u2026 or proceed with ANOVA on each dependent variable.\n\n\nGoing further\n\u00b6\n\n\nPackage \nlme4\n has excellent facilities for fitting linear and generalized linear mixed-effects models.\n\n\n(M)ANOVA Assumptions\n\u00b6\n\n\nOutliers\n\u00b6\n\n\nOutliers can severely affect normality and homogeneity of variance.\n\n\n1\n2\n# dataset\n\n\nhead\n(\nmtcars\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##                mpg cyl disp  hp drat   wt qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# detect outliers\n\n\n# ordered squared robust Mahalanobis distances of the observations against the empirical distribution function of the MD2i\n\n\nlibrary\n(\nmvoutlier\n)\n\n\noutliers \n<-\n\naq.plot\n(\nmtcars\n[\nc\n(\n\"mpg\"\n,\n \n\"disp\"\n,\n \n\"hp\"\n,\n \n\"drat\"\n,\n \n\"wt\"\n,\n \n\"qsec\"\n)])\n\n\n\n\n\n\n\n1\n2\n## Projection to the first and second robust principal components.\n## Proportion of total variation (explained variance): 0.974\n\n\n\n\n\n\n\n\n1\noutliers \n# show list of outliers\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n## $outliers\n##           Mazda RX4       Mazda RX4 Wag          Datsun 710 \n##               FALSE               FALSE               FALSE \n##      Hornet 4 Drive   Hornet Sportabout             Valiant \n##               FALSE               FALSE               FALSE \n##          Duster 360           Merc 240D            Merc 230 \n##                TRUE                TRUE                TRUE \n##            Merc 280           Merc 280C          Merc 450SE \n##               FALSE               FALSE               FALSE \n##          Merc 450SL         Merc 450SLC  Cadillac Fleetwood \n##               FALSE               FALSE               FALSE \n## Lincoln Continental   Chrysler Imperial            Fiat 128 \n##               FALSE               FALSE                TRUE \n##         Honda Civic      Toyota Corolla       Toyota Corona \n##               FALSE               FALSE               FALSE \n##    Dodge Challenger         AMC Javelin          Camaro Z28 \n##               FALSE               FALSE                TRUE \n##    Pontiac Firebird           Fiat X1-9       Porsche 914-2 \n##               FALSE               FALSE               FALSE \n##        Lotus Europa      Ford Pantera L        Ferrari Dino \n##               FALSE                TRUE               FALSE \n##       Maserati Bora          Volvo 142E \n##                TRUE               FALSE\n\n\n\n\n\n\n1\npar\n(\nmfrow \n=\n \nc\n(\n1\n,\n1\n))\n\n\n\n\n\n\n\nUnivariate normality\n\u00b6\n\n\n1\n2\n3\n# Q-Q plot\n\nqqnorm\n(\nmtcars\n$\nmpg\n)\n\nqqline\n(\nmtcars\n$\nmpg\n)\n\n\n\n\n\n\n\n\n\n1\n2\n# Shapiro-Wilk test of normality\n\nshapiro.test\n(\nmtcars\n$\nmpg\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mtcars$mpg\n## W = 0.9, p-value = 0.1\n\n\n\n\n\n\n1\n2\n3\n4\nlibrary\n(\nnortest\n)\n\n\n\n# Anderson-Darling test for normality\n\nad.test\n(\nmtcars\n$\nmpg\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Anderson-Darling normality test\n## \n## data:  mtcars$mpg\n## A = 0.6, p-value = 0.1\n\n\n\n\n\n\n1\n2\n# Cramer-von Mises test for normality\n\ncvm.test\n(\nmtcars\n$\nmpg\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Cramer-von Mises normality test\n## \n## data:  mtcars$mpg\n## W = 0.09, p-value = 0.2\n\n\n\n\n\n\n1\n2\n# Lilliefors (Kolmogorov-Smirnov) test for normality\n\nlillie.test\n(\nmtcars\n$\nmpg\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Lilliefors (Kolmogorov-Smirnov) normality test\n## \n## data:  mtcars$mpg\n## D = 0.1, p-value = 0.2\n\n\n\n\n\n\n1\n2\n# Pearson chi-square test for normality\n\npearson.test\n(\nmtcars\n$\nmpg\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Pearson chi-square normality test\n## \n## data:  mtcars$mpg\n## P = 8, p-value = 0.2\n\n\n\n\n\n\n1\n2\n# Shapiro-Francia test for normality\n\nsf.test\n(\nmtcars\n$\nmpg\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Shapiro-Francia normality test\n## \n## data:  mtcars$mpg\n## W = 1, p-value = 0.1\n\n\n\n\n\n\nMultivariate normality\n\u00b6\n\n\n1\nhead\n(\nEuStockMarkets\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##       DAX  SMI  CAC FTSE\n## [1,] 1629 1678 1773 2444\n## [2,] 1614 1688 1750 2460\n## [3,] 1607 1679 1718 2448\n\n\n\n\n\n\n1\n2\n3\n4\nlibrary\n(\nmvnormtest\n)\n\n\n\n# Shapiro-Wilk test for multivariate normality of numeric matrix\n\nmshapiro.test\n(\nt\n(\nEuStockMarkets\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Shapiro-Wilk normality test\n## \n## data:  Z\n## W = 0.9, p-value <2e-16\n\n\n\n\n\n\nWith a p x 1 multivariate normal random vector x vector, the squared Mahalanobis distance between x and ?? is going to be chi-square distributed with p degrees of freedom.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# graphical assessment of multivariate normality\n\nx \n<-\n \nas.matrix\n(\nEuStockMarkets\n)\n \n# n x p numeric matrix\n\ncenter \n<-\n \ncolMeans\n(\nx\n)\n \n# centroid\n\nn \n<-\n \nnrow\n(\nx\n);\n p \n<-\n \nncol\n(\nx\n)\n\ncov \n<-\n cov\n(\nx\n)\n\nd \n<-\n mahalanobis\n(\nx\n,\n center\n,\n cov\n)\n \n# distances\n\n\nqqplot\n(\nqchisq\n(\nppoints\n(\nn\n),\n df \n=\n p\n),\n d\n,\n main \n=\n \n\"QQ Plot Assessing Multivariate Normality\"\n,\n ylab \n=\n \n\"Mahalanobis D2\"\n)\n\nabline\n(\na \n=\n \n0\n,\n b \n=\n \n1\n)\n \n\n\n\n\n\n\n\n\nHeteroscedasticity\n\u00b6\n\n\nNonconstant error variance or non-homogeneity of variances.\n\n\n1\n2\n# dataset\n\n\nhead\n(\nmtcars\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##                mpg cyl disp  hp drat   wt qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n\n\n\n\n\n\n1\n2\n3\n# y is a numeric variable and G is the grouping variable\n\n\n# Bartlett parametric test of homogeneity of variances\n\nbartlett.test\n(\nmpg \n~\n cyl\n,\n data\n=\nmtcars\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Bartlett test of homogeneity of variances\n## \n## data:  mpg by cyl\n## Bartlett's K-squared = 8, df = 2, p-value = 0.02\n\n\n\n\n\n\n1\n2\n# Figner-Killeen non-parametric test of homogeneity of variances\n\nfligner.test\n(\nmpg \n~\n cyl\n,\n data \n=\n mtcars\n)\n \n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Fligner-Killeen test of homogeneity of variances\n## \n## data:  mpg by cyl\n## Fligner-Killeen:med chi-squared = 7, df = 2, p-value = 0.03\n\n\n\n\n\n\n1\n2\n3\n4\n5\nlibrary\n(\nHH\n)\n\n\n\n# y is numeric and G is a grouping factor\n\n\n# G must be of type factor\n\nhov\n(\nmpg \n~\n \nfactor\n(\ncyl\n),\n data \n=\n mtcars\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n## \n##  hov: Brown-Forsyth\n## \n## data:  mpg\n## F = 6, df:factor(cyl) = 2, df:Residuals = 30, p-value = 0.009\n## alternative hypothesis: variances are not identical\n\n\n\n\n\n\n1\n2\n3\n# homogeneity of variance plot\n\n\n# graphic test of homogeneity of variances based on Brown-Forsyth\n\nhovPlot\n(\nmpg \n~\n \nfactor\n(\ncyl\n),\n data \n=\n mtcars\n)\n \n\n\n\n\n\n\n\n\nNon-homogeneity of covariance matrices\n\n\n1\n2\n# dataset\n\n\nhead\n(\niris\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n\n\n\n\n\n\n1\nlibrary\n(\nbiotools\n)\n\n\n\n\n\n\n\n1\n2\n## ---\n## biotools version 3.0\n\n\n\n\n\n\n1\n2\n3\n# Box's M test\n\n\n# very sensitive to violations of normality, leading to rejection in most typical cases\n\nboxM\n(\niris\n[,\n \n-5\n],\n iris\n[,\n \n5\n])\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n## \n##  Box's M-test for Homogeneity of Covariance Matrices\n## \n## data:  iris[, -5]\n## Chi-Sq (approx.) = 100, df = 20, p-value <2e-16\n\n\n\n\n\n\nResampling Statistics\n\u00b6\n\n\nIndependent k-sample location tests\n\u00b6\n\n\n1\n2\n# dataset\n\n\nhead\n(\nmtcars\n,\n \n3\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n##                mpg cyl disp  hp drat   wt qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# exact Wilcoxon Mann Whitney rank sum test\n\n\n# re-randomization or permutation based statistical tests\n\n\n# where y is numeric and A is a binary factor\n\n\nlibrary\n(\ncoin\n)\n\n\nwilcox_test\n(\nmpg \n~\n \nfactor\n(\nam\n),\n data \n=\n mtcars\n,\n distribution \n=\n \n\"exact\"\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n## \n##  Exact Wilcoxon-Mann-Whitney Test\n## \n## data:  mpg by factor(am) (0, 1)\n## Z = -3, p-value = 0.001\n## alternative hypothesis: true mu is not equal to 0\n\n\n\n\n\n\n1\n# lower case letters represent numerical variables and upper case letters represent categorical factors\n\n\n\n\n\n\n\n\n\nMonte-Carlo simulations are available for all tests. Exact tests are available for 2 group procedures.\n\n\nThese tests do not assume random sampling from well-defined populations. They can be a reasonable alternative to     classical procedures when test assumptions can not be met.\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# one-way permutation test based on 9999 Monte-Carlo\n\n\n# resamplings\n\n\n# y is numeric and A is a categorical factor\n\n\nlibrary\n(\ncoin\n)\n\n\noneway_test\n(\nmpg \n~\n \nfactor\n(\nam\n),\n data \n=\n mtcars\n,\n distribution \n=\n approximate\n(\nB \n=\n \n9999\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n## \n##  Approximative Two-Sample Fisher-Pitman Permutation Test\n## \n## data:  mpg by factor(am) (0, 1)\n## Z = -3, p-value = 5e-04\n## alternative hypothesis: true mu is not equal to 0\n\n\n\n\n\n\nSymmetry of a response for repeated measurements\n\u00b6\n\n\n1\n2\n3\n4\n5\n# exact Wilcoxon signed rank test\n\n\n# where y1 and y2 are repeated measures\n\n\nlibrary\n(\ncoin\n)\n\n\nwilcoxsign_test\n(\nmpg \n~\n \nfactor\n(\nam\n),\n data \n=\n mtcars\n,\n distribution \n=\n \n\"exact\"\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n## \n##  Exact Wilcoxon-Pratt Signed-Rank Test\n## \n## data:  y by x (pos, neg) \n##   stratified by block\n## Z = 5, p-value = 5e-10\n## alternative hypothesis: true mu is not equal to 0\n\n\n\n\n\n\n1\n2\n3\n4\n# Freidman Test based on 9999 Monte-Carlo resamplings.\n\n\n# y is numeric, A is a grouping factor, and B is a\n\n\n# blocking factor\n\nfriedman_test\n(\ny \n~\n A \n|\n B\n,\n data \n=\n mydata\n,\n  distribution \n=\n approximate\n(\nB \n=\n \n9999\n))\n\n\n\n\n\n\n\nIndependence of two numeric variables\n\u00b6\n\n\n1\n2\n3\n4\n5\n# Spearman Test of independence based on 9999 Monte-Carlo\n\n\n# resamplings. x and y are numeric variables\n\n\nlibrary\n(\ncoin\n)\n\n\nspearman_test\n(\nmpg \n~\n am\n,\n data \n=\n mtcars\n,\n distribution \n=\n approximate\n(\nB \n=\n \n9999\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n## \n##  Approximative Spearman Correlation Test\n## \n## data:  mpg by am\n## Z = 3, p-value = 0.001\n## alternative hypothesis: true rho is not equal to 0\n\n\n\n\n\n\nIndependence in contingency tables\n\u00b6\n\n\n1\n2\n# dataset\n\n\nhead\n(\nCO2\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## Grouped Data: uptake ~ conc | Plant\n##   Plant   Type  Treatment conc uptake\n## 1   Qn1 Quebec nonchilled   95   16.0\n## 2   Qn1 Quebec nonchilled  175   30.4\n## 3   Qn1 Quebec nonchilled  250   34.8\n## 4   Qn1 Quebec nonchilled  350   37.2\n## 5   Qn1 Quebec nonchilled  500   35.3\n## 6   Qn1 Quebec nonchilled  675   39.2\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\ncases \n<-\n \nc\n(\n4\n,\n \n2\n,\n \n3\n,\n \n1\n,\n \n59\n)\n\nn \n<-\n \nsum\n(\ncases\n)\n\ncochran \n<-\n \ndata.frame\n(\n\n    diphtheria \n=\n \nfactor\n(\n\n        \nunlist\n(\nrep\n(\nlist\n(\nc\n(\n1\n,\n \n1\n,\n \n1\n,\n \n1\n),\n\n                        \nc\n(\n1\n,\n \n1\n,\n \n0\n,\n \n1\n),\n\n                        \nc\n(\n0\n,\n \n1\n,\n \n1\n,\n \n1\n),\n\n                        \nc\n(\n0\n,\n \n1\n,\n \n0\n,\n \n1\n),\n\n                        \nc\n(\n0\n,\n \n0\n,\n \n0\n,\n \n0\n)),\n\n                   cases\n))\n\n    \n),\n\n    media \n=\n \nfactor\n(\nrep\n(\nLETTERS\n[\n1\n:\n4\n],\n n\n)),\n\n    case \n=\n  \nfactor\n(\nrep\n(\nseq_len\n(\nn\n),\n each \n=\n \n4\n))\n\n\n)\n\n\n\nhead\n(\ncochran\n)\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n##   diphtheria media case\n## 1          1     A    1\n## 2          1     B    1\n## 3          1     C    1\n## 4          1     D    1\n## 5          1     A    2\n## 6          1     B    2\n\n\n\n\n\n\n1\n2\n3\n4\n5\n# independence in 2-way contingency table based on\n\n\n# 9999 Monte-Carlo resamplings. A and B are factors\n\n\nlibrary\n(\ncoin\n)\n\n\nchisq_test\n(\nPlant \n~\n Type\n,\n data \n=\n CO2\n,\n distribution \n=\n approximate\n(\nB \n=\n \n9999\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n## \n##  Approximative Linear-by-Linear Association Test\n## \n## data:  Plant (ordered) by Type (Quebec, Mississippi)\n## Z = -8, p-value <2e-16\n## alternative hypothesis: two.sided\n\n\n\n\n\n\n1\n2\n3\n4\n# Cochran-Mantel-Haenzsel Test of 3-way Contingency Table\n\n\n# based on 9999 Monte-Carlo resamplings. A, B, are factors\n\n\n# and C is a stratefying factor\n\nmh_test\n(\ndiphtheria \n~\n media \n|\n case\n,\n data \n=\n cochran\n,\n distribution \n=\n approximate\n(\nB \n=\n \n9999\n))\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n## \n##  Approximative Marginal Homogeneity Test\n## \n## data:  diphtheria by\n##   media (A, B, C, D) \n##   stratified by case\n## chi-squared = 8, p-value = 0.05\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n# linear by linear association test based on 9999\n\n\n# Monte-Carlo resamplings\n\n\n# A and B are ordered factors\n\n\nlibrary\n(\ncoin\n)\n\n\nlbl_test\n(\nPlant \n~\n Type\n,\n data \n=\n CO2\n,\n distribution \n=\n approximate\n(\nB \n=\n \n9999\n))\n \n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n## \n##  Approximative Linear-by-Linear Association Test\n## \n## data:  Plant (ordered) by Type (Quebec, Mississippi)\n## Z = -8, p-value <2e-16\n## alternative hypothesis: two.sided\n\n\n\n\n\n\nMany other univariate and multivariate tests are possible using the functions in the \ncoin\n package.",
            "title": "Statistics with R, Notes"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#group-data",
            "text": "1\n2 # dataset  head ( mtcars ,   3 )    1\n2\n3\n4 ##                mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1   1\n2 # variable grouped by one factor to show a function  by ( mtcars $ mpg ,  mtcars $ cyl ,  FUN  =   function ( x )   {   c ( m  =   mean ( x ),  s  =  sd ( x ))   })      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## mtcars$cyl: 4\n##         m         s \n## 26.663636  4.509828 \n## -------------------------------------------------------- \n## mtcars$cyl: 6\n##         m         s \n## 19.742857  1.453567 \n## -------------------------------------------------------- \n## mtcars$cyl: 8\n##         m         s \n## 15.100000  2.560048   1\n2\n3\n4 # description statistics by group  library ( psych ) \n\ndescribeBy ( mtcars ,  group  =  mtcars $ am )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53 ## \n##  Descriptive statistics by group \n## group: 0\n##      vars  n   mean     sd median trimmed    mad    min    max  range\n## mpg     1 19  17.15   3.83  17.30   17.12   3.11  10.40  24.40  14.00\n## cyl     2 19   6.95   1.54   8.00    7.06   0.00   4.00   8.00   4.00\n## disp    3 19 290.38 110.17 275.80  289.71 124.83 120.10 472.00 351.90\n## hp      4 19 160.26  53.91 175.00  161.06  77.10  62.00 245.00 183.00\n## drat    5 19   3.29   0.39   3.15    3.28   0.22   2.76   3.92   1.16\n## wt      6 19   3.77   0.78   3.52    3.75   0.45   2.46   5.42   2.96\n## qsec    7 19  18.18   1.75  17.82   18.07   1.19  15.41  22.90   7.49\n## vs      8 19   0.37   0.50   0.00    0.35   0.00   0.00   1.00   1.00\n## am      9 19   0.00   0.00   0.00    0.00   0.00   0.00   0.00   0.00\n## gear   10 19   3.21   0.42   3.00    3.18   0.00   3.00   4.00   1.00\n## carb   11 19   2.74   1.15   3.00    2.76   1.48   1.00   4.00   3.00\n##       skew kurtosis    se\n## mpg   0.01    -0.80  0.88\n## cyl  -0.95    -0.74  0.35\n## disp  0.05    -1.26 25.28\n## hp   -0.01    -1.21 12.37\n## drat  0.50    -1.30  0.09\n## wt    0.98     0.14  0.18\n## qsec  0.85     0.55  0.40\n## vs    0.50    -1.84  0.11\n## am     NaN      NaN  0.00\n## gear  1.31    -0.29  0.10\n## carb -0.14    -1.57  0.26\n## -------------------------------------------------------- \n## group: 1\n##      vars  n   mean    sd median trimmed   mad   min    max  range  skew\n## mpg     1 13  24.39  6.17  22.80   24.38  6.67 15.00  33.90  18.90  0.05\n## cyl     2 13   5.08  1.55   4.00    4.91  0.00  4.00   8.00   4.00  0.87\n## disp    3 13 143.53 87.20 120.30  131.25 58.86 71.10 351.00 279.90  1.33\n## hp      4 13 126.85 84.06 109.00  114.73 63.75 52.00 335.00 283.00  1.36\n## drat    5 13   4.05  0.36   4.08    4.02  0.27  3.54   4.93   1.39  0.79\n## wt      6 13   2.41  0.62   2.32    2.39  0.68  1.51   3.57   2.06  0.21\n## qsec    7 13  17.36  1.79  17.02   17.39  2.34 14.50  19.90   5.40 -0.23\n## vs      8 13   0.54  0.52   1.00    0.55  0.00  0.00   1.00   1.00 -0.14\n## am      9 13   1.00  0.00   1.00    1.00  0.00  1.00   1.00   0.00   NaN\n## gear   10 13   4.38  0.51   4.00    4.36  0.00  4.00   5.00   1.00  0.42\n## carb   11 13   2.92  2.18   2.00    2.64  1.48  1.00   8.00   7.00  0.98\n##      kurtosis    se\n## mpg     -1.46  1.71\n## cyl     -0.90  0.43\n## disp     0.40 24.19\n## hp       0.56 23.31\n## drat     0.21  0.10\n## wt      -1.17  0.17\n## qsec    -1.42  0.50\n## vs      -2.13  0.14\n## am        NaN  0.00\n## gear    -1.96  0.14\n## carb    -0.21  0.60   1\n2\n3\n4\n5 # description statistics by group   library ( doBy ) \n\nsummaryBy ( mpg  +  wt  ~  cyl  +  vs ,  data  =  mtcars ,  \n  FUN  =   function ( x )   {   c ( m  =   mean ( x ),  s  =  sd ( x ))   }   )    1\n2\n3\n4\n5\n6 ##   cyl vs    mpg.m     mpg.s     wt.m      wt.s\n## 1   4  0 26.00000        NA 2.140000        NA\n## 2   4  1 26.73000 4.7481107 2.300300 0.5982073\n## 3   6  0 20.56667 0.7505553 2.755000 0.1281601\n## 4   6  1 19.12500 1.6317169 3.388750 0.1162164\n## 5   8  0 15.10000 2.5600481 3.999214 0.7594047",
            "title": "Group data"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#frequency-tables-crosstables-and-independence",
            "text": "Create frequency and contingency tables from categorical variables. Perform tests of independence, measures of association, and graphically display results.",
            "title": "Frequency Tables, CrossTables, and Independence"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#2d-frequency-tables",
            "text": "mytable  <-   table ( A ,  B )  where A are rows, B are columns.  1\n2\n3 # dataset \nmytable  <-   matrix ( c ( 1 , 2 , 3 , 4 ),  nrow  =   2 ) \nmytable   1\n2\n3 ##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4   1\n2 # A frequencies (summed over columns = 1)  margin.table ( mytable ,   1 )    1 ## [1] 4 6   1\n2 # B frequencies (summed over rows = 2)  margin.table ( mytable ,   2 )    1 ## [1] 3 7   1\n2\n3 # A/(A + B)  # cell percentages  prop.table ( mytable )    1\n2\n3 ##      [,1] [,2]\n## [1,]  0.1  0.3\n## [2,]  0.2  0.4   1\n2 # row percentages  prop.table ( mytable ,   1 )    1\n2\n3 ##           [,1]      [,2]\n## [1,] 0.2500000 0.7500000\n## [2,] 0.3333333 0.6666667   1\n2 # column percentages   prop.table ( mytable ,   2 )    1\n2\n3 ##           [,1]      [,2]\n## [1,] 0.3333333 0.4285714\n## [2,] 0.6666667 0.5714286",
            "title": "2D frequency tables"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#3d-frequency-tables",
            "text": "1\n2 # dataset  head ( CO2 ,   3 )    1\n2\n3\n4\n5 ## Grouped Data: uptake ~ conc | Plant\n##   Plant   Type  Treatment conc uptake\n## 1   Qn1 Quebec nonchilled   95   16.0\n## 2   Qn1 Quebec nonchilled  175   30.4\n## 3   Qn1 Quebec nonchilled  250   34.8   1\n2\n3\n4 A  <-   as.numeric ( CO2 [,   'Plant' ]) \nB  <-  CO2 [,   'conc' ] \nC  <-  CO2 [,   'uptake' ] \nmytable  <-   table ( A ,  B ,  C )    1\n2 # several arrays (3D)  dim ( mytable )   # the printout is immense    1 ## [1] 12  7 76   1\n2 # folded table (2D)  dim ( ftable ( mytable ))   # the printout is immense    1 ## [1] 84 76   1\n2\n3\n4 # 3-Way frequency Table \nmytable  <-  xtabs ( ~ A  +  B  +  C ,  data  =  mytable ,  na.action  =  na.omit )  dim ( ftable ( mytable ))   # the printout is immense    1 ## [1] 84 76",
            "title": "3D frequency tables"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#crosstable",
            "text": "1\n2 A  <-   as.numeric ( CO2 [ 1 : 8 ,   'Plant' ]) \nA   1 ## [1] 1 1 1 1 1 1 1 2   1\n2 B  <-  CO2 [ 1 : 8 ,   'conc' ] \nB   1 ## [1]   95  175  250  350  500  675 1000   95   1\n2\n3\n4 # 2-way cross tabulation  library ( gmodels ) \n\nCrossTable ( A ,  B )   # mydata$myrowvar x mydata$mycolvar     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35 ## \n##  \n##    Cell Contents\n## |-------------------------|\n## |                       N |\n## | Chi-square contribution |\n## |           N / Row Total |\n## |           N / Col Total |\n## |         N / Table Total |\n## |-------------------------|\n## \n##  \n## Total Observations in Table:  8 \n## \n##  \n##              | B \n##            A |        95 |       175 |       250 |       350 |       500 |       675 |      1000 | Row Total | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n##            1 |         1 |         1 |         1 |         1 |         1 |         1 |         1 |         7 | \n##              |     0.321 |     0.018 |     0.018 |     0.018 |     0.018 |     0.018 |     0.018 |           | \n##              |     0.143 |     0.143 |     0.143 |     0.143 |     0.143 |     0.143 |     0.143 |     0.875 | \n##              |     0.500 |     1.000 |     1.000 |     1.000 |     1.000 |     1.000 |     1.000 |           | \n##              |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |           | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n##            2 |         1 |         0 |         0 |         0 |         0 |         0 |         0 |         1 | \n##              |     2.250 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |           | \n##              |     1.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.125 | \n##              |     0.500 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |           | \n##              |     0.125 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |           | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n## Column Total |         2 |         1 |         1 |         1 |         1 |         1 |         1 |         8 | \n##              |     0.250 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |     0.125 |           | \n## -------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n## \n##",
            "title": "CrossTable"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#tests-of-independence",
            "text": "There are more tests of independence in section 10, Resampling Statistics.  1\n2 # dataset, a contingency table \ncolors   1\n2\n3\n4\n5 ##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85   1\n2\n3 # chi-square test on 2-way tables  # test independence of the row and column variables, p-value is calculated from the asymptotic chi-squared distribution of the test statistic \nchisq.test ( colors )    1\n2\n3\n4\n5 ## \n##  Pearson's Chi-squared test\n## \n## data:  colors\n## X-squared = 1240, df = 12, p-value < 2.2e-16   1\n2\n3\n4\n5\n6\n7 # dataset, a contingency table in 2x2 matrix form \nTeaTasting  <-  matrix ( c ( 3 ,   1 ,   1 ,   3 ), \n       nrow  =   2 , \n       dimnames  =   list ( Guess  =   c ( \"Milk\" ,   \"Tea\" ), \n                       Truth  =   c ( \"Milk\" ,   \"Tea\" ))) \nTeaTasting   1\n2\n3\n4 ##       Truth\n## Guess  Milk Tea\n##   Milk    3   1\n##   Tea     1   3   1\n2 # Fisher exact test \nfisher.test ( TeaTasting ,  alternative  =   \"greater\" )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## \n##  Fisher's Exact Test for Count Data\n## \n## data:  TeaTasting\n## p-value = 0.2429\n## alternative hypothesis: true odds ratio is greater than 1\n## 95 percent confidence interval:\n##  0.3135693       Inf\n## sample estimates:\n## odds ratio \n##   6.408309    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 # 3D contingency table, where the last dimension refers to the strata \nRabbits  <-  array ( c ( 0 ,   0 ,   6 ,   5 , \n         3 ,   0 ,   3 ,   6 , \n         6 ,   2 ,   0 ,   4 , \n         5 ,   6 ,   1 ,   0 , \n         2 ,   5 ,   0 ,   0 ), \n      dim  =   c ( 2 ,   2 ,   5 ), \n      dimnames  =   list ( \n          Delay  =   c ( \"None\" ,   \"1.5h\" ), \n          Response  =   c ( \"Cured\" ,   \"Died\" ), \n          Penicillin.Level  =   c ( \"1/8\" ,   \"1/4\" ,   \"1/2\" ,   \"1\" ,   \"4\" ))) \nRabbits    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 ## , , Penicillin.Level = 1/8\n## \n##       Response\n## Delay  Cured Died\n##   None     0    6\n##   1.5h     0    5\n## \n## , , Penicillin.Level = 1/4\n## \n##       Response\n## Delay  Cured Died\n##   None     3    3\n##   1.5h     0    6\n## \n## , , Penicillin.Level = 1/2\n## \n##       Response\n## Delay  Cured Died\n##   None     6    0\n##   1.5h     2    4\n## \n## , , Penicillin.Level = 1\n## \n##       Response\n## Delay  Cured Died\n##   None     5    1\n##   1.5h     6    0\n## \n## , , Penicillin.Level = 4\n## \n##       Response\n## Delay  Cured Died\n##   None     2    0\n##   1.5h     5    0   1\n2 # Mantel-Haenszel test / Cochran-Mantel-Haenszel chi-squared test, hypothesis that two nominal variables are conditionally independent in each stratum, assuming that there is no three-way interaction. \nmantelhaen.test ( Rabbits )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## \n##  Mantel-Haenszel chi-squared test with continuity correction\n## \n## data:  Rabbits\n## Mantel-Haenszel X-squared = 3.9286, df = 1, p-value = 0.04747\n## alternative hypothesis: true common odds ratio is not equal to 1\n## 95 percent confidence interval:\n##   1.026713 47.725133\n## sample estimates:\n## common odds ratio \n##                 7   1\n2\n3\n4\n5\n6 # dataset  # 3-way contingency table based on variables A, B, and C \nA  <-  CO2 [,   'Plant' ] \nB  <-  CO2 [,   'conc' ] \nC  <-  CO2 [,   'uptake' ] \nmytable  <-  xtabs ( ~ A + B + C )   # a 3D array    1\n2\n3\n4\n5 # loglinear Models  # mutual independence: A, B, and C are pairwise independent  library ( MASS ) \n\nloglm ( ~ A  +  B  +  C ,  mytable )    1\n2\n3\n4\n5\n6\n7 ## Call:\n## loglm(formula = ~A + B + C, data = mytable)\n## \n## Statistics:\n##                        X^2   df  P(> X^2)\n## Likelihood Ratio  720.1035 6291 1.0000000\n## Pearson          6300.0000 6291 0.4656775   1\n2 # conditional independence: A is independent of B, given C \nloglm ( ~ A  +  B  +  C  +  A  *  C  +  B  *  C ,  mytable )    1\n2\n3\n4\n5\n6\n7 ## Call:\n## loglm(formula = ~A + B + C + A * C + B * C, data = mytable)\n## \n## Statistics:\n##                       X^2   df P(> X^2)\n## Likelihood Ratio 12.13685 5016        1\n## Pearson               NaN 5016      NaN   1\n2 # no three-way interaction \nloglm ( ~ A  +  B  +  C  +  A  *  B  +  A  *  C  +  B  *  C ,  mytable )    1\n2\n3\n4\n5\n6\n7 ## Call:\n## loglm(formula = ~A + B + C + A * B + A * C + B * C, data = mytable)\n## \n## Statistics:\n##                       X^2   df P(> X^2)\n## Likelihood Ratio 1.038376 4950        1\n## Pearson               NaN 4950      NaN",
            "title": "Tests of independence"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#measures-of-association",
            "text": "Association between two nominal variables, giving a value between 0 and +1 (inclusive). It is based on Pearson\u2019s chi-squared statistic.  1\n2 # Dataset \nstr ( Arthritis )    1\n2\n3\n4\n5\n6 ## 'data.frame':    84 obs. of  5 variables:\n##  $ ID       : int  57 46 77 17 36 23 75 39 33 55 ...\n##  $ Treatment: Factor w/ 2 levels \"Placebo\",\"Treated\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ Sex      : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ Age      : int  27 29 30 32 46 58 59 59 63 63 ...\n##  $ Improved : Ord.factor w/ 3 levels \"None\"<\"Some\"<..: 2 1 1 3 3 3 1 3 1 1 ...   1\n2 tab  <-  xtabs ( ~ Improved  +  Treatment ,  data  =  Arthritis ) \ntab   1\n2\n3\n4\n5 ##         Treatment\n## Improved Placebo Treated\n##   None        29      13\n##   Some         7       7\n##   Marked       7      21   1 summary ( assocstats ( tab ))     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 ## \n## Call: xtabs(formula = ~Improved + Treatment, data = Arthritis)\n## Number of cases in table: 84 \n## Number of factors: 2 \n## Test for independence of all factors:\n##  Chisq = 13.055, df = 2, p-value = 0.001463\n##                     X^2 df  P(> X^2)\n## Likelihood Ratio 13.530  2 0.0011536\n## Pearson          13.055  2 0.0014626\n## \n## Phi-Coefficient   : NA \n## Contingency Coeff.: 0.367 \n## Cramer's V        : 0.394   1\n2\n3\n4 # phi coefficient, contingency coefficient, and Cram\u00e9r's V for an 2D table  library ( vcd ) \n\nassocstats ( tab )    1\n2\n3\n4\n5\n6\n7 ##                     X^2 df  P(> X^2)\n## Likelihood Ratio 13.530  2 0.0011536\n## Pearson          13.055  2 0.0014626\n## \n## Phi-Coefficient   : NA \n## Contingency Coeff.: 0.367 \n## Cramer's V        : 0.394   1\n2 # Dataset \nTeaTasting   1\n2\n3\n4 ##       Truth\n## Guess  Milk Tea\n##   Milk    3   1\n##   Tea     1   3   1\n2\n3\n4 # Cohen's kappa and weighted kappa for a confusion matrix  library ( vcd )  kappa ( TeaTasting )    1 ## [1] 2.333333",
            "title": "Measures of association"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#correlations",
            "text": "1\n2 # dataset  head ( mtcars ,   3 )    1\n2\n3\n4 ##                mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1   1\n2\n3 # correlations/covariances among numeric variables in  # a data frame \ncor ( mtcars ,  use  =   \"complete.obs\" ,  method  =   \"kendall\" )   # method = \"pearson\", \"spearman\" or \"kendall\"     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 ##             mpg        cyl       disp         hp        drat         wt\n## mpg   1.0000000 -0.7953134 -0.7681311 -0.7428125  0.46454879 -0.7278321\n## cyl  -0.7953134  1.0000000  0.8144263  0.7851865 -0.55131785  0.7282611\n## disp -0.7681311  0.8144263  1.0000000  0.6659987 -0.49898277  0.7433824\n## hp   -0.7428125  0.7851865  0.6659987  1.0000000 -0.38262689  0.6113081\n## drat  0.4645488 -0.5513178 -0.4989828 -0.3826269  1.00000000 -0.5471495\n## wt   -0.7278321  0.7282611  0.7433824  0.6113081 -0.54714953  1.0000000\n## qsec  0.3153652 -0.4489698 -0.3008155 -0.4729061  0.03272155 -0.1419881\n## vs    0.5896790 -0.7710007 -0.6033059 -0.6305926  0.37510111 -0.4884787\n## am    0.4690128 -0.4946212 -0.5202739 -0.3039956  0.57554849 -0.6138790\n## gear  0.4331509 -0.5125435 -0.4759795 -0.2794458  0.58392476 -0.5435956\n## carb -0.5043945  0.4654299  0.4137360  0.5959842 -0.09535193  0.3713741\n##             qsec         vs          am        gear        carb\n## mpg   0.31536522  0.5896790  0.46901280  0.43315089 -0.50439455\n## cyl  -0.44896982 -0.7710007 -0.49462115 -0.51254349  0.46542994\n## disp -0.30081549 -0.6033059 -0.52027392 -0.47597955  0.41373600\n## hp   -0.47290613 -0.6305926 -0.30399557 -0.27944584  0.59598416\n## drat  0.03272155  0.3751011  0.57554849  0.58392476 -0.09535193\n## wt   -0.14198812 -0.4884787 -0.61387896 -0.54359562  0.37137413\n## qsec  1.00000000  0.6575431 -0.16890405 -0.09126069 -0.50643945\n## vs    0.65754312  1.0000000  0.16834512  0.26974788 -0.57692729\n## am   -0.16890405  0.1683451  1.00000000  0.77078758 -0.05859929\n## gear -0.09126069  0.2697479  0.77078758  1.00000000  0.09801487\n## carb -0.50643945 -0.5769273 -0.05859929  0.09801487  1.00000000   1 cov ( mtcars ,  use  =   \"complete.obs\" )      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36 ##              mpg         cyl        disp          hp         drat\n## mpg    36.324103  -9.1723790  -633.09721 -320.732056   2.19506351\n## cyl    -9.172379   3.1895161   199.66028  101.931452  -0.66836694\n## disp -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915\n## hp   -320.732056 101.9314516  6721.15867 4700.866935 -16.45110887\n## drat    2.195064  -0.6683669   -47.06402  -16.451109   0.28588135\n## wt     -5.116685   1.3673710   107.68420   44.192661  -0.37272073\n## qsec    4.509149  -1.8868548   -96.05168  -86.770081   0.08714073\n## vs      2.017137  -0.7298387   -44.37762  -24.987903   0.11864919\n## am      1.803931  -0.4657258   -36.56401   -8.320565   0.19015121\n## gear    2.135685  -0.6491935   -50.80262   -6.358871   0.27598790\n## carb   -5.363105   1.5201613    79.06875   83.036290  -0.07840726\n##               wt         qsec           vs           am        gear\n## mpg   -5.1166847   4.50914919   2.01713710   1.80393145   2.1356855\n## cyl    1.3673710  -1.88685484  -0.72983871  -0.46572581  -0.6491935\n## disp 107.6842040 -96.05168145 -44.37762097 -36.56401210 -50.8026210\n## hp    44.1926613 -86.77008065 -24.98790323  -8.32056452  -6.3588710\n## drat  -0.3727207   0.08714073   0.11864919   0.19015121   0.2759879\n## wt     0.9573790  -0.30548161  -0.27366129  -0.33810484  -0.4210806\n## qsec  -0.3054816   3.19316613   0.67056452  -0.20495968  -0.2804032\n## vs    -0.2736613   0.67056452   0.25403226   0.04233871   0.0766129\n## am    -0.3381048  -0.20495968   0.04233871   0.24899194   0.2923387\n## gear  -0.4210806  -0.28040323   0.07661290   0.29233871   0.5443548\n## carb   0.6757903  -1.89411290  -0.46370968   0.04637097   0.3266129\n##             carb\n## mpg  -5.36310484\n## cyl   1.52016129\n## disp 79.06875000\n## hp   83.03629032\n## drat -0.07840726\n## wt    0.67579032\n## qsec -1.89411290\n## vs   -0.46370968\n## am    0.04637097\n## gear  0.32661290\n## carb  2.60887097   1\n2\n3\n4 # correlations with significance levels  library ( Hmisc ) \n\nrcorr ( as.matrix ( mtcars ),  type  =   \"pearson\" )   # type can be pearson or spearman     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41 ##        mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n## mpg   1.00 -0.85 -0.85 -0.78  0.68 -0.87  0.42  0.66  0.60  0.48 -0.55\n## cyl  -0.85  1.00  0.90  0.83 -0.70  0.78 -0.59 -0.81 -0.52 -0.49  0.53\n## disp -0.85  0.90  1.00  0.79 -0.71  0.89 -0.43 -0.71 -0.59 -0.56  0.39\n## hp   -0.78  0.83  0.79  1.00 -0.45  0.66 -0.71 -0.72 -0.24 -0.13  0.75\n## drat  0.68 -0.70 -0.71 -0.45  1.00 -0.71  0.09  0.44  0.71  0.70 -0.09\n## wt   -0.87  0.78  0.89  0.66 -0.71  1.00 -0.17 -0.55 -0.69 -0.58  0.43\n## qsec  0.42 -0.59 -0.43 -0.71  0.09 -0.17  1.00  0.74 -0.23 -0.21 -0.66\n## vs    0.66 -0.81 -0.71 -0.72  0.44 -0.55  0.74  1.00  0.17  0.21 -0.57\n## am    0.60 -0.52 -0.59 -0.24  0.71 -0.69 -0.23  0.17  1.00  0.79  0.06\n## gear  0.48 -0.49 -0.56 -0.13  0.70 -0.58 -0.21  0.21  0.79  1.00  0.27\n## carb -0.55  0.53  0.39  0.75 -0.09  0.43 -0.66 -0.57  0.06  0.27  1.00\n## \n## n= 32 \n## \n## \n## P\n##      mpg    cyl    disp   hp     drat   wt     qsec   vs     am     gear  \n## mpg         0.0000 0.0000 0.0000 0.0000 0.0000 0.0171 0.0000 0.0003 0.0054\n## cyl  0.0000        0.0000 0.0000 0.0000 0.0000 0.0004 0.0000 0.0022 0.0042\n## disp 0.0000 0.0000        0.0000 0.0000 0.0000 0.0131 0.0000 0.0004 0.0010\n## hp   0.0000 0.0000 0.0000        0.0100 0.0000 0.0000 0.0000 0.1798 0.4930\n## drat 0.0000 0.0000 0.0000 0.0100        0.0000 0.6196 0.0117 0.0000 0.0000\n## wt   0.0000 0.0000 0.0000 0.0000 0.0000        0.3389 0.0010 0.0000 0.0005\n## qsec 0.0171 0.0004 0.0131 0.0000 0.6196 0.3389        0.0000 0.2057 0.2425\n## vs   0.0000 0.0000 0.0000 0.0000 0.0117 0.0010 0.0000        0.3570 0.2579\n## am   0.0003 0.0022 0.0004 0.1798 0.0000 0.0000 0.2057 0.3570        0.0000\n## gear 0.0054 0.0042 0.0010 0.4930 0.0000 0.0005 0.2425 0.2579 0.0000       \n## carb 0.0011 0.0019 0.0253 0.0000 0.6212 0.0146 0.0000 0.0007 0.7545 0.1290\n##      carb  \n## mpg  0.0011\n## cyl  0.0019\n## disp 0.0253\n## hp   0.0000\n## drat 0.6212\n## wt   0.0146\n## qsec 0.0000\n## vs   0.0007\n## am   0.7545\n## gear 0.1290\n## carb   1\n2\n3 # dataset \nx  <-  mtcars [ 1 : 3 ] \ny  <-  mtcars [ 4 : 6 ]    1\n2 # correlation between two vectors \ncor ( x ,  y )    1\n2\n3\n4 ##              hp       drat         wt\n## mpg  -0.7761684  0.6811719 -0.8676594\n## cyl   0.8324475 -0.6999381  0.7824958\n## disp  0.7909486 -0.7102139  0.8879799   Polychoric correlation  The correlation between two theorised normally distributed continuous latent variables, from two observed ordinal variables.  1\n2\n3 # dataset  # 2-way contingency table of counts \ncolors   1\n2\n3\n4\n5 ##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85   1\n2\n3\n4 # polychoric correlation  library ( polycor ) \n\npolychor ( colors )    1 ## [1] 0.4743984   1\n2\n3\n4\n5\n6 # heterogeneous correlations in one matrix  # pearson (numeric-numeric),  # polyserial (numeric-ordinal),  # and polychoric (ordinal-ordinal)  # a data frame with ordered factors and numeric variables \nhetcor ( colors )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 ## \n## Two-Step Estimates\n## \n## Correlations/Type of Correlation:\n##             fair.hair red.hair medium.hair dark.hair black.hair\n## fair.hair           1  Pearson     Pearson   Pearson    Pearson\n## red.hair       0.8333        1     Pearson   Pearson    Pearson\n## medium.hair    0.2597   0.6467           1   Pearson    Pearson\n## dark.hair     -0.7091  -0.2251      0.1911         1    Pearson\n## black.hair     -0.781  -0.3875    -0.06329    0.9674          1\n## \n## Standard Errors:\n##             fair.hair red.hair medium.hair dark.hair\n## fair.hair                                           \n## red.hair      0.03628                               \n## medium.hair    0.3607   0.1383                      \n## dark.hair     0.09977   0.3733      0.3841          \n## black.hair    0.06019   0.3004      0.4092  0.001491\n## \n## n = 4 \n## \n## P-values for Tests of Bivariate Normality:\n##             fair.hair red.hair medium.hair dark.hair\n## fair.hair                                           \n## red.hair       0.8306                               \n## medium.hair    0.6939   0.8609                      \n## dark.hair      0.7465   0.6335      0.7161          \n## black.hair     0.6582   0.5927      0.6711    0.9873",
            "title": "Correlations"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#t-tests",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # independent 2-group t-test \nt.test ( y  ~  x )   # where y is numeric and x is a binary factor  # independent 2-group t-test \nt.test ( y1 ,  y2 )   # where y1 and y2 are numeric   # paired t-test \nt.test ( y1 ,  y2 ,  paired  =   TRUE )   # where y1 & y2 are numeric   # one sample t-test \nt.test ( y ,  mu  =   3 )   # Ho: mu=3    1\n2\n3 # dataset  # 2-way contingency table \ncolors   1\n2\n3\n4\n5 ##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85   1 mean ( as.numeric ( colors [ 1 ,]))   # row 1 average    1 ## [1] 143.6   1\n2 # independent 2-group t-test \nt.test ( as.numeric ( colors [ 1 ,]),   as.numeric ( colors [ 2 ,]))   # where y1 and y2 are numeric      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## \n##  Welch Two Sample t-test\n## \n## data:  as.numeric(colors[1, ]) and as.numeric(colors[2, ])\n## t = -1.164, df = 5.5777, p-value = 0.2918\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -541.5725  196.7725\n## sample estimates:\n## mean of x mean of y \n##     143.6     316.0   1\n2 # one sample t-test \nt.test ( as.numeric ( colors [ 1 ,]),  mu  =   0 )   # Ho: mu=0     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## \n##  One Sample t-test\n## \n## data:  as.numeric(colors[1, ])\n## t = 2.348, df = 4, p-value = 0.07868\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  -26.2009 313.4009\n## sample estimates:\n## mean of x \n##     143.6   1 t.test ( as.numeric ( colors [ 1 ,]),  mu  =   0 ,  alternative = \"greater\" )   # Ho: mu=<0     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## \n##  One Sample t-test\n## \n## data:  as.numeric(colors[1, ])\n## t = 2.348, df = 4, p-value = 0.03934\n## alternative hypothesis: true mean is greater than 0\n## 95 percent confidence interval:\n##  13.22123      Inf\n## sample estimates:\n## mean of x \n##     143.6    alternative=\"less\"  or  alternative=\"greater\"  option to specify a one tailed test.  var.equal = TRUE  option to specify equal variances and a pooled variance estimate.   For multivariate tests and ANOVA, see sections 8 and 9.",
            "title": "t-tests"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#nonparametric-statistics",
            "text": "Nonnormal distributions.",
            "title": "Nonparametric statistics"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#bivariate-tests",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 # independent 2-group Mann-Whitney U test \nwilcox.test ( y  ~  A )   # where y is numeric and A is A binary factor  # independent 2-group Mann-Whitney U test \nwilcox.test ( y ,  x )   # where y and x are numeric  # dependent 2-group Wilcoxon signed rank test \nwilcox.test ( y1 ,  y2 ,  paired  =   TRUE )   # where y1 and y2 are numeric    1\n2 # 2-way contingency table \ncolors   1\n2\n3\n4\n5 ##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85   1\n2 # independent 2-group Mann-Whitney U Test \nwilcox.test ( as.numeric ( colors [ 1 ,]),   as.numeric ( colors [ 2 ,]))   # where y and x are numeric    1\n2\n3\n4\n5\n6 ## \n##  Wilcoxon rank sum test\n## \n## data:  as.numeric(colors[1, ]) and as.numeric(colors[2, ])\n## W = 8, p-value = 0.4206\n## alternative hypothesis: true location shift is not equal to 0   alternative=\"less\"  or  alternative=\"greater\"  option to specify a one \ntailed test.",
            "title": "Bivariate tests"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#anova",
            "text": "1\n2\n3\n4\n5\n6\n7\n8 # Kruskal Wallis test one-Way ANOVA by ranks \nkruskal.test ( y  ~  A )   # where y1 is numeric and A is a factor  # randomized block design - Friedman test \nfriedman.test ( y  ~  A  |  B )   # where y are the data values, A is a grouping factor and B is a blocking factor  # Kruskal Wallis test one-way ANOVA by ranks \nkruskal.test ( y ,  x )   # where y and x are numeric    1\n2\n3 # dataset  # 2-way contingency table \ncolors   1\n2\n3\n4\n5 ##             fair.hair red.hair medium.hair dark.hair black.hair\n## blue eyes         326       38         241       110          3\n## light eyes        688      116         584       188          4\n## medium eyes       343       84         909       412         26\n## dark eyes          98       48         403       681         85   1\n2 # Kruskal Wallis test one-way ANOVA by ranks \nkruskal.test ( as.numeric ( colors [ 1 ,]),   as.numeric ( colors [ 2 ,]))   # where y and x are numeric    1\n2\n3\n4\n5 ## \n##  Kruskal-Wallis rank sum test\n## \n## data:  as.numeric(colors[1, ]) and as.numeric(colors[2, ])\n## Kruskal-Wallis chi-squared = 4, df = 4, p-value = 0.406",
            "title": "ANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#multiple-regressions",
            "text": "",
            "title": "Multiple Regressions"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#fitting-the-model",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 # multiple linear regression \nfit  <-  lm ( y  ~  x1  +  x2  +  x3 ,  data  =  mydata )  summary ( fit )   # show results  # useful functions \ncoefficients ( fit )   # model coefficients \nconfint ( fit ,  level  =   0.95 )   # CIs for model parameters \nfitted ( fit )   # predicted values \nresiduals ( fit )   # residuals \nanova ( fit )   # ANOVA table \nvcov ( fit )   # covariance matrix for model parameters \ninfluence ( fit )   # regression diagnostics     1\n2 # dataset \nstr ( longley )    1\n2\n3\n4\n5\n6\n7\n8 ## 'data.frame':    16 obs. of  7 variables:\n##  $ GNP.deflator: num  83 88.5 88.2 89.5 96.2 ...\n##  $ GNP         : num  234 259 258 285 329 ...\n##  $ Unemployed  : num  236 232 368 335 210 ...\n##  $ Armed.Forces: num  159 146 162 165 310 ...\n##  $ Population  : num  108 109 110 111 112 ...\n##  $ Year        : int  1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ...\n##  $ Employed    : num  60.3 61.1 60.2 61.2 63.2 ...   1\n2\n3 # multiple linear regression example \nfit  <-  lm ( Armed.Forces  ~  GNP  +  Population ,  data  =  longley )  summary ( fit )   # show results     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 ## \n## Call:\n## lm(formula = Armed.Forces ~ GNP + Population, data = longley)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -70.349 -33.569   5.076  16.409 104.037 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)   \n## (Intercept) 4123.922   1276.579   3.230  0.00657 **\n## GNP            3.365      0.986   3.413  0.00463 **\n## Population   -44.011     14.089  -3.124  0.00807 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 50.56 on 13 degrees of freedom\n## Multiple R-squared:  0.5426, Adjusted R-squared:  0.4723 \n## F-statistic: 7.712 on 2 and 13 DF,  p-value: 0.006191   1\n2 # other useful functions \ncoefficients ( fit )   # model coefficients    1\n2 ## (Intercept)         GNP  Population \n## 4123.922484    3.365215  -44.010955   1 confint ( fit ,  level  =   0.95 )   # CIs for model parameters    1\n2\n3\n4 ##                   2.5 %      97.5 %\n## (Intercept) 1366.042123 6881.802846\n## GNP            1.235097    5.495333\n## Population   -74.447969  -13.573942   1 fitted ( fit )   # predicted values    1\n2\n3\n4 ##     1947     1948     1949     1950     1951     1952     1953     1954 \n## 176.4245 215.9487 161.1151 199.5681 298.4663 306.5279 288.1248 230.9633 \n##     1955     1956     1957     1958     1959     1960     1961     1962 \n## 295.1332 308.9566 313.0359 252.7794 318.8698 297.7176 240.7975 266.2711   1 residuals ( fit )   # residuals    1\n2\n3\n4\n5\n6 ##        1947        1948        1949        1950        1951        1952 \n## -17.4245114 -70.3487085   0.4848669 -34.5681071  11.4336564  52.8721086 \n##        1953        1954        1955        1956        1957        1958 \n##  66.5752438 104.0367029   9.6668098 -23.2566323 -33.2359499  10.9205505 \n##        1959        1960        1961        1962 \n## -63.6698197 -46.3175746  16.4025069  16.4288577   1 anova ( fit )   # ANOVA table    1\n2\n3\n4\n5\n6\n7\n8\n9 ## Analysis of Variance Table\n## \n## Response: Armed.Forces\n##            Df Sum Sq Mean Sq F value   Pr(>F)   \n## GNP         1  14479 14478.7  5.6649 0.033301 * \n## Population  1  24941 24940.8  9.7583 0.008068 **\n## Residuals  13  33226  2555.9                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1 vcov ( fit )   # covariance matrix for model parameters    1\n2\n3\n4 ##             (Intercept)          GNP   Population\n## (Intercept) 1629652.882 1239.7478355 -17970.27388\n## GNP            1239.748    0.9721911    -13.76775\n## Population   -17970.274  -13.7677545    198.49444   1 influence ( fit )   # regression diagnostics      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40 ## $hat\n##       1947       1948       1949       1950       1951       1952 \n## 0.27412252 0.17438942 0.31563199 0.16765687 0.21219668 0.21127212 \n##       1953       1954       1955       1956       1957       1958 \n## 0.11339116 0.08602104 0.10270245 0.12845683 0.13251347 0.11070418 \n##       1959       1960       1961       1962 \n## 0.15598638 0.15164367 0.32488437 0.33842686 \n## \n## $coefficients\n##      (Intercept)          GNP  Population\n## 1947  128.042531  0.131479474 -1.53731106\n## 1948   29.040635  0.121992502 -0.69544934\n## 1949   -6.396740 -0.005738655  0.07379997\n## 1950  177.778426  0.175668519 -2.11609661\n## 1951  133.333008  0.093997366 -1.43810938\n## 1952  638.680267  0.462229807 -6.92955762\n## 1953  422.107967  0.305133565 -4.56222462\n## 1954 -385.998493 -0.325674565  4.42308458\n## 1955   54.458118  0.042127997 -0.59713302\n## 1956 -163.371695 -0.131240587  1.81041089\n## 1957 -212.039316 -0.179084306  2.37664757\n## 1958  -51.395691 -0.033854337  0.55600613\n## 1959 -329.488802 -0.311550864  3.79446941\n## 1960    3.116881 -0.049904757  0.10916689\n## 1961 -242.199361 -0.158976865  2.60043035\n## 1962 -194.416431 -0.113799395  2.04462752\n## \n## $sigma\n##     1947     1948     1949     1950     1951     1952     1953     1954 \n## 52.28757 47.63741 52.61955 51.47046 52.48826 49.73420 48.50003 42.21357 \n##     1955     1956     1957     1958     1959     1960     1961     1962 \n## 52.53729 52.12610 51.60167 52.51353 48.66817 50.57779 52.30331 52.29577 \n## \n## $wt.res\n##        1947        1948        1949        1950        1951        1952 \n## -17.4245114 -70.3487085   0.4848669 -34.5681071  11.4336564  52.8721086 \n##        1953        1954        1955        1956        1957        1958 \n##  66.5752438 104.0367029   9.6668098 -23.2566323 -33.2359499  10.9205505 \n##        1959        1960        1961        1962 \n## -63.6698197 -46.3175746  16.4025069  16.4288577",
            "title": "Fitting the Model"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#diagnostic-plots",
            "text": "1\n2\n3 # diagnostic plots \nlayout ( matrix ( c ( 1 , 2 , 3 , 4 ), 2 , 2 ))   # optional 4 graphs/page \nplot ( fit )     1 layout ( matrix ( c ( 1 , 1 , 1 , 1 ), 2 , 2 ))",
            "title": "Diagnostic plots"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#comparing-two-models-with-anova",
            "text": "1\n2\n3\n4\n5 # compare models \nfit1  <-  lm ( y  ~  x1  +  x2  +  x3  +  x4 ,  data  =  mydata ) \nfit2  <-  lm ( y  ~  x1  +  x2 ) \n\nanova ( fit1 ,  fit2 )     1\n2\n3\n4\n5 # compare models \nfit1  <-  lm ( Armed.Forces  ~  GNP  +  Population ,  data  =  longley ) \nfit2  <-  lm ( Armed.Forces  ~  GNP ,  data  =  longley ) \n\nanova ( fit1 ,  fit2 )     1\n2\n3\n4\n5\n6\n7\n8\n9 ## Analysis of Variance Table\n## \n## Model 1: Armed.Forces ~ GNP + Population\n## Model 2: Armed.Forces ~ GNP\n##   Res.Df   RSS Df Sum of Sq      F   Pr(>F)   \n## 1     13 33226                                \n## 2     14 58167 -1    -24941 9.7583 0.008068 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
            "title": "Comparing two models with ANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#cross-validation",
            "text": "1\n2\n3\n4\n5 # dataset  library ( DAAG ) \n\ndata ( houseprices )  head ( houseprices ,   3 )    1\n2\n3\n4 ##    area bedrooms sale.price\n## 9   694        4        192\n## 10  905        4        215\n## 11  802        4        215   1\n2\n3\n4\n5 # k-fold cross-validation  library ( DAAG )  # case 1, # 3 fold cross-validation \nCVlm ( houseprices ,  form.lm  =  formula ( sale.price  ~  area ),  m  =   3 ,  dots  =   FALSE ,  seed  =   29 ,  plotit  =   c ( \"Observed\" , \"Residual\" ),  main  =   \"Small symbols show cross-validation predicted values\" ,  legend.pos  =   \"topleft\" ,  printit  =   TRUE )    1\n2\n3\n4\n5\n6\n7\n8 ## Analysis of Variance Table\n## \n## Response: sale.price\n##           Df Sum Sq Mean Sq F value Pr(>F)  \n## area       1  18566   18566       8  0.014 *\n## Residuals 13  30179    2321                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 ## \n## fold 1 \n## Observations in test set: 5 \n##              11  20    21     22   23\n## area        802 696 771.0 1006.0 1191\n## cvpred      204 188 199.3  234.7  262\n## sale.price  215 255 260.0  293.0  375\n## CV residual  11  67  60.7   58.3  113\n## \n## Sum of squares = 24351    Mean square = 4870    n = 5 \n## \n## fold 2 \n## Observations in test set: 5 \n##              10   13    14      17     18\n## area        905  716 963.0 1018.00 887.00\n## cvpred      255  224 264.4  273.38 252.06\n## sale.price  215  113 185.0  276.00 260.00\n## CV residual -40 -112 -79.4    2.62   7.94\n## \n## Sum of squares = 20416    Mean square = 4083    n = 5 \n## \n## fold 3 \n## Observations in test set: 5 \n##                 9   12     15    16     19\n## area        694.0 1366 821.00 714.0 790.00\n## cvpred      183.2  388 221.94 189.3 212.49\n## sale.price  192.0  274 212.00 220.0 221.50\n## CV residual   8.8 -114  -9.94  30.7   9.01\n## \n## Sum of squares = 14241    Mean square = 2848    n = 5 \n## \n## Overall (Sum over all 5 folds) \n##   ms \n## 3934   1\n2 # dataset  head ( longley ,   3 )    1\n2\n3\n4 ##      GNP.deflator GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234        236          159        108 1947     60.3\n## 1948         88.5 259        232          146        109 1948     61.1\n## 1949         88.2 258        368          162        110 1949     60.2   1\n2 # case 2, # 3 fold cross-validation \nCVlm ( longley ,  form.lm  =  formula ( Armed.Forces  ~  GNP  +  Population ),  m  =   3 ,  dots  =   FALSE ,  seed  =   29 ,  plotit  =   c ( \"Observed\" , \"Residual\" ),  main  =   \"Small symbols show cross-validation predicted values\" ,  legend.pos  =   \"topleft\" ,  printit  =   TRUE )    1\n2\n3\n4\n5\n6\n7\n8\n9 ## Analysis of Variance Table\n## \n## Response: Armed.Forces\n##            Df Sum Sq Mean Sq F value Pr(>F)   \n## GNP         1  14479   14479    5.66 0.0333 * \n## Population  1  24941   24941    9.76 0.0081 **\n## Residuals  13  33226    2556                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 ## \n## fold 1 \n## Observations in test set: 5 \n##               1953 1954  1957  1960  1962\n## Predicted    288.1  231 313.0 297.7 266.3\n## cvpred       281.4  219 310.6 295.7 263.1\n## Armed.Forces 354.7  335 279.8 251.4 282.7\n## CV residual   73.3  116 -30.8 -44.3  19.6\n## \n## Sum of squares = 22004    Mean square = 4401    n = 5 \n## \n## fold 2 \n## Observations in test set: 6 \n##               1948   1949   1955   1958  1959  1961\n## Predicted    215.9 161.12 295.13 252.78 318.9 240.8\n## cvpred       231.9 171.41 306.33 255.07 324.5 234.9\n## Armed.Forces 145.6 161.60 304.80 263.70 255.2 257.2\n## CV residual  -86.3  -9.81  -1.53   8.63 -69.3  22.3\n## \n## Sum of squares = 12917    Mean square = 2153    n = 6 \n## \n## fold 3 \n## Observations in test set: 5 \n##               1947  1950  1951  1952   1956\n## Predicted    176.4 199.6 298.5 306.5 308.96\n## cvpred       192.8 211.9 282.3 288.9 295.17\n## Armed.Forces 159.0 165.0 309.9 359.4 285.70\n## CV residual  -33.8 -46.9  27.6  70.5  -9.47\n## \n## Sum of squares = 9161    Mean square = 1832    n = 5 \n## \n## Overall (Sum over all 5 folds) \n##   ms \n## 2755",
            "title": "Cross validation"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#variable-selection-heuristic-methods",
            "text": "1\n2 # dataset  head ( longley ,   3 )    1\n2\n3\n4 ##      GNP.deflator GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234        236          159        108 1947     60.3\n## 1948         88.5 259        232          146        109 1948     61.1\n## 1949         88.2 258        368          162        110 1949     60.2   1\n2\n3\n4\n5 # Stepwise Regression  library ( MASS ) \n\nfit  <-  lm ( Armed.Forces  ~  GNP  +  Population  +  Employed  +  Unemployed ,  data  =  longley ) \nstep  <-  stepAIC ( fit ,  direction  =   \"both\" )    1\n2\n3\n4\n5\n6\n7\n8\n9 ## Start:  AIC=125\n## Armed.Forces ~ GNP + Population + Employed + Unemployed\n## \n##              Df Sum of Sq   RSS AIC\n## <none>                    21655 125\n## - Unemployed  1      4508 26163 126\n## - Population  1      5522 27177 127\n## - Employed    1     10208 31863 130\n## - GNP         1     15323 36978 132   1 step $ anova  # display results     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 ## Stepwise Model Path \n## Analysis of Deviance Table\n## \n## Initial Model:\n## Armed.Forces ~ GNP + Population + Employed + Unemployed\n## \n## Final Model:\n## Armed.Forces ~ GNP + Population + Employed + Unemployed\n## \n## \n##   Step Df Deviance Resid. Df Resid. Dev AIC\n## 1                         11      21655 125   The goal is to reduce the AIC. Adding variable does not improve the model. Let\u2019s opt for the most valuable variable.  1\n2 fit  <-  lm ( Armed.Forces  ~  Unemployed ,  data  =  longley ) \nstep  <-  stepAIC ( fit ,  direction  =   \"both\" )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 ## Start:  AIC=138\n## Armed.Forces ~ Unemployed\n## \n##              Df Sum of Sq   RSS AIC\n## - Unemployed  1      2287 72646 137\n## <none>                    70359 138\n## \n## Step:  AIC=137\n## Armed.Forces ~ 1\n## \n##              Df Sum of Sq   RSS AIC\n## <none>                    72646 137\n## + Unemployed  1      2287 70359 138   1 step $ anova  # display results     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 ## Stepwise Model Path \n## Analysis of Deviance Table\n## \n## Initial Model:\n## Armed.Forces ~ Unemployed\n## \n## Final Model:\n## Armed.Forces ~ 1\n## \n## \n##           Step Df Deviance Resid. Df Resid. Dev AIC\n## 1                                 14      70359 138\n## 2 - Unemployed  1     2287        15      72646 137",
            "title": "Variable selection -- Heuristic methods"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#variable-selection-graphical-methods",
            "text": "1\n2 # model \nfit  <-  lm ( Armed.Forces  ~  GNP  +  Population  +  Employed  +  Unemployed ,  data  =  longley )    1\n2\n3\n4\n5\n6\n7 # all Subsets Regression  library ( leaps ) \n\nleaps  <-  regsubsets ( Armed.Forces  ~  GNP  +  Population  +  Employed  +  Unemployed ,  data  =  longley ,  nbest  =   10 )  # view results  summary ( leaps )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27 ## Subset selection object\n## Call: regsubsets.formula(Armed.Forces ~ GNP + Population + Employed + \n##     Unemployed, data = longley, nbest = 10)\n## 4 Variables  (and intercept)\n##            Forced in Forced out\n## GNP            FALSE      FALSE\n## Population     FALSE      FALSE\n## Employed       FALSE      FALSE\n## Unemployed     FALSE      FALSE\n## 10 subsets of each size up to 4\n## Selection Algorithm: exhaustive\n##          GNP Population Employed Unemployed\n## 1  ( 1 ) \" \" \" \"        \"*\"      \" \"       \n## 1  ( 2 ) \"*\" \" \"        \" \"      \" \"       \n## 1  ( 3 ) \" \" \"*\"        \" \"      \" \"       \n## 1  ( 4 ) \" \" \" \"        \" \"      \"*\"       \n## 2  ( 1 ) \"*\" \"*\"        \" \"      \" \"       \n## 2  ( 2 ) \"*\" \" \"        \" \"      \"*\"       \n## 2  ( 3 ) \" \" \"*\"        \" \"      \"*\"       \n## 2  ( 4 ) \" \" \" \"        \"*\"      \"*\"       \n## 2  ( 5 ) \" \" \"*\"        \"*\"      \" \"       \n## 2  ( 6 ) \"*\" \" \"        \"*\"      \" \"       \n## 3  ( 1 ) \"*\" \"*\"        \"*\"      \" \"       \n## 3  ( 2 ) \"*\" \" \"        \"*\"      \"*\"       \n## 3  ( 3 ) \"*\" \"*\"        \" \"      \"*\"       \n## 3  ( 4 ) \" \" \"*\"        \"*\"      \"*\"       \n## 4  ( 1 ) \"*\" \"*\"        \"*\"      \"*\"   1\n2\n3 # plot a table of models showing variables in each model  # models are ordered by the selection statistic \nplot ( leaps ,  scale  =   \"r2\" )     1\n2\n3\n4 # plot statistic by subset size  library ( car ) \n\nsubsets ( leaps ,  statistic  =   \"rsq\" ,  legend  =   FALSE )   # available criteria are rsq, rss, adjr2, cp, bic     1\n2\n3\n4\n5 ##            Abbreviation\n## GNP                   G\n## Population            P\n## Employed              E\n## Unemployed            U   1 subsets ( leaps ,  statistic  =   \"bic\" ,  legend  =   FALSE )   # available criteria are rsq, rss, adjr2, cp, bic     1\n2\n3\n4\n5 ##            Abbreviation\n## GNP                   G\n## Population            P\n## Employed              E\n## Unemployed            U",
            "title": "Variable selection -- Graphical methods"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#variable-selection-relative-importance",
            "text": "Model:  fit  <-  lm ( Armed.Forces  ~  GNP  +  Population  +  Employed  +  Unemployed ,  data  =  longley ) .  Warning:  eval=FALSE .  1\n2\n3\n4 # calculate the relative importance of each predictor  library ( relaimpo ) \n\ncalc.relimp ( fit ,  type  =   c ( \"lmg\" ,   \"last\" ,   \"first\" ,   \"pratt\" ),  rela  =   TRUE )    Results.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 Response variable: Armed.Forces \nTotal response variance: 4843 \nAnalysis based on 16 observations\n\n4 Regressors: \nGNP Population Employed Unemployed \nProportion of variance explained by model: 70.2%\nMetrics are normalized to sum to 100% (rela=TRUE).\n\nRelative importance metrics:\n\n             lmg  last first  pratt\nGNP        0.328 0.431 0.348  4.566\nPopulation 0.241 0.155 0.232 -1.934\nEmployed   0.217 0.287 0.365 -1.780\nUnemployed 0.215 0.127 0.055  0.148\n\nAverage coefficients for different model sizes:\n\n               1X     2Xs     3Xs     4Xs\nGNP         0.313   1.301   3.641   5.026\nPopulation  3.646 -14.815 -24.637 -37.260\nEmployed    9.062  17.646 -34.249 -54.144\nUnemployed -0.132  -0.511  -0.584  -0.436   Bootstrapping  Model:  fit  <-  lm ( Armed.Forces  ~  GNP  +  Population  +  Employed  +  Unemployed ,  data  =  longley ) .  Warning:  eval=FALSE .  1\n2\n3 # bootstrap measures of relative importance (1000 samples) \nboot  <-  boot.relimp ( fit ,  b  =   1000 ,  type  =   c ( \"lmg\" ,   \"last\" ,   \"first\" ,   \"pratt\" ),  rank  =   TRUE ,  diff  =   TRUE ,  rela  =   TRUE ) \nbooteval.relimp ( boot )   # print result    Results.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90 Response variable: Armed.Forces \nTotal response variance: 4843 \nAnalysis based on 16 observations\n\n4 Regressors: \nGNP Population Employed Unemployed \nProportion of variance explained by model: 70.2%\nMetrics are normalized to sum to 100% (rela=TRUE).\n\nRelative importance metrics:\n\n             lmg  last first  pratt\nGNP        0.328 0.431 0.348  4.566\nPopulation 0.241 0.155 0.232 -1.934\nEmployed   0.217 0.287 0.365 -1.780\nUnemployed 0.215 0.127 0.055  0.148\n\nAverage coefficients for different model sizes:\n\n               1X     2Xs     3Xs     4Xs\nGNP         0.313   1.301   3.641   5.026\nPopulation  3.646 -14.815 -24.637 -37.260\nEmployed    9.062  17.646 -34.249 -54.144\nUnemployed -0.132  -0.511  -0.584  -0.436\n\n\n Confidence interval information ( 1000 bootstrap replicates, bty= perc ): \nRelative Contributions with confidence intervals:\n\n                                 Lower  Upper\n                 percentage 0.95 0.95    0.95   \nGNP.lmg           0.3280    ABC_  0.1488  0.4030\nPopulation.lmg    0.2410    _BCD  0.1572  0.3060\nEmployed.lmg      0.2170    ABCD  0.1107  0.3240\nUnemployed.lmg    0.2150    ABCD  0.0586  0.5550\n\nGNP.last          0.4310    ABCD  0.0101  0.5660\nPopulation.last   0.1550    _BCD  0.0003  0.3850\nEmployed.last     0.2870    ABCD  0.0476  0.6280\nUnemployed.last   0.1270    ABCD  0.0009  0.7810\n\nGNP.first         0.3480    ABCD  0.0151  0.3960\nPopulation.first  0.2320    _BCD  0.0070  0.3060\nEmployed.first    0.3650    ABCD  0.0159  0.4020\nUnemployed.first  0.0550    ABCD  0.0004  0.9280\n\nGNP.pratt         4.5660    ABCD -1.0160 10.5380\nPopulation.pratt -1.9340    ABCD -6.0800  2.2050\nEmployed.pratt   -1.7800    _BCD -5.0290  0.8510\nUnemployed.pratt  0.1480    ABC_ -0.3250  1.0910\n\nLetters indicate the ranks covered by bootstrap CIs. \n(Rank bootstrap confidence intervals always obtained by percentile method) \nCAUTION: Bootstrap confidence intervals can be somewhat liberal.\n\n\n Differences between Relative Contributions:\n\n                                            Lower   Upper\n                            difference 0.95 0.95    0.95   \nGNP-Population.lmg           0.0871         -0.0188  0.1521\nGNP-Employed.lmg             0.1106         -0.0479  0.1949\nGNP-Unemployed.lmg           0.1130         -0.4021  0.3281\nPopulation-Employed.lmg      0.0236         -0.1091  0.1124\nPopulation-Unemployed.lmg    0.0259         -0.3904  0.2417\nEmployed-Unemployed.lmg      0.0023         -0.4315  0.2295\n\nGNP-Population.last          0.2756         -0.1062  0.4502\nGNP-Employed.last            0.1438         -0.5564  0.4011\nGNP-Unemployed.last          0.3041         -0.7408  0.5502\nPopulation-Employed.last    -0.1318         -0.6191  0.2653\nPopulation-Unemployed.last   0.0285         -0.7382  0.3622\nEmployed-Unemployed.last     0.1603         -0.6784  0.4852\n\nGNP-Population.first         0.1161         -0.0590  0.1736\nGNP-Employed.first          -0.0172         -0.0893  0.0934\nGNP-Unemployed.first         0.2930         -0.9047  0.3766\nPopulation-Employed.first   -0.1333         -0.2134  0.0688\nPopulation-Unemployed.first  0.1769         -0.8967  0.2969\nEmployed-Unemployed.first    0.3102         -0.8937  0.3823\n\nGNP-Population.pratt         6.4995         -2.2069 15.7978\nGNP-Employed.pratt           6.3461         -1.3879 15.0093\nGNP-Unemployed.pratt         4.4180         -1.9134 10.6456\nPopulation-Employed.pratt   -0.1534         -4.1860  5.9691\nPopulation-Unemployed.pratt -2.0815         -6.1439  2.2348\nEmployed-Unemployed.pratt   -1.9281         -5.2057  0.1725\n\n* indicates that CI for difference does not include 0. \nCAUTION: Bootstrap confidence intervals can be somewhat liberal.   Warning:  eval=FALSE .  1 plot ( booteval.relimp ( boot ,  sort  =   TRUE ))   # to plot the results    The .png file.",
            "title": "Variable selection -- Relative importance"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#going-further",
            "text": "The  nls  package provides functions for nonlinear regression.  Perform robust regression with the  rlm  function in the  MASS  package.  The  robust  package provides a comprehensive library of robust methods, including regression.  The  robustbase  package also provides basic robust statistics including model selection methods.",
            "title": "Going further"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#regression-diagnostics",
            "text": "1\n2\n3 # assume that we are fitting a multiple linear regression \nfit  <-  lm ( mpg  ~  disp  +  hp  +  wt  +  drat ,  data  =  mtcars ) \nfit   1\n2\n3\n4\n5\n6\n7 ## \n## Call:\n## lm(formula = mpg ~ disp + hp + wt + drat, data = mtcars)\n## \n## Coefficients:\n## (Intercept)         disp           hp           wt         drat  \n##    29.14874      0.00382     -0.03478     -3.47967      1.76805",
            "title": "Regression diagnostics"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#outliers",
            "text": "1\n2\n3\n4 library ( car )  # assessing outliers \noutlierTest ( fit )   # Bonferonni p-value for most extreme obs    1\n2\n3\n4\n5 ## \n## No Studentized residuals with Bonferonni p < 0.05\n## Largest |rstudent|:\n##                rstudent unadjusted p-value Bonferonni p\n## Toyota Corolla     2.52             0.0184        0.588   1 qqPlot ( fit ,  main  =   \"QQ Plot\" )   #qq plot for studentized resid     1 leveragePlots ( fit )   # leverage plots",
            "title": "Outliers"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#influential-observations",
            "text": "Model:  fit  <-  lm ( mpg  ~  disp  +  hp  +  wt  +  drat ,  data  =  mtcars ) .  1\n2\n3\n4\n5 # influential observations  # added variable plots  library ( car ) \n\navPlots ( fit )     1\n2\n3\n4\n5\n6 par ( mfrow  =   c ( 1 , 1 ))  # Cook's D plot  # identify D values > 4/(n-k-1) \ncutoff  <-   4 / (( nrow ( mtcars )   -   length ( fit $ coefficients )   -   2 )) \nplot ( fit ,  which  =   4 ,  cook.levels  =  cutoff )     Warning:  eval=FALSE ; interactive function.  1\n2 # influence plot \ninfluencePlot ( fit ,  id.method  =   \"identify\" ,  main  =   \"Influence Plot\" ,  sub  =   \"Circle size is proportial to Cook's Distance\"   )",
            "title": "Influential observations"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#nonnormality",
            "text": "Model:  fit  <-  lm ( mpg  ~  disp  +  hp  +  wt  +  drat ,  data  =  mtcars ) .  1\n2\n3\n4\n5 # normality of residuals  # qq plot for studentized resid  library ( car ) \n\nqqPlot ( fit ,  main  =   \"QQ Plot\" )     1\n2\n3\n4\n5\n6\n7\n8 # distribution of studentized residuals  library ( MASS ) \n\nsresid  <-  studres ( fit ) \nhist ( sresid ,  freq  =   FALSE ,  main  =   \"Distribution of Studentized Residuals\" ) \nxfit  <-   seq ( min ( sresid ),   max ( sresid ), length  =   40 ) \nyfit  <-  dnorm ( xfit ) \nlines ( xfit ,  yfit )",
            "title": "Nonnormality"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#heteroscedasticity",
            "text": "Nonconstant error variance.  Model:  fit  <-  lm ( mpg  ~  disp  +  hp  +  wt  +  drat ,  data  =  mtcars ) .  1\n2\n3 # evaluate homoscedasticity  # non-constant error variance test \nncvTest ( fit )    1\n2\n3 ## Non-constant Variance Score Test \n## Variance formula: ~ fitted.values \n## Chisquare = 1.43    Df = 1     p = 0.232   1\n2 # plot studentized residuals vs. fitted values \nspreadLevelPlot ( fit )     1\n2 ## \n## Suggested power transformation:  0.662",
            "title": "Heteroscedasticity"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#multicollinearity",
            "text": "Model:  fit  <-  lm ( mpg  ~  disp  +  hp  +  wt  +  drat ,  data  =  mtcars ) .  1\n2 # evaluatecCollinearity \nvif ( fit )    1\n2 ## disp   hp   wt drat \n## 8.21 2.89 5.10 2.28   1 sqrt ( vif ( fit ))   >   2   # benchmark = 1.96, rounded to 2    1\n2 ##  disp    hp    wt  drat \n##  TRUE FALSE  TRUE FALSE",
            "title": "Multicollinearity"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#nonlinearity",
            "text": "Model:  fit  <-  lm ( mpg  ~  disp  +  hp  +  wt  +  drat ,  data  =  mtcars ) .  1\n2\n3 # evaluate nonlinearity  # component + residual plot \ncrPlots ( fit )     1\n2 # Ceres plots \nceresPlots ( fit )",
            "title": "Nonlinearity"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#autocorrelation",
            "text": "Serial correlation or non-independence of errors.  Model:  fit  <-  lm ( mpg  ~  disp  +  hp  +  wt  +  drat ,  data  =  mtcars ) .  1\n2 # test for autocorrelated errors \ndurbinWatsonTest ( fit )    1\n2\n3 ##  lag Autocorrelation D-W Statistic p-value\n##    1           0.101          1.74    0.29\n##  Alternative hypothesis: rho != 0",
            "title": "Autocorrelation"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#global-diagnostic",
            "text": "Model:  fit  <-  lm ( mpg  ~  disp  +  hp  +  wt  +  drat ,  data  =  mtcars ) .  1\n2\n3\n4\n5 # global test of model assumptions  library ( gvlma ) \n\ngvmodel  <-  gvlma ( fit )  summary ( gvmodel )      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36 ## \n## Call:\n## lm(formula = mpg ~ disp + hp + wt + drat, data = mtcars)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.508 -1.905 -0.506  0.982  5.688 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 29.14874    6.29359    4.63  8.2e-05 ***\n## disp         0.00382    0.01080    0.35   0.7268    \n## hp          -0.03478    0.01160   -3.00   0.0058 ** \n## wt          -3.47967    1.07837   -3.23   0.0033 ** \n## drat         1.76805    1.31978    1.34   0.1915    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.6 on 27 degrees of freedom\n## Multiple R-squared:  0.838,  Adjusted R-squared:  0.814 \n## F-statistic: 34.8 on 4 and 27 DF,  p-value: 2.7e-10\n## \n## \n## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\n## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\n## Level of Significance =  0.05 \n## \n## Call:\n##  gvlma(x = fit) \n## \n##                      Value p-value                   Decision\n## Global Stat        13.9382 0.00750 Assumptions NOT satisfied!\n## Skewness            4.3131 0.03782 Assumptions NOT satisfied!\n## Kurtosis            0.0138 0.90654    Assumptions acceptable.\n## Link Function       8.7166 0.00315 Assumptions NOT satisfied!\n## Heteroscedasticity  0.8947 0.34421    Assumptions acceptable.",
            "title": "Global diagnostic"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#anova_1",
            "text": "Analysis of variance (ANOVA) is an alternative to regressions among other applications.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 # lower case letters are numeric variables  # upper case letters are factors  # one-way ANOVA (completely randomized design) \nfit  <-  aov ( y  ~  A ,  data  =  mydataframe )  # randomized block design (B is the blocking factor) \nfit  <-  aov ( y  ~  A  +  B ,  data  =  mydataframe )  # two-way factorial design \nfit  <-  aov ( y  ~  A  +  B  +  A : B ,  data  =  mydataframe ) \nfit  <-  aov ( y  ~  A * B ,  data  =  mydataframe )   # same thing  # analysis of covariance \nfit  <-  aov ( y  ~  A  +  x ,  data  =  mydataframe )     1\n2 # dataset  head ( CO2 ,   3 )    1\n2\n3\n4\n5 ## Grouped Data: uptake ~ conc | Plant\n##   Plant   Type  Treatment conc uptake\n## 1   Qn1 Quebec nonchilled   95   16.0\n## 2   Qn1 Quebec nonchilled  175   30.4\n## 3   Qn1 Quebec nonchilled  250   34.8   1\n2\n3 # one-way ANOVA (completely randomized design) \nfit  <-  aov ( uptake  ~  Plant ,  data  =  CO2 ) \nfit    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ## Call:\n##    aov(formula = uptake ~ Plant, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## Estimated effects are balanced   1 summary ( fit )    1\n2\n3\n4\n5 ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1\n2\n3 # randomized block design (B is the blocking factor) \nfit  <-  aov ( uptake  ~  Plant  +  Type ,  data  =  CO2 ) \nfit    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## Call:\n##    aov(formula = uptake ~ Plant + Type, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## 1 out of 13 effects not estimable\n## Estimated effects are balanced   1 summary ( fit )    1\n2\n3\n4\n5 ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1\n2\n3 # two-way factorial design \nfit  <-  aov ( uptake  ~  Plant  +  Type  +  Plant : Type ,  data  =  CO2 ) \nfit    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## Call:\n##    aov(formula = uptake ~ Plant + Type + Plant:Type, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## 12 out of 24 effects not estimable\n## Estimated effects are balanced   1 summary ( fit )    1\n2\n3\n4\n5 ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1\n2 fit  <-  aov ( uptake  ~  Plant * Type ,  data  =  CO2 )   # same thing \nfit    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ## Call:\n##    aov(formula = uptake ~ Plant * Type, data = CO2)\n## \n## Terms:\n##                 Plant Residuals\n## Sum of Squares   4862      4845\n## Deg. of Freedom    11        72\n## \n## Residual standard error: 8.2\n## 12 out of 24 effects not estimable\n## Estimated effects are balanced   1 summary ( fit )    1\n2\n3\n4\n5 ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1\n2\n3 # Analysis of Covariance \nfit  <-  aov ( uptake  ~  uptake  +  conc ,  data  =  CO2 ) \nfit    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ## Call:\n##    aov(formula = uptake ~ uptake + conc, data = CO2)\n## \n## Terms:\n##                 conc Residuals\n## Sum of Squares  2285      7422\n## Deg. of Freedom    1        82\n## \n## Residual standard error: 9.51\n## Estimated effects may be unbalanced   1 summary ( fit )    1\n2\n3\n4\n5 ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## conc         1   2285    2285    25.2 2.9e-06 ***\n## Residuals   82   7422      91                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
            "title": "ANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#evaluate-model-effects",
            "text": "Type I sequential SS: A+B and B+A will produce different results!  Use the  drop1  to produce the familiar Type III results; compare each term with the full model.    1\n2\n3 # display Type I ANOVA table \nfit1  <-  aov ( uptake  ~  Plant  +  Type ,  data  =  CO2 )  summary ( fit1 )    1\n2\n3\n4\n5 ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Plant       11   4862     442    6.57 1.8e-07 ***\n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1\n2\n3 # display Type I ANOVA table \nfit2  <-  aov ( uptake  ~  Type  +  Plant ,  data  =  CO2 )  summary ( fit2 )    1\n2\n3\n4\n5\n6 ##             Df Sum Sq Mean Sq F value  Pr(>F)    \n## Type         1   3366    3366   50.02 8.1e-10 ***\n## Plant       10   1497     150    2.22   0.026 *  \n## Residuals   72   4845      67                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1\n2 # display type III SS and F tests \ndrop1 ( fit1 ,   ~   . ,  test  =   \"F\" )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ## Single term deletions\n## \n## Model:\n## uptake ~ Plant + Type\n##        Df Sum of Sq  RSS AIC F value Pr(>F)  \n## <none>              4845 365                 \n## Plant  10      1497 6341 367    2.22  0.026 *\n## Type    0         0 4845 365                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1 drop1 ( fit2 ,   ~   . ,  test  =   \"F\" )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ## Single term deletions\n## \n## Model:\n## uptake ~ Type + Plant\n##        Df Sum of Sq  RSS AIC F value Pr(>F)  \n## <none>              4845 365                 \n## Type    0         0 4845 365                 \n## Plant  10      1497 6341 367    2.22  0.026 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
            "title": "Evaluate model effects"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#compare-nested-models-directly",
            "text": "1\n2\n3\n4 fit1  <-  aov ( uptake  ~  Plant  +  Type ,  data  =  CO2 ) \nfit2  <-  aov ( uptake  ~  Plant ,  data  =  CO2 ) \n\nanova ( fit1 ,  fit2 )    1\n2\n3\n4\n5\n6\n7 ## Analysis of Variance Table\n## \n## Model 1: uptake ~ Plant + Type\n## Model 2: uptake ~ Plant\n##   Res.Df  RSS Df Sum of Sq F Pr(>F)\n## 1     72 4845                      \n## 2     72 4845  0         0   1\n2\n3\n4 fit2  <-  aov ( uptake  ~  Plant  +  Type ,  data  =  CO2 ) \nfit1  <-  aov ( uptake  ~  Plant ,  data  =  CO2 ) \n\nanova ( fit1 ,  fit2 )    1\n2\n3\n4\n5\n6\n7 ## Analysis of Variance Table\n## \n## Model 1: uptake ~ Plant\n## Model 2: uptake ~ Plant + Type\n##   Res.Df  RSS Df Sum of Sq F Pr(>F)\n## 1     72 4845                      \n## 2     72 4845  0         0",
            "title": "Compare nested models directly"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#multiple-comparisons",
            "text": "Tukey HSD tests for post hoc comparisons on each factor in the model.  Factors as an option.  Type I SS.    1\n2\n3\n4\n5 # model \nfit  <-  aov ( uptake  ~  Plant  +  Type ,  data  =  CO2 )  # Tukey honestly significant differences \nTukeyHSD ( fit )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73 ##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = uptake ~ Plant + Type, data = CO2)\n## \n## $Plant\n##            diff    lwr     upr p adj\n## Qn2-Qn1   1.929 -12.88  16.739 1.000\n## Qn3-Qn1   4.386 -10.42  19.196 0.997\n## Qc1-Qn1  -3.257 -18.07  11.553 1.000\n## Qc3-Qn1  -0.643 -15.45  14.168 1.000\n## Qc2-Qn1  -0.529 -15.34  14.282 1.000\n## Mn3-Qn1  -9.114 -23.92   5.696 0.639\n## Mn2-Qn1  -5.886 -20.70   8.925 0.970\n## Mn1-Qn1  -6.829 -21.64   7.982 0.918\n## Mc2-Qn1 -21.086 -35.90  -6.275 0.000\n## Mc3-Qn1 -15.929 -30.74  -1.118 0.024\n## Mc1-Qn1 -15.229 -30.04  -0.418 0.038\n## Qn3-Qn2   2.457 -12.35  17.268 1.000\n## Qc1-Qn2  -5.186 -20.00   9.625 0.989\n## Qc3-Qn2  -2.571 -17.38  12.239 1.000\n## Qc2-Qn2  -2.457 -17.27  12.353 1.000\n## Mn3-Qn2 -11.043 -25.85   3.768 0.346\n## Mn2-Qn2  -7.814 -22.62   6.996 0.822\n## Mn1-Qn2  -8.757 -23.57   6.053 0.694\n## Mc2-Qn2 -23.014 -37.82  -8.204 0.000\n## Mc3-Qn2 -17.857 -32.67  -3.047 0.006\n## Mc1-Qn2 -17.157 -31.97  -2.347 0.010\n## Qc1-Qn3  -7.643 -22.45   7.168 0.842\n## Qc3-Qn3  -5.029 -19.84   9.782 0.991\n## Qc2-Qn3  -4.914 -19.72   9.896 0.993\n## Mn3-Qn3 -13.500 -28.31   1.311 0.108\n## Mn2-Qn3 -10.271 -25.08   4.539 0.457\n## Mn1-Qn3 -11.214 -26.02   3.596 0.323\n## Mc2-Qn3 -25.471 -40.28 -10.661 0.000\n## Mc3-Qn3 -20.314 -35.12  -5.504 0.001\n## Mc1-Qn3 -19.614 -34.42  -4.804 0.002\n## Qc3-Qc1   2.614 -12.20  17.425 1.000\n## Qc2-Qc1   2.729 -12.08  17.539 1.000\n## Mn3-Qc1  -5.857 -20.67   8.953 0.971\n## Mn2-Qc1  -2.629 -17.44  12.182 1.000\n## Mn1-Qc1  -3.571 -18.38  11.239 1.000\n## Mc2-Qc1 -17.829 -32.64  -3.018 0.006\n## Mc3-Qc1 -12.671 -27.48   2.139 0.167\n## Mc1-Qc1 -11.971 -26.78   2.839 0.233\n## Qc2-Qc3   0.114 -14.70  14.925 1.000\n## Mn3-Qc3  -8.471 -23.28   6.339 0.735\n## Mn2-Qc3  -5.243 -20.05   9.568 0.988\n## Mn1-Qc3  -6.186 -21.00   8.625 0.958\n## Mc2-Qc3 -20.443 -35.25  -5.632 0.001\n## Mc3-Qc3 -15.286 -30.10  -0.475 0.037\n## Mc1-Qc3 -14.586 -29.40   0.225 0.057\n## Mn3-Qc2  -8.586 -23.40   6.225 0.719\n## Mn2-Qc2  -5.357 -20.17   9.453 0.985\n## Mn1-Qc2  -6.300 -21.11   8.511 0.952\n## Mc2-Qc2 -20.557 -35.37  -5.747 0.001\n## Mc3-Qc2 -15.400 -30.21  -0.589 0.034\n## Mc1-Qc2 -14.700 -29.51   0.111 0.054\n## Mn2-Mn3   3.229 -11.58  18.039 1.000\n## Mn1-Mn3   2.286 -12.52  17.096 1.000\n## Mc2-Mn3 -11.971 -26.78   2.839 0.233\n## Mc3-Mn3  -6.814 -21.62   7.996 0.919\n## Mc1-Mn3  -6.114 -20.92   8.696 0.961\n## Mn1-Mn2  -0.943 -15.75  13.868 1.000\n## Mc2-Mn2 -15.200 -30.01  -0.389 0.039\n## Mc3-Mn2 -10.043 -24.85   4.768 0.493\n## Mc1-Mn2  -9.343 -24.15   5.468 0.603\n## Mc2-Mn1 -14.257 -29.07   0.553 0.070\n## Mc3-Mn1  -9.100 -23.91   5.711 0.641\n## Mc1-Mn1  -8.400 -23.21   6.411 0.746\n## Mc3-Mc2   5.157  -9.65  19.968 0.989\n## Mc1-Mc2   5.857  -8.95  20.668 0.971\n## Mc1-Mc3   0.700 -14.11  15.511 1.000",
            "title": "Multiple comparisons"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#visualizing-results",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # two-way interaction plot  attach ( mtcars ) \n\ngear  <-   factor ( gear ) \ncyl  <-   factor ( cyl )  # two-way interactions  library ( car ) \n\ninteraction.plot ( cyl ,  gear ,  mpg ,  type  =   \"b\" ,  col  =   c ( 1 : 3 ),  leg.bty  =   \"o\" ,  leg.bg  =   \"beige\" ,  lwd  =   2 ,  pch  =   c ( 18 , 24 , 22 ),  xlab  =   \"Number of Cylinders\" ,  ylab  =   \"Mean Miles Per Gallon\" ,  main  =   \"Interaction Plot\" )     1\n2\n3\n4 # mean plots for single factors, and includes confidence intervals  library ( gplots ) \n\nplotmeans ( mpg  ~  cyl ,  xlab  =   \"Number of Cylinders\" ,  ylab  =   \"Miles Per Gallon\" ,  main  =   \"Mean Plot\\nwith 95% CI\" )     1 detach ( mtcars )",
            "title": "Visualizing results"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#manova",
            "text": "Multivariate analysis of variance (MANOVA) With more than one dependent variable Y. We can run several ANOVA over different Y, or one MANOVA with one Y built with several variables.  1 head ( longley ,   3 )    1\n2\n3\n4 ##      GNP.deflator GNP Unemployed Armed.Forces Population Year Employed\n## 1947         83.0 234        236          159        108 1947     60.3\n## 1948         88.5 259        232          146        109 1948     61.1\n## 1949         88.2 258        368          162        110 1949     60.2   1\n2\n3\n4\n5\n6\n7\n8 attach ( longley )  # 2x2 factorial MANOVA with 3 dependent variables, Y \nY  <-   cbind ( Unemployed ,  Armed.Forces ,  Employed )   # Y \nfit  <-  manova ( Y  ~  Population * GNP )   # Y ~ X  # display type I SS  summary ( fit ,  test  =   \"Pillai\" )    1\n2\n3\n4\n5\n6\n7 ##                Df Pillai approx F num Df den Df  Pr(>F)    \n## Population      1  0.997     1074      3     10 7.7e-13 ***\n## GNP             1  0.888       26      3     10 4.6e-05 ***\n## Population:GNP  1  0.870       22      3     10 9.4e-05 ***\n## Residuals      12                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1\n2\n3\n4 # test = \"Wilks\", \"Hotelling-Lawley\", and \"Roy\"  # display univariate statistics \nsummary.aov ( fit )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 ##  Response Unemployed :\n##                Df Sum Sq Mean Sq F value  Pr(>F)    \n## Population      1  61739   61739   45.92   2e-05 ***\n## GNP             1  42841   42841   31.87 0.00011 ***\n## Population:GNP  1  10270   10270    7.64 0.01715 *  \n## Residuals      12  16133    1344                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  Response Armed.Forces :\n##                Df Sum Sq Mean Sq F value Pr(>F)   \n## Population      1   9647    9647    4.25 0.0617 . \n## GNP             1  29772   29772   13.10 0.0035 **\n## Population:GNP  1   5958    5958    2.62 0.1314   \n## Residuals      12  27268    2272                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##  Response Employed :\n##                Df Sum Sq Mean Sq F value  Pr(>F)    \n## Population      1  170.6   170.6  546.45 2.2e-11 ***\n## GNP             1   10.5    10.5   33.60 8.5e-05 ***\n## Population:GNP  1    0.1     0.1    0.41    0.54    \n## Residuals      12    3.7     0.3                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1\n2\n3\n4\n5 # display type III SS \nfit1  <-  manova ( Y  ~  Population * GNP ) \nfit2  <-  manova ( Y  ~  GNP * Population )  # type III GNP effect  summary ( fit1 )    1\n2\n3\n4\n5\n6\n7 ##                Df Pillai approx F num Df den Df  Pr(>F)    \n## Population      1  0.997     1074      3     10 7.7e-13 ***\n## GNP             1  0.888       26      3     10 4.6e-05 ***\n## Population:GNP  1  0.870       22      3     10 9.4e-05 ***\n## Residuals      12                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   1\n2 # type III Population effect  summary ( fit2 )    1\n2\n3\n4\n5\n6\n7 ##                Df Pillai approx F num Df den Df  Pr(>F)    \n## GNP             1  0.997     1088      3     10 7.2e-13 ***\n## Population      1  0.786       12      3     10  0.0011 ** \n## GNP:Population  1  0.870       22      3     10 9.4e-05 ***\n## Residuals      12                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1   Multiple comparisons  TukeyHSD  and  plot  do work with a MANOVA fit. Run each dependent variable separately to obtain them\u2026 or proceed with ANOVA on each dependent variable.",
            "title": "MANOVA"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#going-further_1",
            "text": "Package  lme4  has excellent facilities for fitting linear and generalized linear mixed-effects models.",
            "title": "Going further"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#manova-assumptions",
            "text": "",
            "title": "(M)ANOVA Assumptions"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#outliers_1",
            "text": "Outliers can severely affect normality and homogeneity of variance.  1\n2 # dataset  head ( mtcars ,   3 )    1\n2\n3\n4 ##                mpg cyl disp  hp drat   wt qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1   1\n2\n3\n4\n5\n6 # detect outliers  # ordered squared robust Mahalanobis distances of the observations against the empirical distribution function of the MD2i  library ( mvoutlier ) \n\noutliers  <- \naq.plot ( mtcars [ c ( \"mpg\" ,   \"disp\" ,   \"hp\" ,   \"drat\" ,   \"wt\" ,   \"qsec\" )])    1\n2 ## Projection to the first and second robust principal components.\n## Proportion of total variation (explained variance): 0.974    1 outliers  # show list of outliers     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23 ## $outliers\n##           Mazda RX4       Mazda RX4 Wag          Datsun 710 \n##               FALSE               FALSE               FALSE \n##      Hornet 4 Drive   Hornet Sportabout             Valiant \n##               FALSE               FALSE               FALSE \n##          Duster 360           Merc 240D            Merc 230 \n##                TRUE                TRUE                TRUE \n##            Merc 280           Merc 280C          Merc 450SE \n##               FALSE               FALSE               FALSE \n##          Merc 450SL         Merc 450SLC  Cadillac Fleetwood \n##               FALSE               FALSE               FALSE \n## Lincoln Continental   Chrysler Imperial            Fiat 128 \n##               FALSE               FALSE                TRUE \n##         Honda Civic      Toyota Corolla       Toyota Corona \n##               FALSE               FALSE               FALSE \n##    Dodge Challenger         AMC Javelin          Camaro Z28 \n##               FALSE               FALSE                TRUE \n##    Pontiac Firebird           Fiat X1-9       Porsche 914-2 \n##               FALSE               FALSE               FALSE \n##        Lotus Europa      Ford Pantera L        Ferrari Dino \n##               FALSE                TRUE               FALSE \n##       Maserati Bora          Volvo 142E \n##                TRUE               FALSE   1 par ( mfrow  =   c ( 1 , 1 ))",
            "title": "Outliers"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#univariate-normality",
            "text": "1\n2\n3 # Q-Q plot \nqqnorm ( mtcars $ mpg ) \nqqline ( mtcars $ mpg )     1\n2 # Shapiro-Wilk test of normality \nshapiro.test ( mtcars $ mpg )    1\n2\n3\n4\n5 ## \n##  Shapiro-Wilk normality test\n## \n## data:  mtcars$mpg\n## W = 0.9, p-value = 0.1   1\n2\n3\n4 library ( nortest )  # Anderson-Darling test for normality \nad.test ( mtcars $ mpg )    1\n2\n3\n4\n5 ## \n##  Anderson-Darling normality test\n## \n## data:  mtcars$mpg\n## A = 0.6, p-value = 0.1   1\n2 # Cramer-von Mises test for normality \ncvm.test ( mtcars $ mpg )    1\n2\n3\n4\n5 ## \n##  Cramer-von Mises normality test\n## \n## data:  mtcars$mpg\n## W = 0.09, p-value = 0.2   1\n2 # Lilliefors (Kolmogorov-Smirnov) test for normality \nlillie.test ( mtcars $ mpg )    1\n2\n3\n4\n5 ## \n##  Lilliefors (Kolmogorov-Smirnov) normality test\n## \n## data:  mtcars$mpg\n## D = 0.1, p-value = 0.2   1\n2 # Pearson chi-square test for normality \npearson.test ( mtcars $ mpg )    1\n2\n3\n4\n5 ## \n##  Pearson chi-square normality test\n## \n## data:  mtcars$mpg\n## P = 8, p-value = 0.2   1\n2 # Shapiro-Francia test for normality \nsf.test ( mtcars $ mpg )    1\n2\n3\n4\n5 ## \n##  Shapiro-Francia normality test\n## \n## data:  mtcars$mpg\n## W = 1, p-value = 0.1",
            "title": "Univariate normality"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#multivariate-normality",
            "text": "1 head ( EuStockMarkets ,   3 )    1\n2\n3\n4 ##       DAX  SMI  CAC FTSE\n## [1,] 1629 1678 1773 2444\n## [2,] 1614 1688 1750 2460\n## [3,] 1607 1679 1718 2448   1\n2\n3\n4 library ( mvnormtest )  # Shapiro-Wilk test for multivariate normality of numeric matrix \nmshapiro.test ( t ( EuStockMarkets ))    1\n2\n3\n4\n5 ## \n##  Shapiro-Wilk normality test\n## \n## data:  Z\n## W = 0.9, p-value <2e-16   With a p x 1 multivariate normal random vector x vector, the squared Mahalanobis distance between x and ?? is going to be chi-square distributed with p degrees of freedom.  1\n2\n3\n4\n5\n6\n7\n8\n9 # graphical assessment of multivariate normality \nx  <-   as.matrix ( EuStockMarkets )   # n x p numeric matrix \ncenter  <-   colMeans ( x )   # centroid \nn  <-   nrow ( x );  p  <-   ncol ( x ) \ncov  <-  cov ( x ) \nd  <-  mahalanobis ( x ,  center ,  cov )   # distances \n\nqqplot ( qchisq ( ppoints ( n ),  df  =  p ),  d ,  main  =   \"QQ Plot Assessing Multivariate Normality\" ,  ylab  =   \"Mahalanobis D2\" ) \nabline ( a  =   0 ,  b  =   1 )",
            "title": "Multivariate normality"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#heteroscedasticity_1",
            "text": "Nonconstant error variance or non-homogeneity of variances.  1\n2 # dataset  head ( mtcars ,   3 )    1\n2\n3\n4 ##                mpg cyl disp  hp drat   wt qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1   1\n2\n3 # y is a numeric variable and G is the grouping variable  # Bartlett parametric test of homogeneity of variances \nbartlett.test ( mpg  ~  cyl ,  data = mtcars )    1\n2\n3\n4\n5 ## \n##  Bartlett test of homogeneity of variances\n## \n## data:  mpg by cyl\n## Bartlett's K-squared = 8, df = 2, p-value = 0.02   1\n2 # Figner-Killeen non-parametric test of homogeneity of variances \nfligner.test ( mpg  ~  cyl ,  data  =  mtcars )     1\n2\n3\n4\n5 ## \n##  Fligner-Killeen test of homogeneity of variances\n## \n## data:  mpg by cyl\n## Fligner-Killeen:med chi-squared = 7, df = 2, p-value = 0.03   1\n2\n3\n4\n5 library ( HH )  # y is numeric and G is a grouping factor  # G must be of type factor \nhov ( mpg  ~   factor ( cyl ),  data  =  mtcars )    1\n2\n3\n4\n5\n6 ## \n##  hov: Brown-Forsyth\n## \n## data:  mpg\n## F = 6, df:factor(cyl) = 2, df:Residuals = 30, p-value = 0.009\n## alternative hypothesis: variances are not identical   1\n2\n3 # homogeneity of variance plot  # graphic test of homogeneity of variances based on Brown-Forsyth \nhovPlot ( mpg  ~   factor ( cyl ),  data  =  mtcars )      Non-homogeneity of covariance matrices  1\n2 # dataset  head ( iris ,   3 )    1\n2\n3\n4 ##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa   1 library ( biotools )    1\n2 ## ---\n## biotools version 3.0   1\n2\n3 # Box's M test  # very sensitive to violations of normality, leading to rejection in most typical cases \nboxM ( iris [,   -5 ],  iris [,   5 ])    1\n2\n3\n4\n5 ## \n##  Box's M-test for Homogeneity of Covariance Matrices\n## \n## data:  iris[, -5]\n## Chi-Sq (approx.) = 100, df = 20, p-value <2e-16",
            "title": "Heteroscedasticity"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#resampling-statistics",
            "text": "",
            "title": "Resampling Statistics"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#independent-k-sample-location-tests",
            "text": "1\n2 # dataset  head ( mtcars ,   3 )    1\n2\n3\n4 ##                mpg cyl disp  hp drat   wt qsec vs am gear carb\n## Mazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n## Mazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n## Datsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1   1\n2\n3\n4\n5\n6 # exact Wilcoxon Mann Whitney rank sum test  # re-randomization or permutation based statistical tests  # where y is numeric and A is a binary factor  library ( coin ) \n\nwilcox_test ( mpg  ~   factor ( am ),  data  =  mtcars ,  distribution  =   \"exact\" )    1\n2\n3\n4\n5\n6 ## \n##  Exact Wilcoxon-Mann-Whitney Test\n## \n## data:  mpg by factor(am) (0, 1)\n## Z = -3, p-value = 0.001\n## alternative hypothesis: true mu is not equal to 0   1 # lower case letters represent numerical variables and upper case letters represent categorical factors     Monte-Carlo simulations are available for all tests. Exact tests are available for 2 group procedures.  These tests do not assume random sampling from well-defined populations. They can be a reasonable alternative to     classical procedures when test assumptions can not be met.    1\n2\n3\n4\n5\n6 # one-way permutation test based on 9999 Monte-Carlo  # resamplings  # y is numeric and A is a categorical factor  library ( coin ) \n\noneway_test ( mpg  ~   factor ( am ),  data  =  mtcars ,  distribution  =  approximate ( B  =   9999 ))    1\n2\n3\n4\n5\n6 ## \n##  Approximative Two-Sample Fisher-Pitman Permutation Test\n## \n## data:  mpg by factor(am) (0, 1)\n## Z = -3, p-value = 5e-04\n## alternative hypothesis: true mu is not equal to 0",
            "title": "Independent k-sample location tests"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#symmetry-of-a-response-for-repeated-measurements",
            "text": "1\n2\n3\n4\n5 # exact Wilcoxon signed rank test  # where y1 and y2 are repeated measures  library ( coin ) \n\nwilcoxsign_test ( mpg  ~   factor ( am ),  data  =  mtcars ,  distribution  =   \"exact\" )    1\n2\n3\n4\n5\n6\n7 ## \n##  Exact Wilcoxon-Pratt Signed-Rank Test\n## \n## data:  y by x (pos, neg) \n##   stratified by block\n## Z = 5, p-value = 5e-10\n## alternative hypothesis: true mu is not equal to 0   1\n2\n3\n4 # Freidman Test based on 9999 Monte-Carlo resamplings.  # y is numeric, A is a grouping factor, and B is a  # blocking factor \nfriedman_test ( y  ~  A  |  B ,  data  =  mydata ,   distribution  =  approximate ( B  =   9999 ))",
            "title": "Symmetry of a response for repeated measurements"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#independence-of-two-numeric-variables",
            "text": "1\n2\n3\n4\n5 # Spearman Test of independence based on 9999 Monte-Carlo  # resamplings. x and y are numeric variables  library ( coin ) \n\nspearman_test ( mpg  ~  am ,  data  =  mtcars ,  distribution  =  approximate ( B  =   9999 ))    1\n2\n3\n4\n5\n6 ## \n##  Approximative Spearman Correlation Test\n## \n## data:  mpg by am\n## Z = 3, p-value = 0.001\n## alternative hypothesis: true rho is not equal to 0",
            "title": "Independence of two numeric variables"
        },
        {
            "location": "/A_Hands-on_Introduction_to_Statistics_with_R,_Notes/#independence-in-contingency-tables",
            "text": "1\n2 # dataset  head ( CO2 )    1\n2\n3\n4\n5\n6\n7\n8 ## Grouped Data: uptake ~ conc | Plant\n##   Plant   Type  Treatment conc uptake\n## 1   Qn1 Quebec nonchilled   95   16.0\n## 2   Qn1 Quebec nonchilled  175   30.4\n## 3   Qn1 Quebec nonchilled  250   34.8\n## 4   Qn1 Quebec nonchilled  350   37.2\n## 5   Qn1 Quebec nonchilled  500   35.3\n## 6   Qn1 Quebec nonchilled  675   39.2    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 cases  <-   c ( 4 ,   2 ,   3 ,   1 ,   59 ) \nn  <-   sum ( cases ) \ncochran  <-   data.frame ( \n    diphtheria  =   factor ( \n         unlist ( rep ( list ( c ( 1 ,   1 ,   1 ,   1 ), \n                         c ( 1 ,   1 ,   0 ,   1 ), \n                         c ( 0 ,   1 ,   1 ,   1 ), \n                         c ( 0 ,   1 ,   0 ,   1 ), \n                         c ( 0 ,   0 ,   0 ,   0 )), \n                   cases )) \n     ), \n    media  =   factor ( rep ( LETTERS [ 1 : 4 ],  n )), \n    case  =    factor ( rep ( seq_len ( n ),  each  =   4 ))  )  head ( cochran )    1\n2\n3\n4\n5\n6\n7 ##   diphtheria media case\n## 1          1     A    1\n## 2          1     B    1\n## 3          1     C    1\n## 4          1     D    1\n## 5          1     A    2\n## 6          1     B    2   1\n2\n3\n4\n5 # independence in 2-way contingency table based on  # 9999 Monte-Carlo resamplings. A and B are factors  library ( coin ) \n\nchisq_test ( Plant  ~  Type ,  data  =  CO2 ,  distribution  =  approximate ( B  =   9999 ))    1\n2\n3\n4\n5\n6 ## \n##  Approximative Linear-by-Linear Association Test\n## \n## data:  Plant (ordered) by Type (Quebec, Mississippi)\n## Z = -8, p-value <2e-16\n## alternative hypothesis: two.sided   1\n2\n3\n4 # Cochran-Mantel-Haenzsel Test of 3-way Contingency Table  # based on 9999 Monte-Carlo resamplings. A, B, are factors  # and C is a stratefying factor \nmh_test ( diphtheria  ~  media  |  case ,  data  =  cochran ,  distribution  =  approximate ( B  =   9999 ))    1\n2\n3\n4\n5\n6\n7 ## \n##  Approximative Marginal Homogeneity Test\n## \n## data:  diphtheria by\n##   media (A, B, C, D) \n##   stratified by case\n## chi-squared = 8, p-value = 0.05   1\n2\n3\n4\n5\n6 # linear by linear association test based on 9999  # Monte-Carlo resamplings  # A and B are ordered factors  library ( coin ) \n\nlbl_test ( Plant  ~  Type ,  data  =  CO2 ,  distribution  =  approximate ( B  =   9999 ))     1\n2\n3\n4\n5\n6 ## \n##  Approximative Linear-by-Linear Association Test\n## \n## data:  Plant (ordered) by Type (Quebec, Mississippi)\n## Z = -8, p-value <2e-16\n## alternative hypothesis: two.sided   Many other univariate and multivariate tests are possible using the functions in the  coin  package.",
            "title": "Independence in contingency tables"
        }
    ]
}